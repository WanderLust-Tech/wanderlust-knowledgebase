[
  {
    "path": "learning-path-guide",
    "title": "Chromium Learning Path Guide 📚",
    "content": "# Chromium Learning Path Guide 📚\r\n\r\n## 🎯 Recommended Learning Journey\r\n\r\nThe Wanderlust Chromium Knowledge Base is organized into **14 progressive phases** designed to take you from complete beginner to advanced Chromium developer. Each phase builds upon the previous ones, creating a logical learning progression.\r\n\r\n---\r\n\r\n## 📋 Phase Overview\r\n\r\n### **Phase 1: 🌟 Introduction**\r\n**Start Here!** Get oriented with what Chromium is and why it matters.\r\n- Perfect for: Complete beginners\r\n- Time Investment: 30-60 minutes\r\n- Prerequisites: None\r\n\r\n### **Phase 2: 🚀 Getting Started**\r\nSet up your development environment and understand the project structure.\r\n- Perfect for: Developers ready to dive in\r\n- Time Investment: 2-4 hours\r\n- Prerequisites: Phase 1 completed\r\n\r\n### **Phase 3: 🏗️ Core Architecture**\r\nUnderstand Chromium's fundamental design and multi-process architecture.\r\n- Perfect for: Developers who want to understand how Chromium works\r\n- Time Investment: 4-6 hours\r\n- Prerequisites: Basic understanding of web browsers\r\n\r\n### **Phase 4: 🎯 Design Patterns**\r\nLearn the common patterns used throughout the Chromium codebase.\r\n- Perfect for: Developers familiar with software design patterns\r\n- Time Investment: 2-3 hours\r\n- Prerequisites: Object-oriented programming experience\r\n\r\n### **Phase 5: 🔒 Security Architecture**\r\nDeep dive into Chromium's security model and sandbox architecture.\r\n- Perfect for: Security-conscious developers\r\n- Time Investment: 3-4 hours\r\n- Prerequisites: Understanding of OS security concepts\r\n\r\n### **Phase 6: ⚙️ Core Modules**\r\nExplore key Chromium subsystems like V8, networking, and storage.\r\n- Perfect for: Developers working on specific features\r\n- Time Investment: 6-8 hours\r\n- Prerequisites: Phases 1-3 completed\r\n\r\n### **Phase 7: 🔌 APIs & Services**\r\nLearn about modern Chromium APIs and the Mojo service architecture.\r\n- Perfect for: Developers building Chrome extensions or integrations\r\n- Time Investment: 4-5 hours\r\n- Prerequisites: Understanding of IPC concepts\r\n\r\n### **Phase 8: 🛠️ Development Workflow**\r\nMaster the tools and processes for effective Chromium development.\r\n- Perfect for: Developers ready to contribute code\r\n- Time Investment: 6-10 hours\r\n- Prerequisites: Git experience, C++ knowledge\r\n\r\n### **Phase 9: 🧪 Testing & QA**\r\nComprehensive testing strategies and quality assurance practices.\r\n- Perfect for: Developers focused on code quality\r\n- Time Investment: 8-12 hours\r\n- Prerequisites: Testing framework experience\r\n\r\n### **Phase 10: ⚡ Performance & Optimization**\r\nLearn performance analysis and optimization techniques.\r\n- Perfect for: Performance-focused developers\r\n- Time Investment: 4-6 hours\r\n- Prerequisites: Understanding of system performance\r\n\r\n### **Phase 11: 🖥️ Platform-Specific Development**\r\nPlatform-specific considerations for different operating systems.\r\n- Perfect for: Developers targeting specific platforms\r\n- Time Investment: Varies by platform (2-4 hours each)\r\n- Prerequisites: Platform-specific development experience\r\n\r\n### **Phase 12: 🐛 Debugging & Troubleshooting**\r\nTools and techniques for debugging complex Chromium issues.\r\n- Perfect for: Developers maintaining or fixing bugs\r\n- Time Investment: 3-4 hours\r\n- Prerequisites: Debugging experience\r\n\r\n### **Phase 13: 🤝 Contributing to Chromium**\r\nGuidelines for contributing to the open-source Chromium project.\r\n- Perfect for: Developers ready to contribute upstream\r\n- Time Investment: 2-3 hours\r\n- Prerequisites: Phases 1-8 completed\r\n\r\n### **Phase 14: 🎮 Interactive Demos & Examples**\r\nHands-on examples and interactive learning tools.\r\n- Perfect for: All skill levels, anytime\r\n- Time Investment: Varies (30 minutes - 2 hours per demo)\r\n- Prerequisites: None (can be used throughout the journey)\r\n\r\n---\r\n\r\n## 🗺️ Learning Paths by Role\r\n\r\n### **For New Chromium Contributors**\r\n1. 🌟 Introduction\r\n2. 🚀 Getting Started  \r\n3. 🏗️ Core Architecture\r\n4. 🛠️ Development Workflow\r\n5. 🧪 Testing & QA\r\n6. 🤝 Contributing to Chromium\r\n\r\n### **For Chromium Extension Developers**\r\n1. 🌟 Introduction\r\n2. 🏗️ Core Architecture\r\n3. 🔌 APIs & Services\r\n4. 🔒 Security Architecture\r\n5. 🐛 Debugging & Troubleshooting\r\n\r\n### **For Performance Engineers**\r\n1. 🌟 Introduction\r\n2. 🏗️ Core Architecture\r\n3. ⚙️ Core Modules\r\n4. ⚡ Performance & Optimization\r\n5. 🧪 Testing & QA\r\n6. 🐛 Debugging & Troubleshooting\r\n\r\n### **For Security Researchers**\r\n1. 🌟 Introduction\r\n2. 🏗️ Core Architecture\r\n3. 🔒 Security Architecture\r\n4. ⚙️ Core Modules\r\n5. 🐛 Debugging & Troubleshooting\r\n\r\n### **For Platform Developers**\r\n1. 🌟 Introduction\r\n2. 🚀 Getting Started\r\n3. 🏗️ Core Architecture\r\n4. 🖥️ Platform-Specific Development\r\n5. 🧪 Testing & QA\r\n\r\n---\r\n\r\n## 💡 Study Tips\r\n\r\n### **Beginner Tips**\r\n- Start with Phase 1 and don't skip ahead\r\n- Use the interactive demos to reinforce concepts\r\n- Take breaks between phases to absorb information\r\n- Join the Chromium community forums for questions\r\n\r\n### **Intermediate Tips**\r\n- Focus on phases most relevant to your work\r\n- Experiment with the code playground demos\r\n- Practice with real Chromium source code\r\n- Set up a local development environment early\r\n\r\n### **Advanced Tips**\r\n- Contribute to documentation improvements\r\n- Mentor newer developers\r\n- Specialize in specific modules or platforms\r\n- Consider becoming a Chromium committer\r\n\r\n### **For Teams**\r\n- Create study groups for each phase\r\n- Set up shared development environments\r\n- Rotate through different specializations\r\n- Document your team's specific use cases\r\n\r\n---\r\n\r\n## 🏆 Milestones & Achievements\r\n\r\n### **Beginner Milestones**\r\n- [ ] Completed Introduction phase\r\n- [ ] Successfully built Chromium locally\r\n- [ ] Understood the process model\r\n- [ ] Ran first debugging session\r\n\r\n### **Intermediate Milestones**\r\n- [ ] Contributed first patch review\r\n- [ ] Mastered a specific testing framework\r\n- [ ] Optimized a performance bottleneck\r\n- [ ] Implemented a small feature\r\n\r\n### **Advanced Milestones**\r\n- [ ] Became module owner or reviewer\r\n- [ ] Mentored new contributors\r\n- [ ] Led architectural improvements\r\n- [ ] Spoke at Chromium conferences\r\n\r\n---\r\n\r\n## 🔗 Quick Navigation\r\n\r\n**Essential Starting Points:**\r\n- [🌟 Introduction → What is Chromium?](/#/introduction/overview)\r\n- [🚀 Getting Started → Setup & Build](/#/getting-started/setup-build)\r\n- [🏗️ Core Architecture → Architecture Overview](/#/architecture/overview)\r\n\r\n**Popular Deep Dives:**\r\n- [🔒 Security → Security Model](/#/security/security-model)\r\n- [⚙️ V8 JavaScript Engine](/#/modules/javascript-v8)\r\n- [🧪 Testing in Chromium](/#/development/testing/testing_in_chromium)\r\n\r\n**Interactive Learning:**\r\n- [🎮 Code Playground](/#/demo/code-playground)\r\n- [📊 Interactive Diagrams](/#/demo/interactive-diagrams)\r\n- [📈 Progress Tracking](/#/demo/progress-tracking)\r\n\r\n---\r\n\r\n## 📊 Progress Tracking\r\n\r\nUse the built-in progress tracking system to:\r\n- Monitor your learning journey\r\n- Set daily reading goals\r\n- Track completion of each phase\r\n- Build learning streaks\r\n- Get personalized recommendations\r\n\r\nAccess your progress dashboard: [📈 Progress Dashboard](/#/progress)\r\n\r\n---\r\n\r\n## 🤝 Community & Support\r\n\r\n- **Chromium Forums**: [groups.google.com/a/chromium.org](https://groups.google.com/a/chromium.org)\r\n- **IRC**: #chromium on freenode\r\n- **Discord**: Chromium Community Server\r\n- **Stack Overflow**: Use the `chromium` tag\r\n\r\nHappy learning! 🚀\r\n"
  },
  {
    "path": "chromium-docs-index",
    "title": "Chromium docs",
    "content": "# Chromium docs\n\nThis directory contains [chromium project](https://www.chromium.org/Home/)\ndocumentation in [Gitiles-flavored Markdown]. It is automatically\n[rendered by Gitiles].\n\n[Gitiles-flavored Markdown]: https://gerrit.googlesource.com/gitiles/+/master/Documentation/markdown.md\n[rendered by Gitiles]: https://chromium.googlesource.com/chromium/src/+/main/docs/\n\n**If you add new documents, please also add a link to them in the Document Index\nbelow.**\n\n[TOC]\n\n## Creating Documentation\n\n### Guidelines\n\n*   See the [Chromium Documentation Guidelines](documentation_guidelines.md)\n    and the\n    [Chromium Documentation Best Practices](documentation_best_practices.md).\n*   Markdown documents must follow the\n    [Markdown Style\n    Guide](https://chromium.googlesource.com/chromium/src/+/HEAD/styleguide/markdown/markdown.md).\n\n### Previewing changes\n\n#### Locally using [md_browser](../tools/md_browser/)\n\n```bash\n# in chromium checkout\n./tools/md_browser/md_browser.py\n```\n\nThis is only an estimate. The **gitiles** view may differ.\n\n#### Online with Gerrit's links to gitiles\n\n1.  Upload a patch to gerrit, or receive a review request.\n    e.g. https://chromium-review.googlesource.com/c/3362532\n2.  View a specific .md file.\n    e.g. https://chromium-review.googlesource.com/c/3362532/2/docs/README.md\n3.  You will see something like <br>\n    Base\n    [preview](https://chromium.googlesource.com/chromium/src/+/ad44f6081ccc6b92479b12f1eb7e9482f474859d/docs/README.md)\n    -> Patchset 3\n    [preview](https://chromium.googlesource.com/chromium/src/+/refs/changes/32/3362532/3/docs/README.md)\n    | DOWNLOAD <br>\n    at the top left of the page. Click on the second\n    \"[preview](https://chromium.googlesource.com/chromium/src/+/refs/changes/32/3362532/3/docs/README.md)\"\n    link to open the preview for the current patch set.\n\nThis **gitiles** view is the authoritative view, exactly the same as will be\nused when committed.\n\n## Document Index\n\n**Note**: this is not an exhaustive list of all documents.\n\n### Checking Out and Building\n*   [Linux Build Instructions](linux/build_instructions.md) - Linux\n*   [Mac Build Instructions](mac_build_instructions.md) - MacOS\n*   [Windows Build Instructions](windows_build_instructions.md) - Windows\n*   [Android Build Instructions](android_build_instructions.md) - Android target\n    (on a Linux host)\n*   [Cast Build Instructions](linux/cast_build_instructions.md) - Cast target\n    (on a Linux host)\n*   [Cast for Android Build Instructions](android_cast_build_instructions.md) -\n    Cast for Android (on a Linux host)\n*   [Fuchsia Build Instructions](fuchsia/build_instructions.md) -\n    Fuchsia target (on a Linux host)\n*   [iOS Build Instructions](ios/build_instructions.md) - iOS target (on a MacOS\n    host)\n*   [Chrome OS Build Instructions](chromeos_build_instructions.md) - Chrome OS\n*   [Linux Chromium ARM Recipes](linux/chromium_arm.md) - Recipes for building\n    Chromium for ARM on Linux.\n*   [Chrome Component Build](component_build.md) - Faster builds using more\n    libraries\n*   [Using the BuildRunner](using_build_runner.md) - Scripts that extract build\n    stops from builders and runs them locally on a slave\n*   [Cr User Manual](cr_user_manual.md) - Manual for `cr`, a tool that tries to\n    hide some of the tools used for working on Chromium behind an abstraction\n    layer\n\n### Design Docs\n*   See [design/README.md](design/README.md)\n\n### Integrated Development Environment (IDE) Set Up Guides\n*   [Android Studio](android_studio.md) - Android Studio for Android builds\n*   [Atom](atom.md) - Atom multi-platform code editor\n*   [CLion](clion.md) - CLion IDE, supports GUI debugging.\n*   [Eclipse for Android](eclipse.md) - Eclipse for Android\n*   [Eclipse for Linux](linux/eclipse_dev.md) - Eclipse for other platforms\n    (This guide was written for Linux, but is probably usable on Windows/MacOS\n    as well)\n*   [EMACS Notes](emacs.md) - EMACS commands/styles/tool integrations\n*   [Qt Creator](qtcreator.md) - Using Qt Creator as an IDE or GUI debugger\n*   [Visual Studio Code](vscode.md) - Visual Studio Code\n\n### Git\n*   [Git Cookbook](git_cookbook.md) - A collection of git recipes for common\n    tasks\n*   [Git Tips](git_tips.md) - More git tips\n*   [Git submodules](git_submodules.md) - Git submodule (tips, FAQ)\n\n### Clang\n*   [Clang Compiler](clang.md) - General information on the clang compiler, used\n    by default on Mac and Linux\n*   [Clang Tool Refactoring](clang_tool_refactoring.md) - Leveraging clang tools\n    to perform refactorings that are AST-aware\n*   [The Clang Static Analyzer](clang_static_analyzer.md) - How to enable static\n    analysis at build time\n*   [Clang Code Coverage Wrapper](clang_code_coverage_wrapper.md) - Enable Clang\n    code coverage instrumentation for a subset of source files.\n*   [Writing Clang Plugins](writing_clang_plugins.md) - Don't write a clang\n    plugin, but if you do, read this\n*   [Updating Clang](updating_clang.md) - Updating the version of Clang used to\n    build\n*   [Using clang-format on Chromium C++ Code](clang_format.md) - Various ways to\n    invoke clang-format on C++ code\n*   [Clang Tidy](clang_tidy.md) - Support for the `clang-tidy` tool in Chromium\n*   [Updating Clang Format Binaries](updating_clang_format_binaries.md) - How up\n    update the clang-format binaries that come with a checkout of Chromium\n\n### General Development\n*   [Contributing to Chromium](contributing.md) - Reference workflow process for\n    contributing to the Chromium code base.\n*   [Commit Checklist](commit_checklist.md) - Streamlined checklist to go\n    through before uploading CLs on Gerrit.\n*   [Code Reviews](code_reviews.md) - Code review requirements and guidelines\n*   [Dependency management](dependencies.md) - Managing dependencies (DEPS, git submodules)\n*   [Respectful Code Reviews](cr_respect.md) - A guide for code reviewers\n*   [Respectful Changes](cl_respect.md) - A guide for code authors\n*   [Mandatory Code-Review Rollout](code_review_owners.md) - Upcoming policy changes related to code review and OWNERS\n*   [LUCI Migration FAQ](luci_migration_faq.md) - FAQ on Buildbot-to-LUCI\n    builder migration for Chromium\n*   [Tour of Continuous Integration UI](tour_of_luci_ui.md) - A tour of our\n    the user interface for LUCI, our continuous integration system\n*   [Parsing Test Results](parsing_test_results.md) - An introduction for how to\n    understand the results emitted by polygerrit and CI builds.\n*   [Closure Compilation](closure_compilation.md) - The _Closure_ JavaScript\n    compiler\n*   [Threading and Tasks in Chrome](threading_and_tasks.md) - How to run tasks\n    and handle thread safety in Chrome.\n*   [Callback<> and Bind()](callback.md) - All about Callbacks, Closures, and\n    Bind().\n*   [Chromium Views UI](ui/index.md) - Working with the desktop UI framework.\n*   [Views Platform Styling](ui/views/platform_style.md) - How views are styled\n    to fit in different native platforms\n*   [Tab Helpers](tab_helpers.md) - Using WebContents/WebContentsObserver to add\n    features to browser tabs.\n*   [Adding third_party Libraries](adding_to_third_party.md) - How to get code\n    into third_party/\n*   [Graphical Debugging Aid for Chromium Views](graphical_debugging_aid_chromium_views.md) -\n    Visualizing view trees during debugging\n*   [Bitmap Pipeline](bitmap_pipeline.md) - How bitmaps are moved from the\n    renderer to the screen.\n*   [Flag Guarding Guidelines](flag_guarding_guidelines.md) - When to use\n    server controlled kill switches and A/B experiments to safely roll out\n    changes.\n*   [Using the Origin Trials Framework](origin_trials_integration.md) - A\n    framework for conditionally enabling experimental APIs for testing.\n*   [Chrome Sync](https://source.chromium.org/chromium/chromium/src/+/main:docs/website/site/developers/design-documents/sync) -\n    Docs for the subsystem that allows one to sync data across devices.\n*   [Ozone Overview](ozone_overview.md) - Ozone is an abstraction layer between\n    the window system and low level input and graphics.\n*   [Guidelines for considering branch dates in project planning](release_branch_guidance.md) -\n    What to do (and not to do) around branch dates when scheduling your project\n    work.\n*   [Watchlists](infra/watchlists.md) - Use watchlists to get notified of CLs\n    you are interested in.\n*   [Shutdown](shutdown.md) - Explains the steps of Chrome shutdown, to make it\n    easier to determine where to add a new shutdown operation.\n*   [API Keys](api_keys.md) - When you need access to Google APIs for a custom\n    build, fork, integration of stock Chromium, or are building ChromiumOS (for\n    login).\n*   [User Education](../components/user_education/README.md) - Create\n    in-product help (IPH) and tutorials to call out Chromium features\n\n### Testing\n*   [Running and Debugging Web Tests](testing/web_tests.md)\n*   [On disabling tests](testing/on_disabling_tests.md)\n*   [Writing Web Tests](testing/writing_web_tests.md) - Web Tests using\n    `content_shell`\n*   [Web Test Expectations and Baselines](testing/web_test_expectations.md) -\n    Setting expected results of web tests.\n*   [Web Tests Tips](testing/web_tests_tips.md) - Best practices for web tests\n*   [Web Tests with Manual Fallback](testing/web_tests_with_manual_fallback.md) -\n    Writing tests that simulate manual interventions\n*   [Extending the Web Test Framework](how_to_extend_web_test_framework.md)\n*   [Fixing Web Test Flakiness](testing/identifying_tests_that_depend_on_order.md) -\n    Diagnosing and fixing web test flakiness due to ordering dependencies.\n*   [Running Web Tests using `content_shell`](testing/web_tests_in_content_shell.md) -\n    Running web tests by hand.\n*   [Web Platform Tests](testing/web_platform_tests.md) - Shared tests across\n    browser vendors\n*   [Using Crashpad with `content_shell`](testing/using_crashpad_with_content_shell.md) -\n    Capture stack traces on layout test crashes without an attached debugger\n*   [Test Descriptions](testing/test_descriptions.md) - Unit test targets that can be\n    built, with associated descriptions.\n*   [Fuzz Testing](../testing/libfuzzer/README.md) - Fuzz testing in Chromium.\n*   [IPC Fuzzer](testing/ipc_fuzzer.md) - Fuzz testing of Chromium IPC interfaces.\n*   [Running Chrome tests with AddressSanitizer (asan) and LeakSanitizer (lsan)](testing/linux_running_asan_tests.md) -\n    Run Chrome tests with ASAN and LSAN builds to detect addressability issues and memory leaks.\n*   [Code Coverage](testing/code_coverage.md) - Code coverage for Chromium.\n*   [Code Coverage in Gerrit](testing/code_coverage_in_gerrit.md) - Per-CL code\n    coverage in Gerrit to assist code reviews.\n\n### Configuration Docs\n\n*   [Configuration: Prefs, Settings, Features, Switches & Flags](configuration.md) - Explains different ways to gate a new feature.\n*   [Adding a new feature flag in chrome://flags](how_to_add_your_feature_flag.md) - Quick guide to add a new feature flag to experiment your feature.\n*   [Runtime Enabled Features](https://chromium.googlesource.com/chromium/src/+/main/third_party/blink/renderer/platform/RuntimeEnabledFeatures.md)\n*   [Initialization of Blink runtime features in content layer](initialize_blink_features.md)\n*   [Integrating a feature with the origin trials framework](origin_trials_integration.md)\n\n### GPU-related docs\n*   [GPU Pixel Wrangling](gpu/pixel_wrangling.md) - Instructions for GPU\n    pixel wrangling (the GPU sheriffing rotation).\n*   [Debugging GPU related code](gpu/debugging_gpu_related_code.md) - Hints for\n    debugging GPU- and graphics-related code.\n*   [GPU Testing](gpu/gpu_testing.md) - Description of Chromium's GPU testing\n    infrastructure.\n*   [GPU Bot Details](gpu/gpu_testing_bot_details.md) - In-depth description of\n    how the bots are maintained.\n\n### Misc Linux-Specific Docs\n*   [Linux Proxy Config](linux/proxy_config.md) - Network proxy sources on Linux\n*   [Debugging SSL on Linux](linux/debugging_ssl.md) - Tips on debugging SSL\n    code in Linux\n*   [Linux Cert Management](linux/cert_management.md) - Managing X.509\n    Certificates in Linux\n*   [Tips for Debugging on Linux](linux/debugging.md)\n*   [Linux GTK Theme Integration](linux/gtk_theme_integration.md) - Having\n    Chrome match the GTK+ theme.\n*   [Browser Plugins on Linux](linux/plugins.md) - A collection of links to\n    information on how browser plugins work on Linux\n*   [Linux Crash Dumping](linux/crash_dumping.md) - How Breakpad uploads crash\n    reports to Google crash servers.\n*   [Linux Minidump to Core](linux/minidump_to_core.md) - How to convert a\n    Breakpad-generated minidump files to a core file readable by most debuggersx\n*   [Linux Sandbox IPC](linux/sandbox_ipc.md) - The lower level UPC system used\n    to route requests from the bottom of the call stack up into the browser.\n*   [Linux Dev Build as Default Browser](linux/dev_build_as_default_browser.md) -\n    How to configure a Dev build of Chrome as the default browser in Linux.\n*   [Linux Chromium Packages](linux/chromium_packages.md) - Packages of Chromium\n    browser (not Chrome) provided by some Linux distributions.\n*   [`seccomp` Sandbox Crash Dumping](seccomp_sandbox_crash_dumping.md) - Notes\n    on crash dumping a process running in a seccomp sandbox.\n*   [Linux Password Storage](linux/password_storage.md) - Keychain integrations\n    between Chromium and Linux.\n*   [Linux Sublime Development](sublime_ide.md) - Using Sublime as an IDE\n    for Chromium development on Linux.\n*   [Building and Debugging GTK](linux/building_debug_gtk.md) - Building\n    Chromium against GTK using lower optimization levels and/or more debugging\n    symbols.\n*   [Debugging GTK](linux/debugging_gtk.md) - Using the GTK Debug packages and\n    related tools.\n*   [Chroot Notes](linux/using_a_chroot.md) - Setting up a chroot to work around\n    libfreetype differences in some versions of Linux.\n*   [Linux Sandboxing](linux/sandboxing.md) - The Linux multi-process model to\n    isolate browser components with different privileges.\n*   [Zygote Process](linux/zygote.md) - How the Linux Zygote process, used to\n    spawn new processes, works.\n*   [Running Web Tests on Linux](testing/web_tests_linux.md) - Linux-specific\n    instructions for running web tests.\n*   [Linux Sysroot Images](linux/sysroot.md) - How builds use libraries on Linux\n*   [Linux Hardware Video Decoding](linux/hw_video_decode.md) - Enabling\n    hardware video decode codepaths on Linux\n\n### Misc MacOS-Specific Docs\n*   [Mac Debugging Tips](mac/debugging.md) - An introduction to debugging on\n    Mac, as well as a collection of useful tips.\n*   [Using CCache on Mac](ccache_mac.md) - Speed up builds on Mac using ccache\n    with clang/ninja\n*   [Cocoa tips and tricks](cocoa_tips_and_tricks.md) - A collection of idioms\n    used when writing Cocoa views and controllers\n*   [MacViews Release Plan](ui/views/macviews_release.md)\n\n### Misc Windows-Specific Docs\n*   [Handling cygwin rebaseall failures](cygwin_dll_remapping_failure.md)\n*   [Hacking on ANGLE in Chromium](angle_in_chromium.md) - OpenGL ES 2.0 built\n    on top of DirectX\n*   [Windows Split DLLs](windows_split_dll.md) - Splitting `chrome.dll` into\n    multiple dlls to work around toolchain limitations on Windows.\n*   [Windows Native Window Occlusion Tracking](windows_native_window_occlusion_tracking.md)\n*   [Windows PWA Integration](windows_pwa_integration.md) - Integration with\n    Progressive Web Apps on Windows\n*   [Windows Shortcut and Taskbar Handling](windows_shortcut_and_taskbar_handling.md)\n*   [Windows Virtual Desktop Integration](windows_virtual_desktop_handling.md)\n\n### Misc Android-Specific Docs\n*   [Google Play Services in Chrome for Android](google_play_services.md)\n*   [Accessing C++ Enums in Java](android_accessing_cpp_enums_in_java.md) - How\n    to use C++-defined enums in Java code\n*   [Profiling Content Shell on Android](profiling_content_shell_on_android.md) -\n    Setting up profiling for `content_shell` on Android\n*   [Working Remotely with Android](working_remotely_with_android.md) - Building\n    on a remote machine for an Android device connected to your local machine\n*   [Android Test Instructions](testing/android_test_instructions.md) - Running a build\n    on an Android device or emulator.\n*   [Android Debugging](android_debugging_instructions.md) - Tools and tips for\n    how to debug Java and/or C/C++ code running on Android.\n*   [Android Logging](android_logging.md) - How Chrome's logging API works with\n    `android.util.Log` on Android, and usage guidelines.\n*   [Android Java Static Analysis](../build/android/docs/lint.md) - Catching\n    Java related issues at compile time with the 'lint' tool.\n*   [Java Code Coverage](../build/android/docs/coverage.md) - Collecting code\n    coverage data with the EMMA tool.\n*   [Dynamic Feature Modules (DFMs)](android_dynamic_feature_modules.md) - What\n    are they and how to create new ones.\n*   [Other build-related Android docs](../build/android/docs/index.md)\n*   [Chrome for Android UI](ui/android/overview.md) - Resources and best practices for\n    developing UI\n\n### Misc iOS-Specific Docs\n*   [Continuous Build and Test Infrastructure for Chromium for iOS](ios/infra.md)\n*   [Opening links in Chrome for iOS](ios/opening_links.md) - How to have your\n    iOS app open links in Chrome.\n*   [User Agent in Chrome for iOS](ios/user_agent.md) - Notes on User Agent\n    strings using Chrome for iOS.\n*   [Running iOS test suites locally](ios/testing.md)\n*   [Working With Project Files in iOS](ios/working_with_files.md) - How\n    to add, remove, and rename files in the iOS Chromium project.\n\n### Misc Chrome-OS-Specific Docs\n*   [Setting up captive portals and other restrictive networks](login/restrictive_networks.md)\n*   [Enterprise Enrollment](enterprise/enrollment.md)\n    *   [Kiosk mode and public sessions](enterprise/kiosk_public_session.md)\n*   [Debugging UI in OOBE/login/lock](login/ui_debugging.md)\n*   [Chrome Logging on Chrome OS](chrome_os_logging.md)\n*   [Debugging tips](testing/chromeos_debugging_tips.md)\n\n### Misc WebUI-Specific Docs\n*   [WebUI Explainer](webui/webui_explainer.md) - An explanation of C++ and\n    TypeScript infrastructural code for Chrome UIs implemented with web\n    technologies (i.e. chrome:// URLs).\n*   [Optimizing Chrome Web UIs](webui/optimizing_web_uis.md) - Notes on making\n    WebUIs more performant\n*   [Trusted Types on WebUI](webui/trusted_types_on_webui.md) - Tips for coding\n    in WebUI with Trusted Types in mind.\n*   [chrome-untrusted:// FAQ](webui/chrome_untrusted.md) - Explainer on the\n    usage of the `chrome-untrusted://` scheme for hosting WebUIs that handle\n    untrustworthy content.\n\n### Media\n*   [Audio Focus Handling](media/audio_focus.md) - How multiple MediaSession\n    audio streams interact\n*   [Autoplay of HTMLMediaElements](media/autoplay.md) - How HTMLMediaElements\n    are autoplayed.\n*   [Latency tracing](media/latency_tracing.md) - How to use the\n    `\"audio.latency\"` tracing category to measure audio latency.\n*   [Piranha Plant](piranha_plant.md) - Future architecture of MediaStreams\n*   [Media Capture](media/capture/README.md) - Features and APIs that enable the\n    browser to capture pixels and audio from itself or the underlying OS.\n*   [Video Encode Accelerator Tests](media/gpu/veatest_usage.md) - How to\n    use the accelerated video encoder test program.\n*   [Video Decoder Tests](media/gpu/video_decoder_test_usage.md) - Running the\n    video decoder tests.\n*   [Video Decoder Performance Tests](media/gpu/video_decoder_perf_test_usage.md) -\n    Running the video decoder performance tests.\n\n### Accessibility\n*   [Accessibility Overview](accessibility/overview.md) - Overview of\n    accessibility concerns and approaches in Chromium.\n*   [Accessibility Tests](accessibility/browser/tests.md) - Where to find\n    accessibility-related tests in the codebase.\n*   [ChromeVox on Chrome OS](accessibility/os/chromevox.md) - Enabling spoken\n    feedback (ChromeVox) on Chrome OS.\n*   [ChromeVox on Desktop Linux](accessibility/os/chromevox_on_desktop_linux.md) -\n    Enabling spoken feedback (ChromeVox) on desktop Linux.\n*   [Offscreen, Invisible and Size](accessibility/browser/offscreen.md) - How Chrome\n    defines offscreen, invisible and size in the accessibility tree.\n*   [Text to Speech](accessibility/browser/tts.md) - Overview of text to speech in\n    Chrome and Chrome OS.\n*   [BRLTTY in Chrome OS](accessibility/os/brltty.md) - Chrome OS integration with\n    BRLTTY to support refreshable braille displays\n*   [PATTS on Chrome OS](accessibility/os/patts.md) - Notes on the PATTS speech\n    synthesis engine used on Chrome OS\n*   [VoiceOver](ios/voiceover.md) - Using Apple's VoiceOver feature with\n    Chromium on iOS.\n\n### Memory\n*   [Memory Overview](memory/README.md)\n*   [Heap Profiling with External Tools](memory/heap_profiling_external.md)\n\n### Memory Infrastructure Timeline Profiling (MemoryInfra)\n*   [Overview](memory-infra/README.md)\n*   [GPU Profiling](memory-infra/probe-gpu.md)\n*   [Adding Tracing to a Component](memory-infra/adding_memory_infra_tracing.md)\n*   [Enabling Startup Tracing](memory-infra/memory_infra_startup_tracing.md)\n*   [Memory Usage in CC](memory-infra/probe-cc.md)\n*   [Memory Benchmarks](memory-infra/memory_benchmarks.md)\n*   [Heap Profiling](memory-infra/heap_profiler.md)\n\n### Metrics\n*   [Histograms](/tools/metrics/histograms/README.md)\n*   [User Actions](/tools/metrics/actions/README.md)\n*   [Code review guidelines](/tools/metrics/histograms/review_guidelines.md)\n\n### Misc\n*   [Useful URLs](useful_urls.md) - A collection of links to various tools and\n    dashboards\n*   [ERC IRC](erc_irc.md) - Using ERC in EMACS to access IRC\n*   [Kiosk Mode](kiosk_mode.md) - Simulating kiosk mode.\n*   [User Handle Mapping](user_handle_mapping.md) - Names of developers across\n    Chromium/IRC/Google\n*   [Documentation Best Practices](documentation_best_practices.md)\n*   [Documentation Guidelines](documentation_guidelines.md)\n*   [Chromium Browser vs Google Chrome](chromium_browser_vs_google_chrome.md) -\n    What's the difference between _Chromium Browser_ and _Google Chrome_?\n*   [Google Chrome branded builds](google_chrome_branded_builds.md)\n*   [Proxy Auto Config using WPAD](proxy_auto_config.md) - How WPAD servers are\n    used to automatically set proxy settings.\n*   [Installing Chromium OS on VMWare](installation_at_vmware.md) - How to\n    install Chromium OS on VMWare.\n*   [User Data Directory](user_data_dir.md) - How the user data and cache\n    directories are determined on all platforms.\n*   [User Data Storage](user_data_storage.md) - Policy documentation for files in User Data.\n\n### Mojo &amp; Services\n*   [Intro to Mojo &amp; Services](mojo_and_services.md) - Quick introduction\n    to Mojo and services in Chromium, with examples\n*   [Mojo API Reference](/mojo/README.md) - Detailed reference documentation for\n    all things Mojo\n*   [Service Development Guidelines](/services/README.md) - Guidelines for\n    service development in the Chromium tree\n*   [Servicifying Chromium Features](servicification.md) - General advice for\n    integrating new and existing subsystems into Chromium as services\n*   [Converting Legacy IPC to Mojo](mojo_ipc_conversion.md) - Tips and common\n    patterns for practical IPC conversion work\n*   [Mojo “Style” Guide](security/mojo.md) - Recommendations for best practices\n    from Mojo and IPC reviewers\n*   [D-Bus Mojo Connection Service](dbus_mojo_connection_service.md) - A service\n    in Chrome to bootstrap CrOS services' Mojo connection.\n\n### Security\n*   [The Rule Of 2](security/rule-of-2.md) - An imoportant security rule when\n    handling untrustworthy contents (like anything downloaded from the web).\n\n### Speed\n*   [Chrome Speed](speed/README.md) - Documentation for performance measurements and regressions in Chrome.\n*   [Chrome Speed Metrics](speed_metrics/README.md) - Documentation about user experience metrics on the web and their JavaScript APIs.\n\n### UI\n*   [Chromium UI Platform](ui/index.md) - All things user interface\n\n### What's Up With That Transcripts\n\nThese are transcripts of [What's Up With\nThat](https://www.youtube.com/playlist?list=PL9ioqAuyl6ULIdZQys3fwRxi3G3ns39Hq),\na video series of interviews with Chromium software engineers.\n\n*   [What's Up With Pointers - Episode 1](transcripts/wuwt-e01-pointers.md)\n*   [What's Up With DCHECKs - Episode 2](transcripts/wuwt-e02-dchecks.md)\n*   [What's Up With //content - Episode 3](transcripts/wuwt-e03-content.md)\n*   [What's Up With Tests - Episode 4](transcripts/wuwt-e04-tests.md)\n*   [What's Up With BUILD.gn - Episode 5](transcripts/wuwt-e05-build-gn.md)\n*   [What's Up With Open Source - Episode 6](transcripts/wuwt-e06-open-source.md)\n*   [What's Up With Mojo - Episode 7](transcripts/wuwt-e07-mojo.md)\n*   [What's Up With Processes - Episode 8](transcripts/wuwt-e08-processes.md)\n*   [What's Up With Site Isolation - Episode 9](transcripts/wuwt-e09-site-isolation.md)\n*   [What's Up With Web Platform - Episode 10](transcripts/wuwt-e10-web-platform.md)\n*   [What's Up With Web Standards - Episode 11](transcripts/wuwt-e11-web-standards.md)\n*   [What's Up With Base - Episode 12](transcripts/wuwt-e12-base.md)\n\n### Probably Obsolete\n*   [TPM Quick Reference](tpm_quick_ref.md) - Trusted Platform Module notes.\n*   [System Hardening Features](system_hardening_features.md) - A list of\n    current and planned Chrome OS security features.\n*   [WebView Policies](webview_policies.md)\n*   [Linux Profiling](linux/profiling.md) - How to profile Chromium on Linux\n*   [Linux Graphics Pipeline](linux/graphics_pipeline.md)\n*   [Linux `SUID` Sandbox](linux/suid_sandbox.md) - Sandboxing renderers using a\n    SUID binary on Linux\n*   [Linux `SUID` Sandbox Development](linux/suid_sandbox_development.md) -\n    Development on the above system.\n*   [Linux PID Namespace Support](linux/pid_namespace_support.md)\n*   [Vanilla msysgit workflow](vanilla_msysgit_workflow.md) - A workflow for\n    using mostly vanilla git on Windows.\n*   [Old Options](chrome_settings.md) - Pre-Material Design chrome://settings\n    notes.\n"
  },
  {
    "path": "security/web_assembly",
    "title": "Web Assembly Security in Chromium",
    "content": "# Web Assembly Security in Chromium\n\nTL;DR\nFrom Chrome's threat model perspective we generally consider WASM and JavaScript\nto be in the same risk category - potentially actively malicious. We will run\narbitrary WASM from the web without any indication of trust and rely on V8's\nsecurity model as well as our renderer sandbox to contain potentially malicious\nbehavior. Any outputs from such WASM and JavaScript need to be regarded as\nuntrusted by the rest of the browser code.\n\n## Background\n\nAs the web continues to evolve, the greater ecosystem brings with it new and\nexciting capabilities. From ajax to shadow DOM, Blink, Chrome, and others\ncontinue to push the web forward.\n\nOne such capability is to run native code. The latest standard to do so on the\nweb is via [web assembly](https://webassembly.org/).\n\nWhen it comes to security, web assembly prompts a series of concerns centered\naround our understanding of the insecurity of native code. Some of these\nspecific issues are addressed below. Native code suffers from a\nvariety of memory vulnerabilities which can result in remote code execution\n(RCE). By some measures, 60-70% of security bugs fall into this category.\n\nWeb assembly baked in memory safety measures to prevent RCE. In particular,\nwasm’s memory model is to give the page a single, continuous block of\nmemory. This memory is isolated from the program’s binary instructions. [Learn\nmore](https://webassembly.org/docs/security/).\n\n## CPU bugs, side channel attacks\n\nWASM is not materially different from JavaScript in this regard and both may be\nvulnerable.\n\n## Malicious Input\n\nWASM is not materially different from JavaScript in this regard and both may be\nvulnerable.\n"
  },
  {
    "path": "security/web-platform-security-guidelines",
    "title": "Web Platform Security guidelines",
    "content": "# Web Platform Security guidelines\n\n[TOC]\n\n## Introduction\nThe Open Web Platform (OWP) is a fast evolving platform, with new features\ncontinuously expanding the scope of what the platform can do. It is also a\nparticularly rich target for would-be attackers. In this context, all new\nfeatures should be reviewed with particular care when it comes to their\nsecurity implications. The goal of this document is to help feature teams go\nthrough the Security part of the S&P review process which ensures that their\nfeatures meet the security requirements expected of a new Web Platform feature.\n\nThe guidelines in this document provide insight on how the Security teams think\nabout the security implications of new Web Platform features. They are here to\nhelp feature teams think about security early on when designing their APIs. As\nnew threats and new mitigations arise, we will update this document to reflect\nour updated recommendations.\n\nCurrently, this document is divided into three sets of guidelines: security\nboundaries, integration with security APIs and security UX. We have written\nthose based on the Web Platform Security team experience of conducting security\nreviews, in partnership with other security teams at Google. This is based on\nconcerns that have come up in security reviews, and a few items that we\nenvision could be problematic. \n\nThe goal of this document is not to provide a checklist, where if every item in\nthe list is checked a feature can be considered secure. If you find that your\nfeature cannot meet some of the security guidelines on this list, please reach\nout to the Web Platform Security team earlier rather than later, and we can\nwork together on how to support your feature’s needs in a secure manner.\n\n## Guidelines\n\n### General guidelines\n\n<a name=\"TOC-safe-api-guidelines\"></a>\n#### Safe API design\n\n> Prefer simple APIs.\n\n* It is easier for developers to use higher-level and well laid out APIs. Try\nto make the easy thing the safe thing in new APIs.\n* If an API really needs potentially risky knobs, they should be well\ndocumented and ideally named to explicitly call out their risk (e.g., the\nsubtle property in WebCrypto -- although this naming could be even more\nexplicit).\n\n<a name=\"TOC-design-with-the-web-ecosystem-in-mind\"></a>\n#### Design with the web ecosystem in mind\n\n> Consider how your feature will interact with the whole web ecosystem. In\nparticular, consider the interactions with workers (ServiceWorkers,\nSharedWorkers, DedicatedWorkers), Fenced Frames and with the back/forward\ncache.\n\n* Some features might need to be restricted in workers. For example, we have\nrestricted access to features like camera/microphone/geolocation because we\nhave no UI surface with which to explain the implication of an API to users.\n* Fenced Frames have particular privacy requirements that might require\ndisabling a feature inside them. For example, CSP EE required particular\nintegration as enforcing a CSP on a Fenced Frame would provide a\ncommunication channel with the Fenced Frame embedder.\n* The interaction of the feature with the back/forward cache might be a\nsecurity concern. For example, audio and video capture should only be allowed\nin pages currently shown to the users, and not in pages located in the\nback/forward cache. See also the [non-fully\nactive](https://www.w3.org/TR/security-privacy-questionnaire/#non-fully-active)\npart of the W3C security and privacy questionnaire.\n\n<a name=\"TOC-enterprise-policies\"></a>\n#### Enterprise policies\n\n> New enterprise policies are not allowed to bypass existing web security\npolicies/protections. Enterprise policies can only be added to bypass newly\nintroduced security restrictions, to maintain compatibility of existing\nenterprise web apps. Enterprise policies should only ever bypass policy\ndecisions made by the browser, and not policies requested by websites.\n\n* When the browser introduces a new security restriction, such as gating Shared\nArray Buffers behind crossOriginIsolation, it is ok to introduce an\nenterprise policy to bypass that restriction in order to maintain\ncompatibility of existing enterprise web apps.\n* It is not ok to create an enterprise policy to bypass longstanding security\nrestrictions that should be supported by existing apps. For example, it would\nnot be ok to introduce an enterprise policy to bypass the same-origin policy.\n* Enterprise policies should only apply to new security restrictions introduced\nto the browser. Enterprise policies should not be used to relax security\npolicies requested by the website themselves. For example, it would not be ok\nto introduce an enterprise policy that bypasses CSP.\n* See the [Chrome Security\nFAQ](https://chromium.googlesource.com/chromium/src/+/master/docs/security/faq.md#Are-enterprise-admins-considered-privileged)\nfor more information on enterprise policies.\n\n### Security boundaries\n\n<a name=\"TOC-security-boundaries\"></a>\n#### Security boundaries\n\n> The security team maintains several security boundaries in the WebPlatform:\norigin, site, secure contexts, cross-origin isolated contexts. Before\nintroducing a new security boundary to support your design, discuss it with\nthe Web Platform Security team to ensure it's equally enforceable.\n\n* Maintaining a security boundary is complex, and might not even be possible\n(e.g. origins and Spectre). An API relying on a new form of security boundary\nshould be thoroughly discussed with the Security team to check if the\nboundary is enforceable and the security guarantees can be met. For example,\nit is impossible to create an iframe that is fully isolated from its parent,\ndue to the risk of Spectre attacks on platforms that do not support Site\nIsolation (low-end Android).\n\n<a name=\"TOC-the-origin-boundary\"></a>\n#### The origin boundary\n\n> The origin is the security boundary we aim to defend. We may make diverge\n> from that (in both directions) in some cases, but those ought to be done in\n> consultation with security folks.\n\n* The origin is the primary security boundary of the web, as per the\n[Same-origin\npolicy](https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy).\n* Note that unlike privacy, security is\nconcerned with same-site but cross-origin interactions.\n\n* Maintaining a security boundary is complex, and might not even be possible\n(e.g. origins and Spectre). An API relying on a new form of security boundary\nshould be thoroughly discussed with the Security team to check if the\nboundary is enforceable and the security guarantees can be met. For example,\nit is impossible to create an iframe that is fully isolated from its parent,\ndue to the risk of Spectre attacks on platforms that do not support Site\nIsolation (low-end Android).\n\n<a name=\"TOC-encryption\"></a>\n#### Encryption\n\n> Prefer secure contexts for new features.\n\n* Any data sent over HTTP can be observed by others on the network and opens\nusers to on-path attackers.\n* The same-origin policy the web security model is built upon is easily abused\nwithout cryptographical authentication of the servers we are talking to.\n* See the [Chrome Security\nFAQ](https://chromium.googlesource.com/chromium/src/+/main/docs/security/faq.md#why-are-some-web-platform-features-only-available-in-https-page_loads)\nfor more information.\n\n<a name=\"TOC-timer-resolution\"></a>\n#### Timer resolution\n\n> Explicit timers' granularity must be limited based on a context's\ncross-origin isolation status. Currently, isolated contexts can support\ntimers coarsened to at least 5 microseconds, while unisolated contexts must\ncoarsen timers to 100 microseconds or more. If an API allows the creation of\ntimers with a precision higher than allowed in unisolated contexts, it should\nbe restricted to crossOriginIsolated contexts.\n\n* High resolution timers open users to timing attacks such as Spectre. This is\nwhy their precision should be limited.\n* In crossOriginIsolated contexts, cross-origin resources are either loaded\nwithout credentials, or they opt into being embedded cross-origin into a\ncontext where they could potentially be read by their embedder. This means\nthat cross-origin resources in a crossOriginIsolated context are either ok\nwith a Spectre attack (opt-in model), or of no interest to an attacker\n(credentialless model). Because of this, we allow higher precision timers in\ncrossOriginIsolated contexts. \n* APIs that can be used to create timers (e.g. SharedArrayBuffers) that are\nmore precise than timers available in cross-origin isolated contexts should\nbe gated behind crossOriginIsolation, in order to avoid introducing high\nresolution timers to the platform.\n\n<a name=\"TOC-accessing-data-of-cross-origin-subresources\"></a>\n#### Accessing data of cross-origin subresources\n\n> New APIs that can access data from cross-origin subresources should be gated\n> behind an appropriate mechanism depending on their surface:\n> * CORS/TAO for access to a single resource.\n> * crossOriginIsolation for access to the agent cluster.\n> * crossOriginIsolation + frame opt-in mechanism for access to the whole page.\n\n* This kind of API bypasses the same-origin policy, which is the base of the\nWeb security model. This is only acceptable if the cross-origin resources opt\ninto sharing this data.\n* CORS or TAO are appropriate when divulging information about a single\nresource, e.g. load timings for a single resource.\n* crossOriginIsolation should be used when the API can divulge information\nabout cross-origin resources located in the same agent cluster (roughly, the\nAPI is scoped to same-origin documents). For example, an API that measures\nthe memory taken by all same-origin documents and their subresources.\n* If the API can divulge information from the resources located in the whole\npage, we will need an opt-in from documents outside the agent cluster, on top\nof crossOriginIsolation. For example, an API that streams a video of the page\nor takes a screenshot of the page. Alternatively, it might be acceptable to\ngate the API behind user interaction with a sufficiently informative UI\nelement.\n\n<a name=\"TOC-side-channels\"></a>\n#### Side channels\n\n> Any new form of cross-origin communication or API should be carefully\n> considered when it comes to side-channel attack risks.\n\n* Cross-origin communication channels and APIs can be abused to leak data from\ncross-origin resources. Any new addition should be carefully reviewed to\nassess the amount of data exposed. Note that unlike privacy, security is\nconcerned with same-site but cross-origin communication and APIs.\n\n<a name=\"TOC-implementation-concerns\"></a>\n#### Implementation concerns\n\n> Any new API whose implementation is particularly risky (e.g. requires new\n> parsers, involves new codecs, requires particular isolation) should see the\n> implementation reviewed in detail, in conjunction with the wider Chrome\n> Security team.\n\n* New parsers and codecs are particularly risky pieces of code that are exposed\nto attacker-controlled inputs. Their implementations are subjected to\nparticular rules (see the [rule of\ntwo](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/security/rule-of-2.md)),\nand they must be fuzzed.\n* Proper isolation is hard to deploy, is heavily dependent on implementation\nand might face constraints on some platforms (e.g. low-end Android devices).\n\n<a name=\"TOC-user-activation\"></a>\n#### User activation\n\n> Consider requiring a feature be gated behind user activation if its UX could\n> be abused by a document the user did not interact with.\n\n* Several features should only be available if the user chooses to interact\nwith the document. If they could be abused otherwise, consider gating them\nbehind user activation. For example, fullscreen is gated behind user\nactivation, as it could be used to trick the user into believing they are on\nanother page by mimicking the Chrome UI.\n* This helps match the platform with the user’s mental model.\n* Features which are security sensitive will likely need aditional protection\nbeyond user activation. User activation is not a security boundary, it is a\nway to protect users from abusive UX behavior from sites.\n\n<a name=\"TOC-navigation-and-document-lifetime\"></a>\n#### Navigation and Document lifetime\n\n> Any feature that impacts the lifetime of documents or that modifies\n> navigation is likely to have far-reaching security implications. Please\n> discuss the implications with the Web Platform Security team as soon as\n> possible. \n\n* Modifying the navigation stack could cause URL spoofing attacks.\n* Modifying the navigation stack or the document lifetime could result in wrong\norigin or security policies being applied to the document.\n\n### Integration with security policies\n\n<a name=\"TOC-document-load\"></a>\n#### Document load\n\n> All changes to how a document is loaded should ensure they uphold the following\n> security policies:\n> [XFO](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Frame-Options),\n> [CSP](https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP),\n> [COOP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Opener-Policy),\n> [COEP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Embedder-Policy),\n> and [Private Network access](https://wicg.github.io/private-network-access/).\n\n* XFO, CSP, COEP and Private Network access can block document load. This\nshould be respected.\n* COOP can trigger a browsing context group switch which should be respected as\nwell.\n* CSP, COEP and Private Network access are computed when loading a document and\nmay apply to all of the document resources. Failure to integrate properly\nwith them could result in the policies being bypassed.\n\n<a name=\"TOC-subresource-load\"></a>\n#### Subresource load\n\n> All changes to how a resource is loaded should uphold the following\n> security policies:\n> [CSP](https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP),\n> [COEP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Embedder-Policy),\n> [CORP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Cross-Origin_Resource_Policy_(CORP)),\n> [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS),\n> [CORB](https://fetch.spec.whatwg.org/#corb),\n> [SRI](https://developer.mozilla.org/en-US/docs/Web/Security/Subresource_Integrity)\n> and [Private Network access](https://wicg.github.io/private-network-access/).\n\n* All of the policies above may block unsafe resource loads and must be\nproperly applied to any subresource load.\n\n<a name=\"TOC-code-execution\"></a>\n#### Code execution\n\n> New ways of executing JavaScript code should integrate with CSP script-src.\n\n* New ways of executing JavaScript should defend themselves against XSS attacks\nby supporting CSP.\n\n> New ways of executing code should consider the cross-site scripting (XSS)\n> risks. If any new risk is identified, the new API should integrate with CSP\n> script-src and/or Trusted Types and/or Sanitizer.\n\n* New ways of executing code should consider whether they are open to XSS\nvulnerabilities. If they are, they should defend themselves against attacks\nby supporting CSP and/or Trusted Types and/or Sanitizer.\n* For example, the [import map\nproposal](https://github.com/WICG/import-maps/issues/105) had to be spelled\nas an extension to the\n`<script>` tag to avoid inadvertently creating CSP bypasses.\n\n<a name=\"TOC-CORS\"></a>\n#### CORS\n\n> New types of elements should require CORS when loading resources.\n\n* CORS allows an origin to control how its authenticated data is embedded in\nother origins. In an ideal world, all cross-origin authenticated requests\nwould require CORS, but this is impossible for compatibility reasons.\nHowever, new elements should not add a new source of non CORS requests to the\nplatform. \n* Elements that are allowed to make cross-origin authenticated requests without\nCORS introduce a hole in the platform that may be exploited in a MIME\nmismatch attack to bypass CORS protections for resources that are normally\nloaded through Fetch or into elements that require CORS. \n\n<a name=\"TOC-mime-types\"></a>\n#### MIME types\n\n> New resource types should require strict MIME type matching, and avoid\n> relying upon [sniffing](https://mimesniff.spec.whatwg.org/).\n\n* Mismatches between a resource's asserted MIME type and the way it's used by\nthe browser can cause security issues. For example, browsers currently\nattempt to execute practically anything via `<script>` tags because of\nwidespread mislabeling of script resources as `text/html` or text/plain`,\nwhich can expose those resources to side-channel attacks like Spectre, and\nmore direct XSSI attacks.\n[CORB](https://chromium.googlesource.com/chromium/src/+/HEAD/services/network/cross_origin_read_blocking_explainer.md)\nmitigates some, but not all, of these risks.\n* New resource types should avoid these risks entirely by specifying clear MIME\ntypes, and accepting only those resources that assert themselves to be of the\nproper type.\n\n### Security UX\n\n<a name=\"TOC-iframes\"></a>\n#### Iframes\n\n> A document element should not be allowed to draw outside its frame.\n\n* A document drawing over its embedding frame allows it to perform clickjacking\nattacks and should never be allowed.\n* Drawing over the browser-controlled UX surface allows to perform all sorts of\nattacks. See the browser-controlled surface guideline below for more\ninformation.\n\n<a name=\"TOC-browser-controlled-surface\"></a>\n#### Browser-controlled surface\n\n> Browser-controlled surfaces should not be drawn over. Converting a\n> browser-controlled UX surface into a content-controlled surface can only be\n> considered in specific cases and must be gated behind appropriate mechanisms.\n\n* Drawing over the omnibox allows to perform URL spoof attacks and should never\nbe allowed. It can also lead to displaying incorrect SSL state information to\nthe user.\n* It is possible to consider converting some of the browser-controlled surfaces\ninto content-controlled surfaces in cases such as installed PWAs. This should\nstill be gated behind explicit signals from the user, such as a permission\ngrant or other in-context UI affordances that allow the user to toggle\nbetween modes.\n* Browser-controlled surfaces include the top browser chrome, but also the\nfullscreen disclosure bubble, Payment Handler dialogs, permission dialogs,\netc.\n\n<a name=\"TOC-site-identity-and-security-indicators\"></a>\n#### Site identity and security indicators\n\n> Communicating site identity and security indicators should only be done\n> through browser-controlled UI.\n\n* Security state and site identity are state tracked by the browser and it is\ndifficult (at best) to show this information inside the content area in a\ntrustworthy way. See the [Chrome Security\nFAQ](https://chromium.googlesource.com/chromium/src/+/master/docs/security/faq.md#Certificates-Connection-Indicators)\nfor more details.\n* Interstitials (e.g., SSL error pages) are shown in the content area but are\nbrowser controlled committed navigations.\n* Other security displays overlaid on the content area (e.g., autofill\nwarnings) should be handled with care and should be controlled by the\nbrowser.\n\n<a name=\"TOC-browser-controlled-ui\"></a>\n#### Browser-controlled UI\n\n> Changes to browser-controlled UI or new features that will require new\n> browser-controlled UI should go through Chrome Browser security review.\n\n* Browser-controlled UI has many considerations around spoofing, abuse, etc.\n* New UI needs to be reviewed by various cross-functional groups (not just\nsecurity).\n* This is especially true when the UI involves the user making a decision or\nthe UI communicates site identity to the user in any way.\n\n<a name=\"TOC-permissions\"></a>\n#### Permissions\n\n> Powerful new capabilities should in most cases integrate with\n> Permissions-Policy and only be accessible to top-level frames by default.\n> The Permissions Team should be brought in early to consult on new\n> permissions.\n\n* Permissions often require users to make a trust decision about a site. In\nChrome, the only visible site identity is the top-level frame (whose origin\nis shown in the Omnibox).\n* User confusion about subframe origins was a motivation for permission\ndelegation, where top-level contexts must explicitly delegate their\npermissions to subframes, allowing the user to only have to reason about\ntop-level frames.\n\n<a name=\"TOC-mixed-content\"></a>\n#### Mixed content\n\n> New features should not be able to relax or work around mixed content\n> restrictions.\n\n* Chrome now upgrades or blocks all mixed content (insecure resources or\nconnections embedded in secure contexts). This greatly simplifies Chrome’s\nsecurity state model.\n* Allowing new features to bypass these security restrictions can undermine\nother security features in Chrome which assume no mixed content (such as\nHTTPS-First Mode).\n\n<a name=\"TOC-foreground-background-execution\"></a>\n#### Foreground/background execution\n\n> Consider whether a new feature might be abusable or confusing to a user if a\n> site can use it while in the background. Ensure that the user has sufficient\n> context for something triggering and won’t be caught by surprise.\n\n* This will often co-occur with “Browser-controlled UI” (see above), but it is\ngood to think about whether an API should be restricted to foreground tabs\nonly if they have a risk of being surprising to the user when used by a page.\n* For example, we restrict HTML fullscreen to foreground contexts (and display\na disclosure UI). APIs that trigger permission prompts should only show the\nprompts when the tab is in the foreground.\n\n"
  },
  {
    "path": "security/web-mitigation-metrics",
    "title": "Web Mitigation Metrics",
    "content": "# Web Mitigation Metrics\n\nThe web platform offers a number of tools to web developers which enable\nmitigation of a few important threats. In order to understand how these are\nbeing used in the wild, and evaluate our success at promulgating them, we\ncollect some usage statistics; this document outlines those counters and\npoints to some helpful graphs.\n\n## Content Security Policy\n\nWe believe that a carefully-crafted [Content Security Policy][csp] can help\nprotect web applications from injection attacks that would otherwise lead to\nscript execution. [Strict CSP][strict-csp] is a reasonable approach, one which\nwe'd like to encourage.\n\n[csp]: https://w3c.github.io/webappsec-csp/\n[strict-csp]: https://csp.withgoogle.com/docs/strict-csp.html\n\nIn order to understand CSP's use in the wild, we can look at a few counters that\ngive some insight into the percentage of Chrome users' page views that use CSP\nin a given way:\n\n*   `kContentSecurityPolicy`\n    ([graph](https://chromestatus.com/metrics/feature/timeline/popularity/15))\n    tracks the overall usage of `Content-Security-Policy` headers. Likewise,\n    `kContentSecurityPolicyReportOnly`\n    ([graph](https://chromestatus.com/metrics/feature/timeline/popularity/16))\n    tracks the report-only variant.\n\nTo get a feel for the general quality of policies in the wild, we want to\nevaluate how closely developers are hewing to the strictures of Strict CSP.\nWe've boiled that down to three categories:\n\n*   Does the policy reasonably restrict [`object-src`][object-src]? The only\n    \"reasonable\" restriction, unfortunately, is `object-src 'none'`.\n    `kCSPWithReasonableObjectRestrictions` and\n    `kCSPROWithReasonableObjectRestrictions` track that directive value in\n    enforced and report-only modes respectively.\n\n*   Does the policy reasonably restrict `base-uri` in order to avoid malicious\n    redirection of relative URLs? `base-uri 'none'` and `base-uri 'self'` are\n    both appropriate, and are tracked via `kCSPWithReasonableBaseRestrictions`\n    and `kCSPROWithReasonableBaseRestrictions` in enforced and report-only modes\n    respectively.\n\n*   Does the policy avoid using a list of hosts or schemes (which [research has\n    shown to be mostly ineffective at mitigating attack][csp-is-dead])?\n    `kCSPWithReasonableScriptRestrictions` and\n    `kCSPROWithReasonableScriptRestrictions` track the policies whose\n    [`script-src`][script-src] directives rely upon cryptographic nonces and/or\n    hashes rather than lists of trusted servers, and which also avoid relying\n    upon `'unsafe-inline'`.\n\nPolicies that sufficiently restrict all of the directives above (`object-src`,\n`base-uri`, and `script-src`) are tracked via `kCSPWithReasonableRestrictions`\nand `kCSPROWithReasonableRestrictions`. This is the baseline we'd like pages\ngenerally to meet, and a number we hope we can drive up over time.\n\nWe're also tracking a higher bar, which includes all the restrictions above,\nbut also avoids relying upon `'strict-dynamic'`, via\n`kCSPWithBetterThanReasonableRestrictions` and\n`kCSPROWithBetterThanReasonableRestrictions`.\n\n[object-src]: https://w3c.github.io/webappsec-csp/#directive-object-src\n[base-uri]: https://w3c.github.io/webappsec-csp/#directive-base-uri\n[script-src]: https://w3c.github.io/webappsec-csp/#directive-script-src\n[csp-is-dead]: https://research.google/pubs/pub45542/\n\n#### Embedded Enforcement\n\n`kIFrameCSPAttribute` records the overall usage of the `csp` attribute on\n`<iframe>` elements, which enables pages to enforce a policy on documents\nthey embed.\n\n## Trusted Types\n\n[Trusted Types][tt] gives page authors a means to protect their sites against\ncross-site scripting attacks. In order to understand real-world Trusted Types\nusage we obtain the following usage counts:\n\n* General use:`kTrustedTypesEnabled`, `kTrustedTypesEnabledEnforcing`, and\n  `kTrustedTypesEnabledReportOnly`. The first tells us (relative to all page\n  loads) how many pages have any form of Trusted Types enabled, while the other\n  two allow us to determine which percentage of pages run in enforcing or\n  report-only mode (or both).\n\n* Tracking specific features: `kTrustedTypesPolicyCreated` tracks\n  creation of all Trusted Types policies, `kTrustedTypesDefaultPolicyCreated`\n  notes whether a \"default\" policy has been created. `kTrustedTypesAllowDuplicates`\n  records whether an 'allow-duplicates' keyword has been used.\n\n* Error tracking: `kTrustedTypesAssignmentError` tracks whether Trusted Types\n  has blocked a string assignment.\n\n[tt]: https://github.com/w3c/webappsec-trusted-types/\n\n## Cross Origin Isolation policies\n\nCross Origin Isolation policies refer to a number of header based policies that\ndevelopers can send to enforce specific rules about how their content can be\nembedded, opened from, etc. It is also used to gate certain APIs that would be\notherwise too powerful to use in a post-Spectre world.\n\n[Cross-Origin-Resource-Policy][corp] restricts a resource to only be fetched by\n\"same-origin\" or \"same-site\" pages.\n\n* \"success\": The CORP check passes successfully.\n* \"same-origin violation\": \"same-origin\" is specified on a cross-origin\n  response.\n* \"same-origin violation with COEP involvement\": No CORP header\n  is specified but that is treated as \"same-origin\" because the initiator\n  context enables Cross-Origin Embedder Policy (see below), and the response\n  comes from cross-origin.\n* \"same-site violation\": \"same-site\" is specified on a cross-site response.\n\n[Cross-Origin-Opener-Policy][coop] is used to restrict the usage of window\nopeners. Pages can choose to restrict this relation to same-origin pages with\nsimilar COOP value, same-origin unless they are opening popups or put no\nrestriction by default.\n\n* Usage of COOP is tracked via:\n  - `kCrossOriginOpenerPolicySameOrigin`\n  - `kCrossOriginOpenerPolicySameOriginAllowPopups`\n  * `kCoopAndCoepIsolated`\nThey correspond respectively to the values: \"same-origin\",\n\"same-origin-allow-popups\" and \"same-origin\" used conjointly with COEP.\n\n* Usage of COOP in report-only mode is tracked symmetrically via:\n  - `kCrossOriginOpenerPolicySameOriginReportOnly`\n  - `kCrossOriginOpenerPolicySameOriginAllowPopupsReportOnly`\n  * `kCoopAndCoepIsolatedReportOnly`\n\n* We track how often same-origin documents are present in two pages with\n  different COOP values via `kSameOriginDocumentsWithDifferentCOOPStatus`. We\n  might restrict synchronous access between those in order to allow COOP\n  \"same-origin-allow-popups\" to enable crossOriginIsolated when used in\n  conjunction with COEP.\n\n[Cross-Origin-Embedder-Policy][coep] is used to restrict the embedding of\nsubresources to only those that have explicitly opted in via\n[Cross-Origin-Resource-Policy].\n\n* Usage of COEP is tracked via:\n  - `kCrossOriginEmbedderPolicyCredentialless`\n  - `kCrossOriginEmbedderPolicyRequireCorp`.\n\n* Usage of COEP in report-only mode is tracked symmetrically via:\n  - `kCrossOriginEmbedderPolicyCredentiallessReportOnly`\n  - `kCrossOriginEmbedderPolicyRequireCorpReportOnly`.\n\n* Usage of COEP in SharedWorker is tracked via:\n  - `kCoepNoneSharedWorker`,\n  - `kCoepRequireCorpSharedWorker`\n  - `kCoepCredentiallessSharedWorker`.\n\nNote that some APIs having precise timers or memory measurement are enabled only\nfor pages that set COOP to \"same-origin\" and COEP to \"require-corp\".\n\n* We track such pages via `kCoopAndCoepIsolated`.\n\n\n[corp]: https://developer.mozilla.org/en-US/docs/Web/HTTP/Cross-Origin_Resource_Policy_(CORP)\n[coep]: https://wicg.github.io/cross-origin-embedder-policy/\n[coop]: https://gist.github.com/annevk/6f2dd8c79c77123f39797f6bdac43f3e\n\n## Sanitizer API\n\n[The Sanitizer API][sanitizer] provides a browser-maintained \"ever-green\", safe,\nand easy-to-use library for user input sanitization as part of the general web\nplatform.\n\n* Sanitizer creation: `kSanitizerAPICreated` and\n  `kSanitizerAPIDefaultConfiguration` tell us how many Sanitizers are\n  created and how many Sanitizers are created without custom configurations.\n* Sanitizer method: `kSanitizerAPIToFragment`, `kSanitizerAPISanitizeFor`,\n  and `kSanitizerAPIElementSetSanitized` measure which API entry point has been\n  called.\n* `kSanitizerAPIActionTaken` shows how many times a sanitize action has been\n  performed while calling the Sanitizer APIs. (That is, on how many sanitizer\n  calls did the sanitizer remove nodes from the input sets.)\n* Input type: `kSanitizerAPIFromString`, `kSanitizerAPIFromDocument` and\n  `kSanitizerAPIFromFragment` tell us what kind of input people are using.\n\n[sanitizer]: https://wicg.github.io/sanitizer-api/\n\n## Private Network Access\n\n[Private Network Access][pna] helps to prevent the user agent from\ninadvertently enabling attacks on devices running on a user's local intranet,\nor services running on the user's machine directly.\n\n* Use of PNA in workers tracked via:\n  - `kPrivateNetworkAccessFetchesWorkerScript`\n  - `kPrivateNetworkAccessWithWorker`\n\n* `kPrivateNetworkAccessNullIpAddress` is an experimental use counter for\n  accesses to the 0.0.0.0 IP address (and the corresponding `[::]` IPv6 address).\n  These can be used to access localhost on MacOS and Linux and bypass Private\n  Network Access checks. We intent to block all such requests. See\n  https://crbug.com/1300021 and https://github.com/whatwg/fetch/issues/1117.\n\n[pna]: https://wicg.github.io/private-network-access/\n"
  },
  {
    "path": "security/vrp-faq",
    "title": "Chrome Vulnerability Reward Program (VRP) News and FAQ",
    "content": "# Chrome Vulnerability Reward Program (VRP) News and FAQ\n\n[TOC]\n\n## News and Updates\n\nPlease report all Chromium security bugs in the new tracker using [this\nform](https://issues.chromium.org/issues/new?noWizard=true&component=1363614&template=1922342)\nor https://bughunters.google.com/report/vrp -> Chrome VRP.\n\nPlease check here for any news and updates about the Chrome VRP.\n\n* 30 December 2024: Announcing the top 20 Chrome VRP reporters of 2024:\n  https://crbug.com/386306231 -- congratulations to all who made the list and\n  thank you to all Chrome VRP reporters for your contributions this year and\n  helping make Chrome Browser and Chromium more secure for all users!\n\n* 13 November 2024: Updates to the [V8 Sandbox\n  Bypass](#v8-sandbox-bypass-rewards) scope and reward amounts.[Past\n  submissions of V8 sandbox bypasses](https://issues.chromium.org/hotlists/4802478)\n  have been opened for disclosure; please ensure your bypass has not already\n  been reported and use these reports as a resource to learn about the V8\n  sandbox and known bypass techniques.\n\n* 28 August 2024: Major updates to Chrome VRP rewards structure and amounts to\n  incentize deeper research; please see our [blog\n  post](https://bughunters.google.com/blog/5302044291629056/chrome-vrp-reward-updates-to-incentivize-deeper-research)\n  announcing these changes or review our Chrome VRP [policies and rewards\n  page](https://g.co/chrome/vrp) for full details.\n\n* 20 June 2024: All Google VRPs, including **Chrome VRP, have a new payments\n  processing option through Bugcrowd**. To use the Bugcrowd option to receive\n  your Chrome VRP reward payments, you must:\n    * Set up your profile in bughunters.google.com and associate it with the\n      email address you use to report issues in chromium.issues.com.\n    * Be registered or [register with](https://bugcrowd.com/user/sign_up)\n      Bugcrowd.\n    * In your [Bughunters profile](https://bughunters.google.com/profile),\n      select `Bugcrowd` under `Payment Options` and enter the email address for\n      your Bugcrowd account.\n    * Hit `Save` on your profile and you're ready to roll!\n\n* 4 April 2024: We have launched the [V8 Sandbox Bypass\n  Rewards](#v8-sandbox-bypass-rewards).\n\n* 4 February 2024: The Chromium issue tracker migration is now complete. Please\n  submit all issues using the [new issue tracker](https://issues.chromium.org)\n  and use [this\n  form](https://issues.chromium.org/issues/new?noWizard=true&component=1363614&template=1922342)\n  for directly reporting security issue to the security team.\n\n* 25 January 2024: We have updated our VRP policy on duplicates and collisions\n  for actionable versus non-actionable reports. Please see the [Chrome VRP\n  qualifying vulnerabilities](https://g.co/chrome/vrp/#qualifying-vulnerabilities)\n  for details.\n\n* December 2023: We announced the [Chrome VRP Top Researchers of\n  2023](https://crbug.com/1509898). Congratulations to all who made the list!\n  Thank you to all of our researchers for your contributions in 2023 and helping\n  us make Chrome Browser more secure for all users.\n\n## About the Program\n\nThe Chrome Vulnerability Rewards Program (VRP) is the \"security bug bounty\" for\nGoogle Chrome Browser. Please visit the [Chrome VRP Rewards and\nPolicies](https://g.co/chrome/vrp) page for full details.\n\n### Interesting Security Bug Write-Ups\n\nHere are some interesting write-ups of past Chrome security bugs:\n\n* [A Bug's Life: CVE-2021-21225](https://tiszka.com/blog/CVE_2021_21225.html)\n* [Exploiting CVE-2021-21225 and disabling\n  W^X](https://tiszka.com/blog/CVE_2021_21225_exploit.html)\n* [ZIP embedding attack on Google Chrome\n  extension](https://readme.synack.com/exploits-explained-zip-embedding-attack-on-google-chrome-extensions),\n  by Malcolm Stagg, reporter of CVE-2024-0333\n\nWe only post links to articles with the author's consent. Please let us know if\nyou would like your work to be shared here.\n\n## Best Practices for Security Bug Reporting\n\nTo help make the process of security bug triage as efficient and smooth as\npossible, please consider the following best practices for Chromium security bug\nreports:\n\n* Use the [security bug reporting\n  form](https://issues.chromium.org/issues/new?noWizard=true&component=1363614&template=1922342)\n  * This will allow the bug report to be included in the security bug triage\n    queue immediately.\n* Include the version number and OS used to reproduce the bug. For an [extra\n  bonus reward](https://g.co/chrome/vrp/#bisect-bonus), please consider\n  including a bisection.\n\n### Proof of Concept (PoC)\n\n* Upload PoC files directly to the report itself as separate attachments.\n* Please name the PoC file *poc.html* or *index.html* in the case of multiple\n  files.\n  * This allows [Security Shepherds](shepherd.md) to run the PoC in ClusterFuzz\n  immediately for triage.\n* Ensure the PoC is as minimized as possible.\n  * This is useful in root cause analysis and also improves the changes of\n    reproduction by ClusterFuzz or the Security Shepherd triaging your report.\n* Please format your PoCs so they do not require running python as root.\n  * Your PoC should be constructed to reproduce locally when at all possible.\n* Do *not* provide links (public or unlisted) to / as PoCs.\n  * The PoC should ALWAYS be a file directly attached to the report, even if it\n    cannot be reproduced in ClusterFuzz.\n\n### Steps to Reproduce\n\n* Please include clear, concise, numbered steps to reproduce the bug you are\n  reporting.\n* Provide the full command line and any build flags to reproduce.\n\n### Report Attachments\n\n* Do *not* upload attachments as a single compressed file, such as .zip,\n  .gzip, or .tar files.\n  * Please upload all necessary attachments (PoCs, patch, fix patches, video\n    demonstrations, exploits) as individual files on the bug report.\n* ASAN stack traces should be symbolized, and include all `additional\n  information` fields, and the sanitizer should be built using Chrome's standard\n  mitigation and hardening flags.\n* Patch.diff files that are not suggested fixes should explain - as part of the\n  steps to repro - the reason for the patch, such as to simulate a compromised\n  renderer or allow for stable reproduction, such as running the PoC in a loop\n  or winning a race condition.\n* Do not provide links (public or unlisted) to demonstration videos.\n  * If possible, please upload your demonstration video directly to the report.\n  * Please keep the video as concise as possible.\n    * We do not need to see you loading Chrome or displaying the version page\n      multiple times in a video.\n  * DO feel free to include music, sound effects, or animal pics in your\n    videos. :)\n\n### Bug Reporting\n\n* Please ensure your report is concise and clearly articulates the bug,\n  explaining and demonstrating (through PoC, exploit, and/or video), such as:\n  * How the issue is reachable and exploitable by web content or a compromised\n    renderer.\n  * The security consequences / user harm resulting from exploitation of the\n    bug.\n* Avoid reporting theoretical bugs; reports that simply state a potential bug\n  from static analysis without demonstration of the security issue.\n  * These reports will be considered as unactionable and will be triaged at a\n    lower priority, and may be closed as WontFix without demonstrable evidence\n    of a security bug.\n  * These reports are also considered below baseline quality and qualify for\n    reduced or no VRP reward.\n* At a minimum, please include the version number and release channel\n  (Stable/Beta/Dev) on which you discovered and reproduced the issue you are\n  reporting.\n  * For an opportunity at increasing your potential reward amount to receive a\n    [Bisect Bonus](https://g.co/chrome/vrp/#bisect-bonus),\n    please consider performing a full bisection, detailing the commit that\n    introduced the issue and / or all the active release channels impacted by\n    the bug.\n\n### Suggested Fix / Patch Rewards\n\n* If suggesting a patch to fix the issue you are reporting, please upload it to\n  the report as its own attachment.\n* We reward [bonuses for your patches](https://g.co/chrome/vrp/#patch-bonus)\n  that end up being used as the fix.\n  * Bonuses are $500 - $2000 depending on how substantial the patch is.\n  * To maximize your patch rewards, please commit the patch directly to Chromium\n    and include the Gerrit (Chromium code review tool) link in the report or\n    report comment.\n  * Remember to include the Chromium bug tracker issue number in the CL, if you\n    land the patch after filing the bug report, so that it can be linked to the\n    report.\n\n## Frequently Asked Questions (FAQ)\n\n### Scope / Reward Eligibility\n\n#### How do I know if my bug report is possibly eligible for a VRP reward?\n\n* All validated, [qualifying vulnerability\n  reports](https://g.co/chrome/vrp/#qualifying-vulnerabilities) are\n  automatically considered for a reward once they are fixed. At which point you\n  will see the reward-topanel hotlist signifier added to your bug report. This\n  indicates that it will be reviewed at a Chrome VRP panel meeting for a\n  reward decision.\n* The bug will be updated again once the panel has made a reward decision.\n\n#### I want to report a bug through a broker / not directly to you.\n\n* We believe it is against the spirit of the program to privately disclose\n  security vulnerabilities to third parties for purposes other than fixing the\n  bug. Consequently, such reports will not qualify for a reward.\n\n#### What if someone else reported the same bug?\n\n* Only the first actionable report of a security bug that we were previously\n  unaware of is eligible for a potential VRP reward. In the event of a duplicate\n  submission, the earliest actionable report in the tracker is considered the\n  first report.\n* If the issue is discovered by one of our internal fuzzers within 48 hours of\n  your report, it is considered a known issue and is not eligible for a reward.\n\n#### What does actionable report mean?\n\n* A security bug report is only considered actionable once it contains\n  appropriate information that allows for validation and triage, such as a\n  minimized test case / PoC, steps to reproduce, a symbolized stack trace,\n  and/or other demonstrable evidence of the security bug being reported.\n* A report lacking this actionable information and does not allow us to\n  reproduce the issue to validate and investigate the bug is not considered to\n  be a complete, actionable submission.\n\n#### Are bugs in unlaunched features / behind command line flags VRP-eligible?\n\n* Yes, we are interested in bugs in any code that has shipped to even a fraction\n  of our users.\n* *The only exception at this time are security bugs in V8 behind\n  --experimental*; this flag is for early and experimental V8 development\n  efforts and this configuration should not be used in production. *Security\n  bugs specific to this configuration are not eligible for Chrome VRP rewards.*\n* Please ensure you include what command line flags are required to trigger the\n  bug in your report and remove any unnecessary flags.\n* The report will be triaged with the appropriate security\n  severity and as Security_Impact-None. This categorization of\n  Security_Impact-None does not impact the potential reward amount.\n* Please note that these security bugs do not have the same SLO to fix as bugs\n  impacting Stable, Beta, or Dev. The only expectation is that the bug is fixed\n  before the feature is exposed to users, such as part of an origin trial or\n  field experiment.\n\n#### What about bugs in channels other than Stable?\n\n* We are interested in security bugs in Stable, Beta, and Dev channels because\n  it's best for everyone to find and fix security bugs before they impact Stable\n  channel users.\n* We do, however, discourage hunting in Canary or Trunk builds. These do not go\n  through release testing and exhibit short-lived regression that are typically\n  identified and fixed quickly. Reports of bugs in new code in trunk may collide\n  with on-going work of the engineers as part of \"trunk-churn\".\n* Reports for bugs in newly landed code on Trunk / Head landed within 48 hours\n  of the report are not eligible for VRP rewards.\n  * VRP eligibility for reports in Head will be based on assessment of ongoing\n    development efforts and discussion with the engineering team to determine if\n    the VRP report was used in identifying and fixing that issue.\n  * Bugs for issues resulting from newly landed commits on Head that are seven\n    (7) or fewer days old are likely to not be eligible for a VRP reward.\n\n#### Are bugs in third-party components in scope and eligible for VRP rewards?\n\n* Yes, we are interested in reports for bugs in our third-party components, such\n  as libxml, sqlite, and image and compression libraries. The security impact\n  must manifest in and result in security consequences in shipped configurations\n  of Chrome.\n* Reports should demonstrate that the vulnerable code is reachable in Chrome.\n* We are interested in rewarding any information that enables us to better\n  protect our users. In the event of bugs in an external component (such as an\n  upstream dependency), we may also ask that you file a bug directly with the\n  vendor or maintainer for that component, so that the product or project owners\n  can set about fixing the bug immediately.\n* This also ensures that you have direct access to the status of the report, can\n  directly communicate with that vendor or project owner, and receive credit or\n  acknowledgement (if they have such a mechanism to do so).\n\n#### Can I submit my report(s) and provide a working exploit later?\nIs there a time limit for submitting an exploit?\n\n* Most definitely! We realize that developing an exploit is a lengthy process\n  and we very much encourage this approach, as it allows us to work on fixing\n  the bugs as soon as possible. It also reduces the chance that someone else\n  reports the same issue while you are working on the exploit.\n* Although we don't have a set time limit, we would expect that the exploit\n  would follow within six  weeks of the initial report. If more time is needed,\n  we are happy to discuss extended timelines.\n  * Please reach out to security-vrp@chromium.org to discuss exploit extensions.\n\n#### Will you reward for types of bugs that are not specifically listed?\n\n* Yes. We are interested in rewarding any information that enables us to better\n  protect Chrome users. Reward amounts are based on the potential security harm\n  of the bug being reported and the quality of the report.\n\n### Report Status / Bug Lifecycle\n\n#### What happens after I report a security bug to you?\n\n* Please see [Life of a Security Issue](life-of-a-security-issue.md).\n\n#### Will I receive a CVE for my bug?\n\n* If the issue you report was an exploitable security bug impacting Stable or\n  older version of Chrome, and it was the first actionable report of that\n  issue, your bug will be issued a CVE at the time the fix ships in a Stable\n  channel update of Chrome.\n* The CVE number will be updated directly on the report itself and listed in\n  the Chrome Browser release notes for that Stable channel update.\n\n### Disclosure / Report Visibility\n\n#### What if I disclose the bug publicly before you have fixed or disclosed it?\n\n* Please read [our stance on vulnerability\n  disclosure](https://about.google/appsecurity/#:~:text=Google's%20vulnerability%20disclosure%20policy,a%2090%2Dday%20disclosure%20deadline)\n* Essentially, our pledge to you is to respond promptly and fix bugs in a\n  sensible timeframe, and in exchange, we ask for a reasonable notice of\n  potential disclosure.\n* Disclosures that go against this principle will usually not qualify for a VRP\n  reward, but we will evaluate them on a case-by-case basis.\n\n#### When will the bug I reported be publicly disclosed?\n\n* Most security bugs are automatically opened for public access 14 weeks after\n  the bug is closed as Fixed, meaning the fix commit is landed on Chromium main.\n* Our automation removes the view restrictions, opening the report for public\n  visibility at that time.\n\n#### Can you keep my identity confidential from the public?\n\n* Your email address is obscured by default (by elision) in the Chromium bug\n  tracker and is not revealed once the bug is publicly disclosed.\n* If you do not want to receive public acknowledgement for your report, please\n  let us know in the report or before the issue is listed in the release notes.\n  * We will credit the finding to \"anonymous\" researcher or we are happy to\n    credit it to whatever pseudonym or tag you provide to us.\n* If you receive a VRP reward for your report and accept it, Google or Bugcrowd\n  (depending on who you select to process your VRP reward) will need to\n  privately collect some identifying information to process your reward\n  payment.\n\n#### Can you keep my report under Security Embargo?\n\n* Security Embargo prevents issues from being disclosed beyond the security team\n  and engineers working to resolve the bug. Once the issue is fixed, the\n  (Security Notify) community of embedders and developers of other\n  Chromium-based products are reliant on the access to bug reports. Because\n  Security Embargo restricts this access indefinitely, it is used only on a\n  specific case-by-case basis, such as when the bug should never be publicly\n  disclosed.\n\n### Rewards / Reward Decisions\n\n#### My bug was rewarded under older amounts, will you pay the difference?\n\n* We often increase reward amounts and introduce new bonus opportunities. We\n  reward bug reports based only on the rules that were in effect at the time of\n  bug submission and VRP Panel assessment.\n\n#### The exploit market pays more for bugs!\n\n* Yes, we are aware that the exploit brokers, underground markets, three cats in\n  a trenchcoat on your local street corner may pay more for bugs. We understand\n  that many pockets of society may pay you more money to purchase information\n  about the vulnerabilities you may find or exploits you develop. These people\n  buy vulnerabilities and exploits for offensive purposes to target people\n  using Chrome across the internet. We believe the reward you get under those\n  circumstances comes with strings attached -- including buying your silence\n  and accepting that any bug you sell may be used to target other people without\n  their knowledge, such as activists, dissidents, and vulnerable populations.\n* We understand our reward amounts may be less than these alternatives, but we\n  also believe we provide additional benefits when reporting directly to us,\n  such as public acknowledgement of your findings and skills, the opportunity to\n  engage directly with the security team and engineers working to resolve your\n  bug, a quick resolution and seeing a fix you contributed to go out into the\n  world, the opportunity to publicly discuss/blog/present/share your amazing\n  work, and *the knowledge that you are helping keep Chrome secure for billions\n  of people across the world*!\n* Also, many of our researchers receive gifts of swag and are invited to events.\n* You'll additionally have the peace of mind to know your bug findings were\n  never used by shady people for nefarious purposes.\n\n#### I provided a new POC or new information-- will you increase the VRP reward?\n\n* Not generally, unless there are expectional circumstances. Criteria for Chrome\n  VRP reward decisions is inclusive of information included in the initial\n  report to the time the bug is fully resolved. Information that is provided\n  after the bug is resolved and assessed by the VRP does not benefit the\n  effective and efficient investigation and fix of the bug and should not be\n  expected to impact VRP reward assessment or reassessment.\n* We want to incentivize the earliest reporting of security issues, however,\n  security bug reports should only be submitted once they are fully inclusive\n  of all necessary aspects of a security bug report to allow for validation\n  triage, and resolution. If you can demonstrate an issue is not mitigated or\n  can be reliably reproduced and triggered remotely, that should be\n  demonstrated as part of the initial report or within the period of\n  investigation and fix.\n* We do, however, welcome reports of a functional exploit after the report is\n  submitted and even resolved for a potential higher VRP reward.\n\n#### When will I receive my reward?\n\n* Once the bug has been assessed by the VRP Panel, the bug report is updated\n  with a reward decision and information.\n* There are two options for payment of a VRP Reward -- direct through Google\n  or through Bugcrowd.\n  * Through Google:\n    * If this is your first VRP reward for a Google program, a member of the\n      finance p2p-vrp team will reach out to enroll you in the Google payment\n      system.\n    * VRP payments are handled by the p2p-vrp finance team. Once you have been\n       enrolled, you will receive you payment within 1-2 weeks of a reward\n      decision.\n    * Please reach out to p2p-vrp@google.com with questions about the payment\n      enrollment process or assistance with any payments issues.\n  * Through Bugcrowd:\n    * You must already have or create a new Google\n      [Bughunters](https://bughunters.google.com/profile) profile.\n      (Please note, you can set your Bughunters profile to be private if you\n      prefer to not have a public profile).\n    * Associate your Bughunters profile with the email address you use for\n      reporting Chrome security issues.\n    * Have a Bugcrowd account or [register](https://bugcrowd.com/user/sign_up)\n      with Bugcrowd.\n    * In your [Bughunters profile](https://bughunters.google.com/profile)\n      change your `Payment Options` from `Legacy` to `Bugcrowd` and enter the\n      email address for your Bugcrowd account (and hit `Save`)!\n    * Future reward payments will be sent to Bugcrowd for processing and you\n      will receive an email directly from Bugcrowd to accept those rewards.\n*  If at any point you want to change the method by which you receive VRP reward\n   payments, this can be done through your Google Bughunters profile > `Payments\n   Options`:\n    * Select `Legacy` to receive your payments through Google p2p payments\n       processing.\n    * Select `Bugcrowd` to select payments through Bugcrowd. Remember you must\n      register with Bugcrowd first and enter your Bugcrowd account email in your\n      Bughunters profile.\n\n#### I don't agree with the reward amount. Can I get the reward reassessed?\n\n* We always try our best to be fair and consistent, but sometimes we may get it\n  wrong or miss something in our assessment. If you feel that is the case,\n  please reach out to us at security-vrp@chromium.org detailing why you believe\n  we should reassess your report.\n\n### Other\n\nI have a security-related question that is not listed here.\n\n* This is a statement not a question, but we're happy to help. Take a look at\n  the [Chrome Security FAQ](faq.md) to see if your question is answered there.\n* Also, if you have not already, please check the [Chrome VRP Rewards and\n  Policies page](https://g.co/chrome/vrp).\n* If you still need assistance, please reach out to security-vrp@chromium.org.\n\n\n"
  },
  {
    "path": "security/updates",
    "title": "Chrome Security Update FAQ",
    "content": "# Chrome Security Update FAQ\n\n_Bookmark this page as https://g.co/chrome/security-update-faq_\n\n## TL:DR\n\nAlmost all Chrome updates contain security fixes, and should be prioritized\nequally. The most secure option is to automatically update Chrome as soon as any\nupdate is available, independent of the specific details of any security fixes\nincluded in the update.\n\n## Chrome Updates\n\nAlmost all updates have security fixes. Every week, Chrome releases a new\nversion that materially improves security. The Chrome Security team believes the\nbest option for security is to apply all updates to Chrome as soon as they are\navailable. We recommend against and do not support using the security release\nnotes as a method to prioritize updates. Instead, all updates should be\nprioritized equally, and considered to be important security fixes.\n\nPatching, remediating, and protecting against security vulnerabilities is a core\npart of the Chrome engineering process. The Chrome Security team works to\nprotect Chrome users and make it safe to click on links by building features and\narchitecting systems to defend against exploitation, network tampering, malware\nand phishing. Every release of Chrome includes not just security patches, but\nnew security features and other defensive improvements developed by the Chrome\nSecurity team.\n\nChrome releases [stable milestones][release-cycle] every four weeks (MXXX), with “refresh”\nreleases in-between milestones. Milestones are refreshed with updates that can\ncontain security fixes and functional bug fixes for high-impact bugs. There are\nplanned security refreshes weekly for every milestone, with all of the\nsecurity fixes since the previous release. Chrome may have unscheduled updates\nbetween milestones to fix critical security issues, address breaking functional\nbugs, or patch known in-the-wild exploits. Updates and rapid response are part\nof the Chrome security development lifecycle, and Chrome invests in its ability\nto quickly refresh a release when needed. While we do try to reduce the number\nof unplanned updates, these releases are an important part of how Chrome works\nto secure its users, and are an example of Chrome’s vulnerability reporting and\nsecurity engineering processes working as intended.\n\n## FAQ\n\n**Are security fixes included in every new milestone release?** Yes.\n\n**How do I avoid compatibility issues with web platform changes if I automatically\nupdate Chrome?** Breaking changes to the web platform are communicated in the\n[enterprise release notes][ent-rel-notes]. For changes that may cause\ncompatibility issues, Chrome includes a temporary enterprise policy to maintain\nthe old behavior for several milestones beyond the change, which gives\nenterprises time to address compatibility issues. To proactively identify these\nissues, we recommend enterprises test their critical workflows, or have a cohort\nof users running the Beta version of Chrome.\n\nYou can read more details and best practices on the [Chrome Update Management\nStrategies][update-strategy] technical doc.  Enterprises can also reach out to their [support\nrepresentatives][ent-support] for help, or [file an issue][issue-tracker] if they identify a\nbug in a new version of Chrome.\n\n**How do I prioritize updates that patch vulnerabilities known to be under\nexploitation in the wild (zero-days)?** Don't---this strategy puts you at risk.\nSecurity fixes are important regardless of whether or not there is known\nexploitation. There are two main reasons for this:\n\nFirst, we know that we do not have full visibility into every exploit in the\nwild. Chrome has an industry-leading multi-layered approach to finding possible\nexploits, including code reviews, testing and fuzzing, working with researchers\nand external organizations, and a vulnerability rewards program. However, it's\nimpossible to guarantee that a fixed vulnerability was never exploited in the\nwild, so you should always roll out the fix.\n\nSecond, vulnerabilities may be exploited after our fix is rolled out. Exploiting\npatched vulnerabilities in unpatched installations is known as N-day\nexploitation, and it gets easier and cheaper for attackers after we’ve made a\nfix available in a new version. If you don't update, you're putting your\norganization at risk of N-day exploits, which are much more accessible to bad\nactors than 0-day exploits.\n\nRemember, a bug is only regarded as a security bug if we consider it\nexploitable, so we recommend Chrome users treat all security fixes with equal\npriority and always patch their installation. The [best defense][cisa-patches] against attackers\nexploiting patched vulnerabilities in Chrome is to automatically update Chrome\nwhenever an update is available.\n\n**I've noticed that Chrome has a lot of security updates. Does that mean something\nis wrong?** No. Fast and frequent updates are one of Chrome's security strengths.\nWe've spent years building advanced release infrastructure in order to increase\nthe number of security updates you receive, and the frequency that we can send\nthem out. This keeps you a step ahead of bad actors.\n\n**Updating every week (or more) is a lot of work. How can my IT department\nkeep up?** Chrome is designed to be easy for IT admins to manage and update. If\nyou're finding that it's a lot of work to keep up with Chrome's update schedule,\nthere's a good chance that there's an easier way to accomplish your goals.\nEnterprises that disable auto-updates and instead roll out manual updates are\nboth decreasing their security and greatly increasing the amount of work they\nhave to do.\n\nRead through different update strategies and best practices on the [Chrome Update\nManagement Strategies][update-strategy] technical doc.\n\n**Is it possible that a Chrome update could address a functional bug, and no\nsecurity bugs?** Yes, there are releases containing only functional fixes. When\nthis occurs, there are no security fix notes for the corresponding desktop stable\nrelease.\n\n**How do I know what security fixes are included in a specific Android Chrome\nrelease?** Android releases contain the same security fixes as their\ncorresponding desktop release (first three segments of the version number are\nthe same), unless otherwise noted.\n\n**How do I know what security fixes are included in a specific Chrome on iOS\nrelease?** Due to Apple platform limitations, the browsing engine for Chrome on\niOS is Webkit, which is also used by Safari. Apple maintains Webkit and tracks\nsecurity issues as part of their iOS release process. To receive WebKit security\nfixes, users should always update iOS to the latest version. Whenever possible,\nChrome will ship patches on iOS to mitigate issues in Webkit. In the event of a\nChrome on iOS specific security flaw, Chrome will note the fix in the release\nnotes for the corresponding desktop release.\n\n**My product includes one of Chrome's components (such as V8 or WebRTC). How\ncan I absorb security fixes?** As with Chrome itself, you should assume that\nall security fixes are important and arrange to include them in your product.\nWe recommend tracking Chrome's \"stable\" branch - more recent branches may have\na higher density of ephemeral security bugs which we will fix before shipping\nto stable. For some of Chrome's components (such as WebRTC) security bugs are\nrelatively infrequent and it may be possible to manually absorb new versions\nas necessary by monitoring Chrome release notes. For others (such as V8)\nsecurity bugs are so frequent that the better strategy is to absorb a new\nversion of the component every week. For these components, Chrome does not\nsupport making update decisions based upon the exact security content of the\nrelease - instead, just take a new version each week.\n\n[release-cycle]: https://chromium.googlesource.com/chromium/src/+/main/docs/process/release_cycle.md\n[ent-rel-notes]: https://support.google.com/chrome/a/answer/7679408\n[update-strategy]: https://support.google.com/chrome/a/answer/9982578\n[ent-support]: https://chromeenterprise.google/browser/support/\n[issue-tracker]: https://issues.chromium.org\n[cisa-patches]: https://www.cisa.gov/tips/st04-006\n[chrome-versions]: https://www.chromium.org/developers/version-numbers/\n[rel-dash]: https://chromiumdash.appspot.com/releases?platform=Windows\n"
  },
  {
    "path": "security/tls-sha1-server-signatures",
    "title": "TLS SHA-1 Server Signatures",
    "content": "# TLS SHA-1 Server Signatures\n\n* Specification: [RFC 9155](https://www.rfc-editor.org/rfc/rfc9155.html)\n* Chrome Status: [Deprecate TLS SHA-1 server signatures](https://chromestatus.com/feature/4832850040324096)\n\n[SHA-1](https://www.nist.gov/news-events/news/2022/12/nist-retires-sha-1-cryptographic-algorithm) is an insecure hash algorithm with increasingly significant attacks discovered ([2015](https://en.wikipedia.org/wiki/SHA-1#The_SHAppening), [2017](https://shattered.io/), [2020](https://sha-mbles.github.io/)). To ensure these attacks cannot be used to impersonate a web server, browsers have steadily removed SHA-1 dependencies from HTTPS. In 2017, Chrome removed support for SHA-1 in [certificates](https://www.chromium.org/Home/chromium-security/education/tls/sha-1). In 2020, Chrome disabled [TLS 1.0 and 1.1](https://www.chromestatus.com/feature/5759116003770368), which use SHA-1 throughout.\n\nChrome 117 is [removing](https://chromestatus.com/feature/4832850040324096) support for SHA-1 from the TLS 1.2 [server signature](https://www.rfc-editor.org/rfc/rfc9155.html).\n\n\n## Background\n\nTLS authenticates servers in two parts. First, the certificate, signed by the CA, tells the client what is `example.com`'s public key. Second, the TLS server software uses the corresponding private key in the handshake, to bind it to the connection. In most cipher suites, this involves the server making a signature. This change is to no longer allow SHA-1 in this signature. That is, Chrome will no longer [offer SHA-1 in the signature\\_algorithms field](https://www.rfc-editor.org/rfc/rfc9155.html) and, correspondingly will no longer [accept it in ServerKeyExchange signatures](https://www.rfc-editor.org/rfc/rfc9155.html#section-4).\n\nThis use of SHA-1 is _not_ the same as:\n\n*   SHA-1 in the server's certificate\n*   \"SHA\" in cipher suites, which refers to HMAC-SHA-1 in legacy CBC cipher suites\n\n\n## Is my website affected?\n\nYou can test your website by toggling the \"Allow SHA-1 server signatures in TLS\" flag. Go to `chrome://flags/#use-sha1-server-handshakes`. If setting it to \"Enabled\" causes the site to work, but setting it to \"Disabled\" causes it to break, the website is affected. This flag is temporary and will be removed in the future. It may be used for now to help diagnose issues, but, long-term, the server should be fixed.\n\nDepending on the exact cause, this issue can appear differently, such as an `ERR_SSL_PROTOCOL_ERROR` or `ERR_CONNECTION_RESET`, though this is not the only possible cause of those errors.\n\n\n## Troubleshooting\n\nAll correctly-implemented TLS 1.2 servers already support SHA-2 and pick a common algorithm based on server support and what the client offers. While it is possible to misconfigure servers such that SHA-1 is the only algorithm, this is rare. This is most commonly a bug in the server software.\n\nHere are known issues and how to fix them. If your server software is not listed, contact your software vendor for a fix.\n\n\n### OpenSSL\n\nOpenSSL versions from 1.0.1 to 1.0.1i (August 2014), as 1.0.2 to 1.0.2l (May 2017) did not correctly track signature algorithm state and, as a result, some server applications would lose track of the peer's preferences and only sign SHA-1 when [SNI](https://en.wikipedia.org/wiki/Server_Name_Indication) is offered.\n\nImpacted servers will usually fail with `ERR_SSL_PROTOCOL_ERROR`. Impacted servers will also respond to connections with and without SNI distinctively. Replace `EXAMPLE.COM` with the name of the server:\n\n\n```\n$ openssl s_client -connect EXAMPLE.COM:443 -servername EXAMPLE.COM -sigalgs rsa_pkcs1_sha256 -tls1_2 -quiet\n...\n40E7F950417F0000:error:0A000172:SSL routines:tls12_check_peer_sigalg:wrong signature type:../ssl/t1_lib.c:1594:\n\n$ openssl s_client -connect EXAMPLE.COM:443 -noservername -sigalgs rsa_pkcs1_sha256 -tls1_2 -quiet\n(connection succeeds)\n```\n\n\nThis was [fixed for the 1.0.1](https://github.com/openssl/openssl/commit/4e05aedbcab7f7f83a887e952ebdcc5d4f2291e4) series in 1.0.1j, released October 2014. There was a [partial fix](https://github.com/openssl/openssl/commit/1ce95f19601bbc6bfd24092c76c8f8105124e857) for 1.0.2, but it was incomplete, so some applications continued to be affected until a [complete fix](https://github.com/openssl/openssl/pull/4577) in 1.0.2m, released November 2017. Impacted servers should update to a sufficiently new version. The 1.0.1 and 1.0.2 series have been end-of-life since December 2016 and December 2019, respectively, so updating to a supported version is recommended.\n\nAdditionally, there have been many [OpenSSL security advisories](https://www.openssl.org/news/vulnerabilities.html) since these bugs were fixed. While unrelated to this Chrome change, we would also recommend reviewing the missed advisories for further actions. In particular, very old OpenSSL versions may be vulnerable to [Heartbleed](https://heartbleed.com/), in which case the server private key should be assumed compromised.\n\n\n### IIS with SHA-1 server certificates\n\nThe `signature_algorithms` field is primarily used for negotiating the server signature, but it is also used to guide server certificate selection. Some servers, notably IIS, apply this strictly and will reject the connection if any server certificate is inconsistent with the client's `signature_algorithms`.\n\nChrome already removed support for SHA-1 server certificates in 2017. However, some servers are configured to send an unnecessary self-signed SHA-1 root certificate. This certificate is normally ignored by the client and thus harmless. However, older versions of IIS will still apply strict checks to it and then reject the connection.\n\nIf the impacted server uses IIS and sends a server certificate chain with SHA-1 at the root (or elsewhere), this is a likely cause.\n\nNewer versions of Windows Schannel no longer check the extraneous certificate. Impacted servers should apply Windows updates. Alternatively, impacted servers can reconfigure their servers to [not send the unnecessary certificate](https://www.rfc-editor.org/rfc/rfc5246.html#section-7.4.2) and reduce bandwidth.\n\n\n### IIS with SHA-1 client certificates\n\nTLS client certificates (sometimes referred to with the non-standard term “mTLS”) authenticate analogously to server certificates: the certificate tells the server the public key, then the client makes a client signature with the client private key. As in the analogous direction, the server sends a list of signature algorithms, which the client picks from.\n\nWhile this change does not impact SHA-1 in either client certificates or client private key, older versions of IIS would apply the client's preferences (which control the server certificate and signature) to the client ones. Thus, deployments that depend on SHA-1 client certificates or signatures would break when SHA-1 _server_ signatures are no longer allowed. Impacted servers will usually fail _after_ the client sends the client certificate.\n\nNewer versions of Windows Schannel have fixed this issue. Impacted servers should apply Windows updates. Additionally, while unrelated to this deprecation, enterprises relying on SHA-1 client certificates or signatures are recommended to migrate to SHA-2, to ensure attackers cannot use SHA-1 weaknesses to impersonate clients.\n\n\n## Enterprise policy\n\nEnterprise administrators who need more time can set the [InsecureHashesInTLSHandshakesEnabled](https://chromeenterprise.google/policies/#InsecureHashesInTLSHandshakesEnabled) enterprise policy as a temporary workaround. However, this is a temporary policy and will be removed in Chrome 123. Additionally, as this allows an insecure hash function in a critical part of the TLS handshake, enabling this policy does increase the risk of attackers impersonating servers within an enterprise deployment.\n"
  },
  {
    "path": "security/side-channel-threat-model",
    "title": "Post-Spectre Threat Model Re-Think",
    "content": "# Post-Spectre Threat Model Re-Think\n\nContributors: awhalley, creis, dcheng, jschuh, jyasskin, lukasza, mkwst, nasko,\npalmer, tsepez. Patches and corrections welcome!\n\nLast Updated: 27 April 2021\n\n[TOC]\n\n## Introduction\n\nIn light of [Spectre/Meltdown](https://spectreattack.com/), we needed to\nre-think our threat model and defenses for Chrome renderer processes. Spectre is\na new class of hardware side-channel attack that affects (among many other\ntargets) web browsers. This document describes the impact of these side-channel\nattacks and our approach to mitigating them.\n\n> The upshot of the latest developments is that the folks working on this from\n> the V8 side are increasingly convinced that there is no viable alternative to\n> Site Isolation as a systematic mitigation to SSCAs [speculative side-channel\n> attacks]. In this new mental model, we have to assume that user code can\n> reliably gain access to all data within a renderer process through\n> speculation. This means that we definitely need some sort of ‘privileged/PII\n> data isolation’ guarantees as well, for example ensuring that password and\n> credit card info are not speculatively loaded into a renderer process without\n> user consent. — Daniel Clifford, in private email\n\nIn fact, any software that both (a) runs (native or interpreted) code from more\nthan one source; and (b) attempts to create a security boundary inside a single\naddress space, is potentially affected. For example, software that processes\ndocument formats with scripting capabilities, and which loads multiple documents\nfrom different sources into the same process, may need to take defense measures\nsimilar to those described here.\n\n### Problem Statement\n\n#### Active Web Content: Renderer Processes\n\nWe must assume that *active web content* (JavaScript, WebAssembly, Native\nClient, Flash, PDFium, …) will be able to read any and all data in the address\nspace of the process that hosts it. Multiple independent parties have developed\nproof-of-concept exploits that illustrate the effectiveness and reliability of\nSpectre-style attacks. The loss of cross-origin confidentiality inside a single\nprocess is thus not merely theoretical.\n\nThe implications of this are far-reaching:\n\n* An attacker that can exploit Spectre can bypass certain native code exploit\n  mitigations, even without an infoleak bug in software.\n   * ASLR\n   * Stack canaries\n   * Heap metadata canaries\n   * Potentially certain forms of control-flow integrity\n* We must consider any data that gets into a renderer process to have no\n  confidentiality from any origins running in that process, regardless of the\n  same origin policy.\n\nAdditionally, attackers may develop ways to read memory from other userland\nprocesses (e.g. a renderer reading the browser’s memory). We do not include\nthose attacks in our threat model. The hardware, microcode, and OS must\nre-establish the process boundary and the userland/kernel boundary. If the\nunderlying platform does not enforce those boundaries, there’s nothing an\napplication (like a web browser) can do.\n\n#### GPU Process\n\nChrome’s GPU process handles data from all origins in a single process. It is\nnot currently practical to isolate different sites or origins into their own GPU\nprocesses. (At a minimum, there are time and space efficiency concerns; we are\nstill trying to get Site Isolation shipped and are actively resolving issues\nthere.)\n\nHowever, WebGL exposed high-resolution clocks that are useful for exploiting\nSpectre. It was possible to temporarily remove some of them, and to coarsen\nanother, with minimal breakage of web compatibility, and so [that has been\ndone](https://bugs.chromium.org/p/chromium/issues/detail?id=808744). However, we\nexpect to reinstate the clocks on platforms where Site Isolation is on by\ndefault. (See [Attenuating Clocks, below](#attenuating-clocks).)\n\nWe do not currently believe that, short of full code execution, an attacker can\ncontrol speculative execution inside the GPU process to the extent necessary to\nexploit Spectre-like vulnerabilities. [As always, evidence to the contrary is\nwelcome!](https://www.google.com/about/appsecurity/chrome-rewards/index.html)\n\n#### Nastier Threat Models\n\nIt is generally safest to assume that an arbitrary read-write primitive in the\nrenderer process will be available to the attacker. The richness of the\nattack/API surface available in a rendering engine makes this plausible.\nHowever, this capability is not a freebie the way Spectre is — the attacker must\nactually find 1 or more bugs that enable the RW primitive.\n\nSite Isolation (SI) gets us closer to a place where origins face in-process\nattacks only from other origins in their `SiteInstance`, and not from any\narbitrary origin. (Origins that include script from hostile origins will still\nbe vulnerable, of course.) However, [there may be hostile origins in the same\nprocess](#multiple-origins-within-a-siteinstance).\n\nStrict origin isolation is not yet being worked on; we must first ship SI on by\ndefault. It is an open question whether strict origin isolation will turn out to\nbe feasible.\n\n## Defensive Approaches\n\nThese are presented in no particular order, with the exception that Site\nIsolation is currently the best and most direct solution.\n\n### Site Isolation\n\nThe first order solution is to simply get cross-origin data out of the Spectre\nattacker’s address space. [Site\nIsolation](https://www.chromium.org/Home/chromium-security/site-isolation) (SI)\nmore closely aligns the web security model (the same-origin policy) with the\nunderlying platform’s security model (separate address spaces and privilege\nreduction).\n\nSI still has some bugs that need to be ironed out before we can turn it on by\ndefault, both on Desktop and on Android. As of May 2018 we believe we can turn\nit on by default, on Desktop (but not Android yet) in M67 or M68.\n\nOn iOS, where Chrome is a WKWebView embedder, we must rely on [the mitigations\nthat Apple is\ndeveloping](https://webkit.org/blog/8048/what-spectre-and-meltdown-mean-for-webkit/).\n\nAll major browsers are working on some form of site isolation, and [we are\ncollaborating publicly on a way for sites to opt in to\nisolation](https://groups.google.com/a/chromium.org/forum/#!forum/isolation-policy),\nto potentially make implementing and deploying site isolation easier. (Chrome\nDesktop’s Site Isolation will be on by default, regardless, in the M67 – M68\ntimeframe.)\n\n#### Limitations\n\n##### Incompleteness of CORB\n\nSite Isolation depends on [cross-origin read\nblocking](https://chromium.googlesource.com/chromium/src/+/main/content/browser/loader/cross_origin_read_blocking_explainer.md)\n(CORB; formerly known as cross-site document blocking or XSDB) to prevent a\nmalicious website from pulling in sensitive cross-origin data. Otherwise, an\nattacker could use markup like `<img src=\"http://example.com/secret.json\">` to\nget cross-origin data within reach of Spectre or other OOB-read exploits.\n\nAs of M65, CORB protects:\n\n* HTML, JSON, and XML responses.\n  Protection requires the resource to be served with the correct\n  `Content-Type` header. [We recommend using `X-Content-Type-Options:\n  nosniff`](https://www.chromium.org/Home/chromium-security/ssca).\n* text/plain responses which sniff as HTML, XML, or JSON.\n\nToday, CORB doesn’t protect:\n\n* Responses without a `Content-Type` header.\n* Particular content types:\n   * `image/*`\n   * `video/*`\n   * `audio/*`\n   * `text/css`\n   * `font/*`\n   * `application/javascript`\n   * PDFs, ZIPs, and other unrecognized MIME types\n* Responses to requests initiated from the Flash plugin.\n\nSite operators should read and follow, where applicable, [our guidance for\nmaximizing CORB and other defensive\nfeatures](https://developers.google.com/web/updates/2018/02/meltdown-spectre).\n(There is [an open bug to add a CORB evaluator to\nLighthouse](https://bugs.chromium.org/p/chromium/issues/detail?id=806070).)\n\n##### Multiple Origins Within A `SiteInstance` {#multiple-origins-within-a-siteinstance}\n\nA *site* is defined as the effective TLD + 1 DNS label (“eTLD+1”) and the URL\nscheme. This is a broader category than the origin, which is the scheme, entire\nhostname, and port number. All of these origins belong to the same site:\n\n* https, www.example.com, 443\n* https, www.example.com, 8443\n* https, goaty-desktop.internal.example.com, 443\n* https, compromised-and-hostile.unmaintained.example.com, 8443\n\nTherefore, even once we have shipped SI on all platforms and have shaken out all\nthe bugs, renderers will still not be perfect compartments for origins. So we\nwill still need to take a multi-faceted approach to UXSS, memory corruption, and\nOOB-read attacks like Spectre.\n\nNote that we are looking into the possibility of disabling assignments to\n`document.domain` (via [origin-wide](https://wicg.github.io/origin-policy)\napplication of [Feature Policy](https://wicg.github.io/feature-policy/) or the\nlike). This would open the possibility that we could isolate at the origin\nlevel.\n\n##### Memory Cost\n\nWith SI, Chrome tends to spawn more renderer processes, which tends to lead to\ngreater overall memory usage (conservative estimates seem to be about 10%). On\nmany Android devices, it is more than 10%, and this additional cost can be\nprohibitive. However, each renderer is smaller and shorter-lived under Site\nIsolation.\n\n##### Plug-Ins\n\n###### PDFium\n\nChrome uses different PPAPI processes per origin, for secure origins. (We\ntracked this as [Issue\n809614](https://bugs.chromium.org/p/chromium/issues/detail?id=809614).)\n\n###### Flash\n\nClick To Play greatly reduces the risk that Flash-borne Spectre (and other)\nexploits will be effective at scale.\nEven so,\n[we might want to consider teaching CORB about Flash flavour of CORS](https://crbug.com/816318).\n\n##### Android `WebView`\n\nAndroid `WebView`s run in their own process as of Android O, so the hosting\napplication gets protection from malicious web content. However, all origins are\nrun in the same `WebView` process.\n\n### Ensure User Intent When Sending Data To A Renderer\n\nBefore copying sensitive data into a renderer process, we should somehow get the\nperson’s affirmative knowledge and consent. This has implications for all types\nof form auto-filling: normal form data, passwords, payment instruments, and any\nothers. It seems like we are [currently in a pretty good place on that\nfront](https://bugs.chromium.org/p/chromium/issues/detail?id=802993), with one\nexception: usernames and passwords get auto-filled into the shadow DOM, and then\nrevealed to the real DOM on a (potentially forged?) user gesture. These\ncredentials are origin-bound, however.\n\nThe [Credential Management\nAPI](https://developer.mozilla.org/en-US/docs/Web/API/Credential_Management_API)\nstill poses a risk, exposing usernames/passwords without a gesture for the\nsubset of users who've accepted the auto-sign-in mechanism.\n\nWhat should count as a secure gesture is a gesture on relevant, well-labeled\nbrowser chrome, handled in the browser process. Tracking the gesture in the\nrenderer, that can be forged by web content that compromises the renderer, does\nnot suffice.\n\n#### Challenge\n\nWe must enable a good user experience with autofill, payments, and passwords,\nwhile also not ending up with a browser that leaks these super-important classes\nof data. (A good password management experience is itself a key security goal,\nafter all.)\n\n### Reducing Or Eliminating Speculation Gadgets\n\nExploiting Spectre requires that the attacker can find (in V8, Blink, or Blink\nbindings), generate, or cause to be generated code ‘gadgets’ that will read out\nof bounds when speculatively executed. By exerting more control over how we\ngenerate machine code from JavaScript, and over where we place objects in memory\nrelative to each other, we can reduce the prevalence and utility of these\ngadgets. The V8 team has been [landing such code generation\nchanges](https://bugs.chromium.org/p/chromium/issues/detail?id=798964)\ncontinually since January 2018.\n\nOf the known attacks, we believe it’s currently only feasible to try to mitigate\nvariant 1 with code changes in C++. We will need the toolchain and/or platform\nsupport to mitigate other types of speculation attacks. We could experiment with\ninserting `LFENCE` instructions or using\n[Retpoline](https://support.google.com/faqs/answer/7625886) before calling into\nBlink.\n\nPDFium uses V8 for its JavaScript support. To the extent that we rely on V8\nmitigations for Spectre defense, we need to be sure that PDFium uses the latest\nV8, so that it gets the latest mitigations. In shipping Chrome/ium products,\nPDFium uses the V8 that is in Chrome/ium.\n\n#### Limitations\n\nWe don’t consider this approach to be a true solution; it’s only a mitigation.\nWe think we can eliminate many of the most obvious gadgets and can buy some time\nfor better defense mechanisms to be developed and deployed (primarily, Site\nIsolation).\n\nIt is very likely impossible to eliminate all gadgets. As with [return-oriented\nprogramming](https://en.wikipedia.org/wiki/Return-oriented_programming), a large\nbody of object code (like a Chrome renderer) is likely to contain so many\ngadgets that the attacker has a good probability to craft a working exploit. At\nsome point, we may decide that we can’t stay ahead of attack research, and will\nstop trying to eliminate gadgets.\n\nAdditionally, the mitigations typically come with a performance cost, and we may\nultimately roll some or all of them back. Some potential mitigations are so\nexpensive that it is impractical to deploy them.\n\n### Attenuating Clocks {#attenuating-clocks}\n\nExploiting Spectre requires a clock. We don’t believe it’s possible to\neliminate, coarsen, or jitter all explicit and implicit clocks in the Open Web\nPlatform (OWP) in a way that is sufficient to fully resolve Spectre. ([Merely\nenumerating all the\nclocks](https://bugs.chromium.org/p/chromium/issues/detail?id=798795) is\ndifficult.) Surprisingly coarse clocks are still useful for exploitation.\n\nWhile it sometimes makes sense to deprecate, remove, coarsen, or jitter clocks,\nwe don’t expect that we can get much long-term defensive value from doing so,\nfor several reasons:\n\n* There are [many explicit and implicit clocks in the\n  platform](https://gruss.cc/files/fantastictimers.pdf)\n* It is not always possible to coarsen or jitter them enough to slow or stop\n  exploitation…\n* …while also maintaining web platform compatibility and utility\n\nIn particular, [clock jitter is of extremely limited\nutility](https://rdist.root.org/2009/05/28/timing-attack-in-google-keyczar-library/#comment-5485)\nwhen defending against side channel attacks.\n\nMany useful and legitimate web applications need access to high-precision\nclocks, and we want the OWP to be able to support them.\n\n### Gating Access To APIs That Enable Exploitation\n\nWe want to support a powerful web, though we recognize that some kinds of APIs\na powerful web requires are more likely than others to facilitate exploitation,\neither because they can be used as very high-resolution timers\n(`SharedArrayBuffer`), or because they provide powerful introspection\ncapabilities (`performance.measureMemory`). We can mitigate the risks these APIs\npose by exposing them only in contexts that have opted-into a set of\nrestrictions that limits access to cross-origin data.\n\nIn particular, [`Cross-Origin-Opener-Policy` (COOP) and\n`Cross-Origin-Embedder-Policy` (COEP)](https://docs.google.com/document/d/1zDlfvfTJ_9e8Jdc8ehuV4zMEu9ySMCiTGMS9y0GU92k/edit)\nseem promising. Together, these mechanisms change web-facing behavior to enable\norigin-level process isolation, and ensure that cross-origin subresources will\nflow into a given process only if they opt-into that usage. These properties\ngive us a higher degree of confidence that otherwise dangerous APIs can be\nexposed safely, as any attack they enable would only gain access to same-origin\ndata, or data that explicitly asserted that it accepted the risk of exposure.\n\nBoth COOP and COEP are enabled as of M83, and we [plan to require both before\nenabling APIs like `SharedArrayBuffer`](https://groups.google.com/a/chromium.org/d/msg/blink-dev/_0MEXs6TJhg/F0UduPfpAQAJ).\nOther browsers seem likely to do the same.\n\n#### Other Gating Mechanisms\n\n**Note:** This section explores ideas but we are not currently planning on\nimplementing anything along these lines.\n\nLooking beyond developer opt-ins such as COOP and COEP, we might be able to find\nother ways of limiting the scope of APIs to reduce their risk. For example, a\nthird-party `iframe` that is trying to exploit Spectre is very different than a\nWebAssembly game, in the top-level frame, that the person is actively playing\n(and issuing many gestures to). We could programmatically detect engagement and\nestablish policies for when certain APIs and features will be available to web\ncontent. (See e.g. [Feature Policy](https://wicg.github.io/feature-policy/).)\n\n*Engagement* could be defined in a variety of complementary ways:\n\n* High [site engagement\n  score](https://www.chromium.org/developers/design-documents/site-engagement)\n* High site popularity, search rank, or similar\n* Frequent gestures on/interactions with the document\n* Document is the top-level document\n* Document is the currently-focused tab\n* Site is bookmarked or added to the Home screen or Desktop\n\nAdditionally, we have considered the possibility of prompting the user for\npermission to run certain exploit-enabling APIs, although there are problems:\nwarning fatigue, and the difficulty of communicating something accurate yet\ncomprehensible to people.\n\n## Conclusion\n\nFor the reasons above, we now assume any active code can read any data in the\nsame address space. The plan going forward must be to keep sensitive\ncross-origin data out of address spaces that run untrustworthy code, rather than\nrelying on in-process checks.\n"
  },
  {
    "path": "security/sheriff",
    "title": "Security Sheriff",
    "content": "# Security Sheriff\n\nHas been renamed to security shepherd - [please see here, instead](shepherd.md).\n"
  },
  {
    "path": "security/shepherd",
    "title": "Security Shepherd",
    "content": "# Security Shepherd\n\n## What is Security Shepherding?\nSecurity Shepherding is a rotational assignment for security bug triage\n(Primary Shepherd) and managing the flow of incoming inquiries and progressing\nsecurity issues (Secondary Shepherd). The Shepherding rota pool is made up of\npeople actively working on security in Chrome.\n\nAll Security Shepherds are Googlers; therefore, some links on this page may\nnot be externally accessible or even further restricted to just Chrome Security\nGooglers.\n\nThere is a Primary and Secondary Shepherd scheduled each rotation, with two\nrotations each week, one Tuesday - Thursday and Friday - Monday.\n\nSecurity Shepherding is *not* an on-call rotation. There’s no pager duty, nor\nare you expected to perform Shepherding duties outside of your usual working\nhours, such as overnight or on holidays, weekends, or other off time.\n\nShepherds are only responsible for triage of security bugs during your shift.\nYou are not responsible for bug triage or updating partially triaged bugs past\nyour shift, unless you have specifically taken ownership of an issue, such as\ndue to a complicated or OS-specific reproduction, and arranged that with the\noncoming shepherd. All shepherds should use the handoff doc to communicate items\nfor handover; however, the oncoming primary shepherd should operate on the\npremise all new or _under_-triaged issues are your responsibility. Please do not\nleave any unaddressed red cells in the dashboard at the end of your shift.\n\n## TL;DR Checklist for Primary Shepherding\n(“I’m Primary Shepherd, what do I do???”)\n\nThe Primary Security Shepherd is the front line of security bug triage during\ntheir shift. The goal is to triage incoming security issues accurately,\nthoroughly, and quickly (_as quickly as realistically possible_).\n\nYour PRIMARY DIRECTIVE as PRIMARY SHEPHERD is to tackle all the red cells on the\nsecurity bug dashboard.\n\nFor [*every new incoming security bug*](#Every-New-Incoming-Security-Bug):\n* Make sure it's [*self-contained*](#Ensure-self_contained-issue).\n* Make sure the report is [*valid and actionable*](#Confirm-Valid-and-Actionable)\n  * Ideally, you’ll be able to do this by [reproducing the bug](#Reproduce-the-bug),\n    more ideally, [with ClusterFuzz](clusterfuzz-for-shepherds.md).\n* [*Assign*](#Assign) to an appropriate or suitable owner or engineering team.\n* Set [*severity*](#Set-severity).\n\nFor every new incoming security bug that is S2 or more severe:\n* Set [*oldest impacted active release channel*](#Set-oldest-impacted-active-release-channel) – AKA FoundIn.\n* Set [*impacted-operating-systems*](#Set-impacted-operating-systems). If the bug is known or\nunderstood to impact only specific platforms, set the OS field regardless of severity.\n\nAll of the above should be completed as soon as possible during your shift,\nand at least, by the [shift-handoff](#shift-handoff).\n\nOne or more of the above actions may be necessary to complete the triage of an\nunder-triaged bug, i.e. covering any of the open red cells in the dashboard that\nwere not completed from ClusterFuzz auto-triage or previous work on the bug.\n\nAll this is hard, so please remember to [ask for help](#Ask-for-help).\n[Yell if you must](https://www.youtube.com/watch?v=5y_SbnPx_cE&t=37s)!\n\n## TL;DR Checklist for Secondary Shepherding\n(“I’m Secondary Shepherd, what do I do???”)\n\n* [*Check in on triaged issues*](#Check-in-on-triaged-issues) to ensure progress\n  is being made on medium+ (S2-S0) severity security bugs.\n* [*Manage incoming security email*](#Handle-incoming-security-emails).\n\n\n[TOC]\n\n## Links to Helpful Resources\n\nHere are some of the important references and resources you need or may need\nduring your shepherding shift:\n\n* [Current Shepherds](https://script.google.com/a/macros/google.com/s/AKfycbz02xD4ghSzZu_tXyNRgjC95wFURATZeD_FHq0KRMHeqA-b0b9sow4NV1lhi0P2vy1j/exec)\n* [Chrome Security Bug Dashboard](https://goto.google.com/chrome-security-bugs)\n* [Security Severity Guidelines](severity-guidelines.md)\n* [Security Labels](security-labels.md)\n* FAQs addressing commonly-raised questions about security and what is / is not\n  considered a security bug, to see if there is an existing stance:\n  * [Chrome Security FAQ](faq.md)\n  * [Extensions Security FAQ](https://chromium.googlesource.com/chromium/src/+/refs/heads/main/extensions/docs/security_faq.md)\n  * [Service Worker Security FAQ](service-worker-security-faq.md)\n* [Redshell for Security Shepherds](https://goto.google.com/redshell-for-chrome-shepherds)\n* [Shepherding Guidelines Changelog](https://goto.google.com/shepherding-changelog) for highlighting\n  any process or policy changes since your last shift.\n* [Guidance for triage of theoretical or speculative issues](https://goto.google.com/chrome-speculative-bug-triage)\n* [Reference for common questions about security bug lifecycle](life-of-a-security-issue.md)\n* [Reference for questions related to security fix merge process](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/process/merge_request.md#Security-merge-triage)\n  for answering questions (you do not need to approve merges).\n* [Shepherding Handoff Log](https://goto.google.com/chrome-security-shepherd-handoff)\n* [GPU for dummies](https://goto.google.com/gpu-for-dummies)\n\n### Every New Incoming Security Bug\n\nMonitor the [Chrome open security bugs dashboard](http://go/chrome-security-bugs).\nTackle all the empty red cells. New bugs populate at the top of the sheet and\nwill need full triage. Partially triaged bugs, such as those triaged by\nClusterFuzz or ones pending updates from a prior shift, may be lower in the\nsheet. Please check the sheet for any red cells and do your best to get any bugs\nto a fully triaged state.\n\nWe aim to have every bug triaged and assigned **within two business days,\npreferably one.** This does not include weekends, but please ensure you leave a\nclear queue before the weekend or a holiday.\n\n### Ensure self-contained issue\n\nThere should be one complete, self-contained report, per root cause. To ensure\nthis is the case when assigning security bugs to engineering teams, you may\nneed to take some specific actions here:\n* If the report is a bug chain with several underlying causes, **open one new\n  bug per root cause** and mark the parent bug as `blocked on` each. The parent\n  bug should be set to the severity of the full chain. Each child bug may have a\n  lower severity.\n  * If taking these actions for a VRP reported issue, update the Reporter field\n    with the email address of the VRP reporter and cc: them on the parent issue\n    so they have access.\n* Get everything you need from a reporter before you try and reproduce - do not\n  feel bad about asking for a clear or minimized POC or repeatable steps before\n  attempting to reproduce.\n* If complicated user gestures are required, encourage the reporter to upload a\n  short video. This will alleviate a lot of back and forth for both them and us.\n\n## Confirm Valid and Actionable\n\nWe expect engineering teams to address security bugs promptly. In order to do\nthat, our goal is to pass them actionable reports with little ambiguity.\n\n*For each bug, please take the appropriate action, either:*\n\n* **WontFix it as invalid** (Many recurring types of invalid reports are covered\n  by the [Security FAQ](faq.md), such as those related physically local attacks\n  or inputting JavaScript in the URL bar or running Javascript directly in\n  DevTools not being an indication of an XSS vulnerability. Mark as WontFix and\n  update the 'Issue access level' to `Default access` so the issue is\n  publicly visible.\n* **Mark as duplicate** – we want exactly one bug per root cause problem. Please\n  check for duplicate issues of a given issue from that or other reporters /\n  sources (such as ClusterFuzz).\n  * Search for similar stack traces or sharing similar keyword traits in the bug\n    tracker.\n  * If there are two open reports of the same issue, please merge as a duplicate\n    in the direction of the oldest report.\n  * Use the `Mark as Duplicate` button at the upper right of the report pane.\n    This will provide a pop-up to input the bug number of the canonical report\n    that you are merging this report into as a duplicate of.\n    * Using `Mark as Duplicate` is the best practice for merging issues as\n      duplicates.\n\n* **Convert functional bugs to Type=Bug** For example, many reports are for\n  crashes of a functional nature, rather than an exploitable security condition,\n  such as most null pointer dereferences. Convert such reports from\n  Type=Vulnerability to Type=Bug. Do NOT remove security@chromium.org from\n  collaborators first (as this will result in orphaning the bug), but update the\n  'Issue Access Level' to the appropriate visibility. You may consider adding\n  other visibility restrictions, such as `Limited visibility + Googlers` and add\n  edit-bug-access@chromium.org to CC (this is similar to\n  'Restrict-View-EditAccess' in the legacy issue tracker) if the immediate\n  disclosure could result in potential abuse (e.g. denial of service issue).\n* **Convert to a privacy bug** - privacy issues (such as issues with incognito)\n  are not considered security bugs, but functional privacy issues.\n  Convert to Type=Bug and add the Privacy component. Add yourself and any other\n  security team members who may potentially need access to the cc: line.\n  Update the 'Issue access level' to `Limited visibility + Googlers` and\n  deselect / remove security@chromium.org from the 'add collaborator groups'.\n* **Add the `Needs-Feedback` hotlist (hotlistID: 5433459) and set a Next Action\n  date of 24-48 hours for more information** if there is no response, close the\n  issue as `WontFix`.\n* **Determine issue to be theoretical** - and follow the [process for such\n  issues](http://go/chrome-speculative-bug-triage) – theoretical issues are\n  ones that appear to be potentially real bugs, but the report is lacking\n  evidence of exploitability or reachability. These cases can be shared with\n  engineering teams with a very clear message conveying the speculative nature\n  of the issue. These reports should generally not be prioritized as a Pri-1 as\n  they do not warrant disruption to the engineering teams to investigate and\n  prioritize without more or new information to demonstrate conditions of\n  exploitability.\n\nNone of these apply? Great – this means the bug may be valid and actionable!\nIt can take multiple discussions with a reporter to understand a bug. Try really\nhard to reach a conclusion by the end of your shift. If this isn’t possible,\nplease discuss outstanding cases with the next shepherd and don’t let bugs fall\nthrough the cracks. You are responsible for any bug reported or in an un-triaged\nstate during your shift.\n\nThe best way to determine the validity of a security bug is to [*reproduce it*](#Reproduce-the-bug).\nIt’s helpful to remember that reporters invested time and energy in their bug\nreports:\n\n![alt text](apf-right-a-wrong.png \"felt: a lot of Chrome vuln reports come from\nwell-meaning people who clearly went out of their way to try to right a wrong.\ni like that.\")\n\n[link](https://twitter.com/__apf__/status/728776130564526080)\n\n\nIf you have to close it, please give an explanation as to why.\n\n### Reproduce the bug\n\nReproducing the bug isn’t always required, but often it’s needed and the only\nway to:\n\n* Understand a report and validate the issue being presented.\n* Provide actionable information to the engineering team responsible for fixing\n  the bug.\n* Setting the oldest impacted release channel correctly.\n\nThese things must be done correctly, so as Security Shepherd, you’ll spend a lot\nof time reproducing bugs. Here are some tips in doing so:\n\n* Assume that test cases may be malicious. You should only reproduce bugs\n  on your local machine if you're completely certain that you understand\n  100% of the test case. If not, use a disposable virtual machine. If you're\n  inside Google, a good way to do this is using\n  [Redshell](https://goto.google.com/redshell-for-chrome-shepherds).\n* For any sort of a crash, CHECK/DCHECK or memory safety problem\n  [use ClusterFuzz](clusterfuzz-for-shepherds.md). As well as reproducing bugs,\n  ClusterFuzz will help you with lots of subsequent bisection and labelling\n  tasks. Currently ClusterFuzz only supports untrusted inputs on Linux. If you\n  use ClusterFuzz to reproduce on any other platform, you should be just as\n  paranoid as if you were running a test case locally.\n* [Instructions for using an Android emulator can be found\n  here](/docs/android_emulator.md). If you're inside Google, we have a\n  [guide for testing using Google infrastructure](https://goto.google.com/android-for-chrome-shepherds).\n* When you can't just build from a specific branch locally, see\n  [https://dev.chromium.org/getting-involved/dev-channel](https://dev.chromium.org/getting-involved/dev-channel)\n  or\n  [https://commondatastorage.googleapis.com/chromium-browser-asan/index.html](https://commondatastorage.googleapis.com/chromium-browser-asan/index.html)\n  for the latest release of a specific version.\n* The [get_asan_chrome.py](https://source.chromium.org/chromium/chromium/src/+/main:tools/get_asan_chrome/get_asan_chrome.py)\n  helper script is a handy way to download ASAN Chrome. The --help flag\n  provides usage instructions, e.g. to fetch builds for various versions and\n  platforms.\n* If you run into issues with a reproducible ClusterFuzz test case (like\n  missing symbols, or if anything else seems off), try uploading the test case\n  again using a different job type with a more mature tool (e.g. ASan on Linux).\n  It may give more complete information.\n\n### Set severity\n\nUse the [Security Severity Guidelines](severity-guidelines.md).\n\nIf you can, [*reproduce it using ClusterFuzz*](clusterfuzz-for-shepherds.md), as\nthe severity is usually set automatically.\n\nFor V8 issues, you can tentatively set the issue as High (S1) severity – see\n[Assign,below](#Assign).\n\nPlease adjust severity as your understanding of the bug evolves - but please\nalways add a comment explaining the change. Higher severity bugs involve\nsignificant disruption for multiple teams; lower severity issues may not be\nfixed and a fix released to users as quickly as may be warranted. That’s why\nit’s important to get the severity as correct as possible.\n\n### Set oldest impacted active release channel\n\nWe do not release severe security regressions, so we need to know the earliest\nimpacted Chrome release branch.\n\nFirst, if an issue [doesn’t impact Chrome users by default (such as be being\nbehind a disabled feature or a command line flag), add the hotlist\n**`Security_Impact-None`**](security-labels.md#when-to-use-security_impact_none-toc_security_impact_none);\notherwise, set a **Found In** milestone in the `Found In` field as follows:\n\nCheck [ChromiumDash](https://chromiumdash.appspot.com/releases?platform=Windows) for the earliest relevant milestone number\n(Extended Stable or Stable – sometimes they are the same).\n* If that branch is affected, set the `Found In` field to, to the appropriate\n  milestone number.\n* Otherwise, move forward through milestone numbers. Set the `Found In` field\n  to the oldest impacted branch you find.\n\nThere is no general reason to test versions older than the current Extended\nStable milestone. If you can [*reproduce using ClusterFuzz*](clusterfuzz-for-shepherds.md)\nthe `Found In` field can often be set automatically if ClusterFuzz can identify\nthe culprit CL.\n\nOtherwise, you may need to [reproduce the bug](#Reproduce-the-bug) manually to\ndetermine the impacted branches.\n\nIf you have a bisection or other convincing evidence, that’s sufficient. You can\nmanually check which milestone has a given commit in\n[ChromiumDash commits check](https://chromiumdash.appspot.com/commits).\n\nPlease *do not* base Found In- on the Chrome version number provided in the\noriginal report. This is often based on the version number the individual is\nusing when discovering this issue or is automatically set in the report by the\ntracker’s report wizard and is not correct in terms of coverage of all active\nrelease channels.\n\nFor V8 bugs, you can set `Found In` as the current extended stable milestone\nunless you have reproduced the issue or an accurate bisection has been provided.\n(See [Assign, below](#Assign).)\n\n### Set impacted operating systems\n\nSet the `OS` field as best you can based on [these guidelines](security-labels.md#OS-Labels).\nYou do not need to reproduce the bug on each platform, but it really helps if\nyou set this field roughly right to ensure the bug has the attention of the\ndifferent desktop and mobile release teams.\n\nSome issues may be specific to a particular platform, if you need to reproduce a\nbug that is platform specific and you do not have access to a device with that\nOS, please [ask for help](#Ask-for-help), there is likely someone on the team\nthat does and can help you.\n\nChromeOS is in the Google issue tracker. VRP reports for ChromeOS should be\n[directly reported to ChromeOS](https://bughunters.google.com/report). Please\nrequest the reporter submit reports directly to ChromeOS in the future. For\nVRP and other human-submitted security bug reports specific to ChromeOS,\nplease move the report corresponding component (componentid:1335705) in the\nGoogle issue tracker. Since this bug is being moved between trackers you will\nneed to use your google.com account to move the bug into that tracker component.\n\nSome machine-discovered (Clusterfuzz, Crash AutoBugFiler, GWP-ASAN) may be\nspecific to ChromeOS. If this is determined to be the case after investigation\n(please remember some GWP-ASAN or crash bug auto-filer bugs may have come from a\nChromeOS crash, but the issue may not be specific to ChromeOS), move the bug\nto the appropriate ChromeOS component (componentid:1214738) in the Google\nissue tracker for these reports. Again, you will need to use your google.com\naccount to move this bug into that component.\n\n### Assign\n\nSecurity bugs are not automatically visible, so you must add people to get them\nfixed. For each bug, set:\n\n* The **Component** – due to a limited set of auto-cc rules, this may add\n  some visibility. This will \"move\" the bug into that component; this is the\n  expected outcome. It can also be helpful to set additional **Component Tags**\n  when a bug does not fall neatly into a single component.\n* An **assignee/owner**. Use `git blame` or look for similar past bugs in the\n  tracker.\n* Lots of **cc**s. Copy everyone who could possibly be relevant. Use the owners\n  file for a particular feature to help achieve this.\n* Add a **comment** so that recipients know what’s expected, and why you think\n  they’re the right person to take action.\n  * Be sure to convey if you have reproduced this issue and your determinations\n  about security relevance or diagnosis.\n\nIt’s okay if you cannot determine or  know the exact right assignee, but please\npass it along to / include someone who can direct it more precisely.\n\n*Some types of bugs have specific assignment needs:*\n* **V8 bugs**. First, [upload benign-looking test cases to\n  ClusterFuzz](clusterfuzz-for-shepherds.md) if it isn't already\n  there (please keep an eye out for any special flags and debug vs release).\n  Hopefully, this will cause ClusterFuzz to reproduce and bisect the bug. If\n  not:\n    * Set a provisional severity of High (S1), assuming this causes renderer\n      memory corruption.\n    * Set a provisional `Found In` of the current Extended Stable.\n    * Assign it to the current [V8\n      Sheriff](https://goto.google.com/current-v8-sheriff) with\n      a comment explaining that the severity and `Found In` are provisional.\n      Note that V8 CHECK failure crashes can have security implications, so\n      don't triage it yourself.\n    * If for any reason you need to discuss the bug with a particular V8 contact,\n      Googlers can look at\n      [the V8 security bug triage instructions](https://goto.google.com/v8-security-issue-triage-how-to)\n      for lists of component owners, but this shouldn't normally be necessary.\n* **V8 Sandbox bypasses**. The V8 Sandbox is still under development, however\n  V8 sandbox bypasses, both VRP and ClusterFuzz reported, should be handled as\n  security bugs. That being said, Chrome Security Shepherds are not expected to\n  to fully triage these reports. You do not need to submit the sandbox bypasses\n  to Clusterfuzz. If the report is clearly a V8 sandbox bypass, simply:\n    * Set a provisional severity of Medium (S2).\n    * Set a provisional priority of P1.\n    * Assign to the current [V8\n      Sheriff](https://goto.google.com/current-v8-sheriff).\n    * Apply the `Security_Impact-None` hotlist (hotlistID:5433277).\n    * If possible, please also apply the `V8 Sandbox` hotlist\n      (hotlistID:4802478).\n* **Skia bugs** can be assigned to hcm@chromium.org. Be careful while triaging\n  these! The place where we're crashing isn't necessarily the place where the\n  bug was introduced, so blame may be misleading. Skia fuzzing bugs can be\n  assigned to kjlubick@chromium.org, as Skia is heavily fuzzed on OSS-Fuzz and\n  some issues reported in Chromium are already known or even fixed upstream.\n* **URL spoofing issues**, especially related to RTL or IDNs? See\n  [go/url-spoofs](http://go/url-spoofs) for a guide to triaging these.\n* **SQLite bugs** can be assigned to an owner from //third_party/sqlite/OWNERS.\n  CC drhsqlite@ for upstream issues.\n* **Fullscreen bugs** There is ongoing and planned work to make overall\n  improvements to the security and functionality of Fullscreen. As of January\n  2025 and until [crbug.com/391919449](https://crbug.com/391919449) is closed,\n  new reports of Fullscreen issues should be merged into\n  [crbug.com/391919449](https://crbug.com/391919449) as a Duplicate. In general,\n  the Open Screen team has taken ownership of Fullscreen issues, including\n  security bugs. When necessary, Fullscreen security issues should be assigned\n  muyaoxu@ and cc: liberato@ and mfoltz@.\n* **BoringSSL** the BoringSSL project has moved into the Chromium tracker.\n  BoringSSL is a library, so security bugs that do not impact Chrome may still\n  be meaningful (e.g. server-side bugs). BoringSSL security issues should be\n  fully assessed by the BoringSSL team. If you come across a BoringSSL bug in\n  the triage queue:\n    * Set a provisional severity based on the issue the report proports; the\n      BoringSSL team may need to adjust based on their assessment.\n    * Set `Component` to: Chromium > BoringSSL.\n    * Assign to an appropriate owner based on `third_party/boringssl/OWNERS`;\n      Add owners to cc: on the bug to ensure visibility.\n    * Add `Security_Impact-None` hotlist; owner will update if this issue\n      does impact Chrome.\n* Report suspected malicious URLs to SafeBrowsing:\n  * Public URLs:\n    * [Report malware](https://safebrowsing.google.com/safebrowsing/report_badware/?hl=en)\n    * [Report phishing](https://safebrowsing.google.com/safebrowsing/report_phish/?hl=en)\n    * [Report incorrect phishing warning](https://safebrowsing.google.com/safebrowsing/report_error/?hl=en)\n  * Googlers: see instructions at [go/safebrowsing-escalation](https://goto.google.com/safebrowsing-escalation)\n  * Report suspected malicious file attachments to SafeBrowsing.\n* If the report is in an upstream package that we pull into our tree via\n  `//third_party` or elsewhere:\n    * Ask the reporter to file a bug report upstream, if there is an active\n      upstream. If they can't / don't, or the report is from a bot\n      (clusterfuzz or similar), ask the `//third_party` package owner to file\n      it.\n    * For the downstream bug (the one on the Chromium tracker):\n        * Add the downstream bug to [the Status-External_Dependency hotlist](https://issues.chromium.org/hotlists/5438152).\n        * Assign that bug to an OWNER from the `//third_party` package.\n        * Ask that owner to ensure that the upstream bug is fixed, the\n          downstream copy in Chromium is rolled, and finally the\n          downstream bug is marked Fixed.\n* For vulnerabilities in services Chrome uses (e.g. Omaha, Chrome Web Store,\n  SafeBrowsing), make sure the affected team is informed and has access to the\n  necessary bugs.\n* Chrome for iOS - bugs suspected to be in **WebKit**:\n    * Reproduce using an iOS device or desktop Safari.\n    * Set Severity, Found In, and set Component Tags fields.\n    * If the issue is in Webkit\n      * Add hotlist `Status_ExternalDependency` (hotlistID: [5438152](https://issues.chromium.org/hotlists/5438152))\n      * If reported by an external VRP reporter, request they report the issue\n      directly to Webkit and provide us the WebKit issue ID after they have done\n      so.\n      * If this is an internally discovered issue, file a security bug in the\n      Security product at [bugs.webkit.org](https://bugs.webkit.org) and\n      cc:chrome-ios-security-bugs@google.com. This alias is monitored by the iOS\n      Chrome team so they can be notified when the WebKit bug is fixed.\n        * Note the WebKit bug ID in the Chromium issue report.\n    * All security issues need owners, the WebKit ones can be assigned to michaeldo@.\n*  GPU driver bugs - bugs in GPU drivers that reachable from a renderer process\n   and triggerable through Chrome, such as bugs in Mesa or Mali drivers,\n   should be assigned to the appropriate Chrome engineering team, such as WebGL\n   or WebGPU, to determine if a shader workaround is appropriate. The report\n   should include a valid test case that demonstrates reachability in an active\n   release channel of Chrome on a supported platform.\n   * Because these driver bugs also impact Android, Pixel, and ChromeOS\n     platforms, we do also want to provide those teams visibility. If deemed\n     necessary they can create a bug in their tracker, that should be tracked as\n     a child bug to the original report we received in the Chromium tracker.\n   * For Mesa driver bugs, please cc: robdclark@ and msturner@ for visibility for\n     ChromeOS.\n   * For Mali driver bugs, please cc: aygupta@, bcreasey@, asdl-kfc@ from\n     Android, mjstokes@ and layog@ from Pixel GPU, and robdclark@ and msturner@\n     from ChromeOS GPU.\n\n### Shift handoff\n\nAs you work through the queue each day, please manage your time and ensure you\nhave addressed all red rows and cells in the sheet to the best of your ability.\nMake sure there are no red cells at the top of your sheet before the end of your\nshift. It’s not okay to leave a backlog for the next oncoming security shepherd.\n\nPlease fill out the [Shepherding Handoff\nLog](https://goto.google.com/chrome-security-shepherd-handoff) to communicate\nissues from your shift that may be helpful to the oncoming shift.\n\n### Ask for help\n\nSecurity bug triage is hard. We receive around 75 bug reports per week on\naverage. **If you are ever stuck or in doubt**, please ask for help from the\n[Chrome Security Shepherds chat](https://goto.google.com/chrome-security-shepherds-chat)\nor the [Chrome Security Chat](https://goto.google.com/chrome-security-chat).\nDuring some shifts, there are just too many incoming bugs. It’s okay to ask for\nhelp, please do!\n\nYou may also like the classic [HOWTO: Be a Security Shepherd deck](https://docs.google.com/presentation/d/1eISJXxyv7dUCGUKk_rvUI9t9s2xb98QY4d_-dZSa7Wg/edit#slide=id.p)\n\nBecause shepherding is fun. You like fun. Don't you? Fun is great.\n\n## Secondary Shepherd\n\n### Check in on triaged issues\n\nReview open security bug reports and check that progress is occurring. This does\nnot apply to the new bug reports as these are handled by the primary shepherd.\nThe rule of thumb is *if there is any red cell on the dashboard, it needs your\nattention*: that especially includes the `last updated` column. Our [severity\nguidelines](severity-guidelines.md) contain the expected duration for shipping\nfixes, but it’s important to remember that to get a fix to all users in 60 days\nor so, this may require us to land a fix in a week or two.\n\n*Suggestions for cultivating progress on security bugs:*\n* Don’t just add a comment to the bug as these can disappear into spam (though a\n  well-crafted, meaningful, actionable comment can be effective).\n* Contact the owner via chat or email in addition to commenting on the bug (so\n  others on the bug can see an update is needed).\n* cc: more relevant people\n* Think about what you can do to unblock the bug. What would _you_ do next?\n Perhaps you bring in a subject matter expert of some aspect of the bug that is\n a particular sticking point or suggest a different approach to reproduce the\n bug. Sometimes a security perspective can help shed light on a different way\n forward.\n* Are there old, open `Security_Impact-None` bugs in unlaunched features, where\n  the response has been that there are no plans to launch that feature? Perhaps\n  inquire as to if that code can be removed rather than keeping vulnerable code\n  production code base. (Removing code that is not being used is a win!)\n* Consider if it is better for you to make meaningful steps forward on three\n  bugs versus simple pings on many bugs.\n\nYou can’t possibly usher all bugs toward meaningful progress during your shift.\nAs a general rule, expect to spend a solid two hours each day  ushering bugs\ntoward progress during your shift. Use the `last updated` column to avoid\nduplicating the work of the previous secondary.\n\n### Handle incoming security emails\n\nEnsure that all incoming inquiries to the [security@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/security),\n[security-dev@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/security-dev),\nand\n[chrome-security@google.com](https://groups.google.com/a/google.com/forum/#!forum/chrome-security)\nlists get a reply (by someone; not necessarily you). See\n[go/chrome-security-emails](https://goto.google.com/chrome-security-emails)\nfor a dashboard.\n\n* When triaging an email to be handled off of the list, make sure to bcc: the\nlist that it arrived on, so that others (including future secondary shepherds)\ncan see that it has been handled.\n* Some of these emails are requests for the inclusion of third-party code.\nBy the time you do shift handoff, please ensure these are either completed or\nhave been acknowledged by some other owner. If not, you may need to do them\nyourself.\n  * Please see [How to do Chrome Third-Party Security Reviews](https://goto.google.com/how-to-do-chrome-third-party-security-reviews) for tips.\n\n## Other Helpful Info\n\n### What do all these bug labels mean?\n\n[Security Labels](security-labels.md).\n\n### An owner is asking for security input on backporting a security fix.\nWhat do I do here?\n\nYou are not responsible for handling merges or approving a fix for backmerge.\nIf the issue is resolved and there is a landed CL, please ensure the bug is\nclosed as Fixed. Please also make sure the bug has a severity and FoundIn set.\nThis will allow the bot (Sheriffbot) to add the appropriately update the Merge\ncustom field with the appropriate request-MMM or review-MMM labels, where MMM =\nthe milestones for backmerge consideration (based on rules driven by severity\n(and security_impact, derived from Found In). See\n[security merge triage](../process/merge_request.md#Security-merge-triage)\nfor more information.\n\nThat issue will be visible to the security merge review queue. There are\ndesignated members of the security team who have the hefty responsibility of\nreviewing security issues for backmerge. Merge approvals will be handled by them\nafter at least the fix has had sufficient bake time on Canary.\n\n### When / how does X happen to a security bug?\n\n(e.g. how and when does a VRP bug get to the Chrome VRP Panel?)\n[See Life of a Security Issue](life-of-a-security-issue.md).\n\n### I have questions related to Chrome VRP policy and scope.\n\n[Chrome VRP policies and rewards page](https://g.co/chrome/vrp) and [Chrome VRP\nNews and FAQs](vrp-faq.md). You can also reach out directly to the Chrome VRP\nTL or ask questions in the\n[Chrome Security Shepherds chat](http://go/chrome-security-shepherds-chat), all\nVRP Panel members are also members of that chat.\n\n### There is PII or other data in a report we do not want to publicly disclose.\n\nFor cases of PII, simply delete the attachment or comment that contains PII\nwithin the issue tracker. If PII is contained in the text of the original\ndescription of the report, simply choose the `Edit description` option and\nremove any PII.\n\nFor cases in which we are just delaying public disclosure (such as when a\nsecurity issue impacts other products or vendors), please add the\n`SecurityEmbargo` hotlist (hotlistID: 5432549) and set a date in the `Next\nAction` field so that disclosure can be re-evaluated at that time.\n\n### Protecting researcher identities\n\nMany researchers report security issues under a pseudonym and from a specific\nemail address pertaining to that pseudonym. Please do not refer to the\nresearcher by the email username directly in any comments of the report.\nWhen reports are publicly disclosed, that becomes visible to all and we have to\ndelete those comments to protect that information. To direct a comment at an\nexternal security researcher, please use “OP”, “reporter”, or \"researcher”.\n\n### Deleted Reports / Issues Marked as Spam or Abuse\n\nYou may come across some reports in the security bug triage queue with a red\nbanner, \"The issue has been deleted. Reason: ABUSE,\" this is generally due to\nthe overactive spam filtering in the issue tracker. Just click `Undelete` in the\nright side of the banner, and triage the report as you normally would.\n\n### Shepherding Scheduling\n\n* [Current Shepherds](http://go/whos-the-shepherd)\n* [Rotation schedule](https://docs.google.com/spreadsheets/d/10sLYZbi6QfLcXrhO-j5eSc82uc7NKnBz_o1pR9y8h7U/edit#gid=0)\n* If you're a Shepherd, you should get a calendar invite.\n  Please accept it to acknowledge your upcoming shepherding duty.\n* If you need to swap shifts, ask around for a volunteer and then just update\n  the [rotation sheet](https://docs.google.com/spreadsheets/d/10sLYZbi6QfLcXrhO-j5eSc82uc7NKnBz_o1pR9y8h7U/edit#gid=0) and wait 10 minutes for the calendar invites to be updated.\n\n### Incident response\n\nSometimes you’ll need to handle a security emergency, such as a critical\nseverity bug or bug known or under active exploitation in the wild. In such\ncases:\n* As soon as possible, reach out to the Shepherds chat for a Chrome Security\n  Incident Responder, so they can take on IR Commander responsibilities.\n* Sometimes features can be switched off using feature flags – for example\n  [in permissions](https://docs.google.com/document/d/17JeYt3c1GgghYoxy4NKJnlxrteAX8F4x-MAzTeXqP4U).  Check with the engineer if that is a possibility in the case of this issue.\n\nThat's a lot of stuff! You have this resource and your peers to lean on for\nquestions and expertise. Hopefully this doc helps.\nYou're gonna do great!\n"
  },
  {
    "path": "security/severity-guidelines",
    "title": "Severity Guidelines for Security Issues",
    "content": "# Severity Guidelines for Security Issues\n\n[TOC]\n\nVendors shipping products based on Chromium might wish to rate the severity of\nsecurity issues in the products they release. This document contains guidelines\nfor how to rate these issues. Check out our\n[security release management page](https://www.chromium.org/Home/chromium-security/security-release-management)\nfor guidance on how to release fixes based on severity.\n\nAny significant mitigating factors will generally reduce an issue's severity by one or\nmore levels:\n* Not web accessible, reliant solely on direct UI interaction to trigger.\n* Unusual or unlikely user interaction will normally reduce severity by one\n  level. This means interaction which may sometimes occur, but would not be\n  typical of an average user engaging with Chrome or a particular feature in\n  Chrome, nor could a user be easily convinced to perform by a persuasive web page.\n* Requiring profile destruction or browser shutdown will normally reduce\n  severity by one level.\n* [MiraclePtr protection](#TOC-MiraclePtr)\n\nBugs that require implausible interaction, interactions a user would not\nrealistically be convinced to perform, will generally be downgraded to a\nfunctional bug and not considered a security bug.\n\nConversely, we do not consider it a mitigating factor if a vulnerability applies\nonly to a particular group of users. For instance, a Critical vulnerability is\nstill considered Critical even if it applies only to Linux or to those users\nrunning with accessibility features enabled.\n\nAlso note that most crashes do not indicate vulnerabilities. Chromium is designed\nto crash in a controlled manner (e.g., with a ```__debugBreak```) when memory is\nexhausted or in other exceptional circumstances.\n\n\n## Critical severity (S0) {#TOC-Critical-severity}\n\nCritical severity (S0) issues allow an attacker to read or write arbitrary\nresources (including but not limited to the file system, registry, network,\netc.) on the underlying platform, with the user's full privileges.\n\nThey are normally assigned Priority **P0** and assigned to the current stable\nmilestone (or earliest milestone affected). For critical severity bugs,\n[SheriffBot](https://www.chromium.org/issue-tracking/autotriage) will\nautomatically assign the milestone.\n\n**For critical severity (S0) vulnerabilities, we aim to deploy the patch to all\nChrome users in under 30 days.**\n\nCritical vulnerability details may be made public in 60 days,\nin accordance with Google's general [vulnerability disclosure recommendations](https://security.googleblog.com/2010/07/rebooting-responsible-disclosure-focus.html),\nor [faster (7 days)](https://security.googleblog.com/2013/05/disclosure-timeline-for-vulnerabilities.html)\nif there is evidence of active exploitation.\n\nExample bugs:\n\n* Memory corruption in the browser process ([319125](https://crbug.com/319125#c10)).\n* Memory corruption in an unsandboxed GPU process when it is reachable directly from web\n  content without compromising the renderer.\n  ([1420130](https://crbug.com/1420130), [1427865](https://crbug.com/1427865)).\n  ([on some platforms we consider the GPU process 'sandboxed'](../../docs/security/process-sandboxes-by-platform.md)).\n* Exploit chains made up of multiple bugs that can lead to code execution\n  outside of the sandbox ([416449](https://crbug.com/416449)).\n* A bug that enables web content to read local files\n  ([962500](https://crbug.com/962500)).\n\nNote that the individual bugs that make up the chain will have lower severity\nratings.\n\n## High severity (S1) {#TOC-High-severity}\n\nHigh severity (S1) vulnerabilities allow an attacker to execute code in the context\nof, or otherwise impersonate other origins or read cross-origin data.\nBugs which would normally be\ncritical severity with unusual mitigating factors may be rated as high severity.\nFor example, renderer sandbox escapes fall into this category as their impact is\nthat of a critical severity bug, but they require the precondition of a\ncompromised renderer. (Bugs which involve using [MojoJS](../../mojo/public/js/README.md)\nto trigger an exploitable browser process crash usually fall into this category).\nAnother example are bugs that result in memory corruption in the browser\nprocess, which would normally be critical severity, but require browser shutdown\nor profile destruction, which would lower these issues to high severity. A\nbug with the precondition of browser shutdown or profile destruction should be\nconsidered to have a maximum severity of high and could potentially be\nreduced by other mitigating factors.\n\nThey are normally assigned Priority **P1** and assigned to the current stable\nmilestone (or earliest milestone affected). For high severity bugs,\n[SheriffBot](https://www.chromium.org/issue-tracking/autotriage) will\nautomatically assign the milestone.\n\n**For high severity (S1) vulnerabilities, we aim to deploy the patch to all\nChrome users in under 60 days.**\n\nExample bugs:\n\n* A bug that allows full circumvention of the same origin policy. Universal XSS\nbugs fall into this category, as they allow script execution in the context of\nan arbitrary origin ([534923](https://crbug.com/534923)).\n* A bug that allows arbitrary code execution within the confines of the sandbox,\nsuch as memory corruption in the renderer process\n([570427](https://crbug.com/570427), [468936](https://crbug.com/468936)).\n* Complete control over the apparent origin in the omnibox\n([76666](https://crbug.com/76666)).\n* Memory corruption in the browser or another high privileged process (e.g. a\n  GPU or network process on a [platform where they're not sandboxed](../../docs/security/process-sandboxes-by-platform.md)),\n  that can only be triggered from a compromised renderer,\n  leading to a sandbox escape ([1393177](https://crbug.com/1393177),\n  [1421268](crbug.com/1421268)).\n* Kernel memory corruption that could be used as a sandbox escape from a\ncompromised renderer ([377392](https://crbug.com/377392)).\n* Memory corruption in the browser or another high privileged process (e.g.\n  GPU or network process on a [platform where they're not sandboxed](../../docs/security/process-sandboxes-by-platform.md))\n  that requires specific user interaction, such as granting a permission ([455735](https://crbug.com/455735)).\n* Site Isolation bypasses:\n    - Cross-site execution contexts unexpectedly sharing a renderer process\n      ([863069](https://crbug.com/863069), [886976](https://crbug.com/886976)).\n    - Cross-site data disclosure\n      ([917668](https://crbug.com/917668), [927849](https://crbug.com/927849)).\n\n\n## Medium severity (S2) {#TOC-Medium-severity}\n\nMedium severity (S2) bugs allow attackers to read or modify limited amounts of\ninformation, or are not harmful on their own but potentially harmful when\ncombined with other bugs. This includes information leaks that could be useful\nin potential memory corruption exploits, or exposure of sensitive user\ninformation that an attacker can exfiltrate. Bugs that would normally be rated\nat a higher severity level with unusual mitigating factors may be rated as\nmedium severity.\n\nThey are normally assigned Priority **P1** and assigned to the current stable\nmilestone (or earliest milestone affected). If the fix seems too complicated to\nmerge to the current stable milestone, they may be assigned to the next stable\nmilestone.\n\nExample bugs:\n\n* An out-of-bounds read in a renderer process\n([281480](https://crbug.com/281480)).\n* An uninitialized memory read in the browser process where the values are\npassed to a compromised renderer via IPC ([469151](https://crbug.com/469151)).\n* Memory corruption that requires a specific extension to be installed\n([313743](https://crbug.com/313743)).\n* Memory corruption in the browser process, triggered by a browser shutdown that\n  is not reliably triggered and/or is difficult to trigger ([1230513](https://crbug.com/1230513)).\n* Memory corruption in the browser process, requiring a non-standard flag and\n  user interaction ([1255332](https://crbug.com/1255332)).\n* An HSTS bypass ([461481](https://crbug.com/461481)).\n* A bypass of the same origin policy for pages that meet several preconditions\n([419383](https://crbug.com/419383)).\n* A bug that allows web content to tamper with trusted browser UI\n([550047](https://crbug.com/550047)).\n* A bug that reduces the effectiveness of the sandbox\n([338538](https://crbug.com/338538)).\n* A bug that allows arbitrary pages to bypass security interstitials\n([540949](https://crbug.com/540949)).\n* A bug that allows an attacker to reliably read or infer browsing history\n([381808](https://crbug.com/381808)).\n* An address bar spoof where only certain URLs can be displayed, or with other\nmitigating factors ([265221](https://crbug.com/265221)).\n* Memory corruption in a renderer process that requires specific user\ninteraction, such as dragging an object ([303772](https://crbug.com/303772)).\n\n\n## Low severity (S3) {#TOC-Low-severity}\n\nLow severity (S3) vulnerabilities are usually bugs that would normally be a\nhigher severity, but which have extreme mitigating factors or highly limited\nscope.\n\nThey are normally assigned Priority **P2**. Milestones can be assigned to low\nseverity bugs on a case-by-case basis, but they are not normally merged to\nstable or beta branches.\n\nExample bugs:\n\n* Bypass requirement for a user gesture ([256057](https://crbug.com/256057)).\n* Partial CSP bypass ([534570](https://crbug.com/534570)).\n* A limited extension permission bypass ([169632](https://crbug.com/169632)).\n* An uncontrolled single-byte out-of-bounds read\n([128163](https://crbug.com/128163)).\n\n## Priority for in the wild vulnerabilities {#TOC-itw-pri}\n\nIf there is evidence of a weaponized exploit or active exploitation in the wild,\nthe vulnerability is considered a P0 priority - regardless of the severity\nrating -with a SLO of 7 days or faster. Our goal is to release a fix in a\nStable channel update of Chrome as soon as possible.\n\n## Can't impact Chrome users by default {#TOC-No-impact}\n\nIf the bug can't impact Chrome users by default, this is denoted instead by\nthe **Security-Impact_None** hotlist (hotlistID: 5433277). See\n[the security labels document](security-labels.md#TOC-Security_Impact-None)\nfor more information. The bug should still have a severity set according\nto these guidelines.\n\n\n## Not a security bug {#TOC-Not-a-security-bug}\n\nThe [security FAQ](faq.md) covers many of the cases that we do not consider to\nbe security bugs, such as [denial of service](faq.md#TOC-Are-denial-of-service-issues-considered-security-bugs-)\nand, in particular, null pointer dereferences with consistent fixed offsets.\n\n\n## \"MiraclePtr\" protection against use-after-free {#TOC-MiraclePtr}\n\n[\"MiraclePtr\"](../../base/memory/raw_ptr.md) is a technology designed to\ndeterministically prevent exploitation of use-after-free bugs. Address\nsanitizer is aware of MiraclePtr and will report on whether a given\nuse-after-free bug is protected or not:\n\n```\nMiraclePtr Status: NOT PROTECTED\nNo raw_ptr<T> access to this region was detected prior to the crash.\n```\n\nor\n\n```\nMiraclePtr Status: PROTECTED\nThe crash occurred while a raw_ptr<T> object containing a dangling pointer was being dereferenced.\nMiraclePtr should make this crash non-exploitable in regular builds.\n```\n\nMiraclePtr is now active on all Chrome platforms in non-renderer processes as\nof 118 and on Fuchsia as of 128. Severity assessments are made with\nconsideration of all active release channels (Dev, Beta, Stable, and Extended Stable);\nBRP is now enabled in all active release channels.\n\nAs of 128, if a bug is marked `MiraclePtr Status:PROTECTED`, it is not\nconsidered a security issue. It should be converted to type:Bug and assigned to\nthe appropriate engineering team as functional issue.\n"
  },
  {
    "path": "security/service-worker-security-faq",
    "title": "Service Worker Security FAQ",
    "content": "# Service Worker Security FAQ\n\n[TOC]\n\nThis FAQ is specifically about service workers. Also see the [general security\nFAQ](faq.md).\n\nLike the general security FAQ, this document is a collaborative effort by many\nChromium developers. (rsesek, estark, falken, slightlyoff, jakearchibald, evn,\nraymes, ainslie, mek, lgarron, elawrence, kinuko, palmer, your name here...)\nLast updated 12 May 2017. If you see an error or have an additional question,\nand have a Chromium account, go ahead and fix it. If you don't have a Chromium\naccount, email security-dev@chromium.org for a fix.\n\n## Service Workers seem extremely risky! Why are they OK?\n\nService Workers (SW) are indeed powerful. They support compelling web\napplications that can run offline or with intermittent connectivity. You can\nedit documents, browse and buy from catalogs, send social media messages, write\nemail, etc. even in the subway! Service Workers can make the web platform more\nviable than ever before, enabling web apps to better compete with native apps\neven while essentially retaining the browse-to-use, sandboxed nature of the\n[Open Web Platform](https://www.w3.org/wiki/Open_Web_Platform) (OWP) that we all\nlove. The rest of this FAQ will explain how the SW designers and implementers\nhave mitigated the risks that necessarily come with this functionality.\n\nService Workers are a replacement for and an improvement on the legacy\n[Application Cache\nAPI](https://developer.mozilla.org/en-US/docs/Web/HTML/Using_the_application_cache),\nwhich has been available in the OWP for a very long time.\n\nFor more background on Service Workers, see [Service Workers\nExplained](https://github.com/w3c/ServiceWorker/blob/master/explainer.md).\n\n## Do Service Workers run in a sandbox?\n\nYes, SWs run in renderer processes. When Chrome starts a SW, it chooses a\nrenderer process that is associated with the SW’s origin. If one does not exist,\nthe browser creates a new one using a new\n[`SiteInstance`](https://cs.chromium.org/chromium/src/content/public/browser/site_instance.h)\nfor the origin.\n\n## What APIs can Service Workers access?\n\nThe [HTML specification partially enumerates the API surface available to\nWorkers](https://html.spec.whatwg.org/#apis-available-to-workers). See also\n[`Client`](https://developer.mozilla.org/en-US/docs/Web/API/Client), and\n[`ServiceWorkerGlobalScope`](https://w3c.github.io/ServiceWorker/#serviceworkerglobalscope-interface).\n(Note that SWs do not have access to synchronous APIs.)\n\nHowever, other web platform specifications can add new API surface. For example,\nthe Permissions API exposes a permissions attribute to workers. Generally, SWs\nhave access to a subset of the web platform APIs, although there are some\nWorker- and Service Worker-specific APIs that do not make sense for in-page\nJavaScript.\n\n(`[Service]WorkerGlobalScope` is of course not necessarily a strict subset of\n`Window`, and similarly `WorkerNavigator` is not necessarily a strict subset of\n`Navigator`. And the various SW events are of course only exposed to SWs.)\n\n## Do Service Workers obey the same-origin policy?\n\nService Worker registration specifies that [Service Workers must run in the same\norigin as their\ncallers](https://w3c.github.io/ServiceWorker/#register-algorithm).\n\nThe origin comparison for finding a Service Worker registration for a request is\n[specified](https://w3c.github.io/ServiceWorker/#scope-match-algorithm) to be to\nbe a longest-prefix match of serialized URLs, including their path. (E.g.\n`https://example.com/` != `https://example.com.evil.com/`.) This specification\ngap seems fragile to us, [and should be fixed to be specified and implemented as\nactual origin equality](https://github.com/w3c/ServiceWorker/issues/1118), but\ndoesn’t currently seem exploitable.\n\nOnly [Secure Contexts can register or use Service\nWorkers](https://w3c.github.io/webappsec-secure-contexts/#example-c52936fc).\n\nBecause SWs can call `importScripts` to import scripts (from any other origin),\nit is a good idea for site operators to set a Content-Security-Policy response\nheader on the ServiceWorker’s JavaScript response, instructing the browser what\nsources of script the origin considers trustworthy. That would reduce an XSS\nattacker’s ability to pull in their own code.\n\n## Do Service Workers live forever?\n\nThere are two concepts of “live” here. One is about the installed registration\nand one is about the running Service Worker thread.\n\nThe installed registration lasts indefinitely, similar to origin-scoped storage\nlike\n[IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API). The\nbrowser performs an update check after any navigation using the Service Worker,\ninvalidating the HTTP cache every 24 hours. Additionally, [browsers will\nrevalidate the HTTP cache for SW\nscripts](https://w3c.github.io/ServiceWorker/#dfn-use-cache) unless the site\nopts into using the cache.\n\nThe browser also performs an update check whenever the SW starts and\nperiodically while the worker is running, if it has not checked in the last 24\nhours (86,400 seconds, [as specified in the Handle Functional Event\nalgorithm](https://w3c.github.io/ServiceWorker/#handle-functional-event-algorithm)).\n\nThe browser can terminate a running SW thread at almost any time. Chrome\nterminates a SW if the SW has been idle for 30 seconds. Chrome also detects\nlong-running workers and terminates them. It does this if an event takes more\nthan 5 minutes to settle, or if the worker is busy running synchronous\nJavaScript and does not respond to a ping within 30 seconds. When a SW is not\nrunning, Developer Tools and chrome://serviceworker-internals show its status as\nSTOPPED.\n\n## How can I see Service Workers in Chrome?\n\nYou can see them in the **Service Workers** field in the **Application** tab of\n**Developer Tools**. You can also look at\n[chrome://serviceworker-internals](chrome://serviceworker-internals).\n\n## Do Service Workers keep running after I close the tab?\n\nIf an origin has any Service Workers running, each worker will be shut down soon\nafter it processes the last event. Events that can keep a worker alive include\npush notifications. (Note that the [push notifications will trigger a\nuser-visible notification if the SW does not create\none](https://cs.chromium.org/chromium/src/chrome/browser/push_messaging/push_messaging_notification_manager.cc?type=cs&l=270),\nand they also require the person to grant the origin permission in a prompt. you\ncan see that in action in this [push notifications demo\napp](https://gauntface.github.io/simple-push-demo/).)\n\n## Can attackers use Service Workers to trigger attacks developed after SW registration?\n\nFor example, could an attacker convince users to visit a malicious website, then\nwait for (e.g.) a V8 bug to show up in Chrome's repository, then write an\nexploit, and then somehow run that exploit on the machines of everyone who\nvisited the malicious website in the last month or so?\n\nWithout explicit permission from the user, the browser won't let the SW poll\nfor/receive any push notification events the attacker's server may (try to)\nsend, and hence the SW won't get a chance to handle the events.\n\nSimilarly, you might imagine a SW that tries to use `importScripts` to\nperiodically (re-)load `maybe-v8-payload.js`. But, the SW would only get to do\nthat as part of an event handler. And if the SW isn't getting any events\n(because the person is not browsing or navigating to attacker.com, and because\nthe person never granted attacker.com push notification permission), then it\nwill never get to run its event handlers, and so again the SW won't get a chance\nto attack.\n\nIf the person is currently browsing attacker.com, then the attacker doesn't gain\nany additional attack benefit from a Service Worker. They can just `<script\nsrc=\"maybe-v8-payload.js\">` as usual, from the in-page JavaScript.\n\n## If a site has an XSS vulnerability, can the attacker permanently compromise that origin for me?\n\nAn XSS attacker can indeed register an evil SW. As before SWs, XSS is a very\npowerful mode of attack on a web origin. To mitigate the risk that an XSS attack\nwill register a malicious SW, the browser requires that the SW registration URL\ncome from the origin itself. Thus, to use an XSS attack to register a malicious\nSW, the attacker needs the additional capability to host their own scripts on\nthe server.\n\nHere is another exploit scenario: If the page with an XSS vulnerability also has\na JSONP endpoint, the attacker could use it to (1) bypass CSP; (2) register a\nSW; and (3) call `importScripts` to import a third-party script to persist until\n\n*    the site operators detect and remediates the issue; and\n*    users navigate to the site again while online.\n\nIn an XSS situation, the 24 hour cache directive limit ensures that a malicious\nor compromised SW will outlive a fix to the XSS vulnerability by a maximum of 24\nhours (assuming the client is online). Site operators can shrink the window of\nvulnerability by setting lower TTLs on SW scripts. We also encourage developers\nto [build a kill-switch\nSW](https://stackoverflow.com/questions/33986976/how-can-i-remove-a-buggy-service-worker-or-implement-a-kill-switch/38980776#38980776).\n\nThe right cleanup strategy (for this and other issues) is\n[Clear-Site-Data](https://www.w3.org/TR/clear-site-data/).\n\nAdditionally, site operators should ignore (e.g. respond with `400 Bad Request`)\nrequests that have the Service-Worker request header for domains or paths that\nthe server doesn’t expect to be serving SW scripts for.\n\n## Can sites opt out of Service Workers?\n\nSites that do not intend to serve Service Workers on particular domains or paths\ncan check for and explicitly reject requests for worker scripts, by checking for\n[the Service-Worker request\nheader](https://w3c.github.io/ServiceWorker/#service-worker-script-request).\n\n## How many SWs can an origin, or Chrome itself, spawn?\n\nThe current specification and the current implementation in Chrome do not define\nany limits.\n\n## Can attackers 'hide' Service Worker scripts in JPEGs or other non-script MIME types?\n\nCan an attacker upload (for example) JPEG files to a site that supports the\ncapability, and then use the uploaded files as SW scripts?\n\nThe SW specification and implementation require that a SW script have the right\nJavaScript MIME type. (Additionally, as noted elsewhere in this document, server\noperators should reject SW script requests except for the exact endpoints they\nintend to serve SW scripts from.)\n\n## Can iframes register Service Workers?\n\nYes, if and only if they are themselves secure contexts. [By\ndefinition](https://w3c.github.io/webappsec-secure-contexts/#examples-framed),\nthat means that they must be nested inside secure contexts, all the way up to\nthe top-level document.\n\nService Workers registered by third-party iframes are partitioned by top-level\nsite in addition to the origin of the third-party iframe, even if third-party\ncookies are blocked. See the\n[Partitioning Storage, Service Workers, and Communication APIs explainer](https://github.com/wanderview/quota-storage-partitioning/blob/main/explainer.md)\nfor more information.\n\n## Why doesn’t Chrome prompt the user before registering a Service Worker?\n\nThe Chrome Team generally prefers to ask people about things that are\nprivacy-relevant, using nouns and verbs that are simple and precise (camera,\nmic, geo-location, and so on). But we avoid asking questions about resource-use\n(caching, persistence, CPU, and so on). We’re better prepared to make those\ntypes of resource decisions automatically. (Consider, for example, that the HTTP\ncache, and even [Google\nGears](https://en.wikipedia.org/wiki/Gears_(software)) also do not/did not\nprompt the user.)\n\n[An informal study by Chrome team members Rebecca Rolfe, Ben Wells, and Raymes\nKhoury](https://docs.google.com/presentation/d/1suzMhtvMtA11jxPUdH1jL1oPh-82rTymCnslgR3ehEE/edit#slide=id.p)\nsuggests that people do not generally have sufficient context to understand\npermission requests triggered by API calls from origins in iframes. It seems\nreasonable that people would similarly lack the context to understand requests\nfrom Service Workers.\n\n## What if I don't want *any* SWs?\n\nClearing browser data (CBD; the **Clear browsing data...** button in\n**Settings** or chrome://settings/clearBrowserData) also deletes SWs. You can\nverify that by following this test procedure:\n\n1. Visit https://gauntface.github.io/simple-push-demo/\n1. In a second tab, visit chrome://serviceworker-internals/ to see the ACTIVATED\n   and RUNNING SW\n   *    Note that the origin/the origin's SW cannot actually send any push notifications\n        until you grant it that permission\n1. In a third tab, go to chrome://settings/clearBrowserData to clear browsing data;\n   clear it by clicking **Clear browsing data**\n1. Reload chrome://serviceworker-internals/ to see that the SW's status is now\n   REDUNDANT and STOPPED\n1. Close the Simple Push Demo tab\n1. Reload chrome://serviceworker-internals/ to see that the SW is now gone\n\nYou can also remove individual SW registrations with\nchrome://serviceworker-internals/.\n\nAnother way to avoid SWs is to use one of the browsers that don't (yet) support\nSWs. But, eventually, the Open Web Platform will continue to evolve into a\npowerful, useful platform supporting applications that are [secure, linkable,\nindexable, composable, and ephemeral](https://paul.kinlan.me/slice-the-web/).\nYes, SWs make web apps somewhat less ephemeral, but we believe the increased\napplicability of the OWP is worth it.\n\nBrowser vendors are committed to ensuring the security of the OWP improves even\nas we give it new capabilities. This process happens in the open, in fora like\n[W3C Technical Architecture Group](https://www.w3.org/2001/tag/), [W3C’s Web\nPlatform Incubator Community Group](https://www.w3.org/blog/2015/07/wicg/), and\n[blink-dev@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/blink-dev).\nSecurity and privacy reviews are part of the process and we invite knowledgeable\nexperts to participate in those open fora.\n\n## What are some SW best practices for site operators?\n\n*    [Build a kill-switch SW](https://stackoverflow.com/questions/33986976/how-can-i-remove-a-buggy-service-worker-or-implement-a-kill-switch/38980776#38980776).\n*    Use [Clear-Site-Data](https://www.w3.org/TR/clear-site-data/).\n*    Be aware of the need for longer session lifetimes, since clients may go\n     offline and SWs might need to POST cached requests after coming back\n     online. [Here is one way to handle\n     that](https://developers.google.com/web/updates/2016/06/2-cookie-handoff).\n\n## What SW bugs would qualify for a bounty under [Chrome’s VRP](https://www.google.com/about/appsecurity/chrome-rewards/index.html)?\n\nIf you could break one or more of the security assertions we make in this FAQ,\nthat would be potentially rewardable under the Vulnerability Rewards Program\n(VRP). Here is a non-exhaustive list of examples:\n\n*    Over-long registration/lifetime (e.g. a SW able to run or stay alive even\n     without incoming events to handle)\n*    Same-origin bypass or off-origin SW registration Access to APIs that\n*    require prompts, choosers, or permissions, without\n     permission having been granted to the origin\n     *    Geolocation\n     *    Hardware sensors\n     *    Microphone, camera, media devices\n     *    USB, Bluetooth\n\nHere is [a list of historical SW security\nbugs](https://bugs.chromium.org/p/chromium/issues/list?can=1&q=Type%3DBug-Security+serviceworker&colspec=ID+Pri+M+Stars+ReleaseBlock+Component+Status+Owner+Summary+OS+Modified&x=m&y=releaseblock&cells=ids)\nin Chromium’s bug tracker.\n\nIf you believe you have found a bug in the SW specification, please [file a new\nChromium bug using the Security\ntemplate](https://bugs.chromium.org/p/chromium/issues/entry?template=Security%20Bug).\nIt’s a good idea to file bugs with all browser vendors that implement the buggy\nsection of the spec.\n\nIf you believe you have found a bug in Chrome’s implementation of SW, please\n[file a new bug using the Security\ntemplate](https://bugs.chromium.org/p/chromium/issues/entry?template=Security%20Bug).\nThe Chrome Security Team will triage it within 1 or 2 business days. Good bug\nreports come with minimal test cases that demonstrate the problem!\n"
  },
  {
    "path": "security/security-model",
    "title": "Security Model",
    "content": "# Security Model\r\n\r\nChromium’s security architecture is layered to protect users from malicious content, compromised sites, and browser vulnerabilities. This article breaks down the key pieces of that model—from sandboxing to origin isolation to transport security—and points you to the source files where each is implemented.\r\n\r\n---\r\n\r\n## 1. Threat Model & Goals\r\n\r\n- **Threats Addressed**  \r\n  - Arbitrary code execution via renderer or plugin bugs  \r\n  - Cross-site data leakage (XSS, CSRF)  \r\n  - Network eavesdropping or man-in-the-middle attacks  \r\n- **Security Goals**  \r\n  1. **Isolation**: ensure that untrusted web content cannot read or write user data outside its scope.  \r\n  2. **Least Privilege**: run each component with minimal OS privileges.  \r\n  3. **Defense in Depth**: multiple overlapping safeguards (sandbox, site isolation, CSP).  \r\n\r\n---\r\n\r\n## 2. Origin & Same-Origin Policy\r\n\r\n- **Origin Definition**  \r\n  - A tuple of (scheme, host, port). Two pages may interact only if all three match.  \r\n- **Enforcement**  \r\n  - Implemented in Blink’s DOM bindings under `third_party/blink/renderer/bindings/`.  \r\n  - Checked in navigation, XHR/fetch, iframe embedding, `document.cookie`.  \r\n- **Key Files**  \r\n  - `security_origin.cc` (defines `SecurityOrigin` and policy checks)  \r\n  - `script_security.cc` (enforces JS‐level checks)\r\n\r\n---\r\n\r\n## 3. Site Isolation & Process-Per-Site\r\n\r\n- **Site-Per-Process**  \r\n  - By default, Chromium maps each “site” (origin) into its own renderer process.  \r\n  - Prevents a compromised renderer from poking into other sites’ memory.  \r\n- **Implementation**  \r\n  - Controlled by GN flag `site_per_process` in `content/browser/site_isolation/`.  \r\n  - `SiteIsolationPolicy` and `SiteInstance` classes coordinate process mapping.  \r\n- **Crash Containment**  \r\n  - Renderer crash shows only that tab’s error page; other tabs unaffected.\r\n\r\n---\r\n\r\n## 4. OS-Level Sandboxing\r\n\r\nChromium runs its renderers and helper processes in strict OS sandboxes:\r\n\r\n| Platform | Mechanism                                          | Source Location                           |\r\n|----------|----------------------------------------------------|-------------------------------------------|\r\n| **Windows** | Job objects + restricted tokens                 | `sandbox/win/`                            |\r\n| **Linux**   | setuid “chrome-sandbox” or seccomp‐bpf filters  | `sandbox/linux/`                          |\r\n| **macOS**   | Seatbelt profiles                                | `sandbox/mac/`                            |\r\n\r\n- **Sandbox Brokers**  \r\n  - A minimal “broker” process handles syscalls (e.g. DNS, font enumeration) on behalf of the sandboxed child.  \r\n- **Key Files**  \r\n  - `sandbox_init.cc`  \r\n  - Platform‐specific policy headers under `sandbox/{win,linux,mac}/`.\r\n\r\n---\r\n\r\n## 5. Permissions & Feature Policy\r\n\r\n- **Permissions API**  \r\n  - JS interfaces for `geolocation`, `notifications`, `camera`, etc.  \r\n  - Backed by `PermissionController` in `content/browser/permissions/`.  \r\n- **Feature Policy / Permissions Policy**  \r\n  - Page‐level opt-in/opt-out controls which APIs iframes can use.  \r\n  - Defined via `FeaturePolicy` in Blink (`third_party/blink/renderer/core/feature_policy/`).  \r\n- **UI Prompt**  \r\n  - Browser UI for granting/denying lives in `chrome/browser/ui/permission_bubble/`.\r\n\r\n---\r\n\r\n## 6. Content Security Policy (CSP)\r\n\r\n- **CSP Headers**  \r\n  - Enforced by Blink’s `CSPContext` in `third_party/blink/renderer/core/loader/`.  \r\n  - Prevents inline script, remote code, or framing per site’s policy.  \r\n- **Reporting**  \r\n  - Violation reports sent via `ReportSender` to configured endpoints.  \r\n\r\n---\r\n\r\n## 7. Transport Security & Certificate Validation\r\n\r\n- **Trust Store**  \r\n  - Uses OS‐provided roots on Windows/macOS; Mozilla’s on Linux.  \r\n  - Managed in `net/cert/` (`cert_verifier.cc`, `root_store.cc`).  \r\n- **HSTS / HPKP**  \r\n  - HSTS enforced by `TransportSecurityState`.  \r\n  - HPKP deprecated but still present in some code paths.  \r\n- **OCSP & CRL Sets**  \r\n  - Stapled OCSP responses validated in `net/ocsp/`.  \r\n  - Chrome uses “CRLSet” updates via Safe Browsing service.\r\n\r\n---\r\n\r\n## 8. Safe Browsing & Malware Protection\r\n\r\n- **Phishing & Malware Lists**  \r\n  - Maintained by Google; downloaded to browser periodically.  \r\n  - Checks happen in `safe_browsing/` under `chrome/browser/`.  \r\n- **Interstitial UI**  \r\n  - Block pages with clear warnings.  \r\n  - Code in `chrome/browser/safe_browsing/`.  \r\n\r\n---\r\n\r\n## 9. Extension Security\r\n\r\n- **Isolated Worlds**  \r\n  - Content scripts run in separate V8 contexts with limited DOM access.  \r\n- **Permission Model**  \r\n  - Declared in `manifest.json`, enforced by `ExtensionPermission` classes.  \r\n- **Native Messaging**  \r\n  - Host apps communicate via a JSON‐over‐stdin bridge with strict path restrictions.\r\n\r\n---\r\n\r\n## 10. Developer Tools & Auditing\r\n\r\n- **chrome://security** (future) and `chrome://sandbox` pages  \r\n- **chrome://webrtc-internals** for inspecting WebRTC security parameters  \r\n- **Auditing Tools**  \r\n  - `chrome://policy` for Enterprise policy enforcement  \r\n  - Tracing categories: `SECURITY`, `NETWORK_SECURITY` in `chrome://tracing`\r\n\r\n---\r\n\r\n## 11. Testing & Hardening\r\n\r\n- **Unit & Integration Tests**  \r\n  - `content/browser/site_isolation_browsertest`  \r\n  - `security_interstitial_browsertest`  \r\n  - `sandbox_browsertest`  \r\n- **Fuzzing**  \r\n  - OSS-Fuzz integration for renderer, V8, PDFium, libogg, etc.  \r\n\r\n---\r\n\r\n## 12. Next Steps\r\n\r\n1. Read **Debugging → Debugging Tools** to learn how to trace sandbox violations.  \r\n2. Explore **Modules → Networking (HTTP)** for TLS handshake internals.  \r\n3. Dive into **Architecture → Process Model** to see how sandboxed processes communicate.  \r\n\r\n---\r\n"
  },
  {
    "path": "security/security-labels",
    "title": "Security Fields, Hotlists, and Issue Access / Visibility",
    "content": "# Security Fields, Hotlists, and Issue Access / Visibility\n\n[TOC]\n\nBug database labels are used very heavily for security bugs. We rely on the\nlabels being correct for a variety of reasons, including driving fixing efforts,\ndriving release management efforts (merges and release notes) and also\nhistorical queries and data mining.\n\nBecause of the extent to which we rely on labels, it is an important part of the\nSecurity Sheriff duty to ensure that all security bugs are correctly tagged and\nmanaged. But even if you are not the Security Shepherd, please fix any labeling\nerrors you happen upon.\n\nAny issue that relates to security should have one of the following:\n\n* **Security** Component Tag: Features that are related to security.\n* **Type=Vulnerability**: Designates a security vulnerability that impacts\nusers. This label should not be used for new features that relate to security,\nor general remediation/refactoring ideas. (Use the **Security** Component Tag\nfor that.)\n\n## Fields and Hotlists Relevant For Any **Type=Vulnerability**\n\n* **Security_Severity-**{**Critical (S0)**, **High (S1)**, **Medium (S2)**,\n**Low(S3)**, **Unknown / Not Yet Assessed (S4)**}: Designates the severity\nof a vulnerability according to our\n[severity guidelines](severity-guidelines.md).\n* **Priority: P#**: Priority should generally match Severity (but should be\n  higher if there is evidence of active exploitation):\n  * **Security_Severity-Critical**: **P0**.\n  * **High** and **Medium**: **P1**.\n  * **Low**: **P2**.\n* **Found In: MMM#**: Designates which milestones of Chrome are\nimpacted by the bug. Multiple milestones may be set in the Found In field,\nbut the most important one is the earliest affected milestone. See\n[ChromiumDash](https://chromiumdash.appspot.com/releases?platform=Windows) for\ncurrent releases.\n* **Security_Impact-**{**Head**, **Beta**, **Stable**, **Extended**, **None**}\n  hotlists: Derived from milestones set in the **Found In** field,, this\n  hotlist specifies the earliest affected release channel. Should not normally\n  be set by humans, except in the case of **Security_Impact-None**\n  (hotlistID: 5433277) which means that the bug is in a disabled feature, or\n  otherwise doesn't impact Chrome: see the section below for more details.\n    * Note that **Severity** should still be set to the appropriate Severity\n    (S0-S3) for **Security_Impact-None** issues, as if the feature were enabled\n    or the code reachable.\n* **Issue access level & collaborator groups**{**collaborators=\n  security@chromium.org**, **collaborators=security-notify@chromium.org**,\n  **Limited Visibility + Google**, **SecurityEmbargo hotlist**}: settings\n  that restrict access to the bug. Meaning and usage guidelines are as follows:\n  * **Issue Access level: Limited Visibility + collaborator group =\n    security@chromium.org**: Restricts access to members of\n    *security@chromium.org*. This is the default that should be used for general\n    security bugs that aren't sensitive otherwise.\n  * **Issue Access level: Limited Visibility + collaborator group =\n    security-notify@chromium.org**: Restricts access to members of\n    *security-notify@chromium.org*, which includes external parties who ship\n    Chromium-based products and who need to know about available bug fixes.\n    *security@chromium.org* is a member of that group so the former is a\n    superset of the latter.\n    **Collaborator group = security-notify@chromium.org** is not suitable for\n    sensitive bugs.\n  * **Issue Access level: Limited Visibility + collaborator group =\n    security-notify@webrtc.org**: As above, but additionally give access to\n    *security-notify@webrtc.org*, a community of downstream WebRTC embedders.\n  * **Issue Access level: Limited Visibility + Googlers**: Restricts access to\n    users that are Google employees (but also via their *chromium.org*\n    accounts). This should be used for bugs that aren't OK for external\n    contributors to see (even if we trust them with security work), for\n    example due to:\n      * legal reasons (bug affects a partner Google is under NDA and the\n        information is subject to that)\n      * the bug affecting more Google products than Chrome and Chrome OS\n  * **SecurityEmbargo hotlist** (hotlistID: 5432549): Keeps issues already\n    set with Issue Access level: Limited Visibility + collaborator group =\n    *security@chromium.org* from being automatically de-restricted by Blintz\n    and keeping the bug from being opened for public disclosure. Use this if\n    the bug in question is subject to disclosure decisions made externally,\n    such as:\n      * We receive advance notice of security bugs from an upstream open source\n        project or Google partner and they organize a coordinated disclosure\n        process. We'd remove the restriction hotlist if/when the embargo gets\n        lifted.\n      * The reporter indicates a preference to remain anonymous and the bug\n        history would give away the reporter's identity (if they file using an\n        anonymous account, this doesn't apply).\n\n* **reward-**{**topanel**, **unpaid**, **na**, **inprocess**, _#_} hotlists:\n  Hotlists used for tracking bugs nominated for our [Vulnerability Reward\nProgram](https://www.chromium.org/Home/chromium-security/vulnerability-rewards-program).\n* **reward_to-external** (hotlistID: 5432589): If a bug is filed by a Google or\nChromium user on behalf of an external party, use **reward_to-external** to\nensure the report is still properly credited to the external reporter in the\nrelease notes. IF, however, _the reporter is an individual with an email\naddress_ you should set the **Reporter** field to reflect the email address\nof the external reporter. If the reporter was an organization or entity with a\nspecific email address, then do not alter the **Reporter** field and use the\n**reward-to_external** hotlist. Despite its name, you should add\nthis label whether or not the reporter is in scope for the vulnerability rewards\nprogram, because external reports are credited in the release notes irrespective.\n* **M-#** field: Target milestone for the fix.\n* Chromium 'Component Tags': For bugs filed as **Type=Vulnerability**, we also\nwant to track which Chromium component(s) the bug is in.\n* **ReleaseBlock field = Stable**: When we find a security bug regression that\nhas not yet shipped to stable, we use this label to try and prevent the security\nregression from ever affecting users of the Stable channel.\n* **OS-**{**Chrome**, **Linux**, **Windows**, ...}: Denotes which operating\nsystems are affected.\n* **Merge: field**{**Request-?**, **Approved-?**, **Merged-?**}: Security fixes\nare frequently merged to earlier release branches.\n* **Security Release: #-M###**: Denotes which exact patch a security fix made\nit into. This is more fine-grained than the **M-#** label. **Release-0-M105**\ndenotes the initial release of an M105 release to Stable channel.\n* **CVE field: -####-####**: For security bugs that get assigned a CVE, we\nupdate the CVE field for appropriate bug(s) with the CVE number  for easy\nsearching.\n**Type=Vulnerability** bugs should always have **Severity of S0-S3**,\n**Found In - ### set**, **Security_Impact** hotlist, **OS**, **Priority**,\n**M**, **Component Tags**, and an **Assigner** set.\n\n### When to use the  Security_Impact-None hotlist {#TOC-Security-Impact-None}\n\n**Security_Impact-None** says that the bug can't affect any users running the\ndefault configuration of Chrome. It's most commonly used for cases where\ncode is entirely disabled or absent in the production build.\n\nOther cases where it's OK to set **Security_Impact-None**:\n\n* The impacted code runs behind a feature flag which is *disabled by default*,\n  and the field trial configuration has not been switched on.\n* The impacted code only runs behind a command-line flag or `chrome://flags`\n  entry. (In particular, if a bug can only affect those who have\n  set `#enable-experimental-web-platform-features`, it is **Security_Impact-None**.\n* It's a V8 feature behind flags such as `--future`, `--es-staging` or\n  `--wasm-staging` or other experimental flags that are disabled by default.\n\nCases where it's *not* OK to set **Security_Impact-None**:\n\n* Features enabled via normal UI or settings which users might happen across\n  in normal usage. For instance, accessibility features and the Chrome Labs\n  experimental features accessible from the toolbar.\n* Origin trials. Origin trials are only active on some websites, but the\n  affected code does run for Chrome users with the default Chrome configuration.\n* The impacted code runs behind a feature flag which is *enabled by default*,\n  even if that field trial configuration has been switched off. That's because\n  the code may be active for devices which can't access the field trial\n  configuration service.\n* The feature is turned on only for a small percent of users, e.g. 1%.\n* Feature or flag checks are done somewhere that the attacker could influence.\n  For example a privilege escalation from a lower-privileged process\n  (e.g. renderer) to a higher-privileged process (e.g. browser)\n  assumes that the lower-privileged process is already compromised. The\n  attacker could overwrite memory for any feature checks performed within\n  that lower-privileged process; the bug only qualifies as impact **None**\n  if checks are performed in the higher-privileged process.\n* If a bug involves a patch to a renderer or use of a flag to turn on\n  [MojoJS](../../mojo/public/js/README.md)\n  this may mean it's a simulation of a compromised renderer and the\n  bug may still be a valid [sandbox escape\n  bug](severity-guidelines.md#TOC-High-severity).\n\nIt's important to get this right, because this label influences how rapidly\nwe merge and release the fix. Ask for help if you're not sure.\n\nSome **Security_Impact-None** bugs may still be subject to VRP rewards, if\nthose bugs are found in code that we're likely to enable in the future.\n\n### OS Field\n\nIt can be hard to know which OS(s) a bug applies to. Here are some guidelines:\n\n* Blink is used on all platforms except iOS. A (say) UAF in Blink is probably\nnot particular to whatever platform it was found on; it's probably applicable\nto all.\n* The same is true of Skia, and the net/ code.\n* If the bug is in a file named `foo_{win,linux,mac,...}.cc`, it's specific to\nthe named platform.\n* Java code is particular to Android.\n* Objective-C++ (`foo.mm`) is particular to macOS and iOS. (But note that most\nof our Objective-C++ is particular to macOS *or* iOS. You can usually tell by\nthe pathname.)\n* Views code (e.g. `ui/message_center/views`) is used on Windows, Linux, Chrome\nOS, and perhaps Fuchsia (?). Views for macOS is increasingly a thing, but Cocoa\ncode (e.g. `ui/message_center/cocoa`) is particular to macOS.\n\n## After the bug is fixed: Merge field labels {#TOC-Merge-labels}\n\nOnce you've landed a complete fix for a security bug, please immediately\nmark the bug as Fixed. Do not request merges: Sheriffbot will request\nappropriate merges to beta or stable according to our guidelines.\nHowever, it is really helpful if you comment upon any unusual stability or\ncompatibility risks of merging.\n\n(Some Chromium teams traditionally deal with merges _before_ marking bugs as\nFixed. Please don't do that for security bugs.)\n\nPlease take the opportunity to consider whether there are any variants\nor related problems. It's very common for attackers to tweak working attack code\nto exploit a similar situation elsewhere. If you've even the remotest thought\nthat there _might_ be equivalent patterns or variants elsewhere, file a bug\nwith type=Bug-Security. It can be nearly blank. The important thing is to record\nthe fact that something may need doing.\n\n## Sheriffbot automation\n\nSecurity labels guide the actions taken by\n[Blintz](https://www.chromium.org/issue-tracking/autotriage). The source of\ntruth for the actual rule set is\n[go/chrome-blintz-source](https://goto.google.com/chrome-blintz-source) (sorry,\nGoogle employees only). The motivation behind these rules is to help automate\nthe security bug life cycle so security shepherds and security engineers in\ngeneral spend less time updating bugs and can do more useful work instead.\n\nThe following sections describe the current set of rules relevant to security\nbugs. The list below only describes rules that change the labels described\nabove. There are additional rules for sending nag messages and janitorial tasks;\ncheck the [Chrome blintz source](https://goto.google.com/chrome-blintz-source)\nfor details.\n\n### Remove Invalid **Release-#** Field Entries\n\nOnly bugs that affect stable should carry a release designator, this rule\nremoves release designators that are set on bugs not affecting stable.\n\n### Remove Invalid **Security_Impact-X** Hotlists\n\nEach bug should be on exactly one **Security_Impact-X** and it should be one of\nthe 5 valid Security_Impact hotlists (None, Extended, Stable, Beta, Head). This\nrule a bug from any invalid and excess Security_Impact hotlists.\n\n### Adjust **Security_Impact-X** Hotlists to match Found In\n\nBased on **Found In #** milestone set in the field this rule assigns\ncorresponding **Security_Impact-X** hotlists if they are incorrect or absent.\n**Security_Impact-None** is never changed.\n\n### Update **M-#** Field\n\nBugs that are set with milestones earlier than the current milestone will\nbe updated to set the field to the current milestone and\n**Security_Impact-Extended**.\n\nBugs that carry a **Security_Impact-X** hotlist but are missing a milestone\nfield being set will be updated so that the **M-#** field reflects the\ncorresponding to the respective milestone.\n\n### Set **ReleaseBlock field** For Regressions\n\nIf there's a high or medium severity security regression in beta or ToT, update\nthe **ReleaseBlock** field to Stable to prevent that regression from being\nshipped to users.\n\nSimilarly, critical security regressions are marked **ReleaseBlock: Beta**.\n\n### Adjust **Priority P#** To Match Severity\n\nAdjust **Priority P#** according to the priority rules for severity labels described\nabove. If there is evidence of active exploitation then a higher priority should\nbe used.\n\n### Drop **Visibility Group Restrictions** From Old, Fixed Bugs for Disclosure\n\nRemove **security@chromium.org, **security-notify@chromium.org** and\n**security-notify@webrtc.org** from **Collaborator Groups** and Update **Issue\nAccess level to Default Visibility** for security bugs that have been closed\n(Fixed, Verified, Duplicate, WontFix,Invalid) more than 14 weeks ago, making\nthem publicly accessible. The idea here is that by 14 weeks, important security\nfixes will have shipped in a Stable channel update and allowing users time to\nupdate.\n\n### Set **security-notify@chromium.org as Collaborator** On Fixed Bugs\n\nWhile **Issue Access level** remains **Limited Visibility** removes\n**security@chromium.org** as Collaborator field / Add Collaborator Groups\nand replaces updates with **security-notify@chromium.org** for fixed security\nbugs. Rationale is that while fixed bugs are generally not intended to become\npublic immediately, we'd like to give access to external parties depending on\nChromium via *security-notify@chromium.org*.\n(Collaborator for WebRTC bugs is instead updated to\n**security-notify@webrtc.org**).\n\n### Update **Merge Field with Request-X** For Fixed Bugs\n\nFixed security bugs that affect stable or beta and are critical or high severity\nwill automatically trigger a merge request for the current beta branch, and\nperhaps stable if also impacted.\n\n### Drop **X from ReleaseBlock field** For **Security_Impact-None** Bugs\n\nNo need to stop a release if the bug doesn't have any consequences.\n\n## An Example\n\nGiven the importance and volume of field, hotlists, and visiblity settings,\nan example might be useful.\n\n1. An external researcher files a security bug, with a repro that demonstrates\nmemory corruption against the latest (e.g.) M123 dev channel. The will present\nas **Type=Vulnerability** and **Visibility / Issue Access level** will be set\nto **Limited Visibility** with **security@chromium.org** set as\n**Collaborator**.\n2. The Security Shepherd triages the issue and uses ClusterFuzz to confirm\nthat the bug is a novel and dangerous-looking buffer overflow in the renderer\nprocess. ClusterFuzz also confirms that all current releases are affected. Since\nM121 is the current Stable release, M120 is Extended Stable, and M122 is in\nBeta, we update the **Found In field** to **120** to reflect Extended Stable as\nthe earliest / oldest affected release channel. The severity of a buffer\noverflow in a renderer implies **High (S1) Severity** and **P1 Priority**. Any\nexternal report for a confirmed vulnerability needs **reward-topanel**. Blintz\nwill usually add it automatically once the bug is fixed. The stack trace\nprovided by ClusterFuzz suggests that the bug is in the component **Blink>DOM**,\nand such bugs should be labeled as applying to all OSs except iOS (where Blink\nis not used): **OS-**{**Linux**, **Windows**, **Android**, **Chrome**,\n**Fuchsia**}. Blintz will check whether 120 is the current extended stable,\nstable, beta or head milestone; and will add the **Security_Impact-Extended\nhotlist**.\n3. Within a day or two, the Security Shepherd was able to get the bug assigned\nand — oh joy! — fixed very quickly. When the bug's status changes to **Fixed**,\nBlintz will update the **Merge field with the appropriate request-MMM** labels,\nand will change update Visibility by changing **Collaborators from\nsecurity@chromium.org to security-notify@chromium.org**.\n4. Later that week, the Chrome Security VRP TL does a sweep of all\n**reward-topanel** bugs. This one gets rewarded, so that one reward label is\nreplaced with two: **reward-7000** and **reward-unpaid**. Later,\n**reward-unpaid** becomes **reward-inprocess** and is later still removed when\nall is done. Of course, **reward-7000** remains forever. (We use it to track\ntotal payout!)\n5. A Chrome Security TPM with the responsibility of doing merge review regularly\nchecks the security merge review queue based on Type=Vulnerability issues with\n**Merge fields** updated with **request-MMM* or **review-MMM** entries. They\nreview this issue and fix, having had enough bake time in Canary and being safe\nto merge, and approve it for merge, updating the Merge fields to\n**Approved-MMM** for each milestone the fix is being approved for backmerge to.\nThere are weekly Stable channel updates, and this fix was approved for merge to\nM121, so it will be shipped in the following week's update of M121.\n6. Just before the release, a Chrome Security TPM runs a series of scripts\nand verifies the security fixes being shipped in that Stable channel update and\napplies the appropriate **#-M121** tag in the **Release** field. Since the bug\nwas externally reported, it will also be issued a CVE ID, and the **CVE** field\nis updated with the appropriate CVE for that issue: **2024-####**.\n7. 14 weeks after the bug is marked **Fixed**, Blintz updates the **Visibility**\nfrom **Issue access level -- Limited Visibility** to **Default Visibility** and\nremoves security-notify@chromium.org from Collaborators, making the issue\npublicly visible. There is one crucial exception -- Blintz will not update the\nVisibility or remove security@chromium.org from Collaborators if the issue is\non the **SecurityEmbargo** hotlist.\n"
  },
  {
    "path": "security/security-issue-guide-for-devs",
    "title": "You’ve just been assigned a security bug…",
    "content": "# You’ve just been assigned a security bug…\n\nIf you have just been assigned a security bug, **don’t panic!** Security bugs\nare a fact of life in Chromium, and the project has a team of people and robust\nprocesses to help analyze and get security issues fixed. This document is meant\nto help Chromium developers handle their first (few) security bug(s). You may\nalso want to review the [Life of a Security Issue](life-of-a-security-issue.md)\nto understand how you as a developer fit into the larger security bug life\ncycle.\n\nChromium has [public commitments](severity-guidelines.md) to fix security issues\nwithin certain timeframes. Please treat security issues as high-priority\ninterrupts to your work, especially if they are **High Severity (S1)** or\n**Critical Severity (S0)**. However, the expectation is that you handle security\nissues within your normal working hours, not after-hours, weeknights, or on\nvacation. Everyone shares the responsibility of keeping our users safe!\n\n## 1. Understand why you were Assigned\n\nAll incoming security bugs are analyzed and triaged by the current [security\nshepherd](shepherd.md). If you have been assigned a security bug, it is because\nthe shepherd thinks you are the responsible owner for the code in question. The\nshepherd assigned you the bug because either:\n\n1. They have verified the bug is valid and the shepherd expects you to fix it\n2. There is a technical question that needs to be answered before the bug can be\n   fully triaged\n\nIn either case, if you are not the correct owner, please suggest a more\nappropriate person and re-assign it to that person. Or, if you do not know the\ncorrect owner, remove yourself from the Assignee field so that the bug\nre-enters the shepherd’s queue. Setting a component alone will not grant view\naccess or alert the component owners, so the shepherd's queue is the best\nway to ensure the bug is properly triaged.\n\nIn the case where the shepherd is asking you technical questions, they will\nfurther triage the bug after considering your responses.\n\nSecurity bugs are also view-restricted until after the fix is released to users.\nIt is okay to CC additional people (including yourself if you re-assign the\nissue) that can help diagnose and fix the bug.\n\n## 2. Participate in the discussion\n\nSome bugs involve discussion with the reporter and/or members of the security\nteam. For example, the issue may be in a feature or system that the shepherd is\nnot well-equipped to reproduce, and they may ask you for help in determining if\nthe bug is valid. The shepherd may also try to determine if the bug is mitigated,\nmeaning that the security impact is smaller or greater than described by the\nreporter. As the developer, you may have questions about certain preconditions\nassumed by the reporter. We encourage you to interact with the reporter and the\nshepherd, directly in the issue tracker, as much as you need in order to identify\nand fix the issue.\n\nPlease do _not_ adjust any of the [security metadata](security-labels.md) on the\nbug (namely the **Severity** field and **Security\\_Impact** hotlists). If you think a\nbug is not a security issue or its severity should be downgraded, discuss it with\nthe security team and let them adjust the metadata. However, you can adjust the\n**Found In** field if you know the versions a bug affects.\n\n## 3. Fix the bug\n\nThis is the normal part of the job! Write a fix and a regression test, upload\nthe CL, and get it reviewed by the appropriate code owner. The shepherd who\nassigned you the bug does not need to be included on the CL. Once the CL has\nlanded, please [_immediately_ mark the bug as\n**Fixed**](https://groups.google.com/a/chromium.org/g/chromium-dev/c/JNJdU-dnjTk/m/4jXI96pdAgAJ).\nThat status change will kick off the security team’s automation to ensure the\nfix is released to users in a timely fashion.\n\nA word on CL descriptions: Do not hide or obscure the fact that the CL is fixing\na security bug; it is okay to mention that the CL fixes a use-after-free.\nHowever, the best CL description isn’t “[component] Fix uaf” – it is better to\ndescribe _what_ lifetimes are being corrected, as well as the faulty underlying\nassumption that led to the bug. As an example, [this\nCL](https://chromium-review.googlesource.com/c/chromium/src/+/2167426) fixes a\nuse-after-free and describes the lifetime issue and change.\n\n## 4. Merge the fix\n\nAfter the bug has been marked **Fixed**, automation (or a member of the security\nteam) will request merge to the applicable release branches. Once the merge\nquestionnaire is posted to the bug, please respond to the questions.\n\nIf the merge is approved, it is your responsibility to merge the CL to the\napproved branches. The merge approval will show up as the 'Merge' custom field as\n\"Approved-&lt;Milestone&gt;\".\n\n## 5. Think about patterns\n\nAfter the reported bug has been fixed and possibly merged, consider if the same\nbug may exist in other places. For example:\n\n* If you fixed one instance of using `base::Unretained` in an unsafe manner,\n  check the surrounding code for other usages that may be unsafe.\n* If you converted an incorrect `DCHECK` to an early return or `CHECK`, look for\n  similar incorrect `DCHECKs`.\n* If there was an integer overflow, look at other arithmetic operations and\n  consider using base/numerics/.\n\n## Summary\n\n**Do:**\n\n* CC additional subject-matter-experts to the bug\n* Have a productive discussion in the bug issue comments\n* Fix the bug as quickly as you can in your normal working hours\n* Set the bug’s status to **Fixed** as soon as the CL lands\n* Merge the CLs to the appropriate branches after receiving merge approval\n* See also our [top security things checklist](checklist.md)\n\n**Don’t:**\n\n* Panic\n* Communicate with the reporter about the issue outside of the bug tracker\n* Adjust the [security labels](security-labels.md) like the **Severity** field\n  or **Security\\_Impact** hotlists.\n"
  },
  {
    "path": "security/security-considerations-for-browser-ui",
    "title": "Security Considerations for Browser UI",
    "content": "# Security Considerations for Browser UI\n\nUsers expect to be able to use browsers to visit any website, including those\nwhich they don't trust to deliver safe web content and code. One of the ways\nthat an unsafe website might try to harm a user is by attacking, abusing, or\nmanipulating the browser's own UI (aka the browser chrome). Browser UI therefore\nhas some special usable security considerations. For most browser UI, there are\ntwo main attacks to think about: spoofing and clickjacking.\n\n## Spoofing\n\nMany aspects of browser security rely on the user understanding the difference\nbetween trustworthy browser UI and untrustworthy web content. Spoofing refers to\nwhen untrustworthy web content mimics browser UI such that the user can't tell\nthe difference. This could result in leaking sensitive data to an attacker,\nconfusing or tricking the user into taking an action that they wouldn't take\notherwise, or other more subtle forms of abuse.\n\nTraditionally, web browsers try to maintain a line of death between trustworthy\nbrowser chrome and untrustworthy web content. Trustworthy browser UI should\nappear above the line of death, or at least be anchored to or overlap it.\n[This blog post](https://textslashplain.com/2017/01/14/the-line-of-death/)\ncontains an excellent overview of this concept, including some of its\nshortcomings. Our research also shows that this concept is not well understood\nby users. The position of a given piece of UI is less important than other\nfactors, such as familiarity and visual cues (for example, the presence of logos\nor account names).\n\nMaintaining an understandable and consistent distinction between browser UI and\nweb content is more of an art than a science. If you are building a new browser\nUI surface, consider whether web content can spoof your UI, how convincingly\nthey might be able to do so, and what harm might come to the user as a result.\n\nHere are some security principles to strive for to protect users and help them\nbuild correct mental models about browser versus web content:\n\n### Prefer negative security indicators to positive or neutral ones\n\nUsually, attackers don't have much incentive to spoof negative security\nindicators (like a warning) compared to a positive or neutral one. Positive\nsecurity indicators also have\n[other usability problems](https://www.troyhunt.com/the-decreasing-usefulness-of-positive-visual-security-indicators-and-the-importance-of-negative-ones/).\nPrefer to warn users when things are amiss, rather than reassure them about\nneutral or positive security properties.\n\n(Note that negative security indicators can be abused too! For example, some\ntech support scam websites show fake Safe Browsing or certificate warnings to\nencourage the user to believe that their computer is infected and they need to\npurchase fraudulent tech support or antivirus software. However, there are many\nother ways that these scams could try to convince the user that their computer\nis broken, and generally negative indicators are a less fruitful target for\nspoofing than positive ones.)\n\n### Prefer existing UI/UX patterns; avoid introducing new ones\n\nReuse familiar patterns rather than introducing new ones to help users maintain\nan accurate mental model of browser UI versus web content. For example, instead\nof building a new browser UX pattern, can you model the functionality as a\nnormal webpage in a normal tab? Instead of introducing a new type of prompt or\nmodal dialog, can you send the user into Settings to make a choice there? The\nfewer UI/UX patterns the user has to learn, the more likely that they will be\nable to accurately discern trustworthy from untrustworthy content when it\nmatters most.\n\n### Avoid mixing trustworthy with untrustworthy content; clearly demarcate untrustworthy content\n\nWe generally try to avoid showing attacker-controlled content in browser UI, and\nwhen we do so, we constrain it and make it clear from context that it is not\ncontent provided by the browser. For example, well-meaning people often propose\nshowing website-provided strings in permission prompts to help the user\nunderstand why a website is asking for a particular permission. We avoid this\nbecause attackers might abuse this functionality to trick the user into granting\na permission that they wouldn't otherwise grant.\n\nWhen demarcating untrustworthy content in browser UI, consider what might happen\nif the content is longer than expected or uses unexpected characters. Take\nspecial care with [URLs](url_display/url_display_guidelines.md).\n\n### Avoid occlusion of browser UI\n\nFor many forms of browser UI, spoofing is not as scary a risk as partial or full\nocclusion. For example, a website that spoofs a browser permission prompt cannot\ndo too much harm, because the spoofed permission prompt will not cause the\npermission to actually be granted. But consider an attacker who can cover the\norigin shown in the permission prompt with an origin of their own choosing --\nthat is much scarier than fully spoofing the dialog, because the user might\nchoose to grant the permission based on a misunderstanding of which origin they\nare interacting with. Another example is the fullscreen disclosure bubble which\ntells users that they are in fullscreen mode and how to exit it; an attacker who\ncan occlude the fullscreen bubble is very powerful as there are no longer any\ntrustworthy pixels on the screen.\n\nWhen introducing a new form of browser UI, take care to ensure that attacker\ncontent can't occlude it, partially or fully.\n\n## Clickjacking and keyjacking\n\nClickjacking is when an attacker tricks the user into clicking or interacting\nwith a UI element that they don't mean to interact with. Historically,\nclickjacking was mainly considered an attack executed by one website against\nanother. For example, `attacker.com` iframes `victim.com` and covers up the\n`victim.com` UI. `attacker.com` then tricks the user into clicking in a\nparticular location (e.g., by convincing them to play a game that incentivizes\nthem to click in a specific location), and at the last minute they remove the\noccluding UI and the click falls through to the `victim.com` frame, causing the\nuser to take some unintentional action on `victim.com`. Keyjacking is a similar\nattack except the user's keypress rather than click falls through to the\noccluded UI. This category of attack is also sometimes called \"UI redressing\".\n\nClickjacking is mostly under control on the web, but it hasn't been\nsystematically tackled for browser UI, and many browser UI surfaces are\nclickjackable. If a website can trick a user into clicking at a particular\nlocation at a particular time, the attacker may be able to trigger some browser\nUI (such as a prompt or dialog) to show up at that location at exactly the right\nmoment such that the user is tricked into interacting with the browser UI\nwithout realizing it.\n\nNot all browser UI surfaces are hardened against clickjacking/keyjacking, but\nnew ones should be if possible. Consider these possible defenses:\n\n### Don't have a default-selected accept button\n\nIf your dialog or UI has a call-to-action triggered by a button that is\ndefault-selected, the dialog is subject to keyjacking. An evil webpage can trick\na user into mashing or repeatedly hitting the Enter key, and then trigger your\ndialog to show, causing the user to unknowingly accept. Users should have to\nmake an explicit selection on security- or privacy-sensitive browser UI\nsurfaces.\n([Example](https://bugs.chromium.org/p/chromium/issues/detail?id=865202#c9))\n\n### Require multiple clicks/gestures before an action is taken\n\nFor example, if introducing a new type of permission prompt, consider whether it\nis feasible for the permission-granting flow to involve two clicks in two\ndifferent locations. Tricking the user into making two clicks in two locations\non browser UI is a lot harder than tricking the user into one click on a single\nlocation.\n\n### Introduce a short delay before the UI's call-to-action activates\n\nIf multiple clicks/gestures aren't feasible, consider introducing a short delay\nbetween when the browser UI is shown and the call-to-action activates. For\nexample, if the user must click a button to grant a permission, introduce a\ndelay before the button becomes active once the permission prompt is shown.\nChrome uses short and long delays in various UI:\n\n-   For large security-sensitive browser surfaces like interstitials, three\n    seconds is typically considered a delay that is long enough to let the user\n    notice that the UI is showing without being too disruptive to the typical\n    user experience.\n\n-   For smaller UI surfaces such as dialog boxes, a shorter delay like 500ms can\n    be more practical.\n    [`InputEventActivationProtector`](ui/views/input_event_activation_protector.h)\n    is a helper class that ignores UI events that happen within 500ms of the\n    sensitive UI being displayed.\n"
  },
  {
    "path": "security/rust-toolchain",
    "title": "",
    "content": "Please see [//docs/rust.md](../rust.md) for documentation on the Rust\ntoolchain in Chromium.\n"
  },
  {
    "path": "security/rules",
    "title": "Security rules",
    "content": "# Security rules\n\nThis is a list of the security policies Chromium has published.\n\n* [Rule of Two](rule-of-2.md) - don't handle untrustworthy data in the browser\n  process in an unsafe language\n* [The browser process should not handle messages from web\n  content](handling-messages-from-web-content.md)\n* [Behavior should be part of Chrome's binaries or delivered via component\n  updater](behavior-over-the-internet.md) rather than delivered dynamically\n* Rules for [Android IPC](android-ipc.md)\n* [Always assume a compromised renderer](compromised-renderers.md)\n* [Use origin not URL for security decisions](origin-vs-url.md)\n* [Controlling access to powerful web platform\n  features](permissions-for-powerful-web-platform-features.md)\n* [Security considerations for browser UI](security-considerations-for-browser-ui.md)\n* [Guidelines for URL display](url_display_guidelines/url_display_guidelines.md)\n* [Avoid adding cross-origin full-page overlays](overlay-policy.md)\n* [Security Guidelines for LLMs and other large models in Chrome](llm-security-guidelines.md)\n\nYou can also find our position on various matters in the [security FAQ](faq.md):\nfor example, on local attackers or on the privilege accorded to enterprise\nadmins.\n"
  },
  {
    "path": "security/rule-of-2",
    "title": "The Rule Of 2",
    "content": "# The Rule Of 2\n\nWhen you write code to parse, evaluate, or otherwise handle untrustworthy inputs\nfrom the Internet — which is almost everything we do in a web browser! — we like\nto follow a simple rule to make sure it's safe enough to do so. The Rule Of 2\nis: Pick no more than 2 of\n\n  * untrustworthy inputs;\n  * unsafe implementation language; and\n  * high privilege.\n\n![alt text](rule-of-2-drawing.png \"Venn diagram showing you should always use\na safe language, a sandbox, or not be processing untrustworthy inputs in the first\nplace.\")\n\n(drawing source\n[here](https://docs.google.com/drawings/d/12WoPI7-E5NAINHUZqEPGn38aZBYBxq20BgVBjZIvgCQ/edit?usp=sharing))\n\n## Why?\n\nWhen code that handles untrustworthy inputs at high privilege has bugs, the\nresulting vulnerabilities are typically of Critical or High severity. (See our\n[Severity Guidelines](severity-guidelines.md).) We'd love to reduce the severity\nof such bugs by reducing the amount of damage they can do (lowering their\nprivilege), avoiding the various types of memory corruption bugs (using a safe\nlanguage), or reducing the likelihood that the input is malicious (asserting the\ntrustworthiness of the source).\n\nFor the purposes of this document, our main concern is reducing (and hopefully,\nultimately eliminating) bugs that arise due to _memory unsafety_. [A recent\nstudy by Matt Miller from Microsoft\nSecurity](https://github.com/Microsoft/MSRC-Security-Research/blob/master/presentations/2019_02_BlueHatIL/2019_01%20-%20BlueHatIL%20-%20Trends%2C%20challenge%2C%20and%20shifts%20in%20software%20vulnerability%20mitigation.pdf)\nstates that \"~70% of the vulnerabilities addressed through a security update\neach year continue to be memory safety issues\". A trip through Chromium's bug\ntracker will show many, many vulnerabilities whose root cause is memory\nunsafety. (As of March 2019, only about 5 of 130 [public Critical-severity\nbugs](https://bugs.chromium.org/p/chromium/issues/list?can=1&q=Type%3DBug-Security+Security_Severity%3DCritical+-status%3AWontFix+-status%3ADuplicate&sort=&groupby=&colspec=ID+Pri+M+Stars+ReleaseBlock+Component+Status+Owner+Summary+OS+Modified&x=m&y=releaseblock&mode=&cells=ids&num=)\nare not obviously due to memory corruption.)\n\nSecurity engineers in general, very much including Chrome Security Team, would\nlike to advance the state of engineering to where memory safety issues are much\nmore rare. Then, we could focus more attention on the application-semantic\nvulnerabilities. 😊 That would be a big improvement.\n\n## What?\n\nSome definitions are in order.\n\n### Untrustworthy Inputs\n\n_Untrustworthy inputs_ are inputs that\n\n  * have non-trivial grammars; and/or\n  * come from untrustworthy sources.\n\nIf there were an input type so simple that it were straightforward to write a\nmemory-safe handler for it, we wouldn't need to worry much about where it came\nfrom **for the purposes of memory safety**, because we'd be sure we could handle\nit. We would still need to treat the input as untrustworthy after\nparsing, of course.\n\nUnfortunately, it is very rare to find a grammar trivial enough that we can\ntrust ourselves to parse it successfully or fail safely. (But see\n[Normalization](#normalization) for a potential example.) Therefore, we do need\nto concern ourselves with the provenance of such inputs.\n\nAny arbitrary peer on the Internet is an untrustworthy source, unless we get\nsome evidence of its trustworthiness (which includes at least [a strong\nassertion of the source's\nidentity](#verifying-the-trustworthiness-of-a-source)). When we can know with\ncertainty that an input is coming from the same source as the application itself\n(e.g. Google in the case of Chrome, or Mozilla in the case of Firefox), and that\nthe transport is integrity-protected (such as with HTTPS), then it can be\nacceptable to parse even complex inputs from that source. It's still ideal,\nwhere feasible, to reduce our degree of trust in the source — such as by parsing\nthe input in a sandbox.\n\n### Unsafe Implementation Languages\n\n_Unsafe implementation languages_ are languages that lack [memory\nsafety](https://en.wikipedia.org/wiki/Memory_safety), including at least C, C++,\nand assembly language. Memory-safe languages include Go, Rust, Python, Java,\nJavaScript, Kotlin, and Swift. (Note that the safe subsets of these languages\nare safe by design, but of course implementation quality is a different story.)\n\n#### Unsafe Code in Safe Languages\n\nSome memory-safe languages provide a backdoor to unsafety, such as the `unsafe`\nkeyword in Rust. This functions as a separate unsafe language subset inside the\nmemory-safe one.\n\nThe presence of unsafe code does not negate the memory-safety properties of the\nmemory-safe language around it as a whole, but _how_ unsafe code is used is\ncritical. Poor use of an unsafe language subset is not meaningfully different\nfrom any other unsafe implementation language.\n\nIn order for a library with unsafe code to be safe for the purposes of the Rule\nof 2, all unsafe usage must be able to be reviewed and verified by humans with\nsimple local reasoning. To achieve this, we expect all unsafe usage to be:\n* Small: The minimal possible amount of code to perform the required task\n* Encapsulated: All access to the unsafe code is through a safe API\n* Documented: All preconditions of an unsafe block (e.g. a call to an unsafe\n  function) are spelled out in comments, along with explanations of how they are\n  satisfied.\n\nBecause unsafe code reaches outside the normal expectations of a memory-safe\nlanguage, it must follow strict rules to avoid undefined behaviour and\nmemory-safety violations, and these are not always easy to verify. A careful\nreview by one or more experts in the unsafe language subset is required.\n\nIt should be safe to use any code in a memory-safe language in a high-privilege\ncontext. As such, the requirements on a memory-safe language implementation are\nhigher: All code in a memory-safe language must be capable of satisfying the\nRule of 2 in a high-privilege context (including any unsafe code) in order to be\nused or admitted anywhere in the project.\n\n### High Privilege\n\n_High privilege_ is a relative term. The very highest-privilege programs are the\ncomputer's firmware, the bootloader, the kernel, any hypervisor or virtual\nmachine monitor, and so on. Below that are processes that run as an OS-level\naccount representing a person; this includes the Chrome Browser process and Gpu\nprocess. We consider such processes to have high privilege. (After all, they\ncan do anything the person can do, with any and all of the person's valuable\ndata and accounts.)\n\nProcesses with slightly reduced privilege will (hopefully soon) include the\nnetwork process. These are still pretty high-privilege processes. We are always\nlooking for ways to reduce their privilege without breaking them.\n\nLow-privilege processes include sandboxed utility processes and renderer\nprocesses with [Site Isolation](\nhttps://www.chromium.org/Home/chromium-security/site-isolation) (very good) or\n[origin isolation](\nhttps://cloud.google.com/docs/chrome-enterprise/policies/?policy=IsolateOrigins)\n(even better).\n\n### Processing, Parsing, And Deserializing\n\nTurning a stream of bytes into a structured object is hard to do correctly and\nsafely. For example, turning a stream of bytes into a sequence of Unicode code\npoints, and from there into an HTML DOM tree with all its elements, attributes,\nand metadata, is very error-prone. The same is true of QUIC packets, video\nframes, and so on.\n\nWhenever the code branches on the byte values it's processing, the risk\nincreases that an attacker can influence control flow and exploit bugs in the\nimplementation.\n\nAlthough we are all human and mistakes are always possible, a function that does\nnot branch on input values has a better chance of being free of vulnerabilities.\n(Consider an arithmetic function, such as SHA-256, for example.)\n\n## Solutions To This Puzzle\n\nChrome Security Team will generally not approve landing a CL or new feature\nthat involves all 3 of untrustworthy inputs, unsafe language, and high\nprivilege. To solve this problem, you need to get rid of at least 1 of those 3\nthings. Here are some ways to do that.\n\n### Safe Languages\n\nWhere possible, it's great to use a memory-safe language. The following\nmemory-safe languages are approved for use in Chromium:\n* Java (on Android only)\n* Swift (on iOS only)\n* [Rust](../docs/rust.md) (for [third-party use](\n  ../docs/adding_to_third_party.md#Rust))\n* JavaScript or WebAssembly (although we don't currently use them in\n  high-privilege processes like the browser/gpu process)\n\nOne can imagine Kotlin on Android, too, although it is not currently\nused in Chromium.\n\nFor an example of image processing, we have the pure-Java class\n[BaseGifImage](https://cs.chromium.org/chromium/src/third_party/gif_player/src/jp/tomorrowkey/android/gifplayer/BaseGifImage.java?rcl=27febd503d1bab047d73df26db83184fff8d6620&l=27).\nOn Android, where we can use Java and also face a particularly high cost for\ncreating new processes (necessary for sandboxing), using Java to decode tricky\nformats can be a great approach. We do a similar thing with the pure-Java\n[JsonSanitizer](https://cs.chromium.org/chromium/src/services/data_decoder/public/cpp/android/java/src/org/chromium/services/data_decoder/JsonSanitizer.java),\nto 'vet' incoming JSON in a memory-safe way before passing the input to the C++\nJSON implementation.\n\nOn Android, many system APIs that are exposed via Java are not actually\nimplemented in a safe language, and are instead just facades around an unsafe\nimplementation. A canonical example of this is the\n[BitmapFactory](https://developer.android.com/reference/android/graphics/BitmapFactory)\nclass, which is a Java wrapper [around C++\nSkia](https://cs.android.com/android/platform/superproject/+/master:frameworks/base/libs/hwui/jni/BitmapFactory.cpp;l=586;drc=864d304156d1ef8985ee39c3c1858349b133b365).\nThese APIs are therefore not considered memory-safe under the rule.\n\nThe [QR code generator](\nhttps://source.chromium.org/chromium/chromium/src/+/main:components/qr_code_generator/;l=1;drc=b185db5d502d4995627e09d62c6934590031a5f2)\nis an example of a cross-platform memory-safe Rust library in use in Chromium.\n\n### Privilege Reduction\n\nAlso known as [_sandboxing_](https://cs.chromium.org/chromium/src/sandbox/),\nprivilege reduction means running the code in a process that has had some or\nmany of its privileges revoked.\n\nWhen appropriate, try to handle the inputs in a renderer process that is Site\nIsolated to the same site as the inputs come from. Take care to validate the\nparsed (processed) inputs in the browser, since only the browser can trust\nitself to validate and act on the meaning of an object.\n\nEquivalently, you can launch a sandboxed utility process to handle the data, and\nreturn a well-formed response back to the caller in an IPC message. See [Safe\nBrowsing's ZIP\nanalyzer](https://cs.chromium.org/chromium/src/chrome/common/safe_browsing/zip_analyzer.h)\nfor an example. The [Data Decoder Service](https://source.chromium.org/chromium/chromium/src/+/main:services/data_decoder/public/cpp/data_decoder.h)\nfacilitates this safe decoding process for several common data formats.\n\n### Verifying The Trustworthiness Of A Source\n\nIf you can be sure that the input comes from a trustworthy source, it can be OK\nto parse/evaluate it at high privilege in an unsafe language. A \"trustworthy\nsource\" means that Chromium can cryptographically prove that the data comes\nfrom a business entity that you can or do trust (e.g.\nfor Chrome, an [Alphabet](https://abc.xyz) company).\n\nSuch cryptographic proof can potentially be obtained by:\n\n  * Component Updater;\n  * The variations framework.\n  * Pinned TLS (see below).\n\nPinned TLS needs to meet all these criteria to be effective:\n\n  * communication happens via validly-authenticated TLS, HTTPS, or QUIC;\n  * the peer's keys are [pinned in Chrome](https://cs.chromium.org/chromium/src/net/http/transport_security_state_static.json?sq=package:chromium&g=0); and\n  * pinning is active on all platforms where the feature will launch.\n    (Currently pinning is not enabled in iOS or Android WebView).\n\nIt is generally preferred to use Component Updater if possible because pinning\nmay be disabled by locally installed root certificates.\n\nOne common pattern is to deliver a cryptographic hash of some content via such\na trustworthy channel, but deliver the content itself via an untrustworthy\nchannel. So long as the hash is properly verified, that's fine.\n\n### Normalization {#normalization}\n\nYou can 'defang' a potentially-malicious input by transforming it into a\n_normal_ or _minimal_ form, usually by first transforming it into a format with\na simpler grammar. We say that all data, file, and wire formats are defined by a\n_grammar_, even if that grammar is implicit or only partially-specified (as is\nso often the case). A data format with a particularly simple grammar is\n[`SkPixmap`](https://source.chromium.org/chromium/chromium/src/+/3df9ac8e76132c586e888d1ddc7d2217574f17b0:third_party/skia/include/core/SkPixmap.h;l=712).\n(The 'grammar' is represented by the private data fields: a region of raw pixel\ndata, the size of that region, and simple metadata (`SkImageInfo`) about how to\ninterpret the pixels.)\n\nIt's rare to find such a simple grammar for input formats, however.\n\nFor example, consider the PNG image format, which is complex and whose [C\nimplementation has suffered from memory corruption bugs in the\npast](https://www.cvedetails.com/vulnerability-list/vendor_id-7294/Libpng.html).\nAn attacker could craft a malicious PNG to trigger such a bug. But if you\ntransform the image into a format that doesn't have PNG's complexity (in a\nlow-privilege process, of course), the malicious nature of the PNG 'should' be\neliminated and then safe for parsing at a higher privilege level. Even if the\nattacker manages to compromise the low-privilege process with a malicious PNG,\nthe high-privilege process will only parse the compromised process' output with\na simple, plausibly-safe parser. If that parse is successful, the\nhigher-privilege process can then optionally further transform it into a\nnormalized, minimal form (such as to save space). Otherwise, the parse can fail\nsafely, without memory corruption.\n\nThe trick of this technique lies in finding a sufficiently-trivial grammar, and\ncommitting to its limitations.\n\nAnother good approach is to\n\n  1. define a new Mojo message type for the information you want;\n  2. extract that information from a complex input object in a sandboxed\n     process; and then\n  3. send the result to a higher-privileged process in a Mojo message using the\n     new message type.\n\nThat way, the higher-privileged process need only process objects adhering to a\nwell-defined, generally low-complexity grammar. This is a big part of why [we\nlike for Mojo messages to use structured types](mojo.md#Use-structured-types).\n\nFor example, it should be safe enough to convert a PNG to an `SkBitmap` in a\nsandboxed process, and then send the `SkBitmap` to a higher-privileged process\nvia IPC. Although there may be bugs in the IPC message deserialization code\nand/or in Skia's `SkBitmap` handling code, we consider this safe enough for a\nfew reasons:\n\n  * we must accept the risk of bugs in Mojo deserialization; but thankfully\n  * Mojo deserialization is very amenable to fuzzing; and\n  * it's a big improvement to scope bugs to smaller areas, like IPC\n    deserialization functions and very simple classes like `SkBitmap` and\n    `SkPixmap`.\n\nUltimately this process results in parsing significantly simpler grammars. (PNG\n→ Mojo + `SkBitmap` in this case.)\n\n> (We have to accept the risk of memory safety bugs in Mojo deserialization\n> because C++'s high performance is crucial in such a throughput- and\n> latency-sensitive area. If we could change this code to be both in a safer\n> language and still have such high performance, that'd be ideal. But that's\n> unlikely to happen soon.)\n\n### Exception: Protobuf\n\nWhile less preferable to Mojo, we also similarly trust Protobuf for\ndeserializing messages at high privilege from potentially untrustworthy senders.\nFor example, Protobufs are sometimes embedded in Mojo IPC messages. It is\nalways preferable to use a Mojo message where possible, though sometimes\nexternal constraints require the use of Protobuf.\n\nProtobuf's threat model does not include parsing a protobuf from shared\nmemory. Always copy the proto buffer bytes from untrustworthy shared\nmemory regions before deserializing to a Message.\n\nIf you must pass protobuf bytes over mojo use\n[mojo_base::ProtoWrapper](https://chromium.googlesource.com/chromium/src/+/main/mojo/public/cpp/base/proto_wrapper.h)\nas this provides limited type safety for the top-level protobuf message and\nensures copies are taken before deserializing.\n\nNote that this exception only applies to Protobuf as a container format;\ncomplex data contained within a Protobuf must be handled according to this\nrule as well.\n\n### Exception: RE2\n\nAs another special case, we trust the\n[RE2](https://cs.chromium.org/chromium/src/third_party/re2/README.chromium)\nregular expression library to evaluate untrustworthy patterns over untrustworthy\ninput strings, because its grammar is sufficiently limited and hostile input is\npart of the threat model against which it's been tested for years. It is **not**\nthe case, however, that text matched by an RE2 regular expression is necessarily\n\"sanitized\" or \"safe\". That requires additional security judgment.\n\n## Safe Types\n\nAs discussed above in [Normalization](#normalization), there are some types that\nare considered \"safe,\" even though they are deserialized from an untrustworthy\nsource, at high privilege, and in an unsafe language. These types are\nfundamental for passing data between processes using IPC, tend to have simpler\ngrammar or structure, and/or have been audited or fuzzed heavily.\n\n* `GURL` and `url::Origin`\n* `SkBitmap` (in [N32 format](https://source.chromium.org/chromium/chromium/src/+/main:third_party/skia/include/core/SkColorType.h;l=54-58;drc=8d399817282e3c12ed54eb23ec42a5e418298ec6) only)\n* `SkPixmap` (in [N32 format](https://source.chromium.org/chromium/chromium/src/+/main:third_party/skia/include/core/SkColorType.h;l=54-58;drc=8d399817282e3c12ed54eb23ec42a5e418298ec6) only)\n* Protocol buffers (see above; this is not a preferred option and should be\n  avoided where possible)\n\nThere are also classes in `//base` that internally hold simple values that\nrepresent potentially complex data, such as:\n\n* `base::FilePath`\n* `base::Token` and `base::UnguessableToken`\n* `base::Time` and `base::TimeDelta`\n\nThe deserialization of these is safe, though it is important to remember that\nthe value itself is still untrustworthy (e.g. a malicious path trying to escape\nits parent using `../`).\n\n## Existing Code That Violates The Rule\n\nWe still have code that violates this rule.  For example, Chrome's Omnibox\n[still parses JSON in the browser\nprocess](https://bugs.chromium.org/p/chromium/issues/detail?id=863193&q=%22rule%20of%202%22%20omnibox&can=1).\nAdditionally, the networking process on Windows is (at present) unsandboxed by\ndefault, though there is [ongoing\nwork](https://bugs.chromium.org/p/chromium/issues/detail?id=841001)\nto change that default.\n"
  },
  {
    "path": "security/process-sandboxes-by-platform",
    "title": "Unsandboxed Processes by Platform",
    "content": "# Unsandboxed Processes by Platform\n\nThis document summarises the sandboxes used for different processes or services\nin Chrome. This informs the [severity of security\nissues](../../docs/security/severity-guidelines.md) in different processes.\nSecurity issues are triaged based on the least-sandboxed platform where an issue\noccurs. Some processes may be sandboxed but contain important credentials or\ncross-origin data, for this table they count as being sandboxed.\n\nThis table will be updated to track the default configuration of the Stable\nChrome channel (i.e. 100% of clients adopt the tighter configuration).\n\nThe utility process type hosts several services with different sandboxing\nrequirements. Find the sandbox used by a utility by finding the\n[`ServiceSandbox` attribute](../../sandbox/policy/mojom/sandbox.mojom) used in\nits main mojo service.\n\nLast updated for M128.\n\n# Not sandboxed on some platforms\n\n| Process / Service | Platform(s) | Sandbox |\n|---|---|---|\n| Browser | all | **unsandboxed** |\n| Network | Android, Windows, Linux | **unsandboxed** |\n| GPU | Android, non-ChromeOS Linux | **unsandboxed** |\n| On Device Model Execution | Android, non-ChromeOS Linux | **unsandboxed** |\n| Video Capture | non-Fuchsia | **unsandboxed** |\n| kNoSandbox | all | **unsandboxed** |\n| kNoSandboxAndElevatedPrivileges | Windows | **Elevated** |\n\n# Sandboxed on specific platforms\n\n* kNetwork (Fuchsia, Mac)\n* kGpu (Fuchsia, Mac, Windows, ChromeOS)\n* kVideoCapture (Fuchsia)\n\n# Sandboxed\n\n* kRenderer (renderer, extensions, PDF renderers)\n* kUtility\n* kService\n* kServiceWithJit\n* kAudio\n* kOnDeviceModelExecution\n* kCdm\n* kPrintCompositor\n* kSpeechRecognition\n* kScreenAI\n* kPpapi\n* kPrintBackend\n* kVideoCapture (Fuchsia only)\n* kIconReader (Windows only)\n* kMediaFoundationCdm (Windows only)\n* kPdfConversion (Windows only)\n* kXrCompositing (Windows only)\n* kWindowsSystemProxyResolver (Windows only)\n* kHardwareVideoDecoding (Linux & Ash)\n* kHardwareVideoEncoding (Linux & Ash)\n* kIme (Ash only)\n* kTts (Ash only)\n* kLibassistant (Ash only)\n* kNearby (Ash only)\n* kMirroring (MacOS only)\n"
  },
  {
    "path": "security/post-spectre-webdev",
    "title": "Post-Spectre Web Development",
    "content": "# Post-Spectre Web Development\n\nContributors: Artur Janc, Camille Lamy, Charlie Reis, Jun Kokatsu, Mike West.\nPatches and corrections welcome!\n\nLast Updated: Mar. 12th, 2021\n\n*** note\n**Note**: This document has been [adopted][cfc] by the W3C's Web Application Security\nWorking Group; [https://w3c.github.io/webappsec-post-spectre-webdev/][ED] is\nproceeding under that group's aegis, and this document will redirect there\nonce the working group is satisfied with its recommendations.\n***\n\n[cfc]: https://lists.w3.org/Archives/Public/public-webappsec/2021Feb/0007.html\n[ED]: https://w3c.github.io/webappsec-post-spectre-webdev/\n\n[TOC]\n\n## Introduction\n\nIn early 2018, [Spectre][spectre] made it clear that a foundational security\nboundary the web aimed to maintain was substantially less robust than expected.\nThis revelation has pushed web browsers to shift their focus from the\nplatform-level origin boundary to an OS-level process boundary. Chromium's\n[threat model][post-spectre-rethink], for instance, now asserts that \"active\nweb content … will be able to read any and all data in the address space of the\nprocess that hosts it\". This  shift in thinking imposes a shift in development\npractice, both for browser vendors, and for web developers. Browsers need to\nalign the origin boundary with the process boundary through fundamental\nrefactoring projects (for example, Chromium's [Site Isolation][site-isolation],\nand Firefox's [Project Fission][project-fission]). Moreover, browsers must\nprovide web developers with tools to mitigate risk in the short term, and should\npush the platform towards safe default behaviors in the long term. The bad news\nis that this is going to be a lot of work, much of it falling on the shoulders\nof web developers. The good news is that a reasonable set of mitigation\nprimitives exists today, ready and waiting for use.\n\nThis document will point to a set of mitigations which seem promising, and\nprovide concrete recommendations for web developers responsible for protecting\nusers' data.\n\n[spectre]: https://spectreattack.com/\n[post-spectre-rethink]: https://chromium.googlesource.com/chromium/src/+/main/docs/security/side-channel-threat-model.md\n[site-isolation]: https://www.chromium.org/Home/chromium-security/site-isolation\n[project-fission]: https://wiki.mozilla.org/Project_Fission\n\n\n### Threat Model\n\nSpectre-like side-channel attacks inexorably lead to a model in which active web\ncontent (JavaScript, WASM, probably CSS if we tried hard enough, and so on) can\nread any and all data which has entered the address space of the process which\nhosts it. While this has deep implications for user agent implementations'\ninternal hardening strategies (stack canaries, ASLR, etc), here we'll remain\nfocused on the core implication at the web platform level, which is both simple\nand profound: any data which flows into a process hosting a given origin is\nlegible to that origin. We must design accordingly.\n\nIn order to determine the scope of data that can be assumed accessible to an\nattacker, we must make a few assumptions about the normally-not-web-exposed\nprocess model which the user agent implements. The following seems like a good\nplace to start:\n\n1.  User agents are capable of separating the execution of a web origin's code\n    into a process distinct from the agent's core. This separation enables the\n    agent itself to access local devices, fetch resources, broker cross-process\n    communication, and so on, in a way which remains invisible to any process\n    potentially hosting untrusted code.\n\n2.  User agents are able to make decisions about whether or not a given resource\n    should be delivered to a process hosting a given origin based on\n    characteristics of both the request and the response (headers, etc).\n\n3.  User agents can consistently separate top-level, cross-origin windows into\n    distinct processes. They cannot consistently separate same-site or\n    same-origin windows into distinct processes given the potential for\n    synchronous access between the windows.\n\n4.  User agents cannot yet consistently separate framed origins into processes\n    distinct from their embedders' origin.\n    \n    Note: Though some user agents support [out-of-process frames][oopif], no\n    agent supports it consistently across a broad range of devices and\n    platforms. Ideally this will change over time, as the frame boundary *must*\n    be one we can eventually consider robust.\n\nWith this in mind, our general assumption will be that an origin gains access to\nany resource which it renders (including images, stylesheets, scripts, frames,\netc). Likewise, embedded frames gain access to their ancestors' content.\n\nThis model is spelled out in more detail in both Chromium's\n[Post-Spectre Threat Model Rethink][post-spectre-rethink], and in Artur Janc's\n[Notes on the threat model of cross-origin isolation][coi-threat-model].\n\n[oopif]: https://www.chromium.org/developers/design-documents/oop-iframes\n[coi-threat-model]: https://arturjanc.com/coi-threat-model.pdf\n\n\n### Mitigation TL;DR\n\n1.  **Decide when (not!) to respond to requests** by examining incoming headers,\n    paying special attention to the `Origin` header on the one hand, and various\n    `Sec-Fetch-` prefixed headers on the other, as described in the article\n    [Protect your resources from web attacks with Fetch Metadata][fetch-metadata].\n\n2.  **Restrict attackers' ability to load your data as a subresource** by\n    setting a [cross-origin resource policy][corp] (CORP) of `same-origin`\n    (opening up to `same-site` or `cross-origin` only when necessary).\n\n3.  **Restrict attackers' ability to frame your data as a document** by opt-ing\n    into framing protections via `X-Frame-Options: SAMEORIGIN` or CSP's more\n    granular `frame-ancestors` directive (`frame-ancestors 'self'\n    https://trusted.embedder`, for example).\n\n4.  **Restrict attackers' ability to obtain a handle to your window** by setting\n    a [cross-origin opener policy][coop] (COOP). In the best case, you can\n    default to a restrictive `same-origin` value, opening up to\n    `same-origin-allow-popups` or `unsafe-none` only if necessary.\n\n5.  **Prevent MIME-type confusion attacks** and increase the robustness of\n    passive defenses like [cross-origin read blocking][corb] (CORB) /\n    [opaque response blocking][orb] (ORB) by setting correct `Content-Type`\n    headers, and globally asserting `X-Content-Type-Options: nosniff`.\n\n[fetch-metadata]: https://web.dev/fetch-metadata/\n[corp]: https://resourcepolicy.fyi/\n[coop]: https://docs.google.com/document/d/1Ey3MXcLzwR1T7aarkpBXEwP7jKdd2NvQdgYvF8_8scI/edit\n[corb]: https://chromium.googlesource.com/chromium/src/+/main/services/network/cross_origin_read_blocking_explainer.md\n[orb]: https://github.com/annevk/orb\n\n\n## Practical Examples\n\n### Subresources\n\nResources which are intended to be loaded into documents should protect\nthemselves from being used in unexpected ways. Before walking through strategies\nfor specific kinds of resources, a few headers seem generally applicable:\n\n1.  Sites should use Fetch Metadata to make good decisions about\n    [when to serve resources][fetch-metadata]. In order to ensure that decision\n    sticks, servers should explain its decision to the browser by sending a\n    `Vary` header containing `Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site`.\n    This ensures that the server has a chance to make different decisions for\n    requests which will be *used* differently.\n\n2.  Subresources should opt-out of MIME type sniffing by sending an\n    `X-Content-Type-Options` header with a value of `nosniff`. This increases\n    the robustness of MIME-based checks like [cross-origin read blocking][corb]\n    and [opaque response blocking][orb], and mitigates some well-known risks\n    around type confusion for scripts.\n\n3.  Subresources are intended for inclusion in a given context, not as\n    independently navigable documents. To mitigate the risk that navigation to a\n    subresource causes script execution or opens an origin up to attack in some\n    other way, servers can assert the following set of headers which\n    collectively make it difficult to meaningfully abuse a subresource via\n    navigation:\n\n    *   Use the `Content-Security-Policy` header to assert the `sandbox`\n        directive. This ensures that these resources remain inactive if\n        navigated to directly as a top-level document. No scripts will execute,\n        and the resource will be pushed into an \"opaque\" origin.\n\n        Note: Some servers deliver `Content-Disposition: attachment;\n        filename=file.name` to obtain a similar effect. This was valuable to\n        mitigate vulnerabilies in Flash, but the `sandbox` approach seems to\n        more straightforwardly address the threats we care about today.\n\n    *   Asserting the [`Cross-Origin-Opener-Policy`][coop] header with a value\n        of `same-origin` prevents cross-origin documents from retaining a handle\n        to the resource's window if it's opened in a popup.\n\n    *   Sending the `X-Frame-Options` header with a value of `DENY` prevents the\n        resource from being framed.\n\nMost subresources, then, should contain the following block of headers, which\nyou'll see repeated a few times below:    \n\n```http\nContent-Security-Policy: sandbox\nCross-Origin-Opener-Policy: same-origin\nVary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\nX-Content-Type-Options: nosniff\nX-Frame-Options: DENY\n```\n\nWith these generic protections in mind, let's sift through a few scenarios to\ndetermine what headers a server would be well-served to assert:\n\n#### Static Subresources\n\nBy their nature, static resources contain the same data no matter who requests\nthem, and therefore cannot contain interesting information that an attacker\ncouldn't otherwise obtain. There's no risk to making these resources widely\navailable, and value in allowing embedders to robustly debug, so something like\nthe following response headers could be appropriate:\n\n```http\nAccess-Control-Allow-Origin: *\nCross-Origin-Resource-Policy: cross-origin\nTiming-Allow-Origin: *\nContent-Security-Policy: sandbox\nCross-Origin-Opener-Policy: same-origin\nVary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\nX-Content-Type-Options: nosniff\nX-Frame-Options: DENY\n```\n\nCDNs are the canonical static resource distribution points, and many use the\npattern above. Take a look at the following common resources' response headers\nfor inspiration:\n\n* <a href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js\">`https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js`</a>\n* <a href=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\">`https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js`</a>\n* <a href=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\">`https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js`</a>\n* <a href=\"https://ssl.google-analytics.com/ga.js\">`https://ssl.google-analytics.com/ga.js`</a>\n\nSimilarly, application-specific static resource servers are a good place to look\nfor this practice. Consider:\n\n* <a href=\"https://static.xx.fbcdn.net/rsrc.php/v3/y2/r/zVvRrO8pOtu.png\">`https://static.xx.fbcdn.net/rsrc.php/v3/y2/r/zVvRrO8pOtu.png`</a>\n* <a href=\"https://www.gstatic.com/images/branding/googlelogo/svg/googlelogo_clr_74x24px.svg\">`https://www.gstatic.com/images/branding/googlelogo/svg/googlelogo_clr_74x24px.svg`</a>\n\n#### Dynamic Subresources\n\nSubresources that contain data personalized to a given user are juicy targets\nfor attackers, and must be defended by ensuring that they're loaded only in\nways that are appropriate for the data in question. A few cases are well worth\nconsidering:\n\n1.  Application-internal resources (private API endpoints, avatar images,\n    uploaded data, etc.) should not be available to any cross-origin requestor.\n    These resources should be restricted to usage as a subresource in\n    same-origin contexts by sending a [`Cross-Origin-Resource-Policy`][corp]\n    header with a value of `same-origin`:\n    \n    ```http\n    Cross-Origin-Resource-Policy: same-origin\n    Content-Security-Policy: sandbox\n    Cross-Origin-Opener-Policy: same-origin\n    Vary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\n    X-Content-Type-Options: nosniff\n    X-Frame-Options: DENY\n    ```\n\n    This header will prevent cross-origin attackers from loading the resource as\n    a response to a `no-cors` request.\n\n    For example, examine the headers returned when requesting endpoints like the\n    following:\n    \n    *   <a href=\"https://myaccount.google.com/_/AccountSettingsUi/browserinfo\">`https://myaccount.google.com/_/AccountSettingsUi/browserinfo`</a>\n    *   <a href=\"https://twitter.com/i/api/1.1/branch/init.json\">`https://twitter.com/i/api/1.1/branch/init.json`</a>\n    *   <a href=\"https://www.facebook.com/ajax/webstorage/process_keys/?state=0\">`https://www.facebook.com/ajax/webstorage/process_keys/?state=0`</a>\n\n2.  Personalized resources intended for cross-origin use (public API endpoints,\n    etc) should carefully consider incoming requests' properties before\n    responding. These endpoints can only safely be enabled by requiring CORS,\n    and choosing the set of origins for which a given response can be exposed by\n    setting the appropriate access-control headers, for example:\n\n    ```http\n    Access-Control-Allow-Credentials: true\n    Access-Control-Allow-Origin: https://trusted.example\n    Access-Control-Allow-Methods: POST\n    Access-Control-Allow-Headers: ...\n    Access-Control-Allow-...: ...\n    Cross-Origin-Resource-Policy: same-origin\n    Content-Security-Policy: sandbox\n    Cross-Origin-Opener-Policy: same-origin\n    Vary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\n    X-Content-Type-Options: nosniff\n    X-Frame-Options: DENY\n    ```\n\n    Note: The `Cross-Origin-Resource-Policy` header is only processed for\n    requests that are _not_ using CORS for access control (\"`no-cors`\n    requests\"). Sending `Cross-Origin-Resource-Policy: same-origin` is\n    therefore not harmful, and works to ensure that `no-cors` usage isn't\n    accidentally allowed.\n    \n    For example, examine the headers returned when requesting endpoints like\n    the following:\n    \n    *   <a href=\"https://api.twitter.com/1.1/jot/client_event.json\">`https://api.twitter.com/1.1/jot/client_event.json`</a>\n    *   <a href=\"https://play.google.com/log?format=json&hasfast=true\">`https://play.google.com/log?format=json&hasfast=true`</a>\n    *   <a href=\"https://securepubads.g.doubleclick.net/pcs/view\">`https://securepubads.g.doubleclick.net/pcs/view`</a>\n    *   <a href=\"https://c.amazon-adsystem.com/e/dtb/bid\">`https://c.amazon-adsystem.com/e/dtb/bid`</a>\n\n3.  Personalized resources that are intended for cross-origin `no-cors`\n    embedding, but which don't intend to be directly legible in that context\n    (avatar images, authenticated media, etc). These should enable cross-origin\n    embedding via `Cross-Origin-Resource-Policy`, but _not_ via CORS access\n    control headers:\n\n    ```http\n    Cross-Origin-Resource-Policy: cross-origin\n    Content-Security-Policy: sandbox\n    Cross-Origin-Opener-Policy: same-origin\n    Vary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\n    X-Content-Type-Options: nosniff\n    X-Frame-Options: DENY\n    ```\n\n    Note, that this allows the resource to be used by any cross-origin document.\n    That's reasonable for some use cases, but requiring CORS, and opting-in a\n    small set of origins via appropriate access-control headers is a possible\n    alternative for some resources. This approach will give those contexts\n    trivial access to the resource's bits, so the granularity is a tradeoff.\n    Still, considering this case to be the same as the \"personalized resources\n    intended for cross-origin use\" isn't unreasonable.\n\n    If we implemented more granular bindings for CORP headers (along the lines\n    of `Cross-Origin-Resource-Policy: https://trusted.example`), we could avoid\n    this tradeoff entirely. That's proposed in\n    [whatwg/fetch#760](https://github.com/whatwg/fetch/issues/670).\n\n\n    For example:\n\n    *   <a href=\"https://lh3.google.com/u/0/d/1JBUaX1xSOZRxBk5bRNZWgnzyJoCQC52TIRokACBSmGc=w512\">`https://lh3.google.com/u/0/d/1JBUaX1xSOZRxBk5bRNZWgnzyJoCQC52TIRokACBSmGc=w512`</a>\n\n\n### Documents {#documents}\n\n#### Fully-Isolated Documents\n\nDocuments that require users to be signed-in almost certainly contain\ninformation that shouldn't be revealed to attackers. These pages should take\ncare to isolate themselves from other origins, both by making _a priori_\ndecisions about [whether to serve the page at all][fetch-metadata], and by\ngiving clients careful instructions about how the page can be used once\ndelivered. For instance, something like the following set of response headers\ncould be appropriate:\n\n```http\nCross-Origin-Opener-Policy: same-origin\nCross-Origin-Resource-Policy: same-origin\nVary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\nX-Content-Type-Options: nosniff\nX-Frame-Options: SAMEORIGIN\n```\n\nNote: Documents which need to make use of APIs that require full cross-origin\nisolation (such as `SharedArrayBuffer`), will also need to serve a\n[`Cross-Origin-Embedder-Policy`][coep] header, as outlined in\n[Making your website \"cross-origin isolated\" using COOP and COEP][coop-coep].\n\nAccount settings pages, admin panels, and application-specific documents are all\ngood examples of resources which would benefit from as much isolation as\npossible. For real-life examples, consider:\n\n*   <a href=\"https://myaccount.google.com/\">`https://myaccount.google.com/`</a>\n\n[coep]: https://wicg.github.io/cross-origin-embedder-policy/\n[coop-coep]: https://web.dev/coop-coep/\n\n\n#### Documents Expecting to Open Cross-Origin Windows\n\nNot every document that requires sign-in can be fully-isolated from the rest of\nthe internet. It's often the case that partial isolation is a better fit.\nConsider sites that depend upon cross-origin windows for federated workflows\ninvolving payments or sign-in, for example. These pages would generally benefit\nfrom restricting attackers' ability to embed them, or obtain their window\nhandle, but they can't easily lock themselves off from all such vectors via\n`Cross-Origin-Opener-Policy: same-origin` and `X-Frame-Options: DENY`. In these\ncases, something like the following set of response headers might be\nappropriate:\n\n```http\nCross-Origin-Opener-Policy: same-origin-allow-popups\nCross-Origin-Resource-Policy: same-origin\nVary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\nX-Content-Type-Options: nosniff\nX-Frame-Options: SAMEORIGIN\n```\n\nThe only difference between this case and the \"Fully-Isolated\" case above is the\n`Cross-Origin-Opener-Policy` value. `same-origin` will break the opener\nrelationship between the document and any cross-origin window, regardless of who\nopened whom. `same-origin-allow-popups` will break cross-origin opener\nrelationships initiated by a cross-origin document's use of `window.open()`, but\nwill allow the asserting document to open cross-origin windows that retain an\nopener relationship.\n\n\n#### Documents Expecting Cross-Origin Openers\n\nFederated sign-in forms and payment providers are clear examples of documents\nwhich intend to be opened by cross-origin windows, and require that relationship\nto be maintained in order to facilitate communication via channels like\n`postMessage()` or navigation. These documents cannot isolate themselves\ncompletely, but can prevent themselves from being embedded or fetched\ncross-origin. Three scenarios are worth considering:\n\n1.  Documents that only wish to be opened in cross-origin popups could loosen\n    their cross-origin opener policy by serving the following headers:\n    \n    ```http\n    Cross-Origin-Resource-Policy: same-origin\n    Cross-Origin-Opener-Policy: unsafe-none\n    Vary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\n    X-Content-Type-Options: nosniff\n    X-Frame-Options: SAMEORIGIN\n    ```\n\n    For example:\n\n    *   > TODO: Find some links.\n\n2.  Documents that only wish to be framed in cross-origin contexts could loosen\n    their framing protections by serving the following headers:\n\n    ```http\n    Cross-Origin-Resource-Policy: same-origin\n    Cross-Origin-Opener-Policy: same-origin\n    Vary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\n    X-Content-Type-Options: nosniff\n    X-Frame-Options: ALLOWALL\n    ```\n\n    \n    Note that this allows embedding by any cross-origin documents. That's\n    reasonable for some widgety use cases, but when possible, a more secure\n    alternative would specify a list of origins which are allowed to embed the\n    document via the `frame-ancestors` CSP directive. That is, in addition to\n    the `X-Frame-Options` header above, the following header could also be\n    included to restrict the document to a short list of trusted embedders:\n\n    ```http\n    Content-Security-Policy: frame-ancestors https://trusted1.example https://trusted2.example\n    ```\n\n    For example:\n\n    *   > TODO: Find some links.\n\n3.  Documents that support both popup and framing scenarios need to loosen both\n    their cross-origin opener policies and framing protections by combining the\n    recommendations above, serving the following headers:\n\n    ```http\n    Cross-Origin-Resource-Policy: same-origin\n    Cross-Origin-Opener-Policy: unsafe-none\n    Vary: Sec-Fetch-Dest, Sec-Fetch-Mode, Sec-Fetch-Site\n    X-Content-Type-Options: nosniff\n    X-Frame-Options: ALLOWALL\n    ```\n\n    For example:\n\n    *   > TODO: Find some links.\n\n## Implementation Considerations\n\n### Explicitly Setting Headers with Default Values\n\nSeveral recommendations above suggest that developers would be well-served to\nset headers like `X-Frame-Options: ALLOWALL` or `Cross-Origin-Opener-Policy:\nunsafe-none` on responses. These map to the web's status quo behavior, and seem\ntherefore superfluous. Why should developers set them?\n\nThe core reason is that these defaults are poor fits for today's threats, and we\nought to be working to change them. Proposals like [Embedding Requires Opt-In][embedding]\nand [COOP By Default][coop-by-default] suggest that we should shift the web's\ndefaults away from requiring developers to opt-into more secure behaviors by\nmaking them opt-out rather than opt-in. This would place the configuration cost\non those developers whose projects require risky settings.\n\nThis document recommends setting those less-secure header values explicitly, as\nthat makes it more likely that we'll be able to shift the web's defaults in the\nfuture.\n\n[embedding]: https://github.com/mikewest/embedding-requires-opt-in\n[coop-by-default]: https://github.com/mikewest/coop-by-default\n\n## Acknowledgements\n\nThis document relies upon a number of excellent resources that spell out much of\nthe foundation of our understanding of Spectre's implications for the web, and\njustify the mitigation strategies we currently espouse. The following is an\nincomplete list of those works, in no particular order:\n\n*   Chris Palmer's\n    [Isolating Application-Defined Principles](https://noncombatant.org/application-principals/)\n\n*   Charlie Reis'\n    [Long-Term Web Browser Mitigations for Spectre](https://docs.google.com/document/d/1dnUjxfGWnvhQEIyCZb0F2LmCZ9gio6ogu2rhMGqi6gY/edit)\n\n*   Anne van Kesteren's\n    [A Spectre-Shaped Web](https://docs.google.com/presentation/d/1sadl7jTrBIECCanuqSrNndnDr82NGW1yyuXFT1Dc7SQ/edit#slide=id.p),\n    [Safely Reviving Shared Memory](https://hacks.mozilla.org/2020/07/safely-reviving-shared-memory/),\n    and [Opaque Resource Blocking](https://github.com/annevk/orb).\n\n*   Artur Janc and Mike West's [How do we stop spilling the beans across origins?](https://www.arturjanc.com/cross-origin-infoleaks.pdf)\n\n*   Mike West's [Cross-Origin Embedder Policy explainer](https://wicg.github.io/cross-origin-embedder-policy/)\n\n*   Charlie Reis and Camille Lamy's [Cross-Origin Opener Policy explainer](https://docs.google.com/document/d/1Ey3MXcLzwR1T7aarkpBXEwP7jKdd2NvQdgYvF8_8scI/edit)\n\n*   Artur Janc, Charlie Reis, and Anne van Kesteren's [COOP and COEP Explained](https://docs.google.com/document/d/1Ey3MXcLzwR1T7aarkpBXEwP7jKdd2NvQdgYvF8_8scI/edit)\n\n*   Artur Janc's [Notes on the threat model of cross-origin isolation](https://arturjanc.com/coi-threat-model.pdf)\n"
  },
  {
    "path": "security/permissions-for-powerful-web-platform-features",
    "title": "Controlling Access to Powerful Web Platform Features",
    "content": "# Controlling Access to Powerful Web Platform Features\n\n_Author: [dominickn@chromium.org](mailto:dominickn@chromium.org)_\n_Contributors: [mgiuca@chromium.org](mailto:mgiuca@chromium.org), [rorymcclelland@chromium.org](mailto:rorymcclelland@chromium.org)_\n\n# Overview\n\n[Fugu](https://blog.chromium.org/2018/11/our-commitment-to-more-capable-web.html)\n is a renewed effort to bring\n [powerful new capabilities](https://bugs.chromium.org/p/chromium/issues/list?can=2&q=Proj%3DFugu+&colspec=ID+Pri+M+Stars+ReleaseBlock+Component+Status+Owner+Summary+OS+Modified&x=m&y=releaseblock&cells=ids)\nto the web -- e.g. filesystem read/write access. Allowing users to control\nwhich sites are able to access such APIs is crucial for maintaining the\nsecurity and privacy properties of the web. The impact of restrictions on the\ndeveloper ergonomics and user utility of the API and the web platform overall\nmust also be considered.\n\nThis document explores approaches to guarding powerful APIs, e.g. using\n[installed web app state](https://developers.google.com/web/progressive-web-apps/)\nor some other proxy for high user\n[engagement](https://www.chromium.org/developers/design-documents/site-engagement).\nThe following general principles summarise the overall approach of the\nChromium project to evaluating how powerful new features should be controlled\non the web:\n\n+   __Access to powerful APIs__ should be available to the entire web platform\n    of secure contexts, with control managed exclusively by choosers, prompts,\n    or other user consent UX at time-of-use.\n+   __API-specific restrictions__ on the scope of access may also be used to\n    guard against potential abuse (e.g. constraints on available directories /\n    files, no access without a currently open foreground tab).\n+   Usage of powerful APIs should be clearly __disclosed__ to users, ideally\n    using a central hub that offers users __control__ over what sites can use\n    which capabilities.\n+   Installing a web app is associated with __persistence__, and thus\n    persistent and/or background access to powerful APIs may only be granted\n    (possibly subject to additional requirements) to installed web apps.\n    Non-installed sites may still request and be granted permission to use\n    powerful APIs, but should not have their access persisted.\n+   Installation or engagement alone __should not act as a vote of trust__ for\n    either granting access or enabling the ability to ask for access to\n    powerful APIs.\n+   Separately, efforts should be made to __curtail the existing persistency__\n    on the web platform outside of installed web apps, e.g. time-limiting\n    permission grants, more aggressively expiring cookies, and restricting\n    background task execution.\n\nThe remainder of this document explains the reasoning behind these principles,\nand summarises why alternative proposals were not taken up.\n\n# Definition of Terms\n\n+   [__Powerful Web Platform APIs__](https://www.chromium.org/Home/chromium-security/prefer-secure-origins-for-powerful-new-features)\n    are capabilities which carry inherent security or privacy risks when used,\n    but also provide users of web apps with significant utility. A canonical\n    example is local file system access (i.e. allowing web sites to directly\n    read and write from certain locations on the user's device). Many such\n    capabilities already exist on the web in every browser (e.g. access to\n    camera and microphone hardware), while new capabilities are under constant\n    development.\n+   [__Installation__](https://www.w3.org/TR/appmanifest/#installable-web-applications)\n    is the process where a web site may be elevated to run with a more native\n    UX treatment on a given platform and device. It is usually tied with being\n    granted a presence in the platform launcher (e.g. the desktop).\n+   [__Progressive Web Apps__](https://developers.google.com/web/progressive-web-apps/)\n    (PWAs) are web sites which are designed to be installable.\n+   [__Engagement__](https://www.chromium.org/developers/design-documents/site-engagement)\n    is a mechanism for measuring how much users interact with a site. Higher\n    engagement may be a signal that the user derives significant utility from\n    a site.\n\n\n# Principles for Access to Powerful APIs\n\nThis section outlines the general principles that the Chromium team believes\nare critical when designing access to powerful new web platform APIs. These\nprinciples will be considered when evaluating how new APIs are designed.\n\n## Control and Transparency\n\n+   Users must be able to see when the powerful APIs in use, and what sites\n    are using them.\n+   Users may revoke access to powerful APIs at any time, and preferably,\n    the revocation UI should be intuitive to locate based on how permission is\n    granted and/or disclosed to the user.\n+   The principle of least privilege should apply as broadly as possible:\n    users should not have to grant access to more resources than necessary to\n    achieve their goals.\n\n## User Ergonomics\n\n+   The web platform should be fully functional without requiring\n    installation.\n+   Sites must not be able to easily socially-engineer the user into\n    granting permissions they wouldn't otherwise want to.\n+   As much as is practical, users should not be bombarded with permission\n    prompts, as this leads to decision fatigue.\n\n## Developer Predictability\n\n+   Developers must be able to know when they can and cannot access\n    powerful APIs.\n+   Access mechanisms should be cross-browser compatible.\n\n# Proposal\n\n### Baseline: secure contexts, top-level frames, user gesture\n\nMinimally, all new powerful APIs must only be available in\n[secure contexts](https://www.chromium.org/Home/chromium-security/deprecating-powerful-features-on-insecure-origins).\nIdeally, availability is restricted to top-level frames and requires a user\ngesture to trigger. When a webpage is running in a secure context in\na top-level frame with an active user gesture, we call this situation\na __baseline context__.\n\n[Browser extensions](https://developer.chrome.com/extensions) are also\nincluded as a baseline context, as they can already make use of web platform\nAPIs (subject to the same access checks as web sites).\n\nThe\n[permission delegation](https://docs.google.com/document/d/1x5QejvpyQ71LPWhMLsaM1lWCfSsBsSQ8Dap9kJ6uLv0/preview?ts=5b857603#heading=h.ib6rctasbt3y)\nmechanism may be used to extend privileges to iframes on a page if it makes\nsense for a powerful capability to be delegated in this way.\n\n### The entire web platform may access new powerful APIs\n\nIn general, any baseline context may access powerful APIs, regardless of its\nwindowing state (in the tabbed browser or in a standalone app window),\ninstallation state (installed or not), or user engagement (highly interacted\nwith or not). This avoids the __fragmentation of the web__ into different,\nsometimes unpredictable states, and encourages careful consideration of new\nAPI surfaces such that they are exposed in a way that is safe for the web at\nlarge.\n\n### Session-based access is granted by direct user consent\n\nIn general, access to powerful APIs must be mediated by direct, informed user\nconsent while the requesting site is open in a foreground tab via mechanisms\nwhich may include:\n\n+   choosers\n+   prompts\n\nThese mechanisms must clearly disclose the origin of the request, and follow\nChromium's\n[guidelines on displaying URLs](https://chromium.googlesource.com/chromium/src/+/main/docs/security/url_display_guidelines/url_display_guidelines.md).\nImplementations may be tested using tools such as\n[Trickuri](https://github.com/chromium/trickuri).\n\nAs much as possible, APIs should avoid a \"double prompt\", e.g. a permission\nprompt requesting access to the file system, followed by a chooser to pick the\nfile/directory to access. There is little security or privacy benefit to such\na double prompt, and it detrimentally affects user and developer ergonomics.\n\nThere are cases where double prompts are unavoidable, e.g. a web site may\nrequest access to contact information, and if the user grants access, the\nbrowser may need to request OS-level permission to service the request.\n\nIn some cases, Chromium may implicitly grant access to an API if it is not\nparticularly dangerous or does not make sense to guard behind a permission\nconsent. An example of this is the\n[Badging API](https://github.com/WICG/badging/blob/master/explainer.md), which\nonly works for installed web apps, and results in a subtle badging effect on\nthe installed app icon that is not invasive or privacy-sensitive. These cases\nshould be relatively rare considering the powerful APIs that are covered by\nthis document.\n\nThe scope of access to APIs follows the web's same-origin model.\n\n### Persistent and/or background access is restricted to installed apps\n\nThe only definite capability granted by installation is __persistence__. By\ninstalling, the user has explicitly indicated that they want the web app to\nhave a persistent presence on their system.\n\nNew powerful APIs should exclusively use session-based permissions for web\nsites that are not installed. In particular, there should be no access while\nthe site is not open in a tab, and access cannot be requested from a\nnon-foreground tab. When the site is closed or navigated away from, it loses\nany granted access to powerful APIs it had, and must re-ask for access the\nnext time the user visits.\n\nInstalled web sites _may_ instead receive a permanent grant, which is removed\nwhen the site is uninstalled. In this way, installed web sites _may_ be\ngranted the ability to access capabilities in the background, depending on the\nparticular details of each capability. It also avoids overloading the\ninstallation decision with consequences that users may not expect.\n\nPersistency for installed web sites may have other requirements, but\nnon-installed sites may never receive persistent grants to access powerful new\nAPIs.\n\nSome powerful APIs act as a proxy for persistence (e.g. a web site with\npermission to write files to disk). We distinguish persistence via a currently\ngranted capability from __persistent access to the capability itself__; it is\nthe latter privilege which is granted by being installed.\n\n### Disclose access and provide obvious user controls\n\nIt should be obvious to users when sites are using powerful APIs, and they\nshould be given the tools needed to effectively manage and revoke access when\nnecessary.\n\n### Improve permissions and installation UX to avoid decision fatigue\n\nRemoving persistent access from the drive-by web may drive up\n[decision fatigue](https://en.wikipedia.org/wiki/Decision_fatigue) due to\noverprompting. However, we anticipate that many of these capabilities will\nhave relatively niche applications that the drive-by web should not commonly\naccess.\n\nRepeated granting of access to powerful APIs can be used as a signal for\ninstallation. For instance, after two successful powerful permission grants,\nChromium could present the user with the option to install the app on the\nthird permission request.\n\n### Administrator policies may override prompts and enforce persistence\n\nPowerful new capabilities may be paired with\n[Chromium policies](https://cloud.google.com/docs/chrome-enterprise/policies)\nwhich permit administrators to enforce persisted access to capabilities\nwithout prompts. Capabilities may also be restricted or blocked by such\npolicies. This is in line with how many existing permissions have admin policy\noverrides.\n\n### Explore ways of curtailing the lifetime of existing persistence\n\nCurrently, web sites which are not installed have access to significant\npersistence mechanisms:\n\n+   all existing permissions have their decisions persisted indefinitely\n+   cookie and local storage lifetime may be indefinite\n\nTo better align the existing web to the proposal presented here, we suggest a\nparallel effort to apply new lifetime limits on existing persistence\nmechanisms. For example, some of the following measures could be explored:\n\n+   forget any granted permissions if the site has not been visited in X weeks\n    +   this could be challenging to apply for some capabilities such as push\n        notifications, where there is a use case for a site being able to send\n        informative notifications without ever needing to be opened.\n+   ignore cookie Max-Age headers for non-installed sites, and erase cookies\n    when the associated URL is no longer in browser history.\n+   restrict durable/persistent storage to installed apps.\n+   limit the storage quota available to sites unless they are installed (and\n    conversely, raise storage quota limits for installed sites).\n+   restrict what Service Workers may do in the background unless they control\n    a site that is installed.\n\n# Case Study -- File System Access\n\nWe describe a high level case study based on the principles in this document\nfor granting a web site access to a) read any file in a certain directory; b)\nwrite files to a directory.\n\n## Granting access\n\n+   The user must give direct consent via a file picker.\n+   The browser should disclose that access to the chosen file or directory\n    will be granted to the web site.\n+   A non-installed site will trigger the picker each page load that the API\n    to read or write to a directory is invoked.\n+   An installed site will have its access to reading or writing persisted.\n\n## Additional considerations for writing to a directory\n\n+   Existing Downloads UI and protections (e.g. malicious file scanning)\n    could be employed to ensure sites cannot use writing to a directory as a\n    bypass.\n+   Where such scanning isn't accessible, confirmation prompts for opening\n    dangerous files in the Downloads UI may be employed instead to ensure the\n    user knows if a potentially malicious file type is being stored.\n+   Non-installed sites may have additional restrictions on which folders\n    are available to choose (e.g. only allowing the user's Downloads directory,\n    or excluding the user's Documents directory).\n\n# Alternatives Considered\n\n## Guard API Access Behind Installation\n\nApps on any desktop or mobile platform require installation to run, and when\ninstalled, apps are automatically granted many privileges. We could extend\nthis concept to the web by restricting powerful APIs like the File System\nAccess API only to installed web apps. That is, the drive-by web could not even\nask for permission to access an API -- the site would need to be installed.\n\nA key argument for using installation in this manner is that some APIs are\nsimply so powerful that the drive-by web should not be able to ask for them.\nHowever, this document takes the position that installation alone as a\nrestriction is undesirable.\n\n### Pros:\n\n+   This is a simple model that is both user controllable and\n    developer-accessible (via installation APIs such as\n    [beforeinstallpromptevent](https://developers.google.com/web/fundamentals/app-install-banners/)).\n+   Installation is already synonymous with some amount of elevated privilege\n    on most platforms.\n\n### Cons:\n\n+   Fragments the web platform into installed and not installed, with\n    different APIs available depending on installed state.\n+   Disempowers the drive-by web, and undermines the \"try-before-you-buy\"\n    ability that the web affords today. Users may also not be willing to\n    install a site from which they cannot ascertain any benefit.\n+   Forces users to install a site to use it, even if they don't want to\n    install.\n+   May encourage web sites to prompt users on every visit to install the PWA\n    to get access to powerful features.\n+   Creates confusing scenarios when web apps are running in tabs:\n    +   If a web app is installed but the user opens the site in a\n        tab, does it get access to the capability or not?\n    +   What about if the web app starts off in a standalone window, but\n        the user reparents it into a tab? Or if a user sets an installed app to\n        open in a tab?\n+   Platforms where installation grants privilege all incur additional\n    friction during installation that the web currently does not exhibit.\n    Examples include:\n    +   requiring something to be downloaded,\n    +   requiring a confirmation prompt, installer, or some explicit\n        privilege or grant during installation,\n    +   requiring an explicit display of permissions that are implicitly\n        granted.\n+   The implicit granting of privilege by installation has proven to be a\n    security and privacy challenge on many platforms, e.g. Android native apps\n    can access many\n    [powerful features](https://developer.android.com/guide/topics/permissions/overview#normal-dangerous)\n    with no permission prompt and without user recourse to revoke access.\n+   The amount of friction generated by PWA installation over the drive-by\n    web is unclear, and gating APIs behind installation increases the\n    incentive for tricking users into installing.\n\n### Security considerations:\n\nRestricting APIs to installed web apps is not a meaningful security improvement\nfor users for several reasons:\n\n+   While it eliminates the drive-by web as an attack surface, per-API\n    security mitigations (e.g. restricting which directories are accessible\n    for reading and writing) would still be necessary to protect users of\n    installed web apps.\n+   The effectiveness of installation as a gate on access to powerful APIs\n    weakens as the installed web app model becomes more successful.\n+   Developers are incentivised to ask for installation to utilise powerful\n    capabilities, contributing to the erosion of installation as an effective\n    security mitigation.\n+   Per-API permission requests would still be necessary in the installed\n    state, making installation effectively equivalent to an implicit permission\n    grant to ask for permission to access powerful features. We should simply\n    ask users directly if they wish to grant permission, rather than use such a\n    two-tiered requirement.\n\n## Guard API Access Behind Engagement\n\nThis is a more general concept than installation: that continual, significant\nusage of a web site should allow that site to access more powerful APIs.\n\n### Pros:\n\n+   Continual usage of a web site can be taken as a signal of trust\n\n### Cons:\n\n+   Engagement is not standardised or exposed to the web platform, making\n    it a highly unpredictable and unergonomic mechanism of controlling access.\n    +   Standardisation and a web-exposed API would both be requirements for\n        using engagement in this way. Chromium's current engagement\n        implementation is local-only and not web-exposed. There are serious\n        privacy questions about exposing such data to the web.\n+   Solving the first-run problem is non-trivial: engagement requires usage\n    to accumulate, but there are apps which legitimately require access to a\n    powerful API immediately to function (e.g. an editor -> files, or a\n    weather site -> location).\n\n### Security considerations:\n\n+   There is no real evidence to support the implicit assumption under\n    this model that continual usage of a web site correlates with user consent\n    to access powerful features.\n    +   Consider users who frequently use some web site for which location\n        data is useful, but denies that site persistent, background access to\n        geolocation permission.\n    +   A proportion of web traffic goes to sites which users may interact\n        with frequently, but may not necessarily want to grant powerful\n        capabilities.\n\nSimilar to installation, the Chromium team does not regard engagement as a\nrobust way of controlling access to APIs.\n"
  },
  {
    "path": "security/overview",
    "title": "Security & Safety",
    "content": "# Security & Safety\r\n\r\nWelcome to the Security section! This area covers the comprehensive security model and safety mechanisms implemented in the Wanderlust custom Chromium browser.\r\n\r\n## What You'll Find Here\r\n\r\nThis section provides essential security information:\r\n\r\n- **[Security Model](security-model.md)**: Comprehensive overview of browser security architecture and principles\r\n- **Sandboxing**: Process isolation and security boundaries\r\n- **Permission Management**: User permission systems and access controls\r\n- **Security Updates**: Keeping the browser secure with regular updates\r\n\r\n## Security Principles\r\n\r\nOur custom Chromium implementation follows these core security principles:\r\n\r\n### Defense in Depth\r\n- **Multiple Security Layers**: No single point of failure\r\n- **Process Isolation**: Separate processes for different security contexts\r\n- **Sandboxing**: Restricted execution environments for untrusted content\r\n- **Permission Models**: Granular control over resource access\r\n\r\n### Security by Design\r\n- **Least Privilege**: Components operate with minimal necessary permissions\r\n- **Secure Defaults**: Safe configurations out of the box\r\n- **Input Validation**: Comprehensive validation of all external inputs\r\n- **Secure Communication**: Encrypted channels for all sensitive data\r\n\r\n## Key Security Components\r\n\r\n### Process Security\r\n- **Site Isolation**: Each site runs in its own process\r\n- **Renderer Sandboxing**: Web content runs in restricted environments\r\n- **Privilege Separation**: Different privilege levels for different components\r\n\r\n### Network Security\r\n- **TLS/HTTPS**: Secure communication protocols\r\n- **Certificate Validation**: Robust certificate checking\r\n- **Mixed Content Protection**: Preventing insecure content on secure pages\r\n\r\n### Content Security\r\n- **Content Security Policy (CSP)**: Preventing code injection attacks\r\n- **Same-Origin Policy**: Controlling cross-origin resource access\r\n- **Subresource Integrity**: Ensuring resource authenticity\r\n\r\n## Security Threats and Mitigations\r\n\r\n### Common Attack Vectors\r\n- **Cross-Site Scripting (XSS)**: Mitigated through CSP and input sanitization\r\n- **Cross-Site Request Forgery (CSRF)**: Protected by SameSite cookies and tokens\r\n- **Code Injection**: Prevented through sandboxing and input validation\r\n- **Man-in-the-Middle**: Blocked by certificate pinning and HSTS\r\n\r\n### Browser-Specific Security\r\n- **Extension Security**: Secure extension architecture and permissions\r\n- **Download Protection**: Scanning and validation of downloaded files\r\n- **Safe Browsing**: Protection against malicious websites and downloads\r\n\r\n## Integration with Browser Architecture\r\n\r\nSecurity integrates deeply with:\r\n- [Architecture](../architecture/overview.md): Security boundaries in the process model\r\n- [Modules](../modules/overview.md): Security considerations for each module\r\n- [Debugging](../debugging/overview.md): Security-focused debugging techniques\r\n\r\n## Security Best Practices\r\n\r\nFor developers working on security-sensitive code:\r\n\r\n1. **Follow Security Guidelines**: Adhere to established security coding practices\r\n2. **Regular Security Reviews**: Code reviews with security focus\r\n3. **Security Testing**: Comprehensive testing including security scenarios\r\n4. **Stay Updated**: Keep informed about latest security threats and mitigations\r\n\r\n## Compliance and Standards\r\n\r\nOur security implementation follows:\r\n- **Web Security Standards**: W3C and WHATWG security specifications\r\n- **Industry Best Practices**: OWASP guidelines and recommendations\r\n- **Regulatory Requirements**: Compliance with relevant security regulations\r\n\r\n---\r\n\r\n*Start with our [security model documentation](security-model.md) to understand the foundational security architecture and principles.*\r\n"
  },
  {
    "path": "security/overlay-policy",
    "title": "No Cross-Origin Full-Page Overlays",
    "content": "# No Cross-Origin Full-Page Overlays\n\nChrome features should not include cross-origin overlays that fully obscure an\nactive web page's content area, even temporarily.\n\nIf the active page is partially obscured, we should provide sufficient cues to\nthe user that the active page is still present and active. Partial overlays can\nbe dangerous or confusing as well and should be designed carefully with input\nfrom security and Chrome UX.\n\n## Why?\n\nChrome has had multiple features in the past that attempt to show a full page,\ncross-origin overlay above an active web page.  In practice, these have created\ndangerous situations with a large variety of bugs and security issues that are\ndifficult to address in general, so we have adopted a security UX policy that\ndisallows full page overlays.\n\nFeatures that have used or proposed full page overlays:\n* **Interstitial Pages**: Security interstitial pages (e.g., for TLS or Safe\n    Browsing warnings, as listed at `chrome://interstitials`) used an overlay\n    approach for many years, with [dozens of bugs](https://crbug.com/392354)\n    due to the active page underneath. This feature eventually migrated to use\n    committed error pages instead in [448486](https://crbug.com/448486), as\n    documented\n    [here](https://chromium.googlesource.com/chromium/src/+/main/docs/navigation_concepts.md#Interstitial-Pages).\n  * Example: [392354](https://crbug.com/392354): Interstitials are weird and do\n    weird things. This lists 68 dependent bugs (!), spanning a wide variety of\n    affected features.\n* **Prerendering v1**: The original version of prerendering loaded the next page\n    in a hidden active state, and it had many challenges with detecting any\n    observable behavior and discarding the page. This created\n    [complexity](https://docs.google.com/presentation/d/1Ag_LJW_OoIZI--YcZIvv6JicGjECTmrM5xKk8zd_wZg/edit#slide=id.g146ba3c676_0_2),\n    [maintenance challenges](https://docs.google.com/document/d/16VCYGGWau483IMSxODpg5faZny1FJ6vNK2v-BuM5EhU/edit#heading=h.tp04wz80qf7b),\n    and posed \"a burden requiring constant investment of 1-2 SWEs.\" The feature\n    was eventually removed and replaced by Prerendering v2, which uses\n    restricted Mojo interfaces to limit what can occur before it is displayed.\n  * Example: [439545](https://crbug.com/439545) - Desktop notifications\n  * Example: [520275](https://crbug.com/520275) - Voice synthesis audio\n  * Example: [247848](https://crbug.com/247848) - Infobars\n* **Google Lens screenshots**: Google Lens showed a screenshot overlay above an\n    active page, and encountered security bugs where the page changed\n    underneath the screenshot. This feature changed to semi-transparent masks.\n  * Example: [1242424](https://crbug.com/1242424): Lens Screenshot URL spoof. A\n    Google Lens screenshot overlay above the active page was not dismissed for\n    all types of navigations, creating a mismatch between the displayed\n    content's origin and the URL in the omnibox. Due to a full-page overlay\n    concern in comment 4, Lens was changed in\n    [1247917](https://crbug.com/1247917) to use a semitransparent mask to show\n    underlying page content.\n* **Cross-Origin Paint Holding**: Chrome shows the last paint of the previously\n    committed page after a new page commits for up to 4 seconds, until the new\n    page paints. This reduces the amount of time the user is looking at a blank\n    page, but also creates a discrepancy between the address bar and the\n    content shown in the tab. The stale content is not interactive, but it has\n    been used in many URL spoof reports for which we haven't had a great\n    response. It is also possible for the new active page to show prompts or\n    other disruptive behavior before painting, while the stale paint is still\n    visible (see [1447077](https://crbug.com/1447077)). This feature is still\n    enabled, with attempts to mitigate the issues (e.g., disabling input). We\n    may choose whether to continue to mitigate these issues or find a safer way\n    to approach this feature.\n  * Discussion of concerns: [721145](https://crbug.com/721145)\n  * URL spoofs requiring fixes to the 4 second timer:\n    [497588](https://crbug.com/497588), [672847](https://crbug.com/672847),\n    [739621](https://crbug.com/739621), [844881](https://crbug.com/844881),\n    [1152894](https://crbug.com/1152894)\n  * URL spoof reports we choose not to fix: [776402](https://crbug.com/776402)\n    (comment 14 discusses our desire to find a safer UX),\n    [755058](https://crbug.com/755058), [784395](https://crbug.com/784395),\n    [1218366](https://crbug.com/1218366), [1237742](https://crbug.com/1237742),\n    [1304041](https://crbug.com/1304041)\n    * In several WontFix cases, the bug was exploitable for a year or more\n      before the repro independently stopped working, but we had no means of\n      addressing it, and the underlying issues remain present even if\n      particular repro steps stop working.  In other cases, we had to close\n      issues explaining that we can't do any better due to paint holding, which\n      can undermine our credibility with vulnerability reporters.\n* **Default Navigation Transitions**: Proposed to show a full-page screenshot of\n    a cross-origin page above an active page while a navigation is in progress.\n    Pursuing alternative approaches on most Chrome platforms, though a full\n    screen overlay is currently in use on Chrome for iOS.\n\n## Examples of Overlay Problems\n\nThere are many things that an active but hidden page can do to pose security or\nfunctional issues while the overlay is displayed, including but not limited\nto:\n* Sensors continue to record\n  * Camera and microphone continue to record (sometimes without or with\n    inaccurate attribution)\n  * Geolocation\n* Input events may be surprisingly sent to the hidden active page\n* Prompts may appear above the overlay, out of context\n* Popups may appear, if a user activation is available\n* Fullscreen mode may activate, obscuring the overlay\n* Audio continues to play\n* The page may continue to update unexpectedly\n  * Videos continue to play past the point the user observed\n* Built-in Chrome features or extensions may not be aware of the overlay and do\n  surprising things\n  * Accessibility\n  * DevTools\n  * Saving or printing pages\n* Many Chrome features use content/ APIs to access the last committed URL or\n  origin for functional or security purposes, being unaware of the cross-origin\n  overlay.\n\nThis list is not exhaustive and the capabilities of web pages continue to expand\nover time, so it is difficult to prevent all observable or dangerous behavior\nwhile a user is unaware that a page is still present. This would be a fail-open\napproach requiring us to diagnose and fix all problematic cases. Attempts to do\nso in the past (e.g., Prerendering v1) have resulted in an unsustainable\nmaintenance burden.\n\nA hidden active page also affects the user's mental model: they are more likely\nto think the page has been unloaded, ending up confused if Chrome later reveals\nit has been active but hidden.\n\nEven temporary cases, such as showing an overlay until a timeout, can pose risks\nto users, such as a \"hot mic\" situation if a video call appears to end but is\nstill in progress.\n\n## Comparisons to Non-Overlay Cases\n\nThere are other situations where active web pages may continue to run while not\nvisible to the user, such as in a background tab.  Chrome's UI indicates the\nchange of security context to the user in these cases (e.g., using a different\ntab and address bar contents), and continues to indicate the previous page\nexists via the tab strip or tab switcher.  In contrast, full-page overlays\nwithin a given tab imply to the user that the active page is no longer present,\neven though it continues to run.\n\nBFcache and Prerendering are two other features that keep pages in a hidden but\nfrozen or restricted state.  These pages are not fully active as in the overlay\ncase above.  However, not all pages can be put into this frozen or restricted\nstate, depending on which APIs are in use, whether the page opts out, which\nother pages are in the same process, etc.  In such cases, BFcache and\nPrerendering evict or discard the page.  As a result, freezing a page is not a\ngeneral technique that can be used when showing an overlay above any arbitrary\npage.\n"
  },
  {
    "path": "security/origin-vs-url",
    "title": "Use origin (rather than URL) for security decisions.",
    "content": "# Use origin (rather than URL) for security decisions.\n\nURLs are often not sufficient for security decisions, since the origin\nmay not be present in the URL (e.g., `about:blank`),\nmay be tricky to parse (e.g., `blob:` or `filesystem:` URLs),\nor may be\n[opaque](https://html.spec.whatwg.org/multipage/origin.html#concept-origin-opaque)\ndespite a normal-looking URL (e.g., the security context may be\n[sandboxed](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe#attr-sandbox)).\nUse origins whenever possible.\n\n\n## Use `GetLastCommittedOrigin()` or `GetSecurityContext()`.\n\n### Good\n\n```c++\n// Example of browser-side code.\ncontent::RenderFrameHost* frame = ...;\nif (safelist.Matches(frame->GetLastCommittedOrigin()) {\n  // ...\n}\n\n// Example of renderer-side code.  Note that browser-side checks are still\n// needed to ensure that a compromised renderer cannot bypass renderer-side-only\n// checks.\nblink::Frame* frame = ...;\nif (safelist.Matches(frame->GetSecurityContext()->GetSecurityOrigin()) {\n  // ...\n}\n```\n\n### Bad\n\n```c++\n// Example of browser-side code.\ncontent::RenderFrameHost* frame = ...;\nif (safelist.Matches(frame->GetLastCommittedURL()) {\n  // BUG: doesn't work for about:blank or sandboxed frames.\n}\n\n// Example of renderer-side code.  Note that browser-side checks are still\n// needed to ensure that a compromised renderer cannot bypass renderer-side-only\n// checks.\nblink::LocalFrame* frame = ...;\nif (safelist.Matches(frame->GetDocument()->Url()) {\n  // BUG: doesn't work for about:blank or sandboxed frames.\n  // BUG: doesn't work for RemoteFrame(s) which don't have a local Document\n  //      object and don't know the URL (only the origin) of the frame.\n}\n```\n\n\n## Don't use the `GURL` type to store origins.\n\n`GURL origin` is an anti-pattern - representing origins as a URL-focused data\ntype means that 1) some information is lost (e.g., origin's nonce and precursor\ninformation) and 2) some unnecessary information may be present (e.g., URL path\nand/or query parts are never part of an origin).\n\nUse the following datatypes to represent origins:\n\n- C++: `url::Origin` or `blink::SecurityOrigin`\n  (instead of `GURL` or `blink::KURL`).\n- Mojo: `url.mojom.Origin`\n  (instead of `url.mojom.Url`).\n  Remember to\n  [validate data](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/security/mojo.md#Validate-privilege_presuming-data-received-over-IPC)\n  received over IPC from untrustworthy processes\n  or even better\n  [avoid sending origins](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/security/mojo.md#Do-not-send-unnecessary-or-privilege_presuming-data)\n  in the first place.\n- Java: `org.chromium.url.Origin`\n  (see also `url::Origin::FromJavaObject` and `url::Origin::ToJavaObject`).\n\n\n## Avoid converting URLs to origins.\n\n### Good\n\n```c++\nurl::Origin origin = GetLastCommittedOrigin();\n```\n\n### Bad\n\n```c++\nGURL url = ...;\nGURL origin = url.DeprecatedGetOriginAsURL();\n// BUG: `origin` will be incorrect if `url` is an \"about:blank\" URL\n// BUG: `origin` will be incorrect if `url` came from a sandboxed frame.\n// BUG: `origin` will be incorrect when `url` (rather than\n//      `base_url_for_data_url`) is used when working with loadDataWithBaseUrl\n//      (see also https://crbug.com/1201514).\n// BUG: `origin` will be empty if `url` is a blob: URL like\n//      \"blob:http://origin/guid-goes-here\".\n// NOTE: `GURL origin` is also an anti-pattern; see the \"Use correct type to\n//       represent origins\" section below.\n\n// Blink-specific example:\nKURL url = ...;\nscoped_refptr<SecurityOrigin> origin = SecurityOrigin::Create(url);\n// BUG: `origin` will be incorrect if `url` is an \"about:blank\" URL\n// BUG: `origin` will be incorrect if `url` came from a sandboxed frame.\n```\n\n### Risky\n\nIf you know what you are doing and really need to convert a URL into an origin,\nthen you can consider using `url::Origin::Create`, `url::SchemeHostPort`, or\n`url::Origin::Resolve` (noting the limitations of those APIs - see below for\nmore details).\n\n```c++\nconst GURL& url = ...;\n\n// WARNING: `url::Origin::Create(url)` can give unexpected results if:\n// 1) `url` is \"about:blank\", or \"about:srcdoc\"\n//    (returning unique, opaque origin rather than the real origin of the frame)\n// 2) `url` comes from a sandboxed frame\n//    (potentially returning a non-opaque origin, when an opaque one is needed)\n// 3) `base_url_for_data_url` should be used instead of `url`\n//\n// WARNING: `url::Origin::Create(url)` has some additional subtleties:\n// 4) if `url` is a blob: or filesystem: URL like \"blob:http://origin/blob-guid\"\n//    then the inner origin will be returned (unlike with `url::SchemeHostPort`)\n// 5) data: URLs will be correctly be translated into opaque origins, but the\n//    precursor origin will be lost (unlike with `url::Resolve`).\nurl::Origin origin = url::Origin::Create(url);\n\n// WARNING: `url::SchemeHostPort(url)` will *mechanically* extract the scheme,\n// host, and port of the URL, discarding other parts of the URL.  This may have\n// unexpected results when:\n// 1) `url` is \"about:blank\", or \"about:srcdoc\"\n// 2) `url` comes from a sandboxed frame, i.e. when an opaque origin is expected\n// 3) `url` is a data: URL, i.e. when an opaque origin is expected\n// 4) `url` is a blob: or filesystem: URL like \"blob:http://origin/blob-guid\"\n//    (the inner origin will *not* be returned - unlike `url::Origin::Create`)\nurl::SchemeHostPort scheme_host_port = url::SchemeHostPort(url);\n\n// `url::Origin::Resolve` should work okay when:\n// 1) `url` is \"about:blank\", or \"about:srcdoc\"\n// 2) `url` comes from a sandboxed frame (i.e. when `base_origin` is opaque)\n// 3) `url` is a data: URL (i.e. propagating precursor of `base_origin`)\n// 4) `url` is a blob: or filesystem: URL like \"blob:http://origin/blob-guid\"\n//\n// WARNING: It is simpler and more robust to just use `GetLastCommittedOrigin`\n// (instead of combining `GetLastCommittedOrigin` and `GetLastCommittedURL`\n// using `url::Origin::Resolve`).\n// WARNING: `url::Origin::Resolve` is unaware of `base_url_for_data_url`.\nconst url::Origin& base_origin = ...\nurl::Origin origin = url::Origin::Resolve(url, base_origin);\n```\n"
  },
  {
    "path": "security/mojo",
    "title": "Mojo \"Style\" Guide",
    "content": "# Mojo \"Style\" Guide\n\nMojo is Chrome's new IPC system and provides lots of useful abstractions. These\nabstractions can make it easier to write code that makes interprocess calls,\nbut can also add significant complexity. Below are some recommendation from\nMojo and IPC reviewers for best practices.\n\nFor questions, concerns, or suggestions, reach out to\n[chromium-mojo@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/chromium-mojo).\n\n> For legacy IPC, please see [security tips for IPC][security-tips-for-ipc].\n\n[TOC]\n\n\n## Simplicity\n\nStrive to write simple interfaces. Minimize the amount of cross-process state\nthat needs to be maintained in sync.\n\n**_Good_**\n\n```c++\ninterface TeleporterFactory {\n  Create(Location start, Location end) => (pending_remote<Teleporter>);\n};\n\ninterface Teleporter {\n  TeleportGoat(Goat) = ();\n};\n```\n\n**_Bad_**\n\n```c++\ninterface Teleporter {\n  // Bad: comments will need to explicitly call out that both locations need to\n  // be bound before calling TeleportGoat()!\n  //\n  // In addition, if untrustworthy processes can talk to trustworthy processes,\n  // the Teleporter implementation will need to also handle the case where the\n  // Location objects are not yet bound.\n  SetStart(Location);\n  SetEnd(Location);\n  TeleportGoat(Goat) = ();\n};\n```\n\nSimilarly, strive to make methods focused. Do not overuse optional types.\n\n**_Good_**\n\n```c++\nstruct TeleporterStats {\n  AnimalStats animal_stats;\n  FungiStats fungi_stats;\n  GoatStats goat_stats;\n  PlantStats plant_stats;\n};\n\ninterface Teleporter {\n  TeleportAnimal(Animal) => ();\n  TeleportFungi(Fungi) => ();\n  TeleportGoat(Goat) = ();\n  TeleportPlant(Plant) => ();\n\n  // TeleporterStats will be have a value if and only if the call was\n  // successful.\n  GetStats() => (TeleporterStats?);\n};\n```\n\n**_Bad_**\n\n```c++\ninterface Teleporter {\n  // The intent of four optional arguments is unclear: can this call teleport\n  // multiple objects of different types at once, or is the caller only\n  // supposed to only pass one non-null argument per call?\n  Teleport(Animal?, Fungi?, Goat?, Plant?) => ();\n\n  // Does this return all stats if success is true? Or just the categories that\n  // the teleporter already has stats for? The intent is uncertain, so wrapping\n  // the disparate values into a result struct would be cleaner.\n  GetStats() =>\n      (bool success, AnimalStats?, FungiStats?, PlantStats?, FungiStats?);\n};\n```\n\n\n## Documentation\n\nMojo structs, interfaces, and methods should all have comments. Make sure the\ncomments cover the \"how\" and the \"why\" of using an interface and its methods,\nand not just the \"what\". Document preconditions, postconditions, and trust: if\nan interface is implemented in the browser process and handles requests from\nthe renderer process, this should be mentioned in the comments. Complex features\nshould also have an external `README.md` that covers the high-level flow of\ninformation through interfaces and how they interact to implement the feature.\n\n**_Good_**\n\n```c++\n// Interface for controlling a teleporter. Lives in the browser process, and\n// used to implement the Teleportation over Mojo IPC RFC.\ninterface Teleporter {\n  // Teleportation helpers for different taxonomic kingdoms. Teleportation is\n  // not complete until the reply callback is invoked. The caller must NOT\n  // release the sender side resources until the reply callback runs; releasing\n  // the resources early will cause splinching.\n  TeleportAnimal(Animal) => ();\n  TeleportFungi(Fungi) => ();\n  // Goats require a specialized teleportation channel distinct from\n  // TeleportAnimal to ensure goatiness isolation.\n  TeleportGoat(Goat) => ();\n  TeleportPlant(Plant) => ();\n\n  // Returns current teleporter stats. On failure (e.g. a teleportation\n  // operation is currently in progress) a null stats object will be returned.\n  GetStats() => (TeleporterStats?);\n};\n```\n\n\n## Security\n\nPolicy should be controlled solely by the browser process. \"Policy\" can mean\nany number of things, such as sizes, addresses, permissions, URLs, origins,\netc. In an ideal world:\n\n1.  Unprivileged process asks for a capability from the privileged process that\n    owns the resource.\n1.  Privileged process applies policy to find an implementation for the\n    capability.\n1.  Unprivileged process performs operations on the capability, constrained in\n    scope.\n\nThe privileged process must own the capability lifecycle.\n\n\n### Do not trust less privileged processes\n\nThis is the overriding principle for all guidelines in this section. When\nreceiving data from a less trusted process, treat the data as if it were\ngenerated by a malicious adversary. Message handlers cannot assume that offsets\nare valid, calculations won't overflow, et cetera.\n\nIn general:\n\n*   the browser process is the most privileged process type and therefore, must\n    be maximally suspicious of its IPC inputs\n*   the renderer and the ARC++ processes are the least privileged and least\n    trustworthy process types\n*   other process types, such as GPU and plugin, fall in between\n\nWhen passing objects up a privilege gradient (from less → more privileged), the\ncallee must validate the inputs before acting on them. When passing objects\ndown a privilege gradient, such as from browser → renderer, it is OK for the\ncallee to trust the caller.\n\n> See also: [Do not Handle Impossible Situations](#Do-not-handle-impossible-situations)\n\n\n### Do not send unnecessary or privilege-presuming data\n\n> Each `BrowserInterfaceBroker` for frames and workers is strongly associated with an\n> origin. Where possible, prefer to use this associated origin rather than\n> sending it over IPC. (See <https://crbug.com/734210> and\n> <https://crbug.com/775792/>).\n\nFor example, the browser process must not (fully) trust the renderer's claims\nabout origins. The browser process should already know what origin the renderer\nis evaluating, and thus should already have this data (for example, see\n`RenderFrameHost::GetLastCommittedOrigin()`). Thus, a method that requires\npassing an origin from the renderer to the browser process has a conceptual\nerror, and quite possibly, a vulnerability.\n\n> Note: there are currently subtle races when using `GetLastCommittedOrigin()`\n> that will be resolved by fixing <https://crbug.com/729021>.\n\nSimilarly, the browser process must not trust the renderer's claims about file\npathnames. It would be unsafe for the browser process to save a downloaded file\nto `~/.bashrc` just because the renderer asked. Instead, it would be better for\nthe browser process to:\n\n1.  Kill the renderer if `basename(pathname) != pathname`, since the renderer is\n    obviously compromised if it makes this mistake.\n1.  Defang the basename, by removing leading dots, et cetera. Note that the\n    definition of proper defanging varies per platform.\n1.  Prepend its own parent directory to the basename, e.g. ~/Downloads.\n\n> TODO(crbug.com/41352236): Even better would be to implement a C++ type\n> performs the appropriate sanitizations and recommend its usage directly here.\n\n\n### Validate privilege-presuming data received over IPC\n\nIf it is not possible to avoid sending privilege-presuming data over IPC (see\nthe previous section), then such data should be verified before being used.\n\n* Browser process:\n    - Use `ChildProcessSecurityPolicy`'s methods like\n      `CanAccessDataForOrigin` or `CanReadFile` to verify IPC messages\n      received from less privileged processes.\n    - When verification fails, ignore the IPC and terminate the renderer process\n      using `mojo::ReportBadMessage` (or using `mojo::GetBadMessageCallback` for\n      messages handled asynchronously).  For legacy IPC, the renderer process\n      may be terminated by calling the `ReceivedBadMessage` function (separate\n      implementations exist for `//content`, `//chrome` and other layers).\n\n\n### Do not define unused or unimplemented things\n\nMojo interfaces often cross privilege boundaries. Having well-defined interfaces\nthat don't contain stubbed out methods or unused parameters makes it easier to\nunderstand and evaluate the implications of crossing these boundaries. Several\ncommon areas to watch out for:\n\n\n#### Do use EnableIf to guard platform-specific constructs\n\nPlatform-specific functionality should only be defined on the platforms where\nit is implemented. Use the Mojo `EnableIf` annotation to guard definitions that\nshould only be visible in certain build configurations.\n\n**_Good_**\n```c++\n// GN file:\nmojom(\"view_bindings\") {\n  // ...\n\n  enabled_features = []\n  if (is_android) {\n    enabled_features += [ \"is_android\" ]\n  }\n}\n\n// mojom definition:\ninterface View {\n  // ...\n\n  [EnableIf=is_android]\n  UpdateBrowserControlsState(bool enable_hiding, bool enable_showing,\n                             bool animate);\n};\n\n// C++ implementation:\nclass View : public mojom::View {\n public:\n  // ...\n\n#if BUILDFLAG(IS_ANDROID)\n  void UpdateBrowserControlsState(bool enable_hiding, bool enable_showing,\n                                  bool animate);\n#endif\n};\n```\n\n**_Bad_**\n```c++\n// mojom definition:\ninterface View {\n  // ...\n\n  UpdateBrowserControlsState(bool enable_hiding, bool enable_showing,\n                             bool animate);\n};\n\n// C++ implementation:\nclass View : public mojom::View {\n public:\n  // ...\n\n#if BUILDFLAG(IS_ANDROID)\n  void UpdateBrowserControlsState(bool enable_hiding, bool enable_showing,\n                                  bool animate) override;\n#else\n  void UpdateBrowserControlsState(bool enable_hiding, bool enable_showing,\n                                  bool animate) override {\n    NOTREACHED();\n  }\n#endif\n};\n```\n\nThe `EnableIf` annotation can be applied to almost anything: imports,\ninterfaces, methods, arguments, constants, structs, struct members, enums,\nenumerator values, et cetera.\n\n\n#### Do not define unimplemented methods\n\nReviewing IPC requires reviewing a concrete implementation of the Mojo\ninterface, to evaluate how the (possibly untrustworthy) inputs are used, what\noutputs are produced, et cetera. If a method is not yet implemented, do not\ndefine it in the interface.\n\n**_Bad_**\n```c++\n// mojom definition:\ninterface Spaceship {\n  EnterHyperspace();\n  ExitHyperspace();\n};\n\n// C++ implementation:\nclass SpaceshipPrototype : public mojom::Spaceship {\n  void EnterHyperspace() { /* TODO(dcheng): Implement. */ }\n  void ExitHyperspace() { /* TODO(dcheng): Implement. */ }\n};\n```\n\n\n#### Do not define placeholder enumerator values\n\nDo not define placeholder enumerator values like `kLast`, `kMax`, `kCount`, et\ncetera. Instead, rely on the autogenerated `kMaxValue` enumerator emitted for\nMojo C++ bindings.\n\nFor UMA histograms, logging a Mojo enum is simple: simply use the two argument\nversion of `UMA_HISTOGRAM_ENUMERATION`:\n\n**_Good_**\n```c++\n// mojom definition:\nenum GoatStatus {\n  kHappy,\n  kSad,\n  kHungry,\n  kGoaty,\n};\n\n// C++:\nUMA_HISTOGRAM_ENUMERATION(\"Goat.Status\", status);\n```\n\nUsing a `kCount` sentinel complicates `switch` statements and makes it harder to\nenforce invariants: code needs to actively enforce that the otherwise invalid\n`kCount` sentinel value is not incorrectly passed around.\n\n**_Bad_**\n```c++\n// mojom definition:\nenum CatStatus {\n  kAloof,\n  kCount,\n};\n\n// C++\nswitch (cat_status) {\n  case CatStatus::kAloof:\n    IgnoreHuman();\n    break;\n  case CatStatus::kCount:\n    // this should never happen\n}\n```\n\nDefining `kLast` manually results in ugly casts to perform arithmetic:\n\n**_Bad_**\n```c++\n// mojom definition:\nenum WhaleStatus {\n  kFail,\n  kNotFail,\n  kLast = kNotFail,\n};\n\n// C++:\nUMA_HISTOGRAM_ENUMERATION(\"Whale.Status\", status,\n                          static_cast<int>(WhaleStatus::kLast) + 1);\n```\n\nFor interoperation with legacy IPC, also use `kMaxValue` rather than defining a\ncustom `kLast`:\n\n**_Good_**\n```c++\nIPC_ENUM_TRAITS_MAX_VALUE(GoatStatus, GoatStatus::kMaxValue);\n```\n\n\n### Use structured types\n\nWhere possible, use structured types: this allows the type system to help\nenforce that the input data is valid. Common ones to watch out for:\n\n*   Files: use `mojo_base.mojom.File`, not raw descriptor types like `HANDLE`\n    and `int`.\n*   File paths: use `mojo_base.mojom.FilePath`, not `string`.\n*   JSON: use `mojo_base.mojom.Value`, not `string`.\n*   Mojo interfaces: use `Interface` or `Interface&`, not `handle` or\n    `handle<message_pipe>`.\n*   Nonces: use `mojo_base.mojom.UnguessableToken`, not `string`.\n*   Origins: use `url.mojom.Origin`, not `url.mojom.Url` and certainly not\n    `string`.\n*   Time types: use `mojo_base.mojom.TimeDelta` /\n    `mojo_base.mojom.TimeTicks` / `mojo_base.mojom.Time`, not `int64` /\n    `uint64` / `double` / et cetera.\n    *   In WebUI, use `mojo_base.mojom.JSTime` for times coming from Javascript\n        Date objects.\n*   URLs: use `url.mojom.Url`, not `string`.\n*   `array<uint8>` or `string` and `memcpy()`: use a Mojo struct and statically\n    define the serialized fields. While `memcpy()` may be tempting for its\n    simplicity, it can leak info in padding. Even worse, `memcpy()` can easily\n    copy [undocumented fields][serialize-struct-tm-safely] or newly introduced\n    fields that were never evaluated for safety by the developer or reviewer.\n\n**_Good_**\n\n```c++\ninterface ReportingService {\n  ReportDeprecation(mojo_base.mojom.TimeTicks time,\n                    url.mojom.Url resource,\n                    uint32 line_number);\n};\n```\n\n**_Bad_**\n\n```c++\ninterface ReportingService {\n  // Bad: unclear what units |time| is or what |data| contains.\n  ReportDeprecation(double time, mojo_base.mojom.Value data);\n};\n```\n\nAvoid parallel arrays of data that require the receiver to validate that the\narrays have matching lengths. Instead, bundle the data together in a struct so\nit is impossible to have a mismatch:\n\n**_Good_**\n\n```c++\nstruct Pixel {\n  int8 reds;\n  int8 greens;\n  int8 blues;\n  int8 alphas;\n};\n\nstruct Bitmap {\n  // Good: it is impossible for there to be mismatched data.\n  array<Pixel> pixels;\n};\n```\n\n**_Bad_**\n\n```c++\n// Bad: code using this struct will need to validate that all the arrays have\n// matching sizes.\nstruct Bitmap {\n  array<int8> reds;\n  array<int8> greens;\n  array<int8> blues;\n  array<int8> alphas;\n};\n```\n\n\n### Beware of arithmetic overflow\n\n> TODO(dcheng): Import the guidance from the legacy IPC doc.\n\nSigned overflow is undefined in C++. If unsure about whether or not something\nwill overflow, use the safe numeric helpers from `//base/numerics`!\n\n**_Good_**\n\n```c++\nbase::CheckedNumeric<int32_t> size = mojo_rect->width();\nsize *= mojo_rect.height();\nif (!size.IsValid()) {\n  mojo::ReportBadMessage(\"Bad size from renderer!\");\n}\n```\n\n**_Bad_**\n\n```c++\n// Bad: Signed overflow is undefined in C++!\nint32_t size = mojo_rect->width() * mojo_rect.height();\n```\n\nNote that even if the types have defined overflow semantics, it is almost always\na good idea to check for overflow.\n\n**_Good_**\n\n```c++\nuint32_t alloc_size;\nif (!CheckMul(request->elements(), request->element_size())\n         .AssignIfValid(&alloc_size)) {\n  // Safe: avoids allocating with a bogus size that overflowed to a smaller than\n  // expected value.\n  mojo::ReportBadMessage(\"Invalid allocation size\");\n}\n\nElement* array = CreateArray(alloc_size);\nfor (size_t i = 0; i < request->element_size(); ++i) {\n  array[i] = PopulateArray(i);\n}\n```\n\n**_Bad_**\n\n```c++\nuint32_t alloc_size = request->elements() * request->element_size();\n\n// Dangerous: alloc_size can overflow so that CreateArray allocates too little\n// memory. Subsequent assignments will turn into an out-of-bound write!\nElement* array = CreateArray(alloc_size);\nfor (size_t i = 0; i < request->element_size(); ++i) {\n  array[i] = PopulateArray(i);\n}\n```\n\n\n### All possible message values are semantically valid\n\nWhen possible, messages should be defined in such a way that all possible values\nare semantically valid. As a corollary, avoid having the value of one field\ndictate the validity of other fields.\n\n**_Good_**\n\n```c++\nunion CreateTokenResult {\n  // Implies success.\n  string token;\n\n  // Implies failure.\n  string error_message;\n};\n\nstruct TokenManager {\n  CreateToken() => (CreateTokenResult result);\n};\n```\n\n**_Bad_**\n```c++\nstruct TokenManager {\n  // Requires caller to handle edge case where |success| is set to true, but\n  // |token| is null.\n  CreateToken() => (bool success, string? token, string? error_message);\n\n  // Requires caller to handle edge case where both |token| and |error_message|\n  // are set, or both are null.\n  CreateToken() => (string? token, string? error_message);\n};\n```\n\nA known exception where we tolerate imperfect message semantics is\nwith weakly typed integer [bitfields](#handling-bitfields).\n\n### Handling bitfields\n\nMojom has no native support for bitfields. There are two common approaches: a\ntype-safe struct of bools which is a bit clunky (preferred) and an integer-based\napproach (allowed but not preferred).\n\n**_Type-safe bitfields_**\n```c++\nstruct VehicleBits {\n  bool has_car;\n  bool has_bicycle;\n  bool has_boat;\n};\n\nstruct Person {\n  VehicleBits bits;\n};\n```\n\n**_Integer based approach_**\n```c++\nstruct Person {\n  const uint64 kHasCar = 1;\n  const uint64 kHasBicycle = 2;\n  const uint64 kHasGoat= 4;\n\n  uint32 vehicle_bitfield;\n};\n```\nIn both cases, consider typemapping these mojo types to your preferred C++ construct\n(e.g. `base::StrongAlias<...>`, `base::EnumSet<...>`, etc.) to improve downstream\nreadability and type safety.\n\n### Avoid object lifetime issues with self-owned receivers\n\nWhen creating new\n[Mojo services](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/mojo_and_services.md)\nin the browser process (exposed to the renderer via `BrowserInterfaceBrokers` in\na host object like `RenderFrameHostImpl`, `DedicatedWorkerHost`, etc.), one\napproach is to have the interface implementation be owned by the `Receiver`\nusing `mojo::MakeSelfOwnedReceiver`. From the\n[`mojo::MakeSelfOwnedReceiver` declaration](https://source.chromium.org/chromium/chromium/src/+/main:mojo/public/cpp/bindings/self_owned_receiver.h;l=129;drc=643cdf61903e99f27c3d80daee67e217e9d280e0):\n```\n// Binds the lifetime of an interface implementation to the lifetime of the\n// Receiver. When the Receiver is disconnected (typically by the remote end\n// closing the entangled Remote), the implementation will be deleted.\n```\nConsider such an interface created in `RenderFrameHostImpl`, and consider that\nand a corresponding `Remote` was created for this interface and owned by\n`RenderFrame`. It may seem logical to think that:\n 1. (true) The `Receiver` owns the interface implementation\n 2. (true) The lifetime of the `Receiver` is based on the lifetime of the\n    `Remote` in the renderer\n 3. (true) The `Remote` is owned by the `RenderFrame`\n 4. (true) The lifetime of the `RenderFrameHostImpl` is based on the lifetime of\n    the `RenderFrame`\n 5. (true) Destroying the `RenderFrame` will cause the `Remote` to be destroyed,\n    ultimately causing the `Receiver` and the interface implementation to be\n    destroyed. The `RenderFrameHostImpl` will likely be destroyed at some point\n    as well.\n 6. (false) It's safe to assume that `RenderFrameHostImpl` will outlive the\n    self-owned `Receiver` and interface implementation\n\nA\n[common](https://microsoftedge.github.io/edgevr/posts/yet-another-uaf/)\nmistake based on the last assumption above is to store and use a raw pointer\nto the `RenderFrameHostImpl` object in the interface implementation. If the\n`Receiver` outlives the `RenderFrameHostImpl` and uses the pointer to it, a\nUse-After-Free will occur. One way a malicious site or compromised renderer\ncould make this happen is to generate lots of messages to the interface and then\nclose the frame. The `Receiver` might have a backlog of messages to process\nbefore it gets the message indicating that the renderer's `Remote` was closed,\nand the `RenderFrameHostImpl` can be destroyed in the meantime.\n\nSimilarly, it's not safe to assume that the `Profile` object (and objects owned\nby it; `StoragePartitionImpl`, for instance) will outlive the `Receiver`. This\nhas been observed to be true for at least incognito windows, where a renderer\ncan generate messages, close the page, and cause the entire window to close\n(assuming no other pages are open), ultimately causing the\n`OffTheRecordProfileImpl` object to be destroyed before the `Receiver` object.\n\nTo avoid these types of issues, some solutions include:\n\n - Using `DocumentService` or `DocumentUserData` instead of\n   `mojo::MakeSelfOwnedReceiver` for document-based interfaces where the\n   interface implementation needs access to a `RenderFrameHostImpl` object. See\n   the\n   [`DocumentService` declaration](https://source.chromium.org/chromium/chromium/src/+/main:content/public/browser/document_service.h;l=27;drc=d4bf612a0258dd80cfc6d17d49419dd878ebaeb0)\n   for more details.\n\n - Having the `Receiver` and/or interface implementation be owned by the object\n   it relies on (for instance, store the `Receiver` in a private member or\n   use a `mojo::UniqueReceiverSet` for storing multiple `Receiver` /\n   interface implementation pairs).\n\n - Using `WeakPtr`s instead of raw pointers to provide a way to check whether\n   an object has been destroyed.\n\n## C++ Best Practices\n\n\n### Use mojo::WrapCallbackWithDefaultInvokeIfNotRun And mojo::WrapCallbackWithDropHandler sparingly\n\nMojo provides several convenience helpers to automatically invoke a callback if\nthe callback has not already been invoked in some other way when the callback is\ndestroyed, e.g.:\n\n```c++\n  {\n    base::OnceCallback<int> cb = mojo::WrapCallbackWithDefaultInvokeIfNotRun(\n        base::BindOnce([](int) { ... }), -1);\n  }  // |cb| is automatically invoked with an argument of -1.\n```\n\nThis can be useful for detecting interface errors:\n\n```c++\n  process->GetMemoryStatistics(\n      mojo::WrapCallbackWithDefaultInvokeIfNotRun(\n          base::BindOnce(&MemoryProfiler::OnReplyFromRenderer), <failure args>));\n  // If the remote process dies, &MemoryProfiler::OnReplyFromRenderer will be\n  // invoked with <failure args> when Mojo drops outstanding callbacks due to\n  // a connection error on |process|.\n```\n\nHowever, due to limitations of the current implementation, it's difficult to\ntell if a callback object has invoke-on-destroy behavior. In general:\n\n1.  Prefer error connection handlers where possible.\n1.  Only use the callback helpers for detecting interface errors. These\n    callbacks may be invoked during destruction and must carefully consider\n    receiver object lifetime. For more information, please see the\n    [Mojo documentation][mojo-doc-process-crashes].\n\n> Note that using the callback wrappers in the renderer is often unnecessary.\n> Message pipes are typically closed as part of a Document shutting down; since\n> many Blink objects already inherit `blink::ContextLifecycleObserver`, it is\n> usually more idiomatic to use this signal to perform any needed cleanup work.\n\n### Use StructTraits\n\nCreating a typemap and defining a `StructTraits` specialization moves the\ncomplexity of serialization, deserialization, and validation into a central\nlocation. We universally recommend this over defining `TypeConverter`\nspecializations: when a value fails deserialization, the receiver method will\nnever even be invoked. As a bonus, it also reduces the number of copies during\nserialization and deserialization. 😄\n\n**_Good_**\n\n```c++\n// In url_gurl_mojom_traits.h:\ntemplate <>\nstruct StructTraits<url::mojom::UrlDataView, GURL> {\n  static base::StringPiece url(const GURL& r);\n\n  // If Read() returns false, Mojo will discard the message.\n  static bool Read(url::mojom::UrlDataView data, GURL* out);\n};\n\n// In url_gurl_mojom_traits.cc:\n// Note that methods that aren't simple getters should be defined\n// out-of-line to avoid code bloat.\nbase::StringPiece StructTraits<url::mojom::UrlDataView, GURL>::url(\n    const GURL& r) {\n  if (r.possibly_invalid_spec().length() > url::kMaxURLChars ||\n      !r.is_valid()) {\n    return base::StringPiece();\n  }\n  return base::StringPiece(r.possibly_invalid_spec().c_str(),\n                           r.possibly_invalid_spec().length());\n}\n\nbool StructTraits<url::mojom::UrlDataView, GURL>::Read(\n    url::mojom::UrlDataView data, GURL* out) {\n  base::StringPiece url_string;\n  if (!data.ReadUrl(&url_string))\n    return false;\n  if (url_string.length() > url::kMaxURLChars)\n    return false;\n  *out = GURL(url_string);\n  if (!url_string.empty() && !out->is_valid())\n    return false;\n  return true;\n}\n```\n\n**_Bad_**\n\n```c++\ntemplate <>\nstruct TypeConverter<url::mojom::UrlPtr, GURL> {\n  // Inefficient: this copies data once off the wire to create a\n  // url.mojom.Url object, then copies it again to create a GURL.\n  static GURL Convert(const url::mojom::UrlPtr url) {\n    if (url.url.is_empty()) return GURL();\n    // Not good: no way to signal errors, so any code that converts the\n    // Mojo struct to a GURL will somehow need to check for errors…\n    // but it can't even be distinguished from the empty URL case!\n    if (url.url.size() > url::kMaxURLChars) return GURL();\n    return GURL(url.url);\n  }\n};\n```\n\nThere are also corresponding `EnumTraits` and `UnionTraits` specializations for\nmojo enums and unions respectively.\n\n\n### StructTraits getters should be simple\n\nWhere possible, `StructTraits` should be returning const references or simple\nread-only views of the data. Having to create temporary data structures during\nserialization should be rare, and it should be even rarer to mutate the input\nargument.\n\n\n### Out-of-line complex serialization/deserialization logic\n\nA `StructTraits` specialization is almost always fully specialized. Only define\n`StructTraits` methods inline in the header if the method is a simple getter\nthat returns a reference, pointer, or other simple POD. Define all other\nmethods out-of-line to avoid code bloat.\n\n\n### Do not write one-off functions to convert to/from Mojo types\n\nThere are some instances where it is simply not possible to define a\n`StructTraits` for type mapping: this commonly occurs with Blink IDL and Oilpan\ntypes. In these instances, add a `TypeConverter` specialization rather than\ndefining a one-off conversion function. This makes it easier to search for and\naudit code that does potentially risky type conversions.\n\n> The use of `TypeConverter` should be limited as much as possible: ideally,\n> only use it in renderers.\n\n**_Good_**\n\n```c++\ntemplate <>\nstruct TypeConverter<IDLDictionary, mojom::blink::DictionaryPtr> {\n  static IDLDictionary* Convert(const mojom::blink::DictionaryPtr& in) {\n    // Note that unlike StructTraits, there is no out-of-band way to signal\n    // failure.\n    IDLDictionary* out = new IDLDictionary;\n    out->int_value = in->int_value;\n    out->str_value = in->str_value;\n    return out;\n  }\n};\n```\n\n**_Bad_**\n\n```c++\n// Using a custom one-off function makes this hard to discover in security\n// audits.\nIDLDictionary* FromMojo(const mojom::blink::DictionaryPtr& in) {\n  IDLDictionary* out = new IDLDictionary;\n  out->int_value = in->int_value;\n  out->str_value = in->str_value;\n  return out;\n}\n```\n\n\n### Use the proper abstractions\n\n`mojo::ReceiverSet` implies multiple clients may connect. If this actually isn't\nthe case, please do not use it. For example, if an interface can be rebound,\nthen use the singular `mojo::Receiver` and simply `reset()` the existing receiver\nbefore reusing it.\n\n\n### Explicitly reject bad input\n\nWhile validation should be done inside `StructTraits` specializations when\npossible, there are situations where additional checks, e.g. overflow checks,\nare needed outside of `StructTraits` specializations. Use\n`mojo::ReportBadMessage()` or `mojo::GetBadMessageCallback()` to reject bad\ninput in these situations. Under the hood, this may record UMAs, kill the\nprocess sending bad input, et cetera.\n\n*   `mojo::ReportBadMessage()`: use to report bad IPC input while a message is\n    being dispatched on the stack.\n*   `mojo::GetBadMessageCallback()`: use to generate a callback to report bad\n    IPC input. The callback must be generated while a message is being\n    dispatched on the stack; however, the returned callback may be invoked be\n    freely invoked in asynchronously posted callbacks.\n\n\n## Java Best Practices\n\nUnfortunately, there are no strongly established conventions here. Most code\ntends to write manual conversion helpers and throw an exception on conversion\nfailure. See [NfcTypeConverter.java] as one example of how to write conversion\ncode.\n\n\n## General Code Health\n\n\n### Naming Conventions\n\nPlace mojo types in `<top-level namespace>.mojom`. Directories for Mojo traits\nshould be named `mojom` (preferable) or `ipc`. Legacy names that are also\nencountered are `public/interfaces`, `interfaces`, or just `mojo`.\n\n`mojom` is preferred for consistency between the directory name and the nested\nnamespace name.\n\n\n### Do not handle impossible situations\n\nDo not clutter the code by handling impossible situations. Omitting it makes\nthe invariants clear. This takes two different forms:\n\n*   A less trustworthy process can be compromised by an adversary and send\n    arbitrary data. When processing data from a less trustworthy process, do\n    not attempt to handle this invalid data: just call\n    `mojo::ReportBadMessage()`. When invoked in the context of processing an\n    IPC from the renderer, this will kill the renderer process.\n*   A more trustworthy process must be trusted, by definition. Do not write\n    code to handle impossible situations \"just in case\" there's a bug. For\n    example, the renderer class `content::RenderFrameImpl` must always be\n    connected to certain control interfaces in the browser. It does not makes\n    sense to handle a Mojo connection error and try to reconnect: a connection\n    error signals that the browser process is in the process of deleting the\n    frame, and any attempt at reconnecting will never succeed.\n\n\n### Using mojo enums directly when possible\n\n`EnumTraits` generally do not add much value: incoming Mojo enum values are\nalready validated before typemapping, so it is guaranteed that the input value\nto `EnumTraits<T>::FromMojom()` is already a valid enum value, so the method\nitself is just a bunch of boilerplate to map between two very similarly named,\nyet slightly different, enums.\n\nTo avoid this, prefer to use the Mojo enum directly when possible.\n\n\n### Consider the cost of setting up message pipes\n\nMessage pipes are fairly inexpensive, but they are not free either: it takes 6\ncontrol messages to establish a message pipe. Keep this in mind: if the\ninterface is used relatively frequently, connecting once and reusing the\ninterface pointer is probably a good idea.\n\n## Copy data out of BigBuffer before parsing\n\n[BigBuffer](mojo/public/mojom/base/big_buffer.mojom) uses shared memory to make\npassing large messages fast. When shmem is backing the message, it may be\nwritable in the sending process while being read in the receiving process. If a\nBigBuffer is received from an untrustworthy process, you should make a copy of\nthe data before processing it to avoid time-of-check time-of-use (TOCTOU) bugs.\nThe |size()| of the data cannot be manipulated.\n\n\n## Ensure An Explicit Grant For WebUI Bindings\n\nWebUI renderers sometimes need to call special, powerful IPC endpoints in a\nprivileged process. It is important to enforce the constraint that the\nprivileged callee previously created and blessed the calling process as a WebUI\nprocess, and not as a (potentially compromised) web renderer or other\nlow-privilege process.\n\n* Use the standard pattern for instantiating `MojoWebUIController`. WebUI\nMojo interfaces must only be exposed through a `MojoWebUIController` subclass.\n* If there is external functionality that the WebUI needs, make sure to route\nit through the Mojo interfaces implemented by the `MojoWebUIController`, to\navoid circumventing access checks.\n\n\n## Not-Yet-Shipped Features Should Be Feature-Checked On The Privileged Side\n\nSometimes, there will be powerful new features that are not yet turned on by\ndefault, such as behind a flag, Finch trial, or [origin\ntrial](https://www.chromium.org/blink/origin-trials). It is not safe to check\nfor the feature's availability on the renderer side (or in another low-privilege\nprocess type). Instead, ensure that the check is done in the process that has\npower to actually enact the feature. Otherwise, a compromised renderer could opt\nitself in to the feature! If the feature might not yet be fully developed and\nsafe, vulnerabilities could arise.\n\n\n[security-tips-for-ipc]: https://www.chromium.org/Home/chromium-security/education/security-tips-for-ipc\n[NfcTypeConverter.java]: https://chromium.googlesource.com/chromium/src/+/e97442ee6e8c4cf6bcf7f5623c6fb2cc8cce92ac/services/device/nfc/android/java/src/org/chromium/device/nfc/NfcTypeConverter.java\n[mojo-doc-process-crashes]: https://chromium.googlesource.com/chromium/src/+/main/mojo/public/cpp/bindings#Best-practices-for-dealing-with-process-crashes-and-callbacks\n[serialize-struct-tm-safely]: https://chromium-review.googlesource.com/c/chromium/src/+/679441\n"
  },
  {
    "path": "security/llm-security-guidelines",
    "title": "Security Guidelines for LLMs and other large models in Chrome",
    "content": "# Security Guidelines for LLMs and other large models in Chrome\n\nLarge language models (LLMs), generative artificial intelligence (GenAI) models,\nand other large machine learning (ML) models will find uses in Chromium and the\nweb. We will refer to all of these as _models_. This document outlines some\nguidelines to help safely implement features using large models.\n\nOur main security goals are to prevent arbitrary code execution, and prevent\nuser information disclosure between origins. It is not possible to prevent\npeople using Chrome from seeing model weights or predictions as this is not\nfeasible on the client devices where Chrome runs.\n\n# Memory Safety\n\nModels are, abstractly, layers of mathematical operations that mix inputs from\ntrustworthy and untrustworthy sources and produce output that will be used\nelsewhere in Chrome. In practice these models are implemented in memory-unsafe\nlanguages and may include convenience functions to parse complex data formats as\npart of their pipelines. They should be treated the same way as other\nmemory-unsafe code implementing a feature in Chrome to comply with the\n[rule-of-2](rule-of-2.md). Models processing untrustworthy complex data must be\nsandboxed, and data should be provided using safe types.\n\n## Complex formats\n\nModels processing complex data -- such as images, audio or video -- could be\nimplemented using format helpers in their pipelines. To ensure memory safety any\nparsing of complex formats should happen in a sandboxed, site-isolated process.\nEither by sandboxing the model, or by parsing complex formats into accepted safe\nformats before sending them to the process hosting the model.\n\n### Exception - Tokenization\n\nWhere the only function of the model is to tokenize a string of text before\nperforming inference to produce an output this is not considered to be complex\nprocessing.\n\n## Untrustworthy input -> untrustworthy output\n\nIf an attacker can control any input to a model it must be assumed that they can\ncontrol all of its output. Models cannot be used to sanitize data, and their\noutput must be treated as untrustworthy content with an untrustworthy format.\n\nModel output will either need to be parsed in a sandboxed process, or limited to\nonly outputting safe types (e.g. an array of floats).\n\n## Mitigations\n\nModels exposed to untrustworthy input can reduce the risk of exposing memory\nsafety flaws.\n\n  * Use a tight sandbox\n  * Provide model inputs over safe mojo types\n  * Validate the size and format of input\n  * Use a pipeline that only tokenizes then performs inference\n  * Ensure input is in the same format as training data\n  * Disable custom ops that might parse complex formatted data\n  * Limit the size of the model output\n  * Fuzz exposed APIs\n\n# Side-Channels\n\nLarge models will necessarily be reused for several purposes. Where this happens\nit is important that appropriate sessionization is used. It is likely that side\nchannels will exist that could leak some information about previous inputs.\n\n# Model APIs\n\nModels themselves are complex formats that represent complex graphs of\ncomputation. APIs that allow web sites to specify and run models should be\ndesigned so that these graphs and model inputs can be provided safely. Model\nhosting should be managed by a trusted process to ensure only the right set of\noperations can be reached by an untrustworthy model.\n\nIf a model's provenance can be verified (such as with Chrome's Component\nUpdater) then we can assume it is as safe as other Chrome code. This means that\nwhere it runs is determined by what the model does, and the safety of the data\nit consumes. Googlers should refer to internal guidelines for approved delivery\nmechanisms in Chrome (go/tf-security-in-chrome,\ngo/chrome-genai-security-prompts).\n\n# Other safety considerations\n\nModels can output very convincing text. They may be used to summarize important\ninformation (e.g. translating a legal form), or to produce writing for people\nusing Chrome (e.g. a letter to a bank). Models can produce incorrect output even\nif they are not being deliberately steered to do so. People using Chrome should\nhave obvious indications that model output is being used, information about the\nsource of its inputs, and opportunity to review any text generated on their\nbehalf before it is submitted to a third party.\n\nModels may output inappropriate material and where possible their output should\nbe filtered using reasonable safety filters and people should have mechanisms to\nreport and improve model outputs.\n\nModel weights trained from on-device data may embody information about a person\nusing Chrome and should be treated like other sensitive data.\n"
  },
  {
    "path": "security/life-of-a-security-issue",
    "title": "Life of a Security Issue",
    "content": "# Life of a Security Issue\n\nThis page will help you understand the life cycle of a manually-reported\nexternal security bug in the Chromium project. Internally reported and\nfuzzer-found bugs follow a similar lifecycle, though specific details vary. The\nprocess can be visualized at a high level using the state diagram below, and\nfurther explanation is provided in the paragraphs that follow.\n\n![alt text](life-of-a-security-issue.png \"Sequence diagram of the life of a security issue\")\n\n<pre style=\"display:none\" data-note=\"Source code for sequence diagram. Gitiles will not display this.\">\n<!--\nhttps://mermaid.live/edit#pako:eNqNU71uwjAQfpWTh070BTJUqgKd2goBYiEdLvZBLBw79Q9thHj3OpAgklCJJUpy39_d2UfGjSCWMEffgTSnqcSdxTLTmcfgjQ5lTrb5qtB6yWWF2sOCKmP9-P-SeLDS17Cii8RtcUoHUqY6s2Kp03h-eenRklYd8rBrcL3iGLyyMTD9B756JvDqnNzpFhihn8YTmAPZazeTG_QmNdoF5dFLo78ujGt5IBx8YSygFqCaR_oORkOJckQaRP9Au2_yADrI2Jv8JZGxi1W_j1Ej5Bxso2eOfN9kd8OAfUZakLU1VJLv78mPgq0Xc5ijJnUP203rOtEnqLAGSz9oxUOEdD17IMQ85Eq6IgorQkfRxnUAHVfnBqeoZ7Q5k7mqQUjHlXEUV8gmrCQb9yLicT9mGuLQfUElZSyJr4K2GBfebOAUoaES6GkmpDeWJVtUjiasuRHLWnOWeBuoA7VXpkWd_gADszf5\n-->\n\nsequenceDiagram\n\tautonumber\n\tparticipant Reporter\n\tparticipant Security Team\n\tparticipant Developer\n\n\tReporter->>Security Team: Report bug\n\tSecurity Team->>Security Team: Triage bug\n\tSecurity Team->>Developer: Assign bug\n\n\tNote over Reporter,Developer: [Consultation]\n\n\tDeveloper->>Developer: Author and land CL on main\n\tDeveloper->>Security Team: Mark bug as \"Fixed\"\n\n\tSecurity Team-->>Developer: Assess for backports\n\n\tDeveloper-->>Developer: Cherry pick\n\n\tSecurity Team->>Security Team: VRP Panel\n\tSecurity Team->>Reporter: Assign & pay reward\n\tSecurity Team->>Reporter: Assign CVE\n\tSecurity Team->>Security Team: Publish release & security notes\n\n\tReporter-->>Reporter: [Publicly disclose]\n</pre>\n\n## 1. Report bug\n\nA security bug begins when a reporter [\ndiscloses](https://www.chromium.org/Home/chromium-security/reporting-security-bugs/)\na bug in the [Chromium issue\ntracker](https://issues.chromium.org/issues/new?noWizard=true&component=1363614&template=1922342).\nThe new bug is placed in a queue of other incoming security bugs, and it is\nview-restricted to the reporter and select individuals on a need-to-know\nbasis.\n\nBug reports that include specific steps to reproduce, analysis, proofs of\nconcept, and/or suggested patches are encouraged. The [Chrome Vulnerability\nRewards Program (VRP) policies page](https://g.co/chrome/vrp/#report-quality)\nhas information about the expected characteristics of baseline and high-quality\nsecurity bug reports. Please also check the [FAQ](faq.md) to learn\nabout issues that are frequently reported.\n\n## 2. Triage bug\n\nAfter the bug is filed, a [security shepherd](shepherd.md) will evaluate the\nreport. The shepherd does several tasks:\n\n- Validate that the bug reproduces\n- Searching for any duplicate reports\n- Tag the bug with components\n- Assess the bug's [severity](severity-guidelines.md)\n- Determine the versions affected\n- Assign the bug to a developer\n\n## 3. Assign bug\n\nThe primary job of the shepherd is to route valid and actionable reports of\nsecurity bugs to the Chromium developer who is best poised to fix the issue.\n\nAfter the issue is assigned, there may be discussion between the developer(s)\ninvolved, members of the security team, and the original reporter.\n\n## 4. Author and land a CL on `main`\n\nThe developer will author a fix and a regression test for the security issue\nThe CL description should mention the bug number in a\n[`Bug:` or `Fixed:` footer](../contributing.md#cl-footer-reference). The CL\ndescription should be as complete as possible and does not need to hide that\nthe CL fixes a security issue. In general the CL should include a regression\ntest - in limited cases where the issue can easily be triggered from a\nJavaScript sample the test can be landed later.\n\nOnce the CL lands, it will not yet be widely available to users, since it is\nonly in the `main` branch. Unless further steps are taken (see below), the fix\nwill roll out as part of the normal [release\nprocess](../process/release_cycle.md).\n\nReporters are welcome to include a suggested patch in the report or to [upload a\nCL](../contributing.md) with the fix. In that case, the developer assigned to\nthe bug can help code review and land it.\n\n## 5. Mark bug as *Fixed*\n\nOnce the CL has landed, the developer should set the bug's status to *Fixed*.\nWhen the bug moves into the *Fixed* state, the security team's automation\nsystems begin processing the bug report. In particular, the tools will update\n[merge request](../process/merge_request.md) the *Merge* fields with the\nappropriate merge request tags, based on the severity and impact assessed by\nthe shepherd during triage.\n\nVRP reports also are updated with the reward-topanel hotlist by the automation.\nThis allows the report to be included in the VRP Panel queue for\nevaluation and consideration of a potential VRP reward at a future VRP Panel\nsession.\n\n## 6. Assess for backports\n\nThe appropriate members of the security team will make the [final determination](https://www.chromium.org/Home/chromium-security/security-release-management/)\nas to whether backports of the fix should occur to Stable and/or pre-Stable\nChrome release channels.\n\n## 7. Cherry pick\n\nIf approved for backporting, the developer will [cherry\npick](../process/merge_request.md#landing-an-approved-merge) the CL to the\nrelease branches identified by the security team member who approved the\nmerge.\n\n## 8. VRP Panel\n\nMembers of the security team meet regularly as a panel to assess [vulnerability\nrewards](vrp-faq.md) for externally reported security bugs. The individuals on\nthe panel will [take into account](https://g.co/chrome/vrp) the severity and\nimpact of the bug, the quality of the bug report, whether a patch/fix was\nproposed with the report, and other mitigating circumstances. The VRP panel will\nassign any reward amount for the bug.\n\n## 9. Assign and pay reward\n\nAfter the VRP panel meets, the reporter will be notified of the VRP reward\ndecision through the bug report, and the *vrp-reward* field will be updated to\nreflect the VRP reward amount.\n\nPayments are not handled by the security team. A member of the Google finance\nteam working on VRP payments (p2p-vrp) will reach out to arrange payment. The\nreporter must first be enrolled in a payment system Google uses to issue\npayments. The p2p-vrp team will assist the reporter in the enrollment process.\nOnce the reporter is enrolled, all potential future VRP payments will be\nprocessed automatically without any action required by the reporter.\n\n## 10. Assign CVE\n\nAt the time that the security fix is shipped to a Stable channel release, a\nsecurity team member will assign the issue a [CVE](https://www.cve.org/) number.\nCVE numbers need to point to a publicly accessible artifact, and Chrome uses the\nreleases blog (see below) for this purpose.\n\n## 11. Publish release & security notes\n\nThe Chrome Release team releases an update of Chrome containing the security\nfix. If the fix is included in a Stable channel release of Chrome, it will be\nlisted and acknowledged in the security fix notes on the [Chrome Releases\nblog](https://googlechromereleases.blogspot.com/). Security issues will be\nhighlighted with a short description, a reward amount, the CVE number, and\nacknowledging the reporter as requested (if they have consented to such).\n\n## 12. Publicly disclose\n\nExcept in rare circumstances where the bug report has been embargoed, 14 weeks\nafter the issue is marked *Fixed*, security automation opens the bug for public\ndisclosure. At that time, the reporter can consider their obligations under\ncoordinated disclosure to be fulfilled.\n"
  },
  {
    "path": "security/ipc-reviews",
    "title": "IPC Reviews",
    "content": "# IPC Reviews\n\n[TOC]\n\n## tl;dr\n\n📎 It looks like you’re making a possibly security-sensitive change! 📎\n\nIPC security review isn’t a rubberstamp, so your friendly security reviewer\nwill need a fair amount of context to review your CL effectively. Please review\nyour CL description and code comments to make sure they provide context for\nsomeone unfamiliar with your project/area. Pay special attention to where data\ncomes from and which processes it flows between (and their privilege levels).\nFeel free to point your security reviewer at design docs, bugs, or other links\nif you can’t reasonably make a self-contained CL description. (Also see\nhttps://cbea.ms/git-commit/).\n\n## What is IPC review and why is it needed?\n\nA critical part of Chrome’s security is built on process isolation.\nUntrustworthy content is isolated into unprivileged and sandboxed child\nprocesses (e.g. renderer process, GPU process). In contrast, the browser\nprocess is fully privileged and is not sandboxed, as it is considered\ntrustworthy.\n\nIn this model, the browser process acts as an operating system kernel for the\nuntrustworthy child processes. The interfaces defined by IPC are system calls\nexposed to these (potentially compromised and) untrustworthy child processes,\nallowing them to use system capabilities (such as the network) as appropriate.\nBecause IPCs cross trust and privilege boundaries between processes, it is\ncritical that IPCs themselves are robust against malicious input.\n\nConsider the example interface below, assuming it is exposed to renderer\nprocesses:\n\n```\ninterface CookieJar {\n  GetCookies(url.mojom.Url) => (string);\n};\n```\n\nIn normal circumstances, this isn’t problematic: a well-behaved renderer won’t\npass arbitrary URLs to `GetCookies()`; it will only pass URLs of Documents it’s\nrendering. If the renderer is displaying email, it won’t need (or request) the\nbank login cookies. However, an attacker with full control over the renderer\nprocess can pass any URL to `GetCookies()`. An interface like this that\nincorrectly trusts the renderer process would allow an attacker to exfiltrate\nmuch more data than they should be able to.\n\nIPC review is intended to catch these types of bugs. An adversary with full\ncontrol over an untrustworthy process must not be able to exploit IPC to escape\nthe sandboxed process. The goal is to ensure:\n\n- IPC invariants are easy to maintain\n- IPC interfaces are documented and understandable\n- IPC handlers are resistant to malicious input\n\nNote that IPC review is distinct from [security review for launches or major\nchanges to Chrome][chrome-security-review]; the latter is generally focused on\nevaluating feature security at a high level, while IPC review is focused on\nspecific implementation details.\n\n## What does an IPC reviewer look for?\n\n- **What are the endpoints of the changed IPCs?** Almost all IPCs cross trust\n  and privilege boundaries, so it is important to understand what processes are\n  communicating, which way data flows, and if any of the endpoints are\n  untrustworthy. This is often documented with interface-level comments, e.g.\n  “the Widget interface is used by the browser process to inform the renderer\n  process of UI state changes”.\n- **What are the changes in capabilities being exposed over IPC?** Many changes\n  provide new capabilities to an untrustworthy process. An IPC reviewer will\n  evaluate if the new capabilities can be abused by an attacker (e.g. retrieve\n  cookies of an unrelated page, write to arbitrary files, et cetera).\n- **What breaks if an attacker provides malicious input?** If an array argument\n  represents a point in 3D space, one assumption might be that it should contain\n  exactly three elements. An IPC handler processing input from an untrustworthy\n  process must not assume this though; it must validate that the sender actually\n  provided exactly three elements. Note: in this case, an even better\n  alternative would be define a `struct Point` with an `x`, `y`, and `z` fields:\n  then it would be impossible for even a malicious sender to pass a malformed\n  point!\n\nTo answer these questions, it’s often necessary to evaluate both the code\nsending and the code reviewing the IPC. Avoid sending out changes where the IPC\nhandler is simply marked `NOTIMPLEMENTED()`: without an actual implementation,\nit is often impossible to evaluate for potential security issues.\n\nPlease also keep in mind that an IPC reviewer often will not have all the\ndomain-specific knowledge in a given area (whether that be accessibility, GPU,\nXR, or something else). Ensure that a change has appropriate context (in the CL\ndescription and associated bugs), link to design documents, and thoroughly\ndocument interfaces/methods/structs. Good documentation also helps future\nreaders of the code understand the system more easily.\n\n## Guidelines and Best Practices for IPC\n\nIPC reviewers will primarily evaluate a CL using the [IPC Review\nReference][ipc-review-reference], which contains guidelines and recommendations\nfor how to structure Mojo IPC definitions as well as the corresponding C++/JS code.\n\nAdditional non-canonical references that may be interesting reading:\n\n- The previous [Mojo best practices guide][mojo-best-practices]: this is\n  somewhat out-of-date and may occasionally conflict with the aforementioned\n  reference. The reference should take priority.\n- [Security Tips for Legacy IPC][legacy-ipc-security]\n\n## When should IPC review happen?\n\nIn general, include an IPC reviewer when sending a change out for review. Even\nif a change is under active development, it’s still OK to add an IPC reviewer.\nWhile the IPC reviewer might not be actively involved if the design is still in\nflux with other reviewers, simply being in the loop often provides useful\ncontext for the change (and can sometimes save significant future pain).\n\nIt’s important to note that IPC review isn’t just a rubberstamp; as mentioned\nabove, an IPC reviewer’s focus is on reviewing cross-process interactions, from\nthe perspective of a hostile attacker trying to hijack a user’s machine. Adding\nan IPC reviewer only after receiving all other LGTMs can sometimes be\nfrustrating for everyone involved, especially if significant revisions are\nrequested.\n\n## Is it OK to TBR a simple change in IPC?\n\nAvoid TBRing CLs with IPC changes. If the change is simple, ping an IPC reviewer\ndirectly and ask them for a quick LGTM. Erring on the side of safety is\npreferred for security-critical changes.\n\n## IPC review is slow!\n\nPlease reach out to <ipc-security-reviewers@chromium.org> (for public issues)\nor <chrome-security-ipc@google.com> (for internal issues). Large and complex\nfeatures can be difficult to evaluate on a change by change basis; reaching out\ncan help provide IPC reviewers with better context on the security properties\nof the overall system, making it much easier to evaluate individual changes.\n\n[chrome-security-review]: https://www.chromium.org/Home/chromium-security/security-reviews\n[ipc-review-reference]: https://docs.google.com/document/d/1Kw4aTuISF7csHnjOpDJGc7JYIjlvOAKRprCTBVWw_E4/edit#\n[mojo-best-practices]: https://chromium.googlesource.com/chromium/src/+/main/docs/security/mojo.md\n[legacy-ipc-security]: https://www.chromium.org/Home/chromium-security/education/security-tips-for-ipc/\n"
  },
  {
    "path": "security/integer-semantics",
    "title": "Integer Semantics, Unsafety, And You",
    "content": "# Integer Semantics, Unsafety, And You\n\n[TOC]\n\nThese handy tips apply in any memory management situation and in any kind of IPC\nsituation (classic Chromium IPC, Mojo, Windows/POSIX IPC, Mach IPC, files,\nsockets, parsing binary formats, ...).\n\nBasically, don't believe the lie that 'computers are good at arithmetic'. In\ngeneral, unless you explicitly check an arithmetic operation, it's safest to\nassume the operation went wrong. The least painful way to systematically check\narithmetic is Chromium's base/numerics templates and helper functions.\n\n## Be Aware Of The Subtleties Of Integer Types\n\nFirst [read about the scary security implications of integer arithmetic in\nC/C++](http://en.wikipedia.org/wiki/Integer_overflow). Adhere to these best\npractices:\n\n* Use the [integer templates and cast templates in\nbase/numerics](../../base/numerics/README.md) to avoid overflows, **especially when\ncalculating the size or offset of memory allocations**.\n* Use unsigned types for values that shouldn't be negative or where defined\noverflow behavior is required. (Overflow is undefined behavior for signed\ntypes!)\n* Across any process boundary, use explicitly sized integer types, such as\n`int32_t`, `int64_t`, or `uint32_t`, since caller and callee could potentially\nuse different interpretations of implicitly-sized types like `int` or `long`.\n(For example, a 64-bit browser process and a 32-bit plug-in process might\ninterpret `long` differently.)\n\n## Be Aware Of The Subtleties Of Integer Types Across Languages\n\n### Java\n\nWhen writing code for Chromium on Android, you will often need to marshall\narrays, and their sizes and indices, across the language barrier (and possibly\nalso across the IPC barrier). The trouble here is that the Java integer types\nare well-defined, but the C++ integer types are whimsical. A Java `int` is a\nsigned 32-bit integer with well-defined overflow semantics, and a Java `long` is\na signed 64-bit integer with well-defined overflow semantics. in C++, only the\nexplicitly-sized types (e.g. `int32_t`) have guaranteed exact sizes, and only\nunsigned integers (of any size) have defined overflow semantics.\n\nEssentially, Java integers **actually are** what people often (incorrectly)\n**assume** C++ integers are. Furthermore, Java `Array`s are indexed with Java\n`int`s, whereas C++ arrays are indexed with `size_t` (often implicitly cast, of\ncourse). Note that this also implies a 2^31 limit on the number of elements in\nan array that is coming from or going to Java. That Should Be Enough For\nAnybody, but it's good to keep in mind.\n\nYou need to make sure that every integer value survives its journey across\nlanguages intact. That generally means explicit casts with range checks; the\neasiest way to do this is with the `base::checked_cast` or (much less likely)\n`base::saturated_cast` templates in base/numerics. Depending on how the integer\nobject is going to be used, and in which direction the value is flowing, it may\nmake sense to cast the value to `jint` (an ID or regular integer), `jlong` (a\nregular long integer), `size_t` (a size or index), or one of the other more\nexotic C/C++ integer types like `off_t`.\n\n### JavaScript And JSON\n\n[Here is some good reading on integers in\nJavaScript](http://2ality.com/2014/02/javascript-integers.html). TL;DR:\n\n* Normal JavaScript `Number`s have a 'safe' integer range of 53 bits (signed).\nSee `Number.isSafeInteger`, `Number.MIN_SAFE_INTEGER`, and\n`Number.MAX_SAFE_INTEGER`.\n* Array indices are unsigned 32-bit values.\n* Character codes (`fromCharCode`, `charCodeAt`) are unsigned 16-bit values.\n"
  },
  {
    "path": "security/handling-messages-from-web-content",
    "title": "The browser process should not handle messages from web content",
    "content": "# The browser process should not handle messages from web content\n\n![alt text](good-bad-ipc.png \"Safe flow of IPC messages from renderer to\nbrowser, via reviewed APIs; together with two example unsafe flows via\npostMessage and via unreviewed APIs\")\n\n(drawing source\n[here](https://docs.google.com/drawings/d/1SmqvOvLY_DnDxeJHKQRB3rACO0aVSHpyfTycV2v1P1w/edit?usp=sharing))\n\nSometimes features are proposed in which the Chrome user interface (in the\nbrowser process) handles messages directly from web content (JavaScript, HTML\netc.). For example, this could be done using the `postMessage` APIs which have\nbeen put in place for Android WebView apps. This is not allowed, because:\n\n* Overall system security relies on simple and predictable security properties.\n  Adding extra message channels causes complexity, non-discoverability and\n  non-predictability.\n* Chrome's security strategy relies on isolating web content using sandboxed\n  renderer processes and site isolation. Any communication outside of that\n  renderer process presents a risk of a sandbox escape. All such communication\n  has to be via Mojo such that the `mojom` interface definition files go through\n  our [IPC security review process](mojo.md) (and will benefit from other future\n  Mojo security improvements).\n* Websites are untrustworthy. TLS can’t guarantee the provenance of a website —\n  even pinning has limits — and so you must assume any messages from websites\n  are malicious. Processing such messages in the browser process in C++ is\n  likely a violation of the [Rule of Two](rule-of-2.md) and is extremely\n  dangerous.\n* Even if you can comply with the Rule of Two (for example by using a safe\n  language) it's simply difficult to produce robust APIs that are safe against\n  malicious data: the open web platform [API review\n  process](https://www.chromium.org/blink/launching-features) is designed to\n  flush out any concerns. Any APIs or functionality accessible to web content\n  therefore needs to go via that process to give the best chance of spotting\n  danger.\n* There are non-security concerns: It does not comply with the spirit of an open\n  web platform which should be equally available on all user agents.\n\nIn order to support WebView, WebLayer, and CCT, APIs exist in Chrome to\nestablish web message channels between the embedding application and web page.\nThese exist only to support these \"embedding the web\" scenarios, which are often\nused to build site- or purpose-specific browsers. General browser features\nshould not use them because of the reasons stated above.\n\nOther mechanisms of bypassing normal processes might include exposing unreviewed\nAPIs to a component extension, and making its APIs available to web content.\nThese are similarly not allowed.\n"
  },
  {
    "path": "security/faq",
    "title": "Chrome Security FAQ",
    "content": "# Chrome Security FAQ\n\n[TOC]\n\n## Process\n\n<a name=\"TOC-Which-bugs-are-valid-for-rewards-under-the-Chrome-Vulnerability-Rewards-program-\"></a>\n### Which bugs are valid for rewards under the Chrome Vulnerability Rewards program?\n\nPlease see [the VRP FAQ page](vrp-faq.md).\n\n<a name=\"TOC-Why-are-security-bugs-hidden-in-the-Chromium-issue-tracker-\"></a>\n### Why are security bugs hidden in the Chromium issue tracker?\n\nWe must balance a commitment to openness with a commitment to avoiding\nunnecessary risk for users of widely-used open source libraries.\n\n<a name=\"TOC-Can-you-please-un-hide-old-security-bugs-\"></a>\n### Can you please un-hide old security bugs?\n\nOur goal is to open security bugs to the public once the bug is fixed and the\nfix has been shipped to a majority of users. However, many vulnerabilities\naffect products besides Chromium, and we don’t want to put users of those\nproducts unnecessarily at risk by opening the bug before fixes for the other\naffected products have shipped.\n\nTherefore, we make all security bugs public within approximately 14 weeks of the\nfix landing in the Chromium repository. The exception to this is in the event of\nthe bug reporter or some other responsible party explicitly requesting anonymity\nor protection against disclosing other particularly sensitive data included in\nthe vulnerability report (e.g. username and password pairs).\n\n<a name=\"TOC-Can-I-get-advance-notice-about-security-bugs-\"></a>\n### Can I get advance notice about security bugs?\n\nVendors of products based on Chromium, distributors of operating systems that\nbundle Chromium, and individuals and organizations that significantly contribute\nto fixing security bugs can be added to a list for earlier access to these bugs.\nYou can email us at security@chromium.org to request to join the list if you\nmeet the above criteria. In particular, vendors of anti-malware, IDS/IPS,\nvulnerability risk assessment, and similar products or services do not meet this\nbar.\n\nPlease note that the safest version of Chrome/Chromium is always the latest\nstable version — there is no good reason to wait to upgrade, so enterprise\ndeployments should always track the latest stable release. When you do this,\nthere is no need to further assess the risk of Chromium vulnerabilities: we\nstrive to fix vulnerabilities quickly and release often.\n\n<a name=\"TOC-How-can-I-know-which-fixes-to-include-in-my-downstream-project-\"></a>\n### How can I know which fixes to include in my downstream project?\n\nChrome is built with mitigations and hardening which aim to prevent or reduce\nthe impact of security issues. We classify bugs as security issues if they are\nknown to affect a version and configuration of Chrome that we ship to the\npublic. Some classes of bug might present as security issues if Chrome was\ncompiled with different flags, or linked against a different C++ standard\nlibrary, but do not with the toolchain and configuration that we use to build\nChrome. We discuss some of these cases elsewhere in this FAQ.\n\nIf we become aware of them, these issues may be triaged as `Type=Vulnerability,\nSecurity_Impact-None` or as `Type=Bug` because they do not affect the production\nversion of Chrome. They may or may not be immediately visible to the public in\nthe bug tracker, and may or may not be identified as security issues. If fixes\nare landed, they may or may not be merged from HEAD to a release branch. Chrome\nwill only label, fix and merge security issues in Chrome, but attackers can\nstill analyze public issues, or commits in the Chromium project to identify bugs\nthat might be exploitable in other contexts.\n\nChromium embedders and other downstream projects may build with different\ncompilers, compile options, target operating systems, standard library, or\nadditional software components. It is possible that some issues Chrome\nclassifies as functional issues will manifest as security issues in a product\nembedding Chromium - it is the responsibility of any such project to understand\nwhat code they are shipping, and how it is compiled. We recommend using Chrome's\n[configuration](https://source.chromium.org/chromium/chromium/src/+/main:build/config/)\nwhenever possible.\n\n<a name=\"TOC-Can-I-see-these-security-bugs-so-that-I-can-back-port-the-fixes-to-my-downstream-project-\"></a>\n### Can I see these security bugs so that I can back-port the fixes to my downstream project?\n\nMany developers of other projects use V8, Chromium, and sub-components of\nChromium in their own projects. This is great! We are glad that Chromium and V8\nsuit your needs.\n\nWe want to open up fixed security bugs (as described in the previous answer),\nand will generally give downstream developers access sooner. **However, please\nbe aware that backporting security patches from recent versions to old versions\ncannot always work.** (There are several reasons for this: The patch won't apply\nto old versions; the solution was to add or remove a feature or change an API;\nthe issue may seem minor until it's too late; and so on.) We believe the latest\nstable versions of Chromium and V8 are the most stable and secure. We also\nbelieve that tracking the latest stable upstream is usually less work for\ngreater benefit in the long run than backporting. We strongly recommend that you\ntrack the latest stable branches, and we support only the latest stable branch.\n\n<a name=\"TOC-Severity-Guidelines\"></a>\n### How does the Chrome team determine severity of security bugs?\n\nSee the [severity guidelines](severity-guidelines.md) for more information.\nOnly security issues are considered under the security vulnerability rewards\nprogram. Other types of bugs, which we call \"functional bugs\", are not.\n\n## Threat Model\n\n<a name=\"TOC-Timing-Attacks\"></a>\n### Are timing attacks considered security vulnerabilities?\n\nSome timing attacks are considered security vulnerabilities, and some are\nconsidered privacy vulnerabilities. Timing attacks vary significantly in terms\nof impact, reliability, and exploitability.\n\nSome timing attacks weaken mitigations like ASLR (e.g.\n[Issue 665930](https://crbug.com/665930)). Others attempt to circumvent the same\norigin policy, for instance, by using SVG filters to read pixels\ncross-origin (e.g. [Issue 686253](https://crbug.com/686253) and\n[Issue 615851](https://crbug.com/615851)).\n\nMany timing attacks rely upon the availability of high-resolution timing\ninformation [Issue 508166](https://crbug.com/508166); such timing data often has\nlegitimate usefulness in non-attack scenarios making it unappealing to remove.\n\nTiming attacks against the browser's HTTP Cache (like\n[Issue 74987](https://crbug.com/74987)) can potentially leak information about\nwhich sites the user has previously loaded. The browser could attempt to protect\nagainst such attacks (e.g. by bypassing the cache) at the cost of performance\nand thus user-experience. To mitigate against such timing attacks, end-users can\ndelete browsing history and/or browse sensitive sites using Chrome's Incognito\nor Guest browsing modes.\n\nOther timing attacks can be mitigated via clever design changes. For instance,\n[Issue 544765](https://crbug.com/544765) describes an attack whereby an attacker\ncan probe for the presence of HSTS rules (set by prior site visits) by timing\nthe load of resources with URLs \"fixed-up\" by HSTS. Prior to Chrome 64, HSTS\nrules [were shared](https://crbug.com/774643) between regular browsing and\nIncognito mode, making the attack more interesting. The attack was mitigated by\nchanging Content-Security-Policy such that secure URLs will match rules\ndemanding non-secure HTTP urls, a fix that has also proven useful to help to\nunblock migrations to HTTPS. Similarly, [Issue 707071](https://crbug.com/707071)\ndescribes a timing attack in which an attacker could determine what Android\napplications are installed; the attack was mitigated by introducing randomness\nin the execution time of the affected API.\n\n<a name=\"TOC-What-if-a-Chrome-component-breaks-an-OS-security-boundary-\"></a>\n### What if a Chrome component breaks an OS security boundary?\n\nIf Chrome or any of its components (e.g. updater) can be abused to\nperform a local privilege escalation, then it may be treated as a\nvalid security vulnerability.\n\nRunning any Chrome component with higher privileges than intended is\nnot a security bug and we do not recommend running Chrome as an\nAdministrator on Windows, or as root on POSIX.\n\n<a name=\"TOC-Why-isn-t-passive-browser-fingerprinting-including-passive-cookies-in-Chrome-s-threat-model-\"></a>\n<a name=\"TOC-What-is-Chrome-s-threat-model-for-fingerprinting-\"></a>\n### What is Chrome's threat model for fingerprinting?\n\n> **Update, August 2019:** Please note that this answer has changed. We have\n> updated our threat model to include fingerprinting.\n\nAlthough [we do not consider fingerprinting issues to be *security\nvulnerabilities*](#TOC-Are-privacy-issues-considered-security-bugs-), we do now\nconsider them to be privacy bugs that we will try to resolve. We distinguish two\nforms of fingerprinting.\n\n* **Passive fingerprinting** refers to fingerprinting techniques that do not\nrequire a JavaScript API call to achieve. This includes (but is not limited to)\nmechanisms like [ETag\ncookies](https://en.wikipedia.org/wiki/HTTP_ETag#Tracking_using_ETags) and [HSTS\ncookies](https://security.stackexchange.com/questions/79518/what-are-hsts-super-cookies).\n* **Active fingerprinting** refers to fingerprinting techniques that do require\na JavaScript API call to achieve. Examples include most of the techniques in\n[EFF's Panopticlick proof of concept](https://panopticlick.eff.org).\n\nFor passive fingerprinting, our ultimate goal is (to the extent possible) to\nreduce the information content available to below the threshold for usefulness.\n\nFor active fingerprinting, our ultimate goal is to establish a [privacy\nbudget](https://github.com/bslassey/privacy-budget) and to keep web origins\nbelow the budget (such as by rejecting some API calls when the origin exceeds\nits budget). To avoid breaking rich web applications that people want to use,\nChrome may increase an origin's budget when it detects that a person is using\nthe origin heavily. As with passive fingerprinting, our goal is to set the\ndefault budget below the threshold of usefulness for fingerprinting.\n\nThese are both long-term goals. As of this writing (August 2019) we do not\nexpect that Chrome will immediately achieve them.\n\nFor background on fingerprinting and the difficulty of stopping it, see [Arvind\nNarayanan's site](https://33bits.wordpress.com/about/) and [Peter Eckersley's\ndiscussion of the information theory behind\nPanopticlick](https://www.eff.org/deeplinks/2010/01/primer-information-theory-and-privacy).\nThere is also [a pretty good analysis of in-browser fingerprinting\nvectors](https://dev.chromium.org/Home/chromium-security/client-identification-mechanisms).\n\n<a name=\"TOC-I-found-a-phishing-or-malware-site-not-blocked-by-Safe-Browsing.-Is-this-a-security-vulnerability-\"></a>\n### I found a phishing or malware site not blocked by Safe Browsing. Is this a security vulnerability?\n\nMalicious sites not yet blocked by Safe Browsing can be reported via\n[https://www.google.com/safebrowsing/report_phish/](https://www.google.com/safebrowsing/report_phish/).\nSafe Browsing is primarily a blocklist of known-unsafe sites; the feature warns\nthe user if they attempt to navigate to a site known to deliver phishing or\nmalware content. You can learn more about this feature in these references:\n\n*    [https://developers.google.com/safe-browsing/](https://developers.google.com/safe-browsing/)\n*    [https://www.google.com/transparencyreport/safebrowsing/](https://www.google.com/transparencyreport/safebrowsing/)\n\nIn general, it is not considered a security bug if a given malicious site is not\nblocked by the Safe Browsing feature, unless the site is on the blocklist but is\nallowed to load anyway. For instance, if a site found a way to navigate through\nthe blocking red warning page without user interaction, that would be a security\nbug. A malicious site may exploit a security vulnerability (for instance,\nspoofing the URL in the **Location Bar**). This would be tracked as a security\nvulnerability in the relevant feature, not Safe Browsing itself.\n\n<a name=\"TOC-I-can-download-a-file-with-an-unsafe-extension-and-it-is-not-classified-as-dangerous-\"></a>\n### I can download a file with an unsafe extension and it is not classified as dangerous - is this a security bug?\n\nChrome tries to warn users before they open files that might modify their\nsystem. What counts as a dangerous file will vary depending on the operating\nsystem Chrome is running on, the default set of file handlers, Chrome settings,\nEnterprise policy and verdicts on both the site and the file from [Safe\nBrowsing](https://code.google.com/apis/safebrowsing/). Because of this it will\noften be okay for a user to download and run a file. However, if you can clearly\ndemonstrate how to bypass one of these protections then we’d like to hear about\nit. You can see if a Safe Browsing check happened by opening\nchrome://safe-browsing before starting the download.\n\n<a name=\"TOC-what-about-dangerous-file-types-not-listed-in-the-file-type-policy-\"></a>\n### What about dangerous file types not listed in the file type policy?\n\nThe [file type\npolicy](https://source.chromium.org/chromium/chromium/src/+/main:components/safe_browsing/content/resources/download_file_types.asciipb?q=download_file_types.asciipb%20-f:%2Fgen%2F&ss=chromium)\ncontrols some details of which security checks to enable for a given file\nextension. Most importantly, it controls whether we contact Safe Browsing about\na download, and whether we show a warning for all downloads of that file type.\nStarting in M74, the default for unknown file types has been to contact Safe\nBrowsing. This prevents large-scale abuse from a previously unknown file type.\nStarting in M105, showing a warning for all downloads of an extension became\nreserved for exceptionally dangerous file types that can compromise a user\nwithout any user interaction with the file (e.g. DLL hijacking). If you discover\na new file type that meets that condition, we’d like to hear about it.\n\n<a name=\"TOC-i-found-a-local-file-or-directory-that-may-be-security-sensitive-and-is-not-blocked-by-file-system-access-api-\"></a>\n### I found a local file or directory that may be security-sensitive and is not blocked by File System Access API - is this a security bug?\n\nThe File System Access API maintains a [blocklist](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/file_system_access/chrome_file_system_access_permission_context.cc;l=266-346)\nof directories and files that may be sensitive such as systems file, and if user\nchooses a file or a directory matching the list on a site using File System\nAccess API, the access is blocked.\n\nThe blocklist is designed to help mitigate accidental granting by users by\nlisting well-known, security-sensitive locations, as a defense in-depth\nstrategy. Therefore, the blocklist coverage is not deemed as a security bug,\nespecially as it requires user's explicit selection on a file or a directory\nfrom the file picker.\n\n<a name=\"TOC-I-can-download-a-file-with-an-unsafe-extension-but-a-different-extension-or-file-type-is-shown-to-the-user-\"></a>\n### I can download a file with an unsafe extension but a different extension or file type is shown to the user - is this a security bug?\n\nSee [file types](#TOC-The-wrong-description-for-a-file-type-is-added-by-Chrome-).\n\n<a name=\"TOC-Extensions-for-downloaded-files-are-not-shown-in-a-file-dialog-\"></a>\n### Extensions for downloaded files are not shown in a file dialog - is this a security bug?\n\nSee [file types](#TOC-The-wrong-description-for-a-file-type-is-added-by-Chrome-).\n\n<a name=\"TOC-The-wrong-description-for-a-file-type-is-added-by-Chrome-\"></a>\n### The wrong description for a file type is added by Chrome - is this a security bug?\n\nChrome tries to let users know what they will be saving and downloading before\nthey do so. Often operating systems will obscure a file’s type or extension and\nthere is little we can do about that. Chrome shows information to help users\nmake these decisions, both in Chrome-owned UI and in information that Chrome\npasses to OS-owned UI. If this information can be manipulated from a web site to\nmislead a user, then we’d like to hear about it.\n[Example](https://crbug.com/1137247).\n\n<a name=\"TOC-I-can-download-a-file-and-OS-indicators-for-its-provenance-are-not-applied-\"></a>\n### I can download a file and OS indicators for its provenance are not applied - is this a security bug?\n\nChrome attempts to label files downloaded from the internet with metadata using\noperating system APIs where these are available – for instance applying the Mark\nof the Web on Windows. This is often not possible (for instance on non-NTFS file\nsystems on Windows, or for files inside downloaded archives) or disabled by\npolicy. If a web site can cause Chrome to download a file without Chrome then\nadding this metadata as usual, we’d like to hear about it.\n\n<a name=\"TOC-I-can-cause-a-hard-or-soft-link-to-be-written-to-a-directory-bypassing-normal-OS-blocks-\"></a>\n### I can cause a hard or soft link to be written to a directory bypassing normal OS blocks - is this a security bug?\n\nChrome should not allow filesystem links to be created by initiating a download.\n[Example](https://crbug.com/1140417). [Example](https://crbug.com/1137247#c12).\n\n<a name=\"TOC-I-can-hijack-a-user-gesture-and-trick-a-user-into-accepting-a-permission-or-downloading-a-file-\"></a>\n### I can hijack a user gesture and trick a user into accepting a permission or downloading a file - is this a security bug?\n\nChrome tries to design its prompts to select safe defaults. If a prompt can\naccidentally be accepted without the user having an opportunity to make a\ndecision about the prompt then we’d like to know. Examples might include poor\ndefaults so that a user holding down an enter key might accept a dialog they\nwould want to dismiss. [Example](https://crbug.com/854455#c11).\n\nNote that a user navigating to a download will cause a file to be\n[downloaded](https://crbug.com/1114592).\n\n<a name=\"TOC-security-properties-not-inherited-using-contextual-menu-\"></a>\n### Sandbox/CSP/etc... security properties are not inherited when navigating using the middle-click/contextual-menu - is this a security bug?\n\nThe security properties of the document providing the URL are not used/inherited\nwhen the user deliberately opens a link in a popup using one of:\n\n- Ctrl + left-click (Open link in new tab)\n- Shift + left-click (Open link in new window)\n- Middle-click (Open a link in a new tab)\n- Right-click > \"Open link in ...\"\n\nThese methods of following a link have more or less the same implications as the\nuser copying the link's URL and pasting it into a newly-opened window. We treat\nthem as user-initiated top-level navigations, and as such will not apply or\ninherit policy restrictions into the new context\n\nExample of security related properties:\n\n- Content-Security-Policy\n- Cross-Origin-Embedder-Policy\n- Cross-Origin-Opener-Policy\n- Origin\n- Referrer\n- Sandbox\n- etc...\n\nThese browser's actions/shortcuts are specific to Chrome. They are different\nfrom the behavior specified by the web-platform, such as using executing\n`window.open()` or opening a link with the `target=_blank` attribute.\n\n<a name=\"TOC-What-is-the-threat-model-for-Chrome-for-Testing\"></a>\n### What is the threat model for Chrome for Testing?\n\n[Chrome for Testing](https://developer.chrome.com/blog/chrome-for-testing) is a\ndistribution of current and older versions of Chrome. It does not auto-update.\nTherefore, it may lack recent fixes for security bugs. Security bugs can more\neasily be exploited once their fixes are [published in the main Chromium source\ncode repository](updates.md) and so it is unsafe to use Chrome for Testing to\naccess any untrusted website.  You should use Chrome for Testing only for\nbrowser automation and testing purposes, consuming only trustworthy content.\n`chrome-headless-shell` also lacks auto-updates and so, for the same reason,\nshould only be used to consume trusted content.\n\n## Areas outside Chrome's Threat Model\n\n<a name=\"TOC-Are-privacy-issues-considered-security-bugs-\"></a>\n### Are privacy issues considered security bugs?\n\nNo. The Chrome Privacy team treats privacy issues, such as leaking information\nfrom Incognito, fingerprinting, and bugs related to deleting browsing data as\nfunctional bugs.\n\nPrivacy issues are not considered under the security vulnerability rewards\nprogram; the [severity guidelines](severity-guidelines.md) outline the types of\nbugs that are considered security vulnerabilities in more detail.\n\n<a name=\"TOC-What-are-the-security-and-privacy-guarantees-of-Incognito-mode-\"></a>\n### What are the security and privacy guarantees of Incognito mode?\n\nBugs in Incognito mode are tracked as privacy bugs, not security bugs.\n\nThe [Help Center](https://support.google.com/chrome/?p=cpn_incognito) explains\nwhat privacy protections Incognito mode attempts to enforce. In particular,\nplease note that Incognito is not a “do not track” mode, and it does not hide\naspects of your identity from web sites. Chrome does offer a way to send Do Not\nTrack request to servers; see chrome://settings/?search=do+not+track\n\nWhen in Incognito mode, Chrome does not store any new history, cookies, or other\nstate in non-volatile storage. However, Incognito windows will be able to access\nsome previously-stored state, such as browsing history.\n\n<a name=\"TOC-Are-XSS-filter-bypasses-considered-security-bugs-\"></a>\n### Are XSS filter bypasses considered security bugs?\n\nNo. Chromium once contained a reflected XSS filter called the [XSSAuditor](https://www.chromium.org/developers/design-documents/xss-auditor)\nthat was a best-effort second line of defense against reflected XSS flaws found\nin web sites. The XSS Auditor was [removed in Chrome 78](https://groups.google.com/a/chromium.org/forum/#!msg/blink-dev/TuYw-EZhO9g/blGViehIAwAJ).\nConsequently, Chromium no longer takes any special action in response to an\nX-XSS-Protection header.\n\n<a name=\"TOC-Are-denial-of-service-issues-considered-security-bugs-\"></a>\n### Are denial of service issues considered security bugs?\n\nNo. Denial of Service (DoS) issues are treated as **abuse** or **stability**\nissues rather than security vulnerabilities.\n\n*    If you find a reproducible crash (e.g. a way to hit a `CHECK`),\n     we encourage you to [report it](https://issues.chromium.org/new).\n*    If you find a site that is abusing the user experience (e.g. preventing you\n     from leaving a site), we encourage you to [report\n     it](https://issues.chromium.org/new).\n\nDoS issues are not considered under the security vulnerability rewards program;\nthe [severity guidelines](severity-guidelines.md) outline the types of bugs that\nare considered security vulnerabilities in more detail.\n\n<a name=\"TOC-Why-aren-t-physically-local-attacks-in-Chrome-s-threat-model-\"></a>\n### Why aren't physically-local attacks in Chrome's threat model?\n\nPeople sometimes report that they can compromise Chrome by installing a\nmalicious DLL in a place where Chrome will load it, by hooking APIs (e.g. [Issue\n130284](https://crbug.com/130284)), or by otherwise altering the configuration\nof the device.\n\nWe consider these attacks outside Chrome's threat model, because there is no way\nfor Chrome (or any application) to defend against a malicious user who has\nmanaged to log into your device as you, or who can run software with the\nprivileges of your operating system user account. Such an attacker can modify\nexecutables and DLLs, change environment variables like `PATH`, change\nconfiguration files, read any data your user account owns, email it to\nthemselves, and so on. Such an attacker has total control over your device,\nand nothing Chrome can do would provide a serious guarantee of defense. This\nproblem is not special to Chrome ­— all applications must trust the\nphysically-local user.\n\nThere are a few things you can do to mitigate risks from people who have\nphysical control over **your** computer, in certain circumstances.\n\n*    To stop people from reading your data in cases of device theft or loss, use\n     full disk encryption (FDE). FDE is a standard feature of most operating\n     systems, including Windows Vista and later, Mac OS X Lion and later, and\n     some distributions of Linux. (Some older versions of Mac OS X had partial\n     disk encryption: they could encrypt the user’s home folder, which contains\n     the bulk of a user’s sensitive data.) Some FDE systems allow you to use\n     multiple sources of key material, such as the combination of both a\n     password and a key file on a USB token. When available, you should use\n     multiple sources of key material to achieve the strongest defense. Chrome\n     OS encrypts users’ home directories.\n*    If you share your computer with other people, take advantage of your\n     operating system’s ability to manage multiple login accounts, and use a\n     distinct account for each person. For guests, Chrome OS has a built-in\n     Guest account for this purpose.\n*    Take advantage of your operating system’s screen lock feature.\n*    You can reduce the amount of information (including credentials like\n     cookies and passwords) that Chrome will store locally by using Chrome's\n     Content Settings (chrome://settings/content) and turning off the form\n     auto-fill and password storage features\n     ([chrome://settings/search#password](chrome://settings/search#password)).\n\nThere is almost nothing you can do to mitigate risks when using a **public**\ncomputer.\n\n*    Assume everything you do on a public computer will become, well, public.\n     You have no control over the operating system or other software on the\n     machine, and there is no reason to trust the integrity of it.\n*    If you must use such a computer, use Incognito mode and close all Incognito\n     windows when you are done browsing to limit the amount of data you leave\n     behind. Note that Incognito mode **provides no protection** if the system has\n     already been compromised as described above.\n\n<a name=\"TOC-Why-aren-t-compromised-infected-machines-in-Chrome-s-threat-model-\"></a>\n### Why aren't compromised/infected machines in Chrome's threat model?\n\nAlthough the attacker may now be remote, the consequences are essentially the\nsame as with physically-local attacks. The attacker's code, when it runs as\nyour user account on your machine, can do anything you can do. (See also\n[Microsoft's Ten Immutable Laws Of\nSecurity](https://web.archive.org/web/20160311224620/https://technet.microsoft.com/en-us/library/hh278941.aspx).)\n\nOther cases covered by this section include leaving a debugger port open to\nthe world, remote shells, and so forth.\n\n<a name=\"TOC-If-a-website-can-open-an-android-app-via-an-intent\"></a>\n### If a website can open an Android app via an intent is this a security bug?\n\nNo - websites can link to external handlers or applications - but there are\nrestrictions around requiring a user gesture and the type of intent that can\nbe launched. Full details are available in the\n[external_intents](../../components/external_intents/README.md) documentation.\n\n<a name=\"TOC-Does-entering-JavaScript:-URLs-in-the-URL-bar-or-running-script-in-the-developer-tools-mean-there-s-an-XSS-vulnerability-\"></a>\n### Does entering JavaScript: URLs in the URL bar or running script in the developer tools mean there's an XSS vulnerability?\n\n[No](https://crbug.com/81697). Chrome does not attempt to prevent the user from\nknowingly running script against loaded documents, either by entering script in\nthe Developer Tools console or by typing a JavaScript: URI into the URL bar.\nChrome and other browsers do undertake some efforts to prevent *paste* of script\nURLs in the URL bar (to limit\n[social-engineering](https://blogs.msdn.microsoft.com/ieinternals/2011/05/19/socially-engineered-xss-attacks/))\nbut users are otherwise free to invoke script against pages using either the URL\nbar or the DevTools console.\n\n<a name=\"TOC-Does-executing-JavaScript-from-a-bookmark-mean-there-s-an-XSS-vulnerability-\"></a>\n### Does executing JavaScript from a bookmark or the Home button mean there's an XSS vulnerability?\n\nNo. Chromium allows users to create bookmarks to JavaScript URLs that will run\non the currently-loaded page when the user clicks the bookmark; these are called\n[bookmarklets](https://en.wikipedia.org/wiki/Bookmarklet).\n\nSimilarly, the Home button may be configured to invoke a JavaScript URL when clicked.\n\n<a name=\"TOC-Does-executing-JavaScript-in-a-PDF-file-mean-there-s-an-XSS-vulnerability-\"></a>\n### Does executing JavaScript in a PDF file mean there's an XSS vulnerability?\n\nNo. PDF files have the ability to run JavaScript, usually to facilitate field\nvalidation during form fill-out. Note that the set of bindings provided to\nthe PDF are more limited than those provided by the DOM to HTML documents, nor\ndo PDFs get any ambient authority based upon the domain from which they are\nserved (e.g. no document.cookie).\n\n<a name=\"TOC-Are-PDF-files-static-content-in-Chromium-\"></a>\n### Are PDF files static content in Chromium?\n\nNo. PDF files have some powerful capabilities including invoking printing or\nposting form data. To mitigate abuse of these capabiliies, such as beaconing\nupon document open, we require interaction with the document (a \"user gesture\")\nbefore allowing their use.\n\n<a name=\"TOC-Are-non_committed-URLs-entered-by-the-user-considered-URL-spoofs-\"></a>\n### Are non-committed URLs entered by the user considered URL spoofs?\n\nNo. When a user enters a URL into the address bar (whether by typing,\ncopy/pasting, drag and drop, or otherwise), Chrome intentionally displays\nit instead of the last committed URL of the currently active page, until\nboth the navigation begins and the new page commits. During this time, the\ncurrently active page can change its appearance to mimic the new URL while\nits own URL is not shown. However, the active page does not have control\nover which URL the user entered into the address bar, limiting the\neffectiveness of a spoof attempt. The new\n[lock-replacement icon](https://blog.chromium.org/2023/05/an-update-on-lock-icon.html)\nis also not present in this state, and in many cases (i.e., once the new\nnavigation has started), the loading indicators are present.\n\nThe confusion between the non-committed URL and the active page's\nappearance is a consequence of the address bar needing to serve two roles:\nshowing both where you are and where you are going.\n\nSee also https://crbug.com/378932942 for context.\n\n<a name=\"TOC-What-about-URL-spoofs-using-Internationalized-Domain-Names-IDN-\"></a>\n### What about URL spoofs using Internationalized Domain Names (IDN)?\n\nWe try to balance the needs of our international userbase while protecting users\nagainst confusable homograph attacks. Despite this, there are a list of known\nIDN display issues we are still working on.\n\n*    Please see [this document](https://docs.google.com/document/d/1_xJz3J9kkAPwk3pma6K3X12SyPTyyaJDSCxTfF8Y5sU)\nfor a list of known issues and how we handle them.\n*    [This document](https://chromium.googlesource.com/chromium/src/+/main/docs/idn.md)\ndescribes Chrome's IDN policy in detail.\n\n<a name=\"TOC-Chrome-silently-syncs-extensions-across-devices.-Is-this-a-security-vulnerability-\"></a>\n### Chrome silently syncs extensions across devices. Is this a security vulnerability?\n\nThis topic has been moved to the [Extensions Security FAQ](https://chromium.googlesource.com/chromium/src/+/main/extensions/docs/security_faq.md).\n\n<a name=\"TOC-Why-arent-null-pointer-dereferences-considered-security-bugs-\"></a>\n### Why aren't null pointer dereferences considered security bugs?\n\nNull pointer dereferences with consistent, small, fixed offsets are not considered\nsecurity bugs. A read or write to the NULL page results in a non-exploitable crash.\nIf the offset is larger than 32KB, or if there's uncertainty about whether the\noffset is controllable, it is considered a security bug.\n\nAll supported Chrome platforms do not allow mapping memory in at least the first\n32KB of address space:\n\n- Windows: Windows 8 and later disable mapping the first 64k of address space;\n  see page 33 of [Exploit Mitigation Improvements in Windows\n  8][windows-null-page-mapping] [[archived]][windows-null-page-mapping-archived].\n- Mac and iOS: by default, the linker reserves the first 4GB of address space\n  with the `__PAGEZERO` segment for 64-bit binaries.\n- Linux: the default `mmap_min_addr` value for supported distributions is at\n  least 64KB.\n- Android: [CTS][android-mmap_min_addr] enforces that `mmap_min_addr` is set to\n  exactly 32KB.\n- ChromeOS: the [ChromeOS kernels][chromeos-mmap_min_addr] set the default\n  `mmap_min_addr` value to at least 32KB.\n- Fuchsia: the [userspace base address][fuchsia-min-base-address] begins at 2MB;\n  this is configured per-platform but set to the same value on all platforms.\n\n[windows-null-page-mapping]: https://media.blackhat.com/bh-us-12/Briefings/M_Miller/BH_US_12_Miller_Exploit_Mitigation_Slides.pdf\n[windows-null-page-mapping-archived]: https://web.archive.org/web/20230608131033/https://media.blackhat.com/bh-us-12/Briefings/M_Miller/BH_US_12_Miller_Exploit_Mitigation_Slides.pdf\n[android-mmap_min_addr]: https://android.googlesource.com/platform/cts/+/496152a250d10e629d31ac90b2e828ad77b8d70a/tests/tests/security/src/android/security/cts/KernelSettingsTest.java#43\n[chromeos-mmap_min_addr]: https://source.chromium.org/search?q=%22CONFIG_DEFAULT_MMAP_MIN_ADDR%3D%22%20path:chromeos%2F&ss=chromiumos%2Fchromiumos%2Fcodesearch:src%2Fthird_party%2Fkernel%2F\n[fuchsia-min-base-address]: https://cs.opensource.google/fuchsia/fuchsia/+/main:zircon/kernel/arch/arm64/include/arch/kernel_aspace.h;l=20;drc=eeceea01eee2615de74b1339bcf6e6c2c6f72769\n\n<a name=\"TOC-Indexing-a-container-out-of-bounds-hits-a-libcpp-verbose-abort--is-this-a-security-bug-\"></a>\n### Indexing a container out of bounds hits a __libcpp_verbose_abort, is this a security bug?\n\n`std::vector` and other containers are now protected by libc++ hardening on all\nplatforms [crbug.com/1335422](https://crbug.com/1335422). Indexing these\ncontainers out of bounds is now a safe crash - if a proof-of-concept reliably\ncauses a crash in production builds we consider these to be functional rather than\nsecurity issues.\n\n<a name=\"TOC-Are-stack-overflows-considered-security-bugs-\"></a>\n### Are stack overflows considered security bugs?\n\nNo. Guard pages mean that stack overflows are considered unexploitable, and\nare regarded as [denial of service bugs](#TOC-Are-denial-of-service-issues-considered-security-bugs-).\nThe only exception is if an attacker can jump over the guard pages allocated by\nthe operating system and avoid accessing them, e.g.:\n\n*    A frame with a very large stack allocation.\n*    C variable length array with an attacker-controlled size.\n*    A call to `alloca()` with an attacker-controlled size.\n\n<a name=\"TOC-Are-tint-ICE-considered-security-bugs-\"></a>\n### Are tint shader compiler Internal Compiler Errors considered security bugs?\n\nNo. When tint fails and throws an ICE (Internal Compiler Error), it will\nterminate the process in an intentional manner and produce no shader output.\nThus there is not security bug that follows from it.\n\n<a name=\"TOC-Are-enterprise-admins-considered-privileged-\"></a>\n### Are enterprise admins considered privileged?\n\nChrome [can't guard against local\nattacks](#TOC-Why-aren-t-physically-local-attacks-in-Chrome-s-threat-model-).\nEnterprise administrators often have full control over the device. Does Chrome\nassume that enterprise administrators are as privileged and powerful as other\nlocal users? It depends:\n\n* On a fully managed machine, for example a [domain-joined Windows\n  machine](https://docs.microsoft.com/en-us/windows-server/identity/ad-fs/deployment/join-a-computer-to-a-domain),\n  a device managed via a Mobile Device Management product, or a device with\n  Chrome managed via machine-level [Chrome Browser Cloud\n  Management](https://support.google.com/chrome/?p=cloud_management),\n  the administrator effectively has privileges to view and mutate any state on\n  the device. Chrome [policy implementations](../enterprise/add_new_policy.md)\n  should still guide enterprise admins to the most user-respectful defaults\n  and policy description text should clearly describe the nature of the\n  capabilities and the user impact of them being granted.\n* On an unmanaged machine, Chrome profiles [can be managed via cloud\n  policy](https://support.google.com/chrome/?p=manage_profiles)\n  if users sign into Chrome using a managed account. These policies are called\n  *user policies*. In this scenario, the Chrome enterprise administrator should\n  have privileges only to *view and mutate state within the profile that they\n  administer*. Any access outside that profile requires end-user consent.\n\nChrome administrators can force-install Chrome extensions without permissions\nprompts, so the same restrictions must apply to the Chrome extension APIs.\n\nChrome has a long history of policy support with many hundreds of policies. We\nrecognize that there may exist policies or policy combinations that can provide\ncapabilities outside of the guidance provided here. In cases of clear violation\nof user expectations, we will attempt to remedy these policies and we will apply\nthe guidance laid out in this document to any newly added policies.\n\nSee the [Web Platform Security\nguidelines](https://chromium.googlesource.com/chromium/src/+/main/docs/security/web-platform-security-guidelines.md#enterprise-policies)\nfor more information on how enterprise policies should interact with Web\nPlatform APIs.\n\n<a name=\"TOC-Can-I-use-EMET-to-help-protect-Chrome-against-attack-on-Microsoft-Windows-\"></a>\n### Can I use EMET to help protect Chrome against attack on Microsoft Windows?\n\nThere are [known compatibility\nproblems](https://sites.google.com/a/chromium.org/dev/Home/chromium-security/chromium-and-emet)\nbetween Microsoft's EMET anti-exploit toolkit and some versions of Chrome. These\ncan prevent Chrome from running in some configurations. Moreover, the Chrome\nsecurity team does not recommend the use of EMET with Chrome because its most\nimportant security benefits are redundant with or superseded by built-in attack\nmitigations within the browser. For users, the very marginal security benefit is\nnot usually a good trade-off for the compatibility issues and performance\ndegradation the toolkit can cause.\n\n<a name=\"TOC-dangling-pointers\"></a>\n### Dangling pointers\n\nChromium can be instrumented to detect [dangling\npointers](https://chromium.googlesource.com/chromium/src/+/main/docs/dangling_ptr.md):\n\nNotable build flags are:\n- `enable_dangling_raw_ptr_checks=true`\n- `use_raw_ptr_asan_unowned_impl=true`\n\nNotable runtime flags are:\n- `--enable-features=PartitionAllocDanglingPtr`\n\nIt is important to note that detecting a dangling pointer alone does not\nnecessarily indicate a security vulnerability. A dangling pointer becomes a\nsecurity vulnerability only when it is dereferenced and used after it becomes\ndangling.\n\nIn general, dangling pointer issues should be assigned to feature teams as\nordinary bugs and be fixed by them. However, they can be considered only if\nthere is a demonstrable way to show a memory corruption. e.g. with a POC causing\ncrash with ASAN **without the flags above**.\n\n<a name=\"TOC-hard-coded-lists\"></a>\n### My domain is on the [Public Suffix List / HSTS preload list / etc.] upstream but this is not yet reflected in Chrome! Is this a security bug?\n\nChrome does not make any guarantees about how soon additions to or removals from\nexternal lists like the [HSTS preload list](https://hstspreload.org) or the\n[Public Suffix List (PSL)](https://publicsuffix.org/) will be incorporated into Chrome.\nIf you believe Chrome's copies of these lists are notably out-of-date, we are\nhappy to field bug reports but we do not consider this to be a vulnerability.\n\n## Certificates & Connection Indicators\n\n<a name=\"TOC-Where-are-the-security-indicators-located-in-the-browser-window-\"></a>\n### Where are the security indicators located in the browser window?\n\nThe topmost portion of the browser window, consisting of the **Omnibox** (or\n**Location Bar**), navigation icons, menu icon, and other indicator icons, is\nsometimes called the browser **chrome** (not to be confused with the Chrome\nBrowser itself). Actual security indicators can only appear in this section of\nthe window. There can be no trustworthy security indicators elsewhere.\n\nFurthermore, Chrome can only guarantee that it is correctly representing URLs\nand their origins at the end of all navigation. Quirks of URL parsing, HTTP\nredirection, and so on are not security concerns unless Chrome is\nmisrepresenting a URL or origin after navigation has completed.\n\nBrowsers present a dilemma to the user since the output is a combination of\ninformation coming from both trustworthy sources (the browser itself) and\nuntrustworthy sources (the web page), and the untrustworthy sources are allowed\nvirtually unlimited control over graphical presentation. The only restriction on\nthe page's presentation is that it is confined to the large rectangular area\ndirectly underneath the chrome, called the **viewport**. Things like hover text\nand URL preview(s), shown in the viewport, are entirely under the control of the\nweb page itself. They have no guaranteed meaning, and function only as the page\ndesires. This can be even more confusing when pages load content that looks like\nchrome. For example, many pages load images of locks, which look similar to the\nmeaningful HTTPS lock in the Omnibox, but in fact do not convey any meaningful\ninformation about the transport security of that page.\n\nWhen the browser needs to show trustworthy information, such as the bubble\nresulting from a click on the lock icon, it does so by making the bubble overlap\nchrome. This visual detail can't be imitated by the page itself since the page\nis confined to the viewport.\n\n<a name=\"TOC-Why-does-Chrome-show-a-lock-even-if-my-HTTPS-connection-is-being-proxied-\"></a>\n### Why does Chrome show a lock, even if my HTTPS connection is being proxied?\n\nSome types of software intercept HTTPS connections. Examples include anti-virus\nsoftware, corporate network monitoring tools, and school censorship software. In\norder for the interception to work, you need to install a private trust anchor\n(root certificate) onto your computer. This may have happened when you installed\nyour anti-virus software, or when your company's network administrator set up\nyour computer. If that has occurred, your HTTPS connections can be viewed or\nmodified by the software.\n\nSince you have allowed the trust anchor to be installed onto your computer,\nChrome assumes that you have consented to HTTPS interception. Anyone who can add\na trust anchor to your computer can make other changes to your computer, too,\nincluding changing Chrome. (See also [Why aren't physically-local attacks in\nChrome's threat model?](#TOC-Why-aren-t-physically-local-attacks-in-Chrome-s-threat-model-).)\n\n<a name=\"TOC-Why-can-t-I-select-Proceed-Anyway-on-some-HTTPS-error-screens-\"></a>\n### Why can’t I select Proceed Anyway on some HTTPS error screens?\n\nA key guarantee of HTTPS is that Chrome can be relatively certain that it is\nconnecting to the true web server and not an impostor. Some sites request an\neven higher degree of protection for their users (i.e. you): they assert to\nChrome (via Strict Transport Security —\n[HSTS](https://tools.ietf.org/html/rfc6797) — or by other means) that any\nserver authentication error should be fatal, and that Chrome must close the\nconnection. If you encounter such a fatal error, it is likely that your network\nis under attack, or that there is a network misconfiguration that is\nindistinguishable from an attack.\n\nThe best thing you can do in this situation is to raise the issue to your\nnetwork provider (or corporate IT department).\n\nChrome shows non-recoverable HTTPS errors only in cases where the true server\nhas previously asked for this treatment, and when it can be relatively certain\nthat the current server is not the true server.\n\n<a name=\"TOC-How-does-key-pinning-interact-with-local-proxies-and-filters-\"></a>\n### How does key pinning interact with local proxies and filters?\n\nTo enable certificate chain validation, Chrome has access to two stores of trust\nanchors (i.e., certificates that are empowered as issuers). One trust anchor\nstore is for authenticating public internet servers, and depending on the\nversion of Chrome being used and the platform it is running on, the\n[Chrome Root Store](https://chromium.googlesource.com/chromium/src/+/main/net/data/ssl/chrome_root_store/faq.md#what-is-the-chrome-root-store)\nmight be in use. The private store contains certificates installed by the user\nor the administrator of the client machine. Private intranet servers should\nauthenticate themselves with certificates issued by a private trust anchor.\n\nChrome’s key pinning feature is a strong form of web site authentication that\nrequires a web server’s certificate chain not only to be valid and to chain to a\nknown-good trust anchor, but also that at least one of the public keys in the\ncertificate chain is known to be valid for the particular site the user is\nvisiting. This is a good defense against the risk that any trust anchor can\nauthenticate any web site, even if not intended by the site owner: if an\notherwise-valid chain does not include a known pinned key (“pin”), Chrome will\nreject it because it was not issued in accordance with the site operator’s\nexpectations.\n\nChrome does not perform pin validation when the certificate chain chains up to a\nprivate trust anchor. A key result of this policy is that private trust anchors\ncan be used to proxy (or\n[MITM](https://en.wikipedia.org/wiki/Man-in-the-middle_attack)) connections,\neven to pinned sites. “Data loss prevention” appliances, firewalls, content\nfilters, and malware can use this feature to defeat the protections of key\npinning.\n\nWe deem this acceptable because the proxy or MITM can only be effective if the\nclient machine has already been configured to trust the proxy’s issuing\ncertificate — that is, the client is already under the control of the person who\ncontrols the proxy (e.g. the enterprise’s IT administrator). If the client does\nnot trust the private trust anchor, the proxy’s attempt to mediate the\nconnection will fail as it should.\n\n<a name=\"TOC-When-is-key-pinning-enabled-\"></a>\n### When is key pinning enabled?\n\nKey pinning is enabled for Chrome-branded non-iOS builds when the local\nclock is within ten weeks of the embedded build timestamp. Key pinning is a\nuseful security measure but it tightly couples client and server configurations\nand completely breaks when those configurations are out of sync. In order to\nmanage that risk we need to ensure that we can promptly update pinning clients\nin an emergency and ensure that non-emergency changes can be deployed in a\nreasonable timeframe.\n\nEach of the conditions listed above helps ensure those properties:\nChrome-branded builds are those that Google provides and they all have an\nauto-update mechanism that can be used in an emergency. Even in cases where\nauto-update is generally effective, there are still non-trivial populations\nof stragglers for various reasons. The ten-week timeout prevents those\nstragglers from causing problems for regular, non-emergency changes and\nallows stuck users to still, for example, conduct searches and access Chrome's\nhomepage to hopefully get unstuck.\n\nIn order to determine whether key pinning is active, try loading\n[https://pinning-test.badssl.com/](https://pinning-test.badssl.com/). If key\npinning is active the load will _fail_ with a pinning error.\n\n<a name=\"TOC-How-does-certificate-transparency-interact-with-local-proxies-and-filters-\"></a>\n### How does Certificate Transparency interact with local proxies and filters?\n\nJust as [pinning only applies to publicly-trusted trust\nanchors](#TOC-How-does-key-pinning-interact-with-local-proxies-and-filters-),\nChrome only evaluates Certificate Transparency (CT) for publicly-trusted trust\nanchors. Thus private trust anchors, such as for enterprise middle-boxes and AV\nproxies, do not need to be publicly logged in a CT log.\n\n<a name=\"TOC-Why-are-some-web-platform-features-only-available-in-HTTPS-page-loads-\"></a>\n### Why are some web platform features only available in HTTPS page-loads?\n\nThe full answer is here: we [Prefer Secure Origins For Powerful New\nFeatures](https://www.chromium.org/Home/chromium-security/prefer-secure-origins-for-powerful-new-features).\nIn short, many web platform features give web origins access to sensitive new\nsources of information, or significant power over a user's experience with their\ncomputer/phone/watch/etc., or over their experience with it. We would therefore\nlike to have some basis to believe the origin meets a minimum bar for security,\nthat the sensitive information is transported over the Internet in an\nauthenticated and confidential way, and that users can make meaningful choices\nto trust or not trust a web origin.\n\nNote that the reason we require secure origins for WebCrypto is slightly\ndifferent: An application that uses WebCrypto is almost certainly using it to\nprovide some kind of security guarantee (e.g. encrypted instant messages or\nemail). However, unless the JavaScript was itself transported to the client\nsecurely, it cannot actually provide any guarantee. (After all, a MITM attacker\ncould have modified the code, if it was not transported securely.)\n\nSee the [Web Platform Security\nguidelines](https://chromium.googlesource.com/chromium/src/+/main/docs/security/web-platform-security-guidelines.md#encryption)\nfor more information on security guidelines applicable to web platform APIs.\n\n<a name=\"TOC-Which-origins-are-secure-\"></a>\n### Which origins are \"secure\"?\n\nSecure origins are those that match at least one of the following (scheme, host,\nport) patterns:\n\n*    (https, *, *)\n*    (wss, *, *)\n*    (*, localhost, *)\n*    (*, 127/8, *)\n*    (*, ::1/128, *)\n*    (file, *, —)\n*    (chrome-extension, *, —)\n\nThat is, secure origins are those that load resources either from the local\nmachine (necessarily trusted) or over the network from a\ncryptographically-authenticated server. See [Prefer Secure Origins For Powerful\nNew\nFeatures](https://sites.google.com/a/chromium.org/dev/Home/chromium-security/prefer-secure-origins-for-powerful-new-features)\nfor more details.\n\n<a name=\"TOC-What-s-the-story-with-certificate-revocation-\"></a>\n### What's the story with certificate revocation?\n\nChrome's primary mechanism for checking certificate revocation status is\n[CRLSets](https://dev.chromium.org/Home/chromium-security/crlsets).\nAdditionally, by default, [stapled Online Certificate Status Protocol (OCSP)\nresponses](https://en.wikipedia.org/wiki/OCSP_stapling) are honored.\n\nAs of 2024, Chrome enforces most security-relevant certificate revocations that\nare visible via Certificate Revocation Lists (CRLs) published to the\n[CCADB](https://www.ccadb.org/) via CRLSets. There is some inherent delay in\ngetting revocation information to Chrome clients, but most revocations should\nreach most users within a few days of appearing on a CA's CRL.\n\nChrome clients do not, by default, perform \"online\" certificate revocation\nstatus checks using CRLs directly or via OCSP URLs included in certificates.\nThis is because online checks offer limited security value unless a client, like\nChrome, refuses to connect to a website if it cannot get a valid response,\n\nUnfortunately, there are many widely-prevalent causes for why a client\nmight be unable to get a valid certificate revocation status response to\ninclude:\n* timeouts (e.g., an OCSP responder is online but does not respond within an\n  acceptable time limit),\n* availability issues (e.g., the OCSP responder is offline),\n* invalid responses (e.g., a \"stale\" or malformed status response), and\n* local network attacks misrouting traffic or blocking responses.\n\nAdditional concern with OCSP checks are related to privacy. OCSP\nrequests reveal details of individuals' browsing history to the operator of the\nOCSP responder (i.e., a third party). These details can be exposed accidentally\n(e.g., via data breach of logs) or intentionally (e.g., via subpoena). Chrome\nused to perform revocation checks for Extended Validation certificates, but that\nbehavior was disabled in 2022 for [privacy reasons](https://groups.google.com/a/mozilla.org/g/dev-security-policy/c/S6A14e_X-T0/m/T4WxWgajAAAJ).\n\nThe following enterprise policies can be used to change the default revocation\nchecking behavior in Chrome, though these may be removed in the future:\n* [enable soft-fail OCSP](https://chromeenterprise.google/policies/#EnableOnlineRevocationChecks)\n* [hard-fail for local trust anchors](https://chromeenterprise.google/policies/#RequireOnlineRevocationChecksForLocalAnchors).\n\n## Passwords & Local Data\n\n<a name=\"TOC-What-about-unmasking-of-passwords-with-the-developer-tools-\"></a>\n### What about unmasking of passwords with the developer tools?\n\nOne of the most frequent reports we receive is password disclosure using the\nInspect Element feature (see [Issue 126398](https://crbug.com/126398) for an\nexample). People reason that \"If I can see the password, it must be a bug.\"\nHowever, this is just one of the [physically-local attacks described in the\nprevious\nsection](#TOC-Why-aren-t-physically-local-attacks-in-Chrome-s-threat-model-),\nand all of those points apply here as well.\n\nThe reason the password is masked is only to prevent disclosure via\n\"shoulder-surfing\" (i.e. the passive viewing of your screen by nearby persons),\nnot because it is a secret unknown to the browser. The browser knows the\npassword at many layers, including JavaScript, developer tools, process memory,\nand so on. When you are physically local to the computer, and only when you are\nphysically local to the computer, there are, and always will be, tools for\nextracting the password from any of these places.\n\n<a name=\"TOC-Is-Chrome-s-support-for-userinfo-in-HTTP-URLs-e.g.-http:-user:password-example.com-considered-a-vulnerability-\"></a>\n### Is Chrome's support for userinfo in HTTP URLs (e.g. http://user:password@example.com) considered a vulnerability?\n\n[Not at this time](https://crbug.com/626951). Chrome supports HTTP and HTTPS\nURIs with username and password information embedded within them for\ncompatibility with sites that require this feature. Notably, Chrome will\nsuppress display of the username and password information after navigation in\nthe URL box to limit the effectiveness of spoofing attacks that may try to\nmislead the user. For instance, navigating to\n`http://trustedsite.com@evil.example.com` will show an address of\n`http://evil.example.com` after the page loads.\n\nNote: We often receive reports calling this an \"open redirect\". However, it has\nnothing to do with redirection; rather the format of URLs is complex and the\nuserinfo may be misread as a host.\n\n<a name=\"TOC-Why-does-the-Password-Manager-ignore-autocomplete-off-for-password-fields-\"></a>\n### Why does the Password Manager ignore `autocomplete='off'` for password fields?\n\nIgnoring `autocomplete='off'` for password fields allows the password manager to\ngive more power to users to manage their credentials on websites. It is the\nsecurity team's view that this is very important for user security by allowing\nusers to have unique and more complex passwords for websites. As it was\noriginally implemented, autocomplete='off' for password fields took control away\nfrom the user and gave control to the web site developer, which was also a\nviolation of the [priority of\nconstituencies](https://www.schemehostport.com/2011/10/priority-of-constituencies.html).\nFor a longer discussion on this, see the [mailing list\nannouncement](https://groups.google.com/a/chromium.org/forum/#!topic/chromium-dev/zhhj7hCip5c).\n\n<a name=\"TOC-Signout-of-Chrome\"></a>\n### Signing out of Chrome does not delete previously-synced data?\n\nIf you have signed into Chrome and subsequently sign out of Chrome, previously\nsaved passwords and other data are not deleted from your device unless you\nselect that option when signing out of Chrome.\n\nIf you change your Google password, synced data will no longer be updated in\nChrome instances until you provide the new password to Chrome on each device\nconfigured to sync. However, previously synced data [remains available](https://crbug.com/792967)\non each previously-syncing device unless manually removed.\n\n<a name=\"TOC-Why-doesn-t-the-Password-Manager-save-my-Google-password-if-I-am-using-Chrome-Sync-\"></a>\n### Why doesn't the Password Manager save my Google password if I am using Chrome Sync?\n\nIn its default mode, Chrome Sync uses your Google password to protect all the\nother passwords in the Chrome Password Manager.\n\nIn general, it is a bad idea to store the credential that protects an asset in\nthe same place as the asset itself. An attacker who could temporarily compromise\nthe Chrome Password Manager could, by stealing your Google password, obtain\ncontinuing access to all your passwords. Imagine you store your valuables in a\nsafe, and you accidentally forget to close the safe. If a thief comes along,\nthey might steal all of your valuables. That’s bad, but imagine if you had also\nleft the combination to the safe inside as well. Now the bad guy has access to\nall of your valuables and all of your future valuables, too. The password\nmanager is similar, except you probably would not even know if a bad guy\naccessed it.\n\nTo prevent this type of attack, Chrome Password Manager does not save the Google\npassword for the account you sync with Chrome. If you have multiple Google\naccounts, the Chrome Password Manager will save the passwords for accounts other\nthan the one you are syncing with.\n\n<a name=\"TOC-Does-the-Password-Manager-store-my-passwords-encrypted-on-disk-\"></a>\n### Does the Password Manager store my passwords encrypted on disk?\n\nChrome generally tries to use the operating system's user storage mechanism\nwherever possible and stores them encrypted on disk, but it is platform\nspecific:\n\n*    On Windows, Chrome uses the [Data Protection API\n     (DPAPI)](https://msdn.microsoft.com/en-us/library/ms995355.aspx) to bind\n     your passwords to your user account and store them on disk encrypted with\n     a key only accessible to processes running as the same logged on user.\n*    On macOS and iOS, Chrome previously stored credentials directly in the user's\n     Keychain, but for technical reasons, it has switched to storing the\n     credentials in \"Login Data\" in the Chrome users profile directory, but\n     encrypted on disk with a key that is then stored in the user's Keychain.\n     See [Issue 466638](https://crbug.com/466638) and [Issue 520437](https://crbug.com/520437) for further explanation.\n*    On Linux, Chrome previously stored credentials directly in the user's\n     Gnome Secret Service or KWallet, but for technical reasons, it has switched to\n     storing the credentials in \"Login Data\" in the Chrome user's profile directory,\n     but encrypted on disk with a key that is then stored in the user's Gnome\n     Secret Service or KWallet. If there is no available Secret Service or KWallet,\n     the data is not encrypted when stored.\n*    On Android, Chrome doesn't store in the profile anymore, instead it uses Google\n     Play Services to access passwords stored on a device.\n*    On ChromeOS passwords are only obfuscated since all profile data is encrypted\n     by the OS.\n\n<a name=\"TOC-If-theres-a-way-to-see-stored-passwords-without-entering-a-password--is-this-a-security-bug-\"></a>\n### If there's a way to see stored passwords without entering a password, is this a security bug?\n\nNo. If an attacker has control of your login on your device, they can get to\nyour passwords by inspecting Chrome disk files or memory. (See\n[why aren't physically-local attacks in Chrome's threat\nmodel](#TOC-Why-aren-t-physically-local-attacks-in-Chrome-s-threat-model-)).\n\nOn some platforms we ask for a password before revealing stored passwords,\nbut this is not considered a robust defense. It’s historically to stop\nusers inadvertently revealing their passwords on screen, for example if\nthey’re screen sharing. We don’t do this on all platforms because we consider\nsuch risks greater on some than on others.\n\n\n<a name=\"TOC-On-some-websites-I-can-use-a-passkey-without-passing-a-lock-screen-or-biometric-challenge-is-this-a-security-bug\"></a>\n### On some websites, I can use passkeys without passing a lock screen or biometric challenge. Is this a security bug?\n\nProbably not. When a website requests a passkeys signature, it can choose\nwhether the authenticator should perform user verification (e.g. with a local\nuser lock screen challenge). Unless the website sets user verification parameter\nin the request to 'required', the passkey authenticator can choose to skip the\nlock screen challenge. Authenticators commonly skip an optional challenge if\nbiometrics are unavailable (e.g. on a laptop with a closed lid).\n\nIf you can demonstrate bypassing the user verification challenge where the\nrequest user verification parameter is set to 'required', please\n[report it](https://issues.chromium.org/issues/new?noWizard=true&component=1363614&template=1922342).\n\n## Other\n\n<a name=\"TOC-What-is-the-security-story-for-Service-Workers-\"></a>\n### What is the security story for Service Workers?\n\nSee our dedicated [Service Worker Security\nFAQ](https://chromium.googlesource.com/chromium/src/+/main/docs/security/service-worker-security-faq.md).\n\n<a name=\"TOC-What-is-the-security-story-for-Extensions-\"></a>\n### What is the security story for Extensions?\n\nSee our dedicated [Extensions Security FAQ](https://chromium.googlesource.com/chromium/src/+/main/extensions/docs/security_faq.md).\n\n<a name=\"TOC-What-is-the-security-model-for-Chrome-Custom-Tabs-\"></a>\n### What's the security model for Chrome Custom Tabs?\n\nSee our [Chrome Custom Tabs security FAQ](custom-tabs-faq.md).\n\n<a name=\"TOC-How-is-security-different-in-Chrome-for-iOS--\"></a>\n### How is security different in Chrome for iOS?\n\nChrome for iOS does not use Chrome's standard rendering engine. Due to Apple's\niOS platform restrictions, it instead uses Apple's WebKit engine and a more\nrestricted process isolation model. This means its security properties are\ndifferent from Chrome on all other platforms.\n\nThe differences in security are far too extensive to list exhaustively, but some\nnotable points are:\n\n* Chromium's [site\n  isolation](https://www.chromium.org/Home/chromium-security/site-isolation/)\n  isn't used; WebKit has its own alternative implementation with different costs\n  and benefits.\n* WebKit has [historically been slower at shipping security\n  fixes](https://googleprojectzero.blogspot.com/2022/02/a-walk-through-project-zero-metrics.html).\n* Chrome's network stack, [root\n  store](https://www.chromium.org/Home/chromium-security/root-ca-policy/) and\n  associated technology are not used, so\n  the platform will make different decisions about what web servers to trust.\n* Sandboxing APIs are not available for native code.\n\nGiven that the fundamentals of the browser are so different, and given these\nlimitations, Chrome for iOS has historically not consistently implemented some\nof Chrome's [standard security guidelines](rules.md). This includes the\nimportant [Rule of Two](rule-of-2.md). Future Chrome for iOS features should\nmeet all guidelines except in cases where the lack of platform APIs make it\nunrealistic. (The use of WebAssembly-based sandboxing is currently considered\nunrealistic though this could change in future.)\n\nIf the Rule of Two cannot be followed, features for Chrome for iOS should\nnevertheless follow it as closely as possible, and adopt additional mitigations\nwhere they cannot:\n\n* First consider adding a validation layer between unsafe code and web contents,\n  or adopting memory-safe parsers at the boundary between the renderer and the\n  browser process. Consider changing the design of the feature so the riskiest\n  parsing can happen in javascript injected in the renderer process.\n* Any unsafe unsandboxed code that is exposed to web contents or other\n  untrustworthy data sources must be extensively tested and fuzzed.\n\nThe Chrome team is enthusiastic about the future possibility of making a version\nof Chrome for iOS that meets our usual security standards if richer platform\nfacilities become widely available: this will require revisiting existing\nfeatures to see if adjustment is required.\n\n<a name=\"TOC-Are-all-Chrome-updates-important--\"></a>\n### Are all Chrome updates important?\n\nYes - see [our updates FAQ](updates.md).\n\n<a name=\"TOC-What-older-Chrome-versions-are-supported--\"></a>\n### What older Chrome versions are supported?\n\nWe always recommend being on the most recent Chrome stable version - see\n[our updates FAQ](updates.md).\n\n<a name=\"TOC-Im-making-a-Chromium-based-browser-how-should-I-secure-it-\"></a>\n### I'm making a Chromium-based browser. How should I secure it?\n\nIf you want to make a browser based on Chromium, you should stay up to date\nwith Chromium's security fixes. There are adversaries who weaponize fixed\nChromium bugs (\"n-day vulnerabilities\") to target browsers which haven’t yet\nabsorbed those fixes.\n\nDecide whether your approach is to stay constantly up to date with Chromium\nreleases, or to backport security fixes onto some older version, upgrading\nChromium versions less frequently.\n\nBackporting security fixes sounds easier than forward-porting features, but in\nour experience, this is false. Chromium releases 400+ security bug fixes per\nyear ([example\nquery](https://bugs.chromium.org/p/chromium/issues/list?q=type%3DBug-Security%20has%3Arelease%20closed%3Etoday-730%20closed%3Ctoday-365%20allpublic&can=1)).\nSome downstream browsers take risks by backporting only Medium+ severity fixes,\nbut that's still over 300 ([example\nquery](https://bugs.chromium.org/p/chromium/issues/list?q=type%3DBug-Security%20has%3Arelease%20closed%3Etoday-730%20closed%3Ctoday-365%20allpublic%20Security_Severity%3DMedium%2CHigh%2CCritical&can=1)).\nMost are trivial cherry-picks; but others require rework and require versatile\nengineers who can make good decisions about any part of a large codebase.\n\nOur recommendation is to stay up-to-date with Chrome's released versions. You\nshould aim to release a version of your browser within just a few days of each\nChrome [stable\nrelease](https://chromereleases.googleblog.com/search/label/Stable%20updates).\nIf your browser is sufficiently widely-used, you can [apply for advance notice\nof fixed vulnerabilities](https://www.chromium.org/Home/chromium-security/) to\nmake this a little easier.\n\nFinally, if you choose the backporting approach, please explain the security\nproperties to your users. Some fraction of security improvements cannot be\nbackported. This can happen for several reasons, for example: because they\ndepend upon architectural changes (e.g. breaking API changes); because the\nsecurity improvement is a significant new feature; or because the security\nimprovement is the removal of a broken feature.\n\n<a name=\"TOC-How-can-I-appeal-a-Safe-Browsing-warning-\"></a>\n### How can I appeal a Safe Browsing warning?\nTo request a review of warnings relating to your own website, use the\n[Security Issues report](https://support.google.com/webmasters/answer/9044101)\npage in your Google Search Console. If the warning applies to another site, you\nmay be able to use\n[https://safebrowsing.google.com/safebrowsing/report_error/](https://safebrowsing.google.com/safebrowsing/report_error/),\nthough you are likely better off contacting the site owner.\n\nIf your concern relates to malware warnings, you may find the warning in your\nSecurity Issues report and request a review from there. There is no separate\nappeal form or process at this time. Please follow these\n[guidelines](https://developers.google.com/search/docs/monitor-debug/security/malware#guidelines)\nto avoid having your binary show warnings from Safe Browsing.\n"
  },
  {
    "path": "security/ev-to-page-info",
    "title": "EV UI Moving to Page Info",
    "content": "# EV UI Moving to Page Info\n\nAs part of a series of data-driven\n[changes](https://blog.chromium.org/2018/05/evolving-chromes-security-indicators.html)\nto Chrome’s security indicators, the Chrome Security UX team is announcing a\nchange to the\n[Extended Validation](https://en.wikipedia.org/wiki/Extended_Validation_Certificate)\ncertificate indicator on certain websites starting in Chrome 77. This doc\nexplains what’s being changed and why, as well as the supporting research\nthat guided this decision.\n\nOn HTTPS websites using [EV](https://en.wikipedia.org/wiki/Extended_Validation_Certificate)\ncertificates, Chrome 76 currently displays an EV badge to the left\nof the URL bar that looks like this:\n\n![Chrome 76 EV UI](ev-to-page-info-images/chrome-76-ev-bar.png \"Chrome 76 EV\nUI\")\n\nStarting in Version 77, Chrome will move this UI to Page Info, which is accessed\nby clicking the lock icon:\n\n![Chrome 77 Page Info UI](ev-to-page-info-images/chrome-77-page-info.png \"Chrome\n77 Page Info UI\")\n\nThrough our own research as well as a survey of prior academic work, the Chrome\nSecurity UX team has determined that the EV UI does not protect users as\nintended (see [Further Reading](#Further-Reading) below). Users do not appear\nto make secure choices (such as not entering password or credit card\ninformation) when the UI is altered or removed, as would be necessary for EV UI\nto provide meaningful protection. Further, the EV badge takes up valuable\nscreen real estate, can present\n[actively confusing company names](https://www.typewritten.net/writer/ev-phishing/)\nin prominent UI, and interferes with Chrome's product direction towards\nneutral, rather than positive,\n[display for secure connections](https://blog.chromium.org/2018/05/evolving-chromes-security-indicators.html).\nBecause of these problems and its limited utility, we believe it belongs better\nin Page Info.\n\nAltering the EV UI is a part of a wider trend among browsers to improve their\nSecurity UI surfaces in light of recent advances in understanding of this\nproblem space. In 2018, Apple\n[announced a similar change](https://cabforum.org/2018/06/06/minutes-for-ca-browser-forum-f2f-meeting-44-london-6-7-june-2018/#Apple-Root-Program-Update)\nto Safari that coincided with the release of iOS 12 and macOS 10.14 and has\nbeen implemented as such ever since.\n\n### Information for embedders\n\nThis change is being incorporated into the Chrome-specific UI code and will not\naffect embedders that are based solely on the underlying content layer.\nEmbedders that incorporate the Chrome-specific code will either take up these\nchanges or maintain a diff from the `main` Chromium branch.\n\n\n## Further Reading\n\nA series of academic research in the 2000s studied the EV UI in lab and survey\nsettings, and found that the EV UI was not protecting against phishing attacks\nas intended. The Chrome Security UX team recently published a study that updated\nthese findings with a large-scale field experiment, as well as a series of\nsurvey experiments.\n\nNo one single study conclusively determines that EV UI is completely ineffective\nor cannot be made to be effective. However, we believe that the body of\nresearch, as well as the product principles outlined above, together strongly\nsuggest that the EV UI does not belong in Chrome’s most visible UI surface.\n\n### External Research:\n\n*   [An evaluation of extended validation and picture-in-picture phishing attacks](https://www.adambarth.com/papers/2007/jackson-simon-tan-barth.pdf):\n\tsurveys participants about IE 7’s EV UI and concludes that it did not help\n    \tusers identify two types of phishing attacks, even after participants\n    \treceived education about the UI.\n*   [Exploring User Reactions to New Browser Cues for Extended Validation Certificates](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.543.2117&rep=rep1&type=pdf):\n\tstudies Firefox 3’s EV UI and found\n\tthat users did not notice it. The researchers presented a re-designed\n\tindicator which some users did notice but did not use in their decision-\n\tmaking.\n*   [Browser interfaces and extended validation SSL certificates: An empirical study](http://people.scs.carleton.ca/~paulv/papers/ccsw09.pdf):\n\texplores a new EV UI design in comparison to IE 7’s design. The researchers\n\tshowed promising results on some axes but did not study whether the new\n\tdesign actually helps users detect attacks.\n*   [The Emperor’s New Security Indicators: An evaluation of website authentication and the effect of role playing on usability studies](http://andyozment.com/papers/emperor.pdf):\n\tdoes not study EV specifically, but studies other positive (non-warning)\n\tsecurity indicators for website authentication via lab study and finds that\n\tusers do not notice their absence.\n\n### Chrome Research:\n\n*   [The Web’s Identity Crisis: Understanding the Effectiveness of Website Identity Indicators](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/400599205ab5a1c9efa03e2a7c127eb8200bf288.pdf):\n\ta large-scale field experiment in which the EV UI was removed for a random\n\tsubset of users, and a wide variety of user behavior metrics did not change,\n\tsuggesting that the EV UI is not having its intended effect. Survey\n\texperiments also confirm that users do not react as intended to positive or\n\tneutral security UI.\n*   [Rethinking Connection Security Indicators](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45366.pdf):\n\tdoes not study EV specifically, but studies users’ reaction to other\n\tconnection security indicators like the lock icon via survey, and finds that\n\tusers are widely confused about their meaning. Informs Chrome’s overall\n\tdirection to remove positive security indicators.\n"
  },
  {
    "path": "security/document-domain",
    "title": "`document.domain` Setting is Deprecated",
    "content": "# `document.domain` Setting is Deprecated\n\nSetting [`document.domain`](https://developer.mozilla.org/en-US/docs/Web/API/Document/domain)\nis [deprecated](https://html.spec.whatwg.org/multipage/origin.html#relaxing-the-same-origin-restriction).\n`document.domain` setting can be used to relax the same-origin restrictions\nbetween different frames, hosted within the same site but on different origins.\nDoing so make themselves effectively same-origin for the purpose of\nsynchronous access.\n\nFor example, `https.//www.example.test/` might host a media player on\n`media.example.org`. If both the main page and the frame execute\n`document.domain = \"example.org\";` they may then access each others' DOM\ntree, which they normally couldn't. (A better way would be to cooperate\nby `postMessage()`.)\n\nThis usage is now being deprecated. (More information can be found\n[here](https://developer.chrome.com/blog/immutable-document-domain/) and\n[there](https://github.com/mikewest/deprecating-document-domain)).\n\n## What is happening, and when?\n\n* M100: Chrome will show deprecation warnings when document.domain is set.\n* M106: Chrome will disable document.domain by default.\n\nNote that the second milestone is tentative: When the time comes, we will\nexamine how many pages will be impacted by this change, and will start a\nseparate discussion (intent to remove) on the\n[blink-dev mailing list](https://groups.google.com/a/chromium.org/g/blink-dev).\n\n##  `document.domain` and Origin-keyed Agent Clusters\n\nMost documentation on this change is phrased in terms of origin-keyed\nagent clusters. This is [a concept in the HTML\nspecification](https://html.spec.whatwg.org/multipage/origin.html#origin-keyed-agent-clusters).\nHere we focus on the behaviour of the `document.domain` setter, which is\nthe visible effect.\n\nA web browser can cluster sites (in order to assign them to operating\nsystem processes) and sites can be clustered by origin, or by site.\nOrigin-keyed agent clustering is preferable for security reasons. However\nwhen sites are clustered by origin, synchronous access to frames outside of\nthat origin (but within the same site) is no longer possible. Thus sites in\norigin-keyed agent clusters disable the `document.domain` setter. This is the\nmechanism underlying this change: From M106 on pages will be assigned to\norigin-keyed agent clusters by default and therefore `document.domain`\nwill no longer be settable by default, either.\n\nThis also gives us an opt-out mechanism for pages who do not wish to follow\nthis change: By setting the `Origin-Agent-Cluster: ?0` http header, a site\ncan request assignment to a site-keyed agent cluster, and `document.domain`\nwill continue to work for them as it currently does. Note that adding this\nheader has no other observable effect and thus retains the current\n(pre-M106) behaviour. This makes it an easy and _safe_ way to opt-out.\n\nSetting this header is a no-op in current versions of Chromium since it\nmatches the default setting, and will preserve this behaviour in the future.\nIt is also a no-op in other browsers, since they either match Chromium's\ncurrent default or have not implemented the `Origin-Agent-Cluster` header at\nall.\n\n## Where are the deprecation warnings found?\n\nThe deprecation warnings are found in the [issues tab](https://developer.chrome.com/docs/devtools/issues/).\n\n## What does the deprecation warning tell me?\n\nThere are two deprecation warnings: One for setting the `document.domain`\naccessors, which modifies the security behaviour. And from M101 on,\na second warning when a cross-domain access is made that is facilitated by\nthe modified `document.domain` property. The first warning tells you where\nthe setup happens, and the second one tells you where it is being used (and\nthus likely why this is being done in the first place).\n\n## How Can I Test This?\n\nIn the DevTools console, for a page `www.example.test`:\n\n```\ndocument.domain = \"example.test\";\ndocument.domain;  // \"example.test\" in a site-keyed agent cluster.\n                  // \"www.example.test\" in an origin-keyed agent cluster.\n```\n\nOne can also directly query whether a page is assigned to an origin-keyed\nagent cluster, by querying `window.originAgentCluster`.\n\n```\nwindow.originAgentCluster;  // true, if page is assigned to an origin-keyed\n                            // agent cluster.\n```\n\nHow to enable/disable the deprecation:\n\n### Enable the Warning (Before M100)\n\n* Start Chrome with `--enable-features=OriginAgentClusterDefaultWarning`\n\n### Enable the Deprecation (Scheduled for M106)\n\n* In [chrome://flags](chrome://flags#origin-agent-cluster-default), go to\n  the \"Origin Agent Cluster Default\" setting and enable it.\n* Or start Chrome with `--enable-features=OriginAgentClusterDefaultEnable`\n* Or add the `Origin-Agent-Cluster: ?1` header to your pages and frames.\n  (E.g. in a testing instance of your site.)\n\n### Testing at Scale / Reporting API\n\nThe deprecation warnings are delivered through the\n[Reporting API](https://web.dev/reporting-api/). They can\nbe pragrammatically processed using `ReportingObserver`. For example, the\nfirst code snippet in\nhttps://developers.google.com/web/updates/2018/07/reportingobserver\nwill report these warnings. The message object delivered by this API is a\n[`DeprecationReportBody`](https://developer.mozilla.org/en-US/docs/Web/API/DeprecationReportBody)\ninstance which offers information about the source code location that triggered\nthe warning.\n\n## What can I do?\n\nIf your site does not use `document.domain` setting you don't have to do\nanything. You could explicitly set the `Origin-Agent-Cluster: ?1` header.\nBut after M106 this would be the default behaviour anyhow.\n\nIf your site uses `document.domain` setting to enable cross-origin Javascript\naccess, you should refactor the code to instead use\n[`window.postMessage()`](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage) (or any other mechanism) to cooperate across origins. Or alternatively\nreduce the need for cross-origin cooperation by moving the cooperating pieces\nonto the same origin.\n\n## What if I don't have the time right now, or want to continue setting `document.domain`?\n\nYou can add the `Origin-Agent-Cluster: ?0` HTTP header to your site. Note that\nthe header must be set both for the main page, as well as for the embedded\nframe(s) that wish to use `document.domain` setting.\n\n## Enterprise Users\n\nUsers of [Chrome for Enterprise](https://chromeenterprise.google/) can set\nthe `OriginAgentClusterDefaultEnabled` policy to `False` to retain the\ncurrent (pre-M106) default for all of their users, until all their internal\nsites and customers have migrated off of `document.domain` usage.\n\n"
  },
  {
    "path": "security/custom-tabs-faq",
    "title": "Chrome Custom Tabs Security FAQ",
    "content": "# Chrome Custom Tabs Security FAQ\n\n## Should apps use WebView when building a browser?\n\n[No, WebView is not intended as a framework for building browsers, and lacks\nsecurity features available in modern\nbrowsers.](https://web.dev/web-on-android/#security-considerations-for-using-webview-as-an-in-app-browser)\n\n## What is the security model for Chrome Custom Tabs?\n\nChrome Custom Tabs (CCT), and Custom Tabs (CT) more generally, allow\nAndroid app developers to use the user's default browser to\nserve embedded web content in their apps.\n\nCT, unlike Android's WebView API, share the same browser state (such as\ncookies) with the browser app. Chromium therefore imposes a strict boundary\nbetween the embedding app and the browsing engine, and the app can normally\nonly get very limited access to web page data and state.\n\nAll considered, there are four parties to consider when evaluating Custom Tabs:\nthe user, the embedding app, the web publisher, and the browser. The native\napp chooses how they want to bring the web in their app, and users choose which\napps to install and use.\n\nGiven this distinct trust relationship between the embedding app and the user\n(which is in general a higher degree of trust than between users and websites\nthey happen upon in their browser), we accept some data exchange between Chrome\nand the underlying app. This is intentional because we believe this\nincentivizes apps to use CT rather than WebView, which was [never designed as a\nfull browser embedding API and has a number of security shortcomings](https://web.dev/web-on-android/#security-considerations-for-using-webview-as-an-in-app-browser).\n\n## What data does Chrome consider permissible for the embedder to have access to?\n\n1. **CCT session specific signals can be shared back to the embedder without user\n   action.** Session specific signals are low-entropy signals about the user's\n   interaction with the tab or page that do not reveal information about the\n   content or identity of the page. Examples of session specific signals include\n   [Custom Tab callbacks](https://developer.android.com/reference/androidx/browser/customtabs/CustomTabsCallback) and [engagement signals](https://developer.chrome.com/docs/android/custom-tabs/guide-engagement-signals/). Session specific signals are\n   designed to avoid malicious actors inferring details about the content or the\n   state of the web page. As such, engagement signals are disabled in some\n   circumstances, such as when pages are opened using [text fragments](https://web.dev/text-fragments/#text-fragments).\n\n2. **Current page URL can be shared with the embedder with explicit user action.**\n   When a user taps on an embedding app action in CCT, the embedding application\n   can see the full URL and origin of the currently visited page. In some instances,\n   verifiable Google app entities can access the current page URL without user\n   intent.\n\n3. **Developers can send and receive messages as if they were a website which they\n   can prove they control.** The postMessage API can be used by developers to\n   establish a 2-way communication channel between the main frame inside the\n   Chrome Custom Tab. For non-verifiable Google entities, this functionality is\n   only supported if a [Digital Asset Link](https://developers.google.com/digital-asset-links)\n   relationship has been established between a website and the embedding app.\n   The website is then used as the origin\n   for the [`window.postMessage()`](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage)\n   Web API, which enables cross-origin communication.\n\n## How else might an embedder appear to interact with web content?\n\n1. **The app may be able to draw over parts of the Chrome browser UI or the website.**\n   Unlike the Chrome browser app which is always displayed in its own Android\n   Task, Custom Tabs are most commonly displayed in the same Android Task as the\n   embedding app. This makes Custom Tabs susceptible to certain tap jacking and\n   phishing attacks. For example, a malicious actor could launch an\n   Activity positioned over the web content or CT toolbar and draw UI to steal a\n   password. The presence of pre-existing browser state and cookies may make the\n   embedded web experience appear more trustworthy and therefore increase the\n   likelihood of the phishing attack succeeding. Note that Android has been\n   pursuing protections within the OS to mitigate against some attacks, and Chrome will\n   continue to work with Android to protect users on older OS versions.\n\n2. **Developers can add app specific actions into CCT**. Chrome provides customization\n   options to embedding apps. The appearance of the bottom toolbar and its\n   contents can be customized and can change during runtime. While this UI surface\n   could be used for malicious purposes, we accept this risk because, overall, CCT\n   has better security properties than WebView, and a high level of UI\n   customisability is necessary to drive Custom Tab adoption. Furthermore, the\n   space that can be occupied by the bottom toolbar is limited and the position is\n   fixed, lowering the risk that users will fall for attacks launched from this\n   surface.\n\n## What data does an embedder not have access to?\n\n**Embedders cannot access data unrelated to the CCT session**. This includes:\n\n* history from past sessions\n* cookies\n* passwords\n* full DOM access\n* arbitrary script injection\n* network request interception\n* etc.\n\nAny future access would require explicit permissions to be accepted.\n"
  },
  {
    "path": "security/cross_origin_isolation",
    "title": "Cross Origin Isolation",
    "content": "# Cross Origin Isolation\n\n[TOC]\n\nCross-Origin Isolation aims to protect sites from side-channel attacks such as\n[Spectre](https://spectreattack.com/) by defining opt-in rules that allow a\nuser agent to place pages in specially restricted origin-keyed processes. See [this\narticle](https://web.dev/articles/why-coop-coep) for more background.\nDevelopers can enable Cross-Origin Isolation by setting both the\n[Cross-Origin-Opener-Policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Opener-Policy)\nand [Cross-Origin-Embedder-Policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Embedder-Policy)\nheaders.\n\nThis document will outline how Cross-Origin Isolation is represented in\nspecifications, and how that is translated into the Chromium codebase.\n\n# Cross-Origin Isolation in Specifications\n\nAt the time that Cross-Origin Isolation (COI) was designed, all major browsers\ncould put top-level documents in dedicated processes, but not all browsers\nsupported out-of-process iframes. Because of this, COI had to be designed to\nprovide side-channel attack mitigation while cross-origin child frames were\nrunning in the same process as the main frame. Much of its complexity comes\nfrom this requirement.\n\nThe Cross-Origin Isolation state of [agents](https://tc39.es/ecma262/\\#agent)\nis represented by the “[cross-origin isolation\nmode](https://html.spec.whatwg.org/multipage/document-sequences.html\\#bcg-cross-origin-isolation)”\nof their browsing context group (BCG), which can have one of [three\nvalues](https://html.spec.whatwg.org/multipage/document-sequences.html\\#groupings-of-browsing-contexts:cross-origin-isolation-mode):\nnone (non-isolated), logical, or concrete. “None” is the default value and will\nbe used in content that doesn’t set the headers needed to opt-in to COI. A BCG\nwill have the “logical” isolation mode if its agent clusters cannot have\ndedicated processes due to resource or implementation constraints, and\n“concrete” if they can. All restrictions imposed by the\nCross-Origin-Opener-Policy and Cross-Origin-Embedder-Policy headers will still\napply to documents in logically isolated BCGs, but they will not have the\ncross-origin isolated capability (explained below). The remainder of this\ndocument will ignore the logical isolation mode.\n\nBCGs with logical or concrete Cross-Origin Isolation modes are considered to be\nCross-Origin Isolated, and will be referred to as Cross-Origin Isolated\nBrowsing Context Groups (COI BCGs) in this document. All top-level documents or\nworkers in a COI BCG must be same-origin.\n\nIf COI is enabled through the Cross-Origin-Opener-Policy and\nCross-Origin-Embedder-Policy headers, then the latter header requires any\ncross-origin resource loaded within the COI BCG to opt-in to cross-origin\nembedding via the [Cross-Origin-Resource-Policy](\nhttps://fetch.spec.whatwg.org/\\#cross-origin-resource-policy-header)\nheader. This header is needed because in browsers without out-of-process\niframes, cross-origin content embedded in a COI page would be vulnerable to\nside-channel attacks from the embedding page. The header limitations ensure\nthat COI documents cannot use side-channel attacks (e.g., Spectre-related\nvulnerabilities using high precision timer APIs) to leak cross-origin content,\nunless that content has acknowledged the risk via the CORP header.\n\n## Cross-Origin Isolated Capability\n\nConcrete Cross-Origin Isolation guarantees that no cross-origin content is\nrunning in a document's process unless it opted-in through the\nCross-Origin-Resource-Policy header. This allows user agents to safely expose\nAPIs that could be used to perform side-channel attacks because there is no\nin-process cross-origin content for the document to leak.\n\nDue to the issue described in\n[github.com/whatwg/html/issues/5435](https://github.com/whatwg/html/issues/5435),\nthere’s also a policy-controlled feature required to grant access to COI-gated\nAPIs. Frames that are missing the “cross-origin-isolated” feature will not be\nable to use COI-gated APIs, but COI-related restrictions will still apply to\nthem. Agents that are allowed to use COI-gated APIs have the “[cross-origin\nisolated\ncapability](https://html.spec.whatwg.org/multipage/webappapis.html\\#concept-settings-object-cross-origin-isolated-capability).”\nTo summarize [yhirano@’s\nresponse](https://github.com/whatwg/html/issues/5435\\#issuecomment-640517912)\non that issue, there are 3 concepts at play here:\n\n* **Cross-Origin Isolation mode** \\- this refers to whether the aforementioned\n  COI restrictions are in place for a given browsing context group.\n* **“cross-origin-isolated” feature** \\- this refers to [“cross-origin-isolated”](\n  https://html.spec.whatwg.org/multipage/infrastructure.html#policy-controlled-features)\n  [policy-controlled feature](\n  https://github.com/w3c/webappsec-permissions-policy/blob/main/permissions-policy-explainer.md).\n  Top-level frames are granted this feature by\n  default but may choose to deny it to child frames. Note that despite its name,\n  this doesn’t actually control whether a frame can request or be given\n  cross-origin isolation, only whether APIs that require cross-origin isolation\n  can be enabled.\n* **Cross-Origin Isolated capability / `window.crossOriginIsolated`** \\- this is\n  a boolean global property representing an agent’s COI capability, which\n  controls whether COI-gated APIs are available. This is equal to `COI mode &&\n  COI feature`.\n\nCOI-gated APIs are only available when the `window.crossOriginIsolated`\nproperty is true, and can be blocked in child frames by not delegating the COI\nfeature.\n\nAll frames and workers within a BCG will have the same COI mode. Child\nframes in a COI BCG will not have the COI capability (`self.crossOriginIsolated\n=== false`) if the “cross-origin-isolated” feature was not delegated to them.\nTwo same-origin frames in the same BCG with different COI capabilities are\nstill synchronously scriptable. This means a same-origin child frame without\nthe COI capability could access COI-gated APIs via its parent document, which\nis the intended behavior. This would be a security issue for most\npolicy-controlled features, which are intended to block access to capabilities,\nbut “cross-origin-isolated” is a bit more subtle. These APIs are not inherently\ndangerous; they're only a threat to cross-origin content running in the same\nprocess. The goal of the “cross-origin-isolated” feature is not to completely\nblock access to COI-gated APIs, but instead to allow blocking access to them\n*in cross-origin iframes*, which in browsers without out-of-process iframes\nwould have to run in-process. This allows embedding frames to protect themselves\nfrom side-channel attacks from cross-origin embedded content.\n\nWorker behavior depends on the type of worker:\n\n* Service Workers will have the COI capability if their worker script's response\n  sets the appropriate Cross-Origin-Embedder/Opener-Policy headers for non-local\n  schemes, or will inherit the COI capability of their creator for local\n  schemes (about, blob, data, filesystem, etc…). Permissions Policy doesn’t\n  apply to workers because it’s a document-level concept (see\n  [github issue](https://github.com/w3c/webappsec-permissions-policy/issues/207)),\n  so it can’t be taken into account here. See step 14.3.4 in section 10.2.4 of\n  the [spec](https://html.spec.whatwg.org/multipage/workers.html\\#worker-processing-model).\n* Shared Workers and Shared Storage Worklets do not currently support COI.\n* Dedicated Workers and Worklets have the COI capability if their\n  hosting document does.\n\n## Isolated Contexts\n\n[Isolated Contexts](https://wicg.github.io/isolated-web-apps/isolated-contexts.html)\nare environments which meet a minimum bar of isolation and integrity\nverification, and are used to gate APIs that are too powerful to expose in\nSecure Contexts, and require review/attestation. They are the security\nfoundation of [Isolated Web Apps](https://github.com/WICG/isolated-web-apps/blob/main/README.md).\nIsolated Contexts require COI, but do not directly extend it at a specification\nlevel.\n\n## Document-Isolation-Policy\n\n[Document-Isolation-Policy](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/security/dip/README.md)\nbuilds on and extends the concepts introduced by Cross-Origin Isolation to\nallow documents to opt-in to COI without the restrictions imposed by COOP/COEP.\nThis document doesn't yet cover the specification or implementation details of\nDocument-Isolation-Policy.\n\n\n# Cross-Origin Isolation in //content\n\n## [WebExposedIsolationInfo](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/web\\_exposed\\_isolation\\_info.h)\n\nWebExposedIsolationInfo (WEII) **represents a browsing context group’s\ncross-origin isolation mode**, and does *not* take into account the\n“cross-origin-isolated” policy-controlled feature. Note in the diagram below\nthat all iframes have the same WEII value regardless of their origin or\nPermissions Policy.\n\nWEII is a non-public class in //content, and is the source of truth for the COI\nmode of a given BCG. It is a property of [BrowsingInstance](\nhttps://source.chromium.org/chromium/chromium/src/+/main:content/browser/browsing\\_instance.h)\n(//content’s representation of a BCG) and\n[ProcessLock](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/process\\_lock.h),\nand can have a value of “not isolated”, “isolated”, or “isolated application.”\nIn the latter two cases it will also contain the origin representing the\nisolation boundary. The origin is necessary to ensure that documents from the\nsame site embedded within two cross-origin COI pages will never be placed in\nthe same process.\n\nAs mentioned above, Isolated Contexts aren’t an extension of the Cross-Origin\nIsolated mode at the specification level, but they are at the //content level.\nThe “isolated application” WEII represents Isolated Contexts. Long term the\n“isolated application” WEII to Isolated Context mapping should be 1:1, but for\nnow Chrome Apps are also considered Isolated Contexts because they need access\nto these APIs but predate Cross-Origin Isolation and often don't work with it\nenabled.\n\n## [WebExposedIsolationLevel](https://source.chromium.org/chromium/chromium/src/+/main:content/public/browser/web\\_exposed\\_isolation\\_level.h)\n\nWebExposedIsolationLevel (WEIL) is a public enum in //content that, like WEII,\nhas three values: kNotIsolated, kIsolated, and kIsolatedApplication. It\n**represents the cross-origin isolated *capability* of a specific frame**\nif accessed from RenderFrameHost::GetWebExposedIsolationLevel(), or\nthe **cross-origin isolated *mode* of a process** if accessed from\nRenderProcessHost::GetWebExposedIsolationLevel(). Unlike kIsolated,\nkIsolatedApplication will not propagate to cross-origin child frames; they will\nhave at most the kIsolated WEIL.\n\nThis means that WEIL and WEII will differ in workers or iframes if their\nprocess is locked to a different origin than the WEII, and will differ in\nframes if the “cross-origin-isolated” feature was not delegated to the frame.\n\n![Diagram of WEII vs WEIL](cross_origin_isolation_levels_diagram.png)\n<!-- Source: https://docs.google.com/drawings/d/1tCeworrk7qYkbyAbjeWVgmBrqe5XOenxJbLlNmm4lN8/edit -->\n\n## Guarding APIs on Isolation Level\n\nAPI access should be guarded on RenderFrameHost::GetWebExposedIsolationLevel(),\nwhich takes permissions policy into account, when a RenderFrameHost is\navailable. Shared and Service Workers don’t have a RenderFrameHost, so they\nneed to use RenderProcessHost::GetWebExposedIsolationLevel() to check their\nisolation mode. This does mean permissions policy will be ignored, which is the\nexpected behavior.\n\n# Isolation Level Representation in Blink\n\nThe application isolation level maps to “Isolated Context” in Blink. APIs can\nbe limited to this isolation mode by adding the \\[IsolatedContext\\] IDL\nattribute, which reflects the static\n[Agent::IsIsolatedContext()](https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/renderer/core/execution_context/agent.h?q=%5CbIsIsolatedContext%5Cb),\nwhich itself reflects the process’s WEIL. There’s also\n[ExecutionContext::IsIsolatedContext()](https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/renderer/core/execution_context/execution_context.h?q=%5CbIsIsolatedContext%5Cb),\nwhich maps to Agent::IsIsolatedContext(). Checking API availability in Blink\ncurrently requires checking both ExecutionContext::IsIsolatedContext() and\n[ExecutionContext::CrossOriginIsolatedCapability()](https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/renderer/core/execution_context/execution_context.h?q=%5CbCrossOriginIsolatedCapability%5Cb).\n\n# Appendix\nOther useful resources:\n * [COOP and COEP explained](https://docs.google.com/document/d/1zDlfvfTJ_9e8Jdc8ehuV4zMEu9ySMCiTGMS9y0GU92k/edit?usp=sharing)\n * [Notes on the threat model of cross-origin isolation](https://arturjanc.com/coi-threat-model.pdf)\n * [Post-Spectre Web Development](https://chromium.googlesource.com/chromium/src/+/main/docs/security/post-spectre-webdev.md)\n"
  },
  {
    "path": "security/compromised-renderers",
    "title": "Threat Model And Defenses Against Compromised Renderers",
    "content": "# Threat Model And Defenses Against Compromised Renderers\n\nGiven the complexity of the browser, our threat model must use a \"defense\nin depth\" approach to limit the damage that occurs if an attacker\nfinds a way around the Same Origin Policy or other security logic in the\nrenderer process.\nFor example, the combination of Chrome's sandbox, IPC security checks, and Site\nIsolation limit what an untrustworthy renderer process can do.  They\nprotect Chrome users against attackers, even when such attackers are able to\nbypass security logic in the renderer process.\nFor other arguments for the \"defense in depth\" approach and why our\nthreat model covers compromised renderers, please see\n[the Site Isolation motivation](https://www.chromium.org/Home/chromium-security/site-isolation#TOC-Motivation).\n\nIn a compromised renderer, an attacker is able to execute\narbitrary native (i.e. non-JavaScript) code within the renderer\nprocess's sandbox.  A compromised renderer can forge\nmalicious IPC messages, impersonate a Chrome Extension content script,\nor use other techniques to trick more privileged parts of the browser.\n\nThe document below gives an overview of features that Chrome attempts to\nprotect against attacks from a compromised renderer.  Newly discovered\nholes in this protection would be considered security bugs and possibly\neligible for the\n[Chrome Vulnerability Rewards Program](https://www.google.com/about/appsecurity/chrome-rewards/).\n\n[TOC]\n\n\n## Site Isolation foundations\n\nMost of the other protections listed in this document implicitly assume that\nattacker-controlled execution contexts (e.g. HTML documents or service workers)\nare hosted in a separate renderer process from other, victim contexts.\nThis separation is called\n[Site Isolation](https://www.chromium.org/Home/chromium-security/site-isolation)\nand allows the privileged browser\nprocess to restrict what origins a renderer process is authorized to read or\ncontrol.\n\nThe privilege restriction can be implemented in various ways - see the\n\"protection techniques\" listed in other sections in this document.\nOne example is validating in the browser process whether an incoming IPC can\nlegitimately claim authority over a given origin (e.g. by checking via\n`CanAccessDataForOrigin` if the process lock matches).\nAnother example is making sure that capabilities handed over to renderer\nprocesses are origin-bound (e.g. by setting `request_initiator_origin_lock`\non a `URLLoaderFactory` given to renderer processes).\nYet another example is making security decisions based on trustworthy knowledge,\ncalculated within the privileged browser process (e.g. using\n`RenderFrameHost::GetLastCommittedOrigin()`).\n\nCompromised renderers shouldn’t be able to commit an execution context\n(e.g. commit a navigation to a HTML document, or create a service worker)\nin a renderer process hosting other, cross-site execution contexts.\nOn desktop platforms all sites (site = scheme plus eTLD+1) should be isolated\nfrom each other.\nOn Android, sites where the user entered a password should be isolated\nfrom each other and from other sites.\n\n**Known gaps in protection**:\n- No form of Site Isolation is active in Android WebView.\n  See also https://crbug.com/769449.\n- Frames with `<iframe sandbox>` attribute are not isolated\n  from their non-opaque precursor origin.\n  See also https://crbug.com/510122.\n- `file:` frames may share a process with other `file:` frames.\n  See also https://crbug.com/780770.\n\n\n## Cross-Origin HTTP resources\n\nCompromised renderers shouldn't be able to read the contents (header + body) of\na cross-site HTTP response, unless it is a valid subresource needed for\ncompatibility (e.g., JavaScript, images, etc), or is successfully allowed via\nCORS.\n\nProtection techniques:\n- Enforcing\n  [Cross-Origin Read Blocking\n  (CORB)](https://www.chromium.org/Home/chromium-security/corb-for-developers)\n  in the NetworkService process\n  (i.e. before the HTTP response is handed out to the renderer process).\n- Only allowing the privileged browser process to create\n  `network::mojom::URLLoaderFactory` objects that handle HTTP requests.\n  This lets the browser process carefully control security-sensitive\n  `network::mojom::URLLoaderFactoryParams` of such factories (such as\n  `request_initiator_origin_lock`, `is_orb_enabled`, `disable_web_security` or\n  `isolation_info`).\n\n**Known gaps in protection**:\n- Content types for which CORB does not apply\n  (e.g. `image/png`, `application/octet-stream`) are not protected by\n  default.  We recommend that HTTP servers protect such resources by\n  either serving a `Cross-Origin-Resource-Policy: same-origin` response header\n  or validating the `Sec-Fetch-Site` request header.\n\n\n## Contents of cross-site frames\n\nCompromised renderers shouldn't be able to read the contents of cross-site\nframes.  Examples:\n- Text or pixels of cross-site frames.\n- Full URL (e.g. URL path or query) of cross-site frames.\n  Note that the origin of other frames\n  needs to be exposed via `window.origin` for legacy reasons.\n\nProtection techniques:\n- Compositing tab contents (both for display and for printing)\n  outside the renderer processes.\n- Isolating PDF plugins.\n- Being careful what URLs are exposed in console messages.\n\n**Known gaps in protection**:\n- Mixed content console messages may disclose cross-site URLs\n  (see also https://crbug.com/726178).\n\n\n## Cookies\n\nCompromised renderers shouldn’t be able to read or write\nany cookies of another site,\nor `httpOnly` cookies even from the same site.\n\nProtection techniques:\n- Renderer processes are only given `network::mojom::RestrictedCookieManager`\n  for origins within their site\n  (see `StoragePartitionImpl::CreateRestrictedCookieManager`).\n- Mojo serialization does not send any cookies from HTTP headers to the renderer\n  process (see\n  `ParamTraits<scoped_refptr<net::HttpResponseHeaders>>::Write`).\n\n\n## Passwords\n\nCompromised renderers shouldn’t be able to read or write passwords of\nother sites.\n\nProtection techniques:\n- Using `CanAccessDataForOrigin` to verify IPCs sent by a renderer process\n  (e.g. `//components/password_manager/content/browser/bad_message.cc`)\n- Using trustworthy, browser-side knowledge\n  to determine which credentials to read or write\n  (e.g. `content::RenderFrameHost::GetLastCommittedURL` in\n  `password_manager::CredentialManagerImpl::GetOrigin`).\n\n\n## Security-sensitive UI/chrome elements (e.g. Omnibox)\n\nCompromised renderers shouldn’t be able to influence/spoof\nsecurity-sensitive UI elements.\n\nExamples:\n- Omnibox\n    - URL (e.g. renderer process locked to foo.com shouldn’t\n      be able to trick the Omnibox into displaying bar.com)\n    - Secure / not secure chip (e.g. a renderer process locked to a HTTP\n      site shouldn’t be able to trick the Omnibox into displaying a\n      HTTPS-associated lock)\n    - Content settings (e.g. a renderer process that has been granted\n      microphone access shouldn’t be able to suppress the mic/camera\n      icon in the Omnibox)\n- Dialogs and prompts (for example a permissions dialog asking to allow\n  a site to show notifications)\n    - Origin in dialogs (e.g. a renderer process locked to foo.com\n      shouldn’t be able to trick the Omnibox into displaying a bar.com\n      URL in permission dialogs)\n\nProtection techniques:\n- `RenderFrameHostImpl::CanCommitOriginAndUrl` verifies that the renderer\n  process is able to commit what it claims, and kills the process otherwise.\n- Work-in-progress: calculating the origin in the browser process,\n  before a navigation commits (https://crbug.com/888079).\n\n\n## Permissions\n\nCompromised renderers shouldn’t be able to gain permissions without user\nconsent.\n\nExamples: microphone access permission, geolocation permission, etc.\n\nProtection techniques:\n- Requesting permissions based on browser-side knowledge of frame's origin\n  (e.g. see `GeolocationServiceImplContext::RequestPermission`).\n\n\n## Web storage\n\nCompromised renderers shouldn’t be able to read from or write into\nstorage of another site.\n\nExamples of protected storage technologies:\n- localStorage\n- sessionStorage\n- indexedDB\n- blob storage\n- webSQL\n\nProtection techniques:\n- Using `CanAccessDataForOrigin` to verify IPCs sent by a renderer process\n  (e.g. see `StoragePartitionImpl::OpenLocalStorage`).\n- Binding Mojo interfaces to a single origin obtained from browser-side\n  information in `RenderFrameHost::GetLastCommittedOrigin()`\n  (e.g. see `RenderFrameHostImpl::CreateIDBFactory`).\n\n\n## Messaging\n\nCompromised renderers shouldn’t be able to:\n- Spoof the `MessageEvent.origin` seen by a recipient of a `postMessage`.\n- Bypass enforcement of the `targetOrigin` argument of `postMessage`.\n- Send or receive `BroadcastChannel` messages for another origin.\n- Spoof the `MessageSender.url`, nor `MessageSender.origin`, nor\n  `MessageSender.id` (i.e. an extension id which can differ from the origin when\n  the message is sent from a content script), as seen by a recipient of a\n  `chrome.runtime.sendMessage`.\n  See also [MessageSender documentation](https://developers.chrome.com/extensions/runtime#type-MessageSender) and [content script security guidance](https://groups.google.com/a/chromium.org/forum/#!topic/chromium-extensions/0ei-UCHNm34).\n- Spoof the id of a Chrome extension initiating\n  [native messaging](https://developer.chrome.com/docs/apps/nativeMessaging/)\n  communication.\n\nProtection techniques:\n- Using `CanAccessDataForOrigin` to verify IPCs sent by a renderer process\n  (e.g. in `RenderFrameProxyHost::OnRouteMessageEvent` or\n  `BroadcastChannelProvider::ConnectToChannel`).\n- Using `ContentScriptTracker` to check if IPCs from a given renderer process\n  can legitimately claim to act on behalf content scripts of a given extension.\n\n\n## JavaScript code cache\n\nCompromised renderers shouldn't be able to poison the JavaScript code cache\nused by scripts executed in cross-site execution contexts.\n\nProtection techniques:\n- Using trustworthy, browser-side origin lock while writing to and fetching from\n  the code cache by using `ChildProcessSecurityPolicyImpl::GetOriginLock` in\n  `GetSecondaryKeyForCodeCache` in\n  `//content/browser/renderer_host/code_cache_host_impl.cc`\n\n\n## Cross-Origin-Resource-Policy response header\n\nA compromised renderer shouldn’t be able to bypass\n[Cross-Origin-Resource-Policy (CORP)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Cross-Origin_Resource_Policy_%28CORP%29),\nwhich prevents or allows responses from being requested cross-origin, more\nexplicitly than CORB.\n\nProtection techniques:\n- Enforcing Cross-Origin-Resource-Policy in the NetworkService process\n  (i.e. before the HTTP response is handed out to the renderer process).\n- Preventing spoofing of `network::ResourceRequest::request_initiator`\n  by comparing against `request_initiator_origin_lock` in\n  `network::CorsURLLoaderFactory::IsValidRequest`.\n\n\n## Frame-ancestors CSP and X-Frame-Options response headers\n\nA compromised renderer shouldn’t be able to bypass `X-Frame-Options`\nor `frame-ancestors` CSP.\n\nFor example, if example.com/page.html sends a `X-Frame-Options: deny` header,\nthen it should never commit in a subframe, even if some renderers have\nbeen compromised.\n\nProtection techniques:\n- `X-Frame-Options: deny` is enforced in the browser process\n  via `content::AncestorThrottle`, an implementation of\n  `content::NavigationThrottle`.\n- `frame-ancestors` is enforced in a renderer process, but\n  this process is considered trustworthy in this scenario\n  (because it hosts the frame that is requesting protection).\n  See also https://crbug.com/759184 which tracks\n  moving this enforcement into the browser process.\n\n\n## HTTP request headers\n\nCompromised renderers shouldn’t be able to control security sensitive HTTP\nrequest headers like `Host`, `Origin`, or `Sec-Fetch-Site`.\n\nProtection techniques:\n- Using `AreRequestHeadersSafe` to reject `Host` and other headers that\n  should only be generated internally within the NetworkService.\n- Preventing spoofing of `network::ResourceRequest::request_initiator`\n  by comparing against `request_initiator_origin_lock` in\n  `network::CorsURLLoaderFactory::IsValidRequest`.\n\n\n## (WIP) SameSite cookies\n\nCompromised renderers shouldn’t be able to send a cross-site HTTP request with\nSameSite cookies.\n\n**Work-in-progress / not protected today**.\n\nTODO(morlovich): Add details.  I assume that this requires trustworthy\n|request_initiator| (similar to the `Origin` header), but probably more\nthan that.\n\nSee also https://crbug.com/927967.\n\n\n## (WIP) User gestures / activations.\n\nCompromised renderers shouldn't be able to spoof user gestures to perform\nactions requiring them:\n\n- A compromised renderer should not be able to forge a gesture that affects\n  the trusted browser UI.  For example, a compromised renderer should not be\n  able to interact with the Omnibox or the WebBluetooth chooser.\n\n- A compromised renderer should not be able to forge a gesture that grants\n  extra capabilities to a web origin.   For example, a compromised renderer\n  should not be able to open an unlimited number of popup\n  windows by forging user gestures.\n  **Work-in-progress / not protected today** - see https://crbug.com/848778.\n\n\n## Web Accessible Resources of Chrome Extensions\n\nCompromised non-extension renderers shouldn’t be able to access\nnon-web-accessible-resources of a Chrome Extension.\n\nProtection techniques:\n- Navigations: Enforcement in the browser process\n  via `extensions::ExtensionNavigationThrottle`, an implementation of\n  `content::NavigationThrottle`.  This relies on non-spoofability\n  of `content::NavigationHandle::GetInitiatorOrigin`.\n- Subresources: Enforcement in the browser process via\n  `ExtensionURLLoaderFactory::CreateLoaderAndStart`.  This relies\n  on process boundaries and therefore doesn't rely on non-spoofability\n  of `network::ResourceRequest::request_initiator`.\n\n\n## Non-Web resources\n\nCompromised *web* renderer processes shouldn’t be able to access\n*local* resources (e.g. `file://...` or `chrome://settings`).\n\nProtection techniques:\n- TODO(lukasza, nasko): need to research\n\n\n## Android-specific protection gaps\n\nDue to resource constraints, on Android platforms only some sites get a\ndedicated renderer process, isolated from other sites.\n(Current heuristic is to isolate the sites where the user has entered a password\nin the past.)\nThis means that some sites are hosted in a renderer process that is\n*not* locked to any particular site.  If an attacker compromises\nan unlocked renderer process, they may try to abuse protection gaps listed\nbelow.\n\n**Known gaps in protection**:\n- When `CanAccessDataForOrigin` runs on the IO thread, it cannot protect\n  isolated sites against being accessed from an unlocked renderer process.\n  Some web storage protections depend on `CanAccessDataForOrigin` calls\n  on the IO thread.\n  See also https://crbug.com/764958.\n\n\n## Renderer processes hosting DevTools frontend\n\nIf an attacker could take control over the DevTools frontend then the attacker\nwould gain access to all the cookies, storage, etc. of any origin within the\npage and would be able to execute arbitrary scripts in any frame of the page.\nThis means that treating the DevTools renderer as untrustworthy wouldn't in\npractice offer additional protection for the same-origin-policy.\n\nBecause of the above:\n\n- Chrome ensures that the DevTools frontend is always hosted in a renderer\n  process separate from renderers hosting web origins.\n- Chrome assumes that the DevTools frontend is always trustworthy\n  (i.e. never compromised, or under direct control of an attacker).\n  For example, when the DevTools process asks to initiate a HTTP request on\n  behalf of https://example.com, the browser process trusts the DevTools\n  renderer to claim authority to initiate requests of behalf of this origin\n  (e.g. attach SameSite cookies, send appropriate Sec-Fetch-Site request header,\n  etc.).\n"
  },
  {
    "path": "security/clusterfuzz-for-shepherds",
    "title": "Security Shepherd ClusterFuzz instructions",
    "content": "# Security Shepherd ClusterFuzz instructions\n\n[TOC]\n\nThis page has instructions for [Security Shepherds](shepherd.md) in how best to use\n[ClusterFuzz](https://clusterfuzz.com) to reproduce and label bugs.\n\n## Basics\n\n[https://clusterfuzz.com/upload-testcase](https://clusterfuzz.com/upload-testcase)\nallows you to upload files to reproduce crashes on various platforms and will\nidentify revision ranges when the regression was introduced.\n\nPrefer using the \"Quick upload\" flow for simple cases. If you choose to use\n\"Upload\" instead, you will have to pick which [job](#useful-jobs) to run.\n\nNote that ClusterFuzz only supports running untrusted inputs on Linux. The UI\nwill warn you of that.\n\nIf a test case requires multiple files, they can be uploaded together in a zip\nor tar archive: the main file needs to contain the words `run`, `fuzz-` `index.`\nor `crash.`.\n\nPlease *do* specify the crbug number when uploading the test case. This allows\nClusterFuzz to keep the crbug updated with progress.\n\n## Useful jobs\n\nYou should chose the right job type depending on the format of file you want to\ntest:\n\n* repro.html [linux_asan_chrome_mp](https://clusterfuzz.com/upload-testcase?upload=true&job=linux_asan_chrome_mp)\n  or [windows_asan_chrome](https://clusterfuzz.com/upload-testcase?upload=true&job=windows_asan_chrome)\n* repro.js [linux_asan_d8](https://clusterfuzz.com/upload-testcase?upload=true&job=linux_asan_d8)\n* repro.pdf [libfuzzer_chrome_asan / pdfium_xfa_fuzzer](https://clusterfuzz.com/upload-testcase?upload=true&job=libfuzzer_chrome_asan&target=pdfium_xfa_fuzzer)\n\n## MojoJS\n\n[MojoJS](../../mojo/public/js/README.md) is a means for a renderer process to use\nMojo IPCs directly from JavaScript. Although it's not enabled in normal production\nChrome builds, it's a great way to simulate how a compromised renderer can attack\nother processes over IPC.\n\nBecause Mojo IPCs change with each version of Chrome, the test case needs to\nuse exactly the right MojoJS bindings. MojoJS bugs typically specify to use\n`python ./copy_mojo_bindings.py` to put such bindings in place, but that does not\nwork for ClusterFuzz where it will need to bisect across many versions of Chrome\nwith many versions of Mojo.\n\nTherefore, do this instead:\n\n* In the PoC, replace all paths where it's loading MojoJS scripts to be prefixed\n  with `file:///gen` instead. For example:\n```\n  <script src=\"file:///gen/mojo/public/js/mojo_bindings_lite.js\">\n```\n  This works because most of the ClusterFuzz Chrome binaries are [now built with](https://chromium-review.googlesource.com/c/chromium/src/+/1119727) `enable_ipc_fuzzer=true`.\n\n* If you believe the bug will reproduce on Linux, use the [linux_asan_chrome_mojo](https://clusterfuzz.com/upload-testcase?upload=true&job=linux_asan_chrome_mojo) job type.\n* If you believe the bug will only reproduce on Android, [ClusterFuzz can't help right now](https://crbug.com/1067103).\n* Otherwise, use any job type but specify extra command-line flags `--enable-blink-features=MojoJS`. In this case, ClusterFuzz might declare that a browser process crash is Critical severity, whereas because of the precondition of a compromised renderer [you may wish to adjust it down to High](severity-guidelines.md).\n\n[Example bug where these instructions have worked](https://crbug.com/1072983).\n\n## Gestures\n\nSome testcases require UI gestures to reproduce them. ClusterFuzz has a\n\"gestures\" field where you _may_ be able to specify such UI interactions. The\nlanguage is platform-specific. On Linux, it's commands for\n[xdotool](https://manpages.ubuntu.com/manpages/trusty/man1/xdotool.1.html) in a\nPython-like list. For instance,\n```\n[ \"type,'qx1sqOqB0ZbEFYn'\",\"key,F3\",\"key,F12\" ]\n```\nAs mouse coordinates are subject to change, it probably only makes sense to try\ngestures if the UI actions can be achieved purely using keystrokes. The relevant\nClusterFuzz [code is in\ngesture_handler.py](https://github.com/google/clusterfuzz/blob/master/src/clusterfuzz/_internal/fuzzing/gesture_handler.py#L22)\nto figure out the languages for other platforms.\n\n## HTTP(S) headers\n\nIf you need to reproduce a test case that involves specific HTTP headers, do this:\n\n1. Make a copy of [page_load_in_process_fuzzer_seed_corpus/network.textproto](https://source.chromium.org/chromium/chromium/src/+/main:chrome/test/fuzzing/page_load_in_process_fuzzer_seed_corpus/network.textproto)\n2. Edit as necessary to give the headers you need\n3. Go to the ClusterFuzz [upload page](https://clusterfuzz.com/upload-testcase)\n4. Select `libfuzzer_chrome_asan` for the job\n5. Select `page_load_in_process_fuzzer` for the fuzzer\n6. Upload `network.textproto` as the test case.\n"
  },
  {
    "path": "security/checklist",
    "title": "Top security things for Chromies to remember",
    "content": "# Top security things for Chromies to remember\n\n## Why:\n\n1. **This is not a rehearsal**. Exploit brokers trade in Chrome security bugs and\nthey're used by [foreign governments in harmful ways](https://blog.google/threat-analysis-group/countering-threats-north-korea/).\n2. **Security bugs are not theoretical bugs**: If it's marked as a security bug,\nthe security team believes it's exploitable in practice. Attackers have\nincredible toolkits to turn niche, timing-dependent, obscure conditions into\npowerful primitives. Unfortunately, they don't need a reliably reproducing bug.\nIf you can't see a way forward, talk to us.\n3. **Attackers deliver data via malicious websites**. Be paranoid when you're\nhandling data delivered from a website. Do not make any assumptions about its\nformat or contents. Code defensively. If you're talking to a lower privilege\nprocess (e.g. a renderer process) from a higher privilege process (e.g.\nbrowser) assume it's compromised and is sending you crafted data.\n4. **Chromium C++ object lifetimes are often too complicated for human brains**.\nYou will introduce use-after-free and data race bugs (readily exploitable).\nAssume the worst and protect against errors using sandboxes, CHECKs and\nfuzzing. If your object lifetimes depend on JavaScript or website data, keep\nownership very simple (avoid raw pointers, avoid `base::Unretained`, single\nownership wherever possible).\n\n## Do's:\n1. **Get in touch** ([security@chromium.org](mailto:security@chromium.org)). We\nare here to help, and we are humans. We won't give you questionnaires or forms\nto fill out, and we will do everything we can to help get you to a secure\noutcome.\n2. **Use checked\n[numerics](https://chromium.googlesource.com/chromium/src/+/HEAD/base/numerics/)\nand use C++\n[containers](https://chromium.googlesource.com/chromium/src/+/HEAD/base/containers/)**\n(Chromium, absl or STL are all OK - they're hardened, or soon-to-be). Prefer\n[`base::span`](https://chromium.googlesource.com/chromium/src/+/HEAD/base/containers/span.h#155)\nover pointer arithmetic.\n3. **Merge fixes with great urgency**. [You fixed a\nbug?](security-issue-guide-for-devs.md) Thank you! It's now\neasier to exploit. N-day attackers are now weaponizing your publicly-visible\ngit commit. Work urgently with the security and release TPMs to handle merges,\nto get the fix out to users before n-day exploitation gets widespread. Step\none: simply mark the bug as Fixed, and then sheriffbot and the TPMs will get in\ntouch to ask for the correct merges.\n4. **Keep things simple**. If your object lifetimes confuse you, or if your\ndata structure can be modified in three places, or if you have to keep two data\nstructures synchronized at all times, you've probably got a security bug.\n5. **Intentionally crash**: it's better than being exploited. `DCHECK`s don't\nprevent security incidents, because release builds don't have them, so use\n`CHECK`s unless calculating the condition is expensive.\n6. **Write fuzzers** for any data that your code is ingesting. It can be [just\na few lines of code](../../testing/libfuzzer/getting_started.md)\nand it can be mesmerizing to see the fuzzer explore your code.\n\n## Don'ts:\n\n1. **Don't use `base::Unretained`** without a comment explaining how you can\nprove the object lifetimes are safe - it's responsible for a high percentage of\nour exploitable bugs. The best alternative? Consider `SafeRef<T>` or\n`WeakPtr<T>` - or ask us!\n2. **Don't run unnecessary code in the browser process**: it has all the\nsecrets. Run the code elsewhere if you can. If it must run in the browser\nprocess, be extra paranoid.\n3. **Don't mess with security UX**. Humankind vaguely knows to trust UI drawn\nby the browser, but not to trust UI drawn by the website. Don't blur the lines.\nThis awareness is [like a fragile\nbutterfly](web-platform-security-guidelines.md#security-ux). [URL display is\nequally delicate](url_display_guidelines/url_display_guidelines.md).\n\nFor more information, see our security [guidelines](rules.md), [FAQ](faq.md)\nand [suggestions for what to do if you get a security\nbug](security-issue-guide-for-devs.md). And don't hesitate to contact\n[security@chromium.org](mailto:security@chromium.org) - we want to help. Thanks\nfor reading!\n\n"
  },
  {
    "path": "security/behavior-over-the-internet",
    "title": "Guidelines for delivery of Chrome behavior over the Internet",
    "content": "# Guidelines for delivery of Chrome behavior over the Internet\n\n**Summary**: It's OK to deliver _content_ to Chrome dynamically over the internet,\nbut Chrome behavior should be a part of the Chrome binary or delivered via\nComponent Updater.\n\nThere are several reasons for this:\n\n* TLS is insufficient to prove categorically that the behavior comes from the\n  Chrome organization (due primarily to the risk that malware, enterprise\n  security software, or governments have arranged to install extra TLS root\n  certificates). So you'd need to build a custom signature verification scheme.\n  This is hard (per the mantra \"don't roll your own crypto\"). The code is\n  fiddly; you'd need to sustain processes for key rotations, revocation,\n  detecting private key compromise etc.\n* Even if the transport is secure, a compromised server could show fake Chrome\n  UI and spoof websites.\n* Users expect Chrome behavior to be part of the Chrome download, and are\n  surprised when it dynamically changes. Chrome does support dynamically\n  changing behavior in several ways, such as the Variations framework. But\n  these mechanisms are well-documented and controlled by e.g. enterprise\n  policies, so it's better to use one of them than re-inventing the wheel\n  -- and they conveniently support out-of-band signing. Dynamic behavior\n  also adds a combinatorial factor which makes it harder to test or fuzz\n  Chrome, or to put in place simple reliable security defenses which apply\n  universally.\n\nWhat do we mean by 'behavior'? 'Behavior' is anything that a reasonable user\nwould consider to be 'part of Chrome' as opposed to content displayed by\nChrome. So, the feed of articles is definitely 'content' whereas the settings\npages are 'behavior'. Web pages themselves are 'content' but any action which\nthe browser applies to those pages is 'behavior'. Generally speaking, code\nwhich runs in the browser process is likely to be 'behavior'. Configuration\ninformation is likely to be 'behavior' too if it meaningfully alters Chrome's\nfunctionality.\n\nA solution is available for fairly-dynamic Chrome behavior:\n[Component Updater](https://chromium.googlesource.com/chromium/src/+/main/components/component_updater/README.md).\nComponents can be updated without an update to Chrome itself, which allows\nthem to have faster or desynchronized release cadences. (The same [signing\ntechnology](https://g3doc.corp.google.com/company/teams/chrome/intelligence/serving_on_device_models.md?cl=head)\ncan be used elsewhere, but this should be avoided if possible\nbecause component updater has documentation and controls such as enterprise\npolicies that might need to be replicated for per-feature signing and code\ndelivery schemes).\n"
  },
  {
    "path": "security/ax-tree-security-guidelines",
    "title": "Security Guidelines for the Accessibility Tree",
    "content": "# Security Guidelines for the Accessibility Tree\n\nTL;DR; The existing\n[AXTreeData](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/mojom/ax_tree_data.mojom)\nand\n[AXTreeUpdate](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/mojom/ax_tree_update.mojom)\nstructures can be walked in the browser process using\n[ax_tree.h](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/ax_tree.h)\nand\n[ax_tree_update.h](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/ax_tree_update.h)\nwhen no alternatives are available. The trees describe complex documents and can\nbe supplied by compromised renderers. Care should be taken when processing AX\ndata, especially when processing updates. New features using the tree should be\nhosted in a sandboxed process.\n\n## Background\n\nOne of Chromium's key features is its multi-process architecture. This brings\nnumerous benefits but presents one key challenge for accessibility.\n\nOperating system accessibility apis expect the ability to synchronously request\ndata about the screen presented as a tree of semantic nodes. These nodes contain\nnumerous attributes describing the node such as its role, display name, bounds,\nand more. Such an api is at the heart of technologies used for persons with\ndisabilities. This includes software that reads the web page to those with\nvisual impairments, highlights words to those with print disabilities,\nauto-scans the actionable views on a page for those with motor impairments, and\nmuch more.\n\nSince Chromium’s renderers hold the semantics about the web page and the browser\nrequires this data synchronously, Chromium developers had to come up with a way\nto bridge this gap while keeping the system itself performant.\n\nAfter trying various approaches, Chromium settled on a model where accessibility\ntrees get serialized from renderers and deserialized in a destination process\nsuch as the browser.\n\nAlthough designed for features and extensions that improve Chrome’s\naccessibility, many other features make use of the trees and updates. The AX\ntree provides a convenient way to snapshot embedded frames, and surface page\nstructure, without the features needing to walk the DOM or inject scripts into\nrenderers.\n\nIn Chrome, the\n[rule-of-two](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/security/rule-of-2.md)\nrequires any C++ code in the browser process to only act on trustworthy data\nfrom renderers. This is usually achieved by passing data to the browser over\nmojo, which validates the data’s format and enforces that it is properly typed.\n[AXTreeData](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/mojom/ax_tree_data.mojom)\nand\n[AXTreeUpdate](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/mojom/ax_tree_update.mojom)\n(along with other AX mojom types) serve this purpose for accessibility tree\nsnapshots and updates. Consumers of these structures should access them via\n[ax_tree.h](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/ax_tree.h)\nand\n[ax_tree_update.h](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/ax_tree_update.h)\nand need not further validate the /format/ of the data they represent. Normal\nweb contents and javascript should not be able to generate or send invalid\nsnapshots or updates.\n\nHowever the AX Tree is complex and its attributes fields are not validated.\nFundamentally the\n[rule-of-two](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/security/rule-of-2.md)\nexists to prevent us acting on web content with bugprone C++ code. As the AXTree\nembeds and is controlled by web content we would implement it differently today.\nIts current design is necessary to work with accessibility tooling on operating\nsystems that were not designed from the ground up to host web content.\n\nToday, a [compromised\nrenderer](https://chromium.googlesource.com/chromium/src/+/master/docs/security/compromised-renderers.md)\ncan cause unexpected trees or updates to be sent to the browser process. Any\ncode performing complex processing of the trees such as comparison of snapshots,\nor updating of internal data structures from updates, should guard itself\nagainst receiving manipulated snapshots or updates. Code using ax_tree.h can\ntrust that the data structures are well-formed, but *cannot* trust that they\nhold valid data. This includes the contents of free-form fields such as\n[strings](https://source.chromium.org/search?q=file:ui%2Faccessibility%2Fmojom%2F.*mojom$%20string&ss=chromium%2Fchromium%2Fsrc:ui%2Faccessibility%2Fmojom%2F),\nor the objects or types of objects referenced by a given ID. The only safe way\nto process this information is in a sandboxed environment or memory-safe\nlanguage.\n\n## Guidance\n\n### The AX Tree is accepted for accessibility features\n\nIf your feature is doing simple things with the AX data you should be fine - it\nwill make security review quicker if you briefly highlight what you are doing\nand why it is safe in your Security Considerations section.\n\nThe existing\n[ax_tree.h](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/ax_tree.h),\n[ax_tree_update.h](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/ax_tree_update.h)\nand associated wrappers are accepted for limited uses in the browser.\n\nConsumers *must* use the ax_tree.h methods to deserialize trees or apply updates\nand not reimplement this.\n\nConsumers should not use the mojom structure of the tree directly and instead\nshould rely on the deserialization & updating methods in\n[ui/accessibility/](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/).\n\n### The AX Tree carries sensitive data or tokens\n\nSome fields in the tree represent sensitive data from the origin in a given\nrenderer, or tokens that can drive features in the browser, and should not be\nshared into unrelated renderer processes:\n\n* The [AXTreeId](https://source.chromium.org/chromium/chromium/src/+/main:ui/accessibility/ax_tree_id.h)\n  is also the embedder token.\n* Page content is present and is site-private user data.\n* Annotations, urls, titles and metadata may be present in string arguments.\n* The AX Tree has string fields that can be manipulated\n\n### The AX tree is complex - and any complex processing could be risky\n\nIf you are doing complicated things with the structures - or if you relate more\nthan one snapshot, or update internal structures from snapshots or updates, then\nyou enter hairy territory - a compromised render has significant control over\nthe tree and can send malicious snapshots or updates.\n\nYou should do this processing in a sandboxed service process or the renderer,\nand your security considerations section should outline how your feature deals\nwith malicious incoming data.\n\n### Fields in the tree are not validated\n\nThe content of the tree is untrustworthy and may come from a compromised\nrenderer.\n\nString might not have expected values or formats:\n\n* String urls should be validated using GURL.\n* Urls should *not* be assumed to refer to the renderer’s origin or its\n  subframe’s origins.\n* Attributes, titles, roles etc. should be sanitized if included or displayed in\n  messages.\n* Matching an entire string, using them as lookups in maps and so on is ok.\n* Smuggling formatted data (such as JSON) in a string is *not* ok.\n* Offsets into strings are untrustworthy - complex string processing should\n  happen in a sandboxed process or a memory-safe language.\n\nA compromised renderer might set or add unexpected attributes or values to\nupdates or snapshots, for instance:\n\n* Rects may have negative or positive x,y coordinates.\n* Offsets into text may lie out of bounds, and may be out of order\n* Object ids in snapshots can be faked or duplicated.\n* Updates or repeated snapshots might describe very different structures.\n* Expected attributes might not be present, or extra attributes might be added.\n* Attribute lists may have inconsistent lengths.\n* A node with five attributes in one snapshot might have seven, or none, in a\n  subsequent snapshot.\n* IDs might be repeated, invalid, or self-referencing.\n\nThe AX Tree ID can sometimes come from a renderer - it is also the embedding\ntoken of the frame - so be very careful when tracking or observing these IDs.\nThese tokens should not be shared between renderers and are unguessable.\n\n## Accessibility / Security FAQs\n\n### Accessibility actions can be used to drive the browser programmatically. Is this a vulnerability?\n\nNo. Accessibility software needs to access and control applications for the\nperson using Chrome. This is intended and supported behavior.\n\n### Operating system accessibility APIs talk directly to the browser process. Is this a security concern?\n\nNo. Accessibility software needs to access and control applications at the same\nlevel as a person using Chrome. This is intended and supported behavior.\n\n### Accessibility transfers data from renderers to the browser process. Is this a security concern?\n\nNo. This data represents the contents and structure of the page for\naccessibility tools, and other Chrome features. Data is safely passed via mojo.\n\nThe tree represents a collection of data from all renderer processes in a tab’s\nframe tree, and holds sensitive content that should not be passed back down to\nrenderers to avoid data leaks between them. The accessibility tree is complex\nand any new complex processing should happen in a sandboxed process.\n\n### Accessibility has a large mojo surface, is this a concern?\n\nNo. This data represents the contents and structure of the page for\naccessibility tools. Data is safely passed via mojo.\n\nLike all IPC in Chrome a compromised renderer can lie about some fields.\nPrivileged processes accessing this data do so via the ax_tree.h APIs, and treat\nany strings or offsets as untrustworthy. The tree itself is complex and features\ncomparing trees should do so in a sandboxed process or memory safe language. See\n“Guidance” above for more discussion.\n\n### Render frames pass accessibility data to one another e.g. hit test requests, accessibility tree ids. Are these requests allowed?\n\nFrames should only pass data to frames that should have it. For instance the\naccessibility tree id is also the frame’s embedder token, and should not be\navailable to unrelated frames.\n\n### Is the accessibility API (either at the OS-level or via extensions) preferred over content script injection?\n\nAll approaches have merits. Extensions are intrinsically safe and allow for\nexperimentation outside of the Chromium. Content script injection is more\nfragile but can access everything it needs within the injected site. The ax tree\nprovides an easy way to access page structure and contents without needing to\nreinvent methods for transmitting the structure, or updates to the structure.\nThe accessibility tree is intended for accessibility surfaces and should not be\nextended for other features in Chrome. The ax tree can also be queried safely\nwithin a renderer.\n\n### Is it considered safer to deserialize accessibility tree data within the browser or a dedicated renderer?\n\nBoth approaches have merits. If a feature needs only limited data from a\nrenderer then it will be better to query the DOM or use the ax tree within the\nrenderer, then pass information to the browser via feature-specific mojo\ndefinitions. If a feature needs to know the full structure of the page, then it\nmight be best to use the ax tree (via the ax_tree.h APIs) as they provide a\nsemi-hardened way to access this.\n\n### Accessibility tree attributes and fields are untrusted. What am I allowed to do with them?\n\nA compromised renderer can supply whatever string or list attributes it wants -\neven when you access these through the ax_tree.h apis - and there is no\nguarantee they will have the format or contents you expect. Features in the\nbrowser or a web ui should sanitize or validate the data (e.g. by rejecting\ninput containing invalid characters, or displaying fields using safe html\nelements in web ui). Accessibility helpers installed in the OS should perform\ntheir own validation.\n\nSee “Guidance” above for more discussion.\n"
  },
  {
    "path": "security/autoupgrade-mixed",
    "title": "Mixed content Autoupgrade",
    "content": "# Mixed content Autoupgrade\n\n## Description\nChrome will now (starting on M80) attempt to upgrade some types of mixed content (HTTP on an HTTPS site) subresources. Subresources that fail to load over HTTPS will not be loaded. For more information see [the official announcement](https://blog.chromium.org/2019/10/no-more-mixed-messages-about-https.html).\n\n## Scope\nAudio, video, and image subresources are upgraded. Blockable (i.e. all other types of) mixed content are blocked without an autoupgrade attempt.\n\n## Opt-out\nUsers can disable autoupgrades, and allow blockable mixed content to load, on a per-site basis through content settings (chrome://settings/content/insecureContent).\n"
  },
  {
    "path": "security/autofill-across-iframes",
    "title": "Autofill across iframes",
    "content": "# Autofill across iframes\n\nChrome Autofill fills in frame-transcending forms like the following pseudo-code\nexample.\n\n```\n<!-- Top-level document URL: https://merchant.example/... -->\n<form>\n  Cardholder name:    <input id=\"name\">\n  Credit card number: <iframe src=\"https://psp.example/...\" allow=\"shared-autofill\"><input id=\"num\"></iframe>\n  Expiration date:    <input id=\"exp\">\n  CVC:                <iframe src=\"https://psp.example/...\" allow=\"shared-autofill\"><input id=\"cvc\"></iframe>\n                      <iframe src=\"https://ads.example/...\"><input id=\"account\"></iframe>\n</form>\n```\n\nThis applies to address and payment information, but not to passwords.\n\n## The security policy\n\nAn autofill fills a form control *candidate* only if one of the following is true:\n\n-   the autofill's origin and the *candidate*'s origin are the [same origin];\n-   [shared-autofill] is enabled in the *candidate*'s [node document] and one of\n    the following is true:\n    -   the autofill's origin and the top-level origin are the [same origin];\n    -   the candidate's origin and the top-level origin are the [same origin] and the\n        *candidate*'s autofill value is non-sensitive.\n\nThe terminology used above is defined in the [appendix](#appendix-terminology).\n\nThis policy is the [eligibility for autofill] definition plus the additional\n\"... and one of the following is true\" conjunct in the [shared-autofill] clause.\n\nThe policy is implemented in [FormForest::GetRendererFormsOfBrowserForm()].\n\n## The rationale\n\nThe example form above exhibits a common pattern: at the time of writing, about\n20% of the payment forms on the web span multiple origins. Most commonly, the\ncardholder name field's origin is the top-level origin, whereas the credit card\nnumber is in a cross-origin iframe hosted by the payment service provider (PSP).\n\nThese iframes are typically styled so that they seamlessly integrate with the\nmerchant's page -- the user is not made aware that multiple frames and origins\nare involved. Yet the different origins isolate the payment information from the\nmerchant's website, which helps them comply with the payment card industry's\ndata security standard (see Section 2.2.3 of the [PCI-DSS best practices]).\n\nChrome Autofill's objective is to fill fields that the user expects to be\nfilled, even if those fields cross origins, while protecting the user against\npossibly malicious sub-frames. Intuitively, we support two \"directions\":\n\n-   \"Downwards\": An autofill may fill fields in descendant documents where\n    [shared-autofill] is enabled. In our example, an autofill initiated on the\n    cardholder name field may fill the credit card number field.\n-   \"Upwards\": An autofill may fill certain values in ancestor documents. In our\n    example, an autofill initiated on the credit card number field may fill the\n    cardholder name field.\n\nWe restrict the values that may be filled \"upwards\" especially to prevent\nleaking sensitive payment information -- credit card numbers and CVCs that the\nPCI-DSS intends to protect -- into the merchant's page. The \"non-sensitive\"\nvalues that we allow to be filled \"upwards\" are credit card types, cardholder\nnames, and expiration dates.\n\nThe terms \"upwards\" and \"downwards\" are imprecise: our security policy doesn't\nrefer to the [top-level traversable]'s [node document], but rather to its\n[origin], the top-level origin. This way, Autofill works the same when, for example,\nthe cardholder name is hosted in a same-origin iframe: `<iframe\nsrc=\"https://merchant.example/...\"><input id=\"name\"></iframe>`.\n\nOur security policy does not allow \"upwards\" or \"downwards\" filling to and from\narbitrary documents. It only allows filling \"upwards to\" main-origin documents\nand \"downwards from\" main-origin documents. This simplifies reasoning about the\nsecurity policy as well as the implementation, and is still sufficient for\nreal-world payment forms.\n\nThe following table illustrates which fields may be filled in our example form\ndepending on the autofill's origin:\n\n| Autofill's origin          | `name`   | `num`    | `exp`    | `cvc`    | `account` |\n|----------------------------|:--------:|:--------:|:--------:|:--------:|:---------:|\n| `https://merchant.example` | &#10004; | &#10004; | &#10004; | &#10004; | &#10006;  |\n| `https://psp.example`      | &#10004; | &#10004; | &#10004; | &#10004; | &#10006;  |\n| `https://ads.example`      | &#10004; | &#10006; | &#10004; | &#10006; | &#10004;  |\n\n## Appendix: Terminology\n\nAn *autofill* is an operation that fills one or many form control elements in\nthe [fully active descendants of a top-level traversable with user attention].\nAn autofill can only be initiated on a [focused] form control element.\n\nAn autofill *fills a form control* if it changes the form control's [value]. The\nvalue after the autofill is the *form control's autofill value*.\n\nA form control's autofill value is *non-sensitive* if it is a credit card type,\na cardholder name, or a credit card expiration date.\n\nA *form control's origin* is its [node document]'s [origin].\nAn *autofill's origin* is the [focused] form control's origin.\nThe *top-level origin* is the [top-level traversable]'s [active document]'s [origin].\n\n*[shared-autofill] is enabled in a document* if the [Is feature enabled in\ndocument for origin?] algorithm on [shared-autofill], the document, and the\ndocument's [origin] returns `Enabled`.\n\n*TODO*: Update link to [eligibility for autofill] once the\n[PR](https://github.com/whatwg/html/pull/8801) is closed.\n\n[FormForest::GetRendererFormsOfBrowserForm()]: https://source.chromium.org/chromium/chromium/src/+/main:components/autofill/content/browser/form_forest.cc;l=618-623;drc=94fbbc584c5d42f0097a9cb28b355853d2b34658\n[active document]: https://html.spec.whatwg.org/#nav-document\n[eligibility for autofill]: https://schwering.github.io/html/#eligible-for-autofill\n[Is feature enabled in document for origin?]: https://w3c.github.io/webappsec-permissions-policy/#algo-is-feature-enabled\n[focused]: https://html.spec.whatwg.org/#focused\n[fully active descendants of a top-level traversable with user attention]: https://html.spec.whatwg.org/#fully-active-descendant-of-a-top-level-traversable-with-user-attention\n[same origin]: https://html.spec.whatwg.org/multipage/browsers.html#same-origin\n[node document]: https://dom.spec.whatwg.org/#concept-node-document\n[origin]: https://dom.spec.whatwg.org/#concept-document-origin\n[PCI-DSS best practices]: https://www.pcisecuritystandards.org/\n[shared-autofill]: https://schwering.github.io/shared-autofill/\n[top-level traversable]: https://html.spec.whatwg.org/#top-level-traversable\n[value]: https://html.spec.whatwg.org/#concept-fe-value\n"
  },
  {
    "path": "security/apparmor-userns-restrictions",
    "title": "AppArmor User Namespace Restrictions vs. Chromium Developer Builds",
    "content": "# AppArmor User Namespace Restrictions vs. Chromium Developer Builds\n\n## Short summary\n\nIf you want to run developer builds of Chromium/Chrome on Ubuntu 23.10+\n(or possibly other Linux distros in the future), you'll need to either globally\nor selectively disable an Ubuntu security feature.\n\n## How can I run developer builds or chromium builds I downloaded from the internet?\n\n### Option 1, the easiest way\n\nThe easiest way is to disable Ubuntu's security feature globally by running\nthese commands in a terminal:\n```\necho 0 | sudo tee /proc/sys/kernel/apparmor_restrict_unprivileged_userns\n```\n\nbut note that this disables a useful Ubuntu security feature.\n\nTo make this setting persist across reboots, create a new file in\n`/etc/sysctl.d`, for example:\n\n```\necho kernel.apparmor_restrict_unprivileged_userns=0 | sudo tee /etc/sysctl.d/60-apparmor-namespace.conf\n```\n\n### Option 2, a safer way\n\nA slightly safer way is to write an AppArmor profile that allows running any\nbinary named \"chrome\" under your chromium build directory:\n```\nexport CHROMIUM_BUILD_PATH=/@{HOME}/chromium/src/out/**/chrome\ncat | sudo tee /etc/apparmor.d/chrome-dev-builds <<EOF\nabi <abi/4.0>,\ninclude <tunables/global>\n\nprofile chrome $CHROMIUM_BUILD_PATH flags=(unconfined) {\n  userns,\n\n  # Site-specific additions and overrides. See local/README for details.\n  include if exists <local/chrome>\n}\nEOF\nsudo service apparmor reload  # reload AppArmor profiles to include the new one\n\n```\n\nNote that an attacker with the ability to create an executable called `chrome`\nanywhere in the above directory will be able to bypass Ubuntu's security\nmechanism.\n\nYou can change `CHROMIUM_BUILD_PATH` to anything you like. ** matches any part\nof a path, * matches one component of a path. Other options of described in\n`man apparmor.d` under the GLOBBING section.\n\n### Option 3, the safest way\n\nIf you have installed Google Chrome, the setuid sandbox helper (the old version\nof the sandbox) is available at `/opt/google/chrome/chrome-sandbox`. You can\ntell developer builds to use it by putting the following in your `~/.bashrc`:\n```\nexport CHROME_DEVEL_SANDBOX=/opt/google/chrome/chrome-sandbox\n```\n\nUbuntu's packaged version of chromium will not install the setuid sandbox\nhelper (it's a snap package that disables the ubuntu security feature at\nruntime for its installed version of chromium).\n\nIf you have not installed Google Chrome, but you do have a chromium source\ncheckout, you can build the SUID sandbox helper yourself and install it. This\nis the old version of the sandbox, but should work without disabling any Ubuntu\nsecurity features. See [Linux SUID Sandbox Development]\n(https://chromium.googlesource.com/chromium/src/+/main/docs/linux/suid_sandbox_development.md)\nfor instructions. This should work permanently.\n\nThe older version of the sandbox may be slightly weaker, and involves installing\na setuid binary.\n\n## Why does this only affect developer builds?\n\nUbuntu ships with an AppArmor profile that applies to Chrome stable binaries\ninstalled at `/opt/google/chrome/chrome` (the default installation path). This\npolicy is stored at `/etc/apparmor.d/chrome`.\n\n## What if I don't have root access to the machine and can't install anything?\n\nYou will need to run developer builds with the `--no-sandbox` command line flag,\nbut be aware that this disables critical security features of Chromium and\nshould never be used when browsing the open web.\n\n## Technical details\n\nOur primary sandbox no longer works on developer builds on some Linux\ndistributions, namely Ubuntu, due to a security feature that restricts access\nto a powerful kernel feature, user namespaces. User namespaces are used by\nChromium (and many containerization applications) to restrict access to the\nfilesystem without requiring root privileges or a setuid binary. For a while,\nuser namespaces have been available to unprivileged (e.g. non-root) users on\nmost Linux distros, but they exposed a lot of extra kernel attack sruface. For\nmore details, see Ubuntu's announcement at\nhttps://ubuntu.com/blog/ubuntu-23-10-restricted-unprivileged-user-namespaces.\n\nIndividual binaries can be allowlisted by filepath using root-owned AppArmor\nprofiles stored in `/etc/apparmor.d/`. Ubuntu ships with an AppArmor profile\nthat applies to Chrome stable binaries installed at\n`/opt/google/chrome/chrome` (the default installation path). Ubuntu's packaged\nversion of Chromium is a snap package, and snap generates an AppArmor profile\nat runtime that allows usage of user namespaces.\n"
  },
  {
    "path": "security/android-sandbox",
    "title": "Chrome Android Sandbox Design",
    "content": "# Chrome Android Sandbox Design\n\nThis document discusses the sandbox and process architecture of Chrome on\nAndroid. Like Chrome's desktop platforms, Android uses a sandbox to isolate and\nde-privilege untrustworthy computing principals. However, the overall\narchitecture is materially different than on desktop platforms, while still\narriving close to the same end result.\n\n## Overall Architecture\n\nThe Chrome Android browser process is the main trusted principal in the system,\nit manages and communicates with a network of processes over inter-process\ncommunication (IPC). The major features of the security architecture are:\n\n- The browser process is protected by the [Android app\n  sandbox](https://source.android.com/docs/security/app-sandbox), which isolates\n  each installed application on the system.\n- Chrome launches its helper processes as Android\n  [Services](https://developer.android.com/reference/android/app/Service).\n- For sandboxed helper processes, the sandbox follows a similar bilayer design\n  as the desktop Linux sandbox. The layer-one sandbox is provided by the\n  [`android:isolatedProcess`](https://developer.android.com/guide/topics/manifest/service-element#isolated)\n  attribute on the Service's manifest definition.\n- Chrome [applies](https://source.chromium.org/chromium/chromium/src/+/main:sandbox/linux/seccomp-bpf-helpers/seccomp_starter_android.h)\n  a layer-two sandbox in the form of a Seccomp-BPF system call filter.\n\n![Diagram of the Android OS components and the parent/child and IPC relationships](android-sandbox-diagram.png)\n\n<!-- Source: https://docs.google.com/drawings/d/11oVsYx_TCrPMglMMW009IFFCStPrLPFbAIVNPp647Lw/edit -->\n\n## Processes\n\nThe below sections discuss three IPC transport mechanisms: Binder, UNIX sockets,\nand Mojo. Binder is Android's primary IPC mechanism, implemented with a kernel\ndriver and a user `ioctl` client. UNIX socket IPC is used between certain\nAndroid OS components, and it underpins Chrome's Mojo IPC mechanism. [Mojo\nIPC](../../mojo/README.md) is the main IPC system used in Chrome to communicate\nbetween components.\n\n### Browser Process\n\nThe Chrome browser process on Android is unique compared to the security\narchitecture of Chrome on other platforms. On desktop systems, all applications\ntypically run as the same user principal (UID), and each application often has\nthe same view of the global filesystem and equal access to the files within.\n\nOn Android, each installed application is given a distinct user ID (UID) and a\nrestricted, scoped storage location. Applications only interact with each other\nusing the high-level\n[Intents](https://developer.android.com/guide/components/intents-filters) IPC\nsystem.\n\nThe implications of this are that Chrome on Android should generally be\nless trusting of data coming from other apps. Similarly, Chrome should either\ntrust or take care to sanitize data it sends to other apps. For more details see\n[Android IPC Security Considerations](android-ipc.md).\n\n### Helper Processes\n\nChrome uses helper processes to segment and isolate the work it performs on\nbehalf of websites and the user. On desktop platforms, Chrome's browser process\nhas a parent-child relationship to the helper processes. But on Android, all\nprocess creation goes through the Activity Manager and a zygote. The entrypoint\nin Chrome for these processes is the\n[`ContentChildProcessService`](https://source.chromium.org/chromium/chromium/src/+/main:content/public/android/java/src/org/chromium/content/app/ContentChildProcessService.java;drc=4e1b7bc33d42b401d7d9ad1dcba72883add3e2af),\nwhich has both privileged (un-sandboxed) and sandboxed subclass variants.\n\nThe zygote architecture is similar to the [one used on\nLinux](../linux/zygote.md), but the Android OS uses it for all processes on the\nsystem. This design exists because starting the Android JVM from scratch in each\nnew process would be expensive, and the zygote enables cloning the running JVM\nto improve performance and reduce memory usage by sharing non-dirty memory\npages.\n\nA consequence of this design is that process creation on Android is more\ninvolved than on other operating systems. Rather than a system call (or two,\ne.g. `fork()` + `execve()`), process creation in Android involves several IPCs\nthrough the system:\n\n1. Browser sends a Binder IPC to Activity Manager for [`Context.bindService()`](https://developer.android.com/reference/android/content/Context#bindService(android.content.Intent,%20android.content.Context.BindServiceFlags,%20java.util.concurrent.Executor,%20android.content.ServiceConnection))\n2. Activity Manager sends the zygote an IPC over a privileged UNIX socket to fork a process\n3. Zygote responds to Activity Manager with the forked process ID\n4. New process sends a Binder IPC to Activity Manager to connect with it\n5. Activity Manager sends a Binder IPC to the new process with information about the APK to load\n6. Activity Manager sends a Binder IPC channel connecting the new process to the browser process\nprocess for [`ServiceConnection.onServiceConnected`](https://developer.android.com/reference/android/content/ServiceConnection#onServiceConnected(android.content.ComponentName,%20android.os.IBinder))\n\nIn Chrome, after the process is created, the browser process sends a file\ndescriptor to the new process over Binder to establish Chrome's Mojo IPC. From\nthen on, Chrome primarily uses Mojo IPC between the browser and the helper\nprocess. The Binder IPC channel is retained in order to control the lifetime of\nthe helper process.\n\n### Zygote\n\nStarting with Android API v29, the OS offers applications the ability to create\nper-app zygotes. An `isolatedProcess <service>` can be declared with\n[`android:useAppZygote`](https://developer.android.com/reference/android/R.styleable#AndroidManifestService_useAppZygote)\nand provide a [`ZygotePreload`](https://developer.android.com/reference/android/app/ZygotePreload)\nimplementation. This instructs the OS to spawn a new zygote, from the system\nzygote, that will be used to launch all instances of that Service. By giving the\napplication the ability to participate via `ZygotePreload`, the application can\nacquire resources and perform initialization that is common to all future\nprocesses. This extends the benefits of lower initialization time and greater\nmemory sharing from the system zygote to individual applications.\n\n## Sandboxing\n\nSimilar to the [Linux sandbox design](../linux/sandboxing.md), which uses a\nbilayer sandbox, Chrome on Android also uses two technologies to secure\nlow-privilege processes.\n\n### SELinux\n\nThe Android OS applies an [SELinux policy](https://source.android.com/docs/security/features/selinux)\nto every process on the system. A full explanation of SELinux is outside the\nscope of this document, but at a high-level it is a [mandatory access control\nmechanism](https://csrc.nist.gov/glossary/term/mandatory_access_control) that\nenforces a capability sandbox. Every object represented in the kernel has an\nassociated security label, and the system policy specifies what subjects can do\nto objects. For Chrome, SELinux acts as the layer-one sandbox.\n\nSystem services have per-service policies, but applications and their components\nare usually assigned to one of three domains:\n\n- [**untrusted_app**](https://cs.android.com/android/platform/superproject/main/+/main:system/sepolicy/private/untrusted_app.te;drc=4aad91d920ad42a7374e7bbd4cb9a50de4e85efb)\n  is the default policy for most user-installed apps\n- [**priv_app**](https://source.android.com/docs/core/permissions/perms-allowlist)\n  is the used for system applications that require powerful capabilities (e.g.\n  backup/restore)\n- [**isolated_app**](https://cs.android.com/android/platform/superproject/main/+/main:system/sepolicy/private/isolated_app.te;drc=941ba723baceac19151560e8a1d2830b9be6493c)\n  is a restrictive sandbox that can be applied via the `<service\n  android:isolatedProcess=\"true\">` tag in an application's manifest\n\nIn Chrome, the browser process runs under the **untrusted_app** SELinux domain,\nwhich enforces separation between distinct apps on the system.\n\nChrome's sandboxed helper processes (e.g. renderers, utility processes,\nsandboxed Mojo services) run under **isolated_app**. When the Android OS starts\nan **isolated_app** process, it also assigns the process to an ephemeral UID, to\nseparate it from other POSIX security principals. The **isolated_app** SELinux\ndomain prevents the process from accessing virtually all system services,\ndevices, the network, and most of the filesystem.\n\nChrome's un-sandboxed helper processes (e.g. GPU, un-sandboxed Mojo services)\nrun under **untrusted_app** and the same UID as the browser process. This means\nthere is no privilege separation between un-sandboxed helper processes and the\nbrowser process.\n\nA unique difference with Android compared to desktop platforms is that the\noperating system controls the sandbox policy (as a result of it being a\nmandatory access control mechanism). This means that Chrome cannot modify the\npolicy, nor create gradations of sandboxes as can be done on other operating\nsystems. A process on Android runs at either the same security principal as the\nbrowser, or in the tightly sandboxed **isolated_app** domain.\n\n### Seccomp-BPF\n\n[Seccomp-BPF](https://www.kernel.org/doc/html/v4.19/userspace-api/seccomp_filter.html)\nis the second layer of the sandbox, which performs kernel attack surface\nreduction. By removing system calls, or filtering specific arguments of system\ncalls, some of the complexity provided by the Linux kernel can be walled off\nfrom a low-privilege process.\n\nThe [policy applied to\nAndroid](https://source.chromium.org/chromium/chromium/src/+/main:sandbox/linux/seccomp-bpf-helpers/baseline_policy_android.h;l=25;drc=f18d3489a59d34c92a8d96ef6a7e7279198a8ec6)\nis based on the desktop Linux policy. But, additional system calls are permitted\ncompared to desktop Linux. The major differences are to allow the JVM to\nfunction properly and to account for differences in Bionic (Android Libc) vs\nGlibc (standard Linux).\n\nAdditionally, the Android OS applies a [seccomp-bpf filter to all\napplications](https://cs.android.com/android/platform/superproject/main/+/main:bionic/libc/seccomp/seccomp_policy.cpp;l=305;drc=704772bda034448165d071f68b6aeca716f4220e).\nThis policy is necessarily looser (since it applies to all applications) than\nthe one applied by Chrome. But, seccomp-bpf policies stack and can only be made\nmore restrictive.\n"
  },
  {
    "path": "security/android-ipc",
    "title": "Android IPC Security Considerations",
    "content": "# Android IPC Security Considerations\n\nGenerally Chrome communicates between its processes using the\n[Mojo](../../mojo/README.md) [inter-process communication (IPC)\nmechanism](mojo.md). For most features, this is the preferred IPC mechanism to\nuse. However, as an Android application, there are certain interactions with\nother applications and the Android OS that necessitate using different IPC\nmechanisms to communicate. This document covers security concerns related to\nthose Android-specific IPC mechanisms.\n\nThe Chrome browser process is typically the only process type that will interact\nwith these different IPC mechanisms.\n\n## Intents\n\n[Intents](https://developer.android.com/guide/components/intents-filters) are\nthe most common type of inter-process communication mechanism on Android. They\nare most commonly used to start Activities and they internally carry data\nassociated with that Activity (e.g. using the `ACTION_SEND` Intent to share a\npiece of content and including either text or image data in the Intent body).\n\n### Inbound Intents\n\nBecause any application can dispatch Intents with Chrome as the receiver, when\nreceiving an inbound Intent, you should never fully trust the data contained\nwithin. Data sent from other applications could be malicious or malformed, and\nso you must validate or sanitze the data before passing it to other trusted\ncomponents of the browser process. Intents are handled in Java though, so\nfollowing the [Rule of 2](rule-of-2.md) is generally easy. (Though take note\nthat certain Android classes are just Java wrappers around native code, which\nwould not be considered safe by that rule.)\n\nInbound Intents may also pose deserialization issues via the data stored in an\nIntent's extras. These issues may result in non-exploitable crashes (e.g.\nhttps://crbug.com/1232099), but it is also possible to have deserialization\nvulnerabilities with security implications. Always use the\n[`IntentUtils.safe*Extra()`](https://source.chromium.org/chromium/chromium/src/+/main:base/android/java/src/org/chromium/base/IntentUtils.java;l=58;drc=7f1297bacd32fe668d4c99cb8963b56aed363acc)\nfamily of methods to access Intent extra fields from inbound Intents.\n\nIt is **fundamentally impossible** to determine the sender of an Intent, unless\nthe Activity was started with\n[`startActivityForResult`](https://developer.android.com/reference/android/app/Activity#startActivityForResult(android.content.Intent,%20int)).\nFor Intents that are started via `startActivityForResult`, you can use\n[`getCallingActivity`](https://developer.android.com/reference/android/app/Activity#getCallingActivity())\nor\n[`getCallingPackage`](https://developer.android.com/reference/android/app/Activity#getCallingPackage())\nto retrieve the identity of the component that called\n[`setResult`](https://developer.android.com/reference/android/app/Activity#setResult(int))\non the started Activity. For all other cases, the security model of your feature\ncannot depend on authenticating the sender of an Intent. Do not trust\n`Intent.EXTRA_REFERRER`. See also the discussion below about [capability\ntokens](#capability-tokens).\n\nOne way to authorize Intents is to use the system's\n[`android:permission`](https://developer.android.com/guide/topics/permissions/overview#permission_enforcement)\nattribute on a component's (e.g. Activity, Service, etc.) manifest declaration.\nYou can [define a custom permission](https://developer.android.com/guide/topics/permissions/defining) and\nset the `android:protectionLevel` of the permission to `\"signature\"` or\n`\"signatureOrSystem\"` to restrict access to just components signed by the same\ncertificate (or trusted system components).\n\n## Outbound Intents {#outbound-intents}\n\nThere are [two types of Intents](https://developer.android.com/guide/components/intents-filters?hl=en#Types):\nimplicit and explicit. With implicit Intents, the receiving application is not\nspecified by the sender and the system uses a resolution process to find the\nmost suitable component to handle it. An implicit Intent can sometimes result in\na chooser being shown to the user when multiple applications could handle it.\nExplicit Intents specify either the package name or a fully qualified\n`ComponentName`, so the recipient is known at the time it is dispatched.\nImplicit Intents can result in an unexpected (and maybe malicious) application\nreceiving user data. If it is possible to know the target application when\nsending an Intent, always prefer using an explicit Intent.\n\n## PendingIntents\n\nA [PendingIntent](https://developer.android.com/reference/android/app/PendingIntent)\nis created by one application and vended to another. The object allows the\nreceiving application to start the component (i.e. Activity, Service, Broadcast)\n_as if the creating application started it_. Similar to a [setuid binary](https://en.wikipedia.org/wiki/Setuid),\nyou must use this with care, as it can even be used to start non-exported\ncomponents of the creating application.\n\nIt is possible to retrieve information about the creator package of the\nPendingIntent using the [`getCreatorPackage()`](https://developer.android.com/reference/android/app/PendingIntent.html#getCreatorPackage())\nmethod. This is the identity under which the Intent, which the PendingIntent\nrepresents, will be started. Note that you cannot retrieve specific information\nabout the Intent (e.g. its target and extras). And as discussed above with\nIntents, it is not possible to determine the application that called\n`PendingIntent.send()`.\n\n## Binder\n\n[Binder](https://developer.android.com/reference/android/os/Binder) is the low\nlevel IPC mechanism on Android, and it is what Intents and other Framework-level\nprimitives are built upon.\n\n### Bound Services\n\nTo communicate between components using Binder, you declare a `<service>` in\nyour manifest and connect to it using [`Context.bindService()`](https://developer.android.com/reference/android/content/Context.html#bindService(android.content.Intent,%2520android.content.ServiceConnection,%2520int)).\nThis is referred to a as a [bound service](https://developer.android.com/guide/components/bound-services).\n\nOne of the powerful properties of a bound service is that you can determine the\nidentity of your communicating peer. This can only be done during a Binder\ntransaction (e.g. in an [AIDL](https://developer.android.com/guide/components/aidl)\nmethod implementation or a [`Handler.Callback`](https://developer.android.com/reference/android/os/Handler.Callback.html))\nthat is **not** marked [`FLAG_ONEWAY`](https://developer.android.com/reference/android/os/IBinder).\nDuring the transaction use [`Binder.getCallingUid()`](https://developer.android.com/reference/android/os/Binder.html#getCallingUid())\nto retrieve the package's UID.\n\nIn Android, every installed application is given a unique user ID (UID). This\ncan be used as a key to query the [PackageManager](https://developer.android.com/reference/android/content/pm/PackageManager),\nto retrieve the [PackageInfo](https://developer.android.com/reference/android/content/pm/PackageInfo)\nfor the application. With the PackageInfo, information about the applications\ncode signing certificates can be retrieved and cryptographically authenticated.\nThis is a strong authentication check and it is the **only** reliable mechanism\nby which you can authenticate your peer.\n\nIn Chrome, the helper functions\n[`ExternalAuthUtils.isCallerValid()`](https://cs.chromium.org/chromium/src/chrome/android/java/src/org/chromium/chrome/browser/externalauth/ExternalAuthUtils.java?l=157&rcl=fa790f69ce80bf2e192d710ea08b8343cad93fbb)\nand `isCallerValidForPackage()` can perform these checks for you.\n\n## Capability Tokens {#capability-tokens}\n\nWe define a **capability token** to be an unforgeable object that the holder may\npresent to another application as authentication to access a specific\ncapability. Binder objects are backed by the kernel (i.e. are unforgeable), are\ntransferable, and are comparable using `isEqual()`, so Binders can be used as\ncapability tokens.\n\nOne security factor to bear in mind is that because capability tokens are\ntransferable, they do not strongly authenticate a caller's identity. One\napplication may deliberately or accidentally transfer a capability token to\nanother application, or a token could be exfiltrated via an application logic\nvulnerability. Therefore, only use capability tokens for access control, not\nidentity authentication.\n\nWhile noting the above factor, capability tokens can be useful for\nauthenticating Intents. If two applications have established a Binder\nconnection, they can use the channel to exchange a capability token. One\napplication constructs a generic Binder (using the\n[`Binder(String)`](https://developer.android.com/reference/android/os/Binder.html#Binder(java.lang.String))\nconstructor) and sends the object over that `ServiceConnection` to the other\napplication, while retaining a reference to it.\n\nThe generic Binder object can then be transmitted as an Intent extra when\nsending Intents between the two applications. By comparing the object with\n`Binder.isEqual()`, you can validate the capability token. Be sure to use an\n[explicit Intent](#outbound-intents) when sending such an Intent.\n\nThis same approach can also be done with using a PendingIntent to a non-exported\ncomponent as a capability token. Internally PendingIntents use a Binder token\napproach, so the only significant difference is the additional capability\nconferred by the PendingIntent to start a component.\n"
  },
  {
    "path": "security/url_display_guidelines/url_display_guidelines",
    "title": "Guidelines for URL Display",
    "content": "# Guidelines for URL Display\n\n*This document covers the best practices and pitfalls for building UI to display URLs in browsers and other apps. It covers the main categories of problems and challenges that we’ve seen in building Chrome. The guidance is intended to be generally applicable, but includes some Chrome-specific notes throughout.*\n\n[TOC]\n\n## Background\n\nThe [URL](https://url.spec.whatwg.org) displayed in a user agent's address bar is often the only security context directly exposed to users, and therefore in many cases it is the only signal users can reasonably rely upon to determine whether or not they should trust a particular website.\n\n### Components of URLs\n\nA URL is made up of a number of components, the majority of which should not have any impact on security decisions:\n\n![Components of a URL](url_components.png)\n\nUsually, the security context that the user cares about is the **[registrable domain](#registrabledomain)** of the top-level page’s URL's origin, even when a given page is made up of components from many different origins. The registrable domain typically consists of a subdomain of an entry on the [Public Suffix list](https://publicsuffix.org/). For instance, bbc.co.uk is a registrable domain under the co.uk public suffix.  The **fully-qualified hostname** consists of a registrable domain, and optionally one or more **subdomain** labels.\n\nThe **scheme** of the URL determines the protocol by which the content from the URL is delivered. If the scheme refers to a non-secure protocol like HTTP, and especially if the protocol traverses an untrusted network, the **registrable domain** information may not accurately describe the true source of the content because the content may have been modified by a **man-in-the-middle** on the network. The **port** number only needs to be specified if it is not the default for the scheme (e.g., 80 for HTTP, 443 for HTTPS).\n\nOther components of the URL (**subdomain**, **userinfo**, **path**, **query**, and **fragment**) are completely under the control of the website and may be crafted in an attempt to spoof the user by misrepresenting the registrable domain.\n\n### Challenges and Threats\n\nMalicious websites are motivated to misrepresent their provenance in order to trick visitors into performing an unsafe action (e.g., phishing, malware install) or to otherwise grant unwarranted trust in the information provided by the site (e.g., \"fake news\").\n\nAs the web platform becomes more capable (introducing new features like device access, etc.), the importance of evaluating the source of web content grows more important and the desire to misrepresent its origins by bad actors increases.\n\n## Best Practices\n\n### TL;DR: Advice on What to Display\n\n**Where possible, avoid displaying URLs**, especially when the user is likely to be making a trust decision. Instead, display only the **[origin](#simplify)**. Additionally, if the connection is **not secure**, add an indicator to that effect:\n\n![Chrome showing Not Secure indicator](not_secure.png)\n\nIf the URL display only applies to secure URLs (for example, a permission prompt that can only be requested by HTTPS pages), omit the scheme and non-default ports (443 for HTTPS).\n\nMore detailed guidance follows.\n\n### Show Origins At All Times\n\nTo avoid spoofing, the current best practice is to always show the\norigin. Whenever the user is interacting with web content, they might be making\na security decision (\"Should I enter my password?\", “Should I trust the\ninformation on this site?”, etc.) and the origin is what we currently rely on to\nhelp them make that decision.\n\nIt’s also important to show an origin when Chrome is explicitly asking the user to make a security decision.\n\nCurrently, in Chrome, origins are hidden in the following special cases:\n\n* Chrome for Android, when a user scrolls down\n\n* Fullscreen mode across platforms\n\n* Installed PWAs on Android\n\n    * The origin is shown in the install dialog\n\n* Installed PWAs on Desktop after animation\n\n    * The origin is shown in the install dialog\n\n### Use Only Security-Reviewed Libraries to Canonicalize and Parse Strings into URLs\n\nParsing and canonicalization of URLs is extremely [error-prone](https://www.blackhat.com/docs/us-17/thursday/us-17-Tsai-A-New-Era-Of-SSRF-Exploiting-URL-Parser-In-Trending-Programming-Languages.pdf), and mistakes can introduce both spoofing vulnerabilities and lower-level problems that can lead to vulnerabilities as severe as remote code execution.\n\nRather than attempting to parse components from a string directly, you should always obtain components of the URL via a trusted object (e.g., GURL/KURL).\n\n### Display URLs in Canonical Form\n\nIn virtually all cases, you should display URLs in **canonical** form, and you should rely on trusted libraries to perform such canonicalization. Canonicalization includes a number of steps that help ensure that URLs are in their simplest form and the one a user is most likely to be able to understand.\n\nCanonicalization includes:\n\n1. Normalizing the hostname (e.g., convert to lowercase, perform IDN-related normalizations)\n\n2. Stripping default ports\n\n3. Path simplification\n\n4. Unnecessarily-encoded octets are decoded\n\nFor example, canonicalization converts `https://ExAmPle.com:443/one/%2e./Tw%2fo/` to the canonical form, `https://example.com/Tw%2fo/`.\n\nDo not attempt to write your own canonicalizer.\n\n### Simplify URLs Whenever Possible {#simplify}\n\n* [Do NOT display the username and password components](https://url.spec.whatwg.org/#url-rendering) of URLs (e.g., `https://user:password@example.com/`) anywhere the user is making a security decision.\n\n    * As these credentials may be sensitive, consider omitting them wherever possible (e.g., do not include username and password when generating printouts).\n\n* When the primary purpose of displaying a URL is to have the user make a security decision, display the **origin**, or if scheme is always HTTPS, just the **domain**. Omit the path, query string, fragment, and any other components of the URL because they provide opportunities for spoofing.\n\n    * Do not display the scheme if it will always be https://. If the scheme is not https://, prefer to show a security indicator icon (dangerous triangle icon + \"Not Secure\" string on http://) rather than the scheme itself.\n\n    * In Chrome, we often remove subdomains \"www\" and “m” as a special case to simplify the origin, except when they are part of the registrable domain. You can use the [`kFormatUrlOmitTrivialSubdomains`](https://cs.chromium.org/chromium/src/components/url_formatter/url_formatter.h?q=kFormatUrlOmitTrivial&sq=package:chromium&g=0&l=63) flag for `url_formatter::FormatURL` ([example usage](https://cs.chromium.org/chromium/src/components/history/core/browser/history_backend.cc?type=cs&q=OmitTrivialSubdomains&sq=package:chromium&g=0&l=127)).\n\n    * Omit default ports (80 for http, 443 for https).\n\n    * If in a space-constrained environment, it's acceptable to use registrable domain instead of the full origin.\n\n### Eliding URLs\n\n* tl;dr: use [`url_formatter::ElideUrl`](https://source.chromium.org/chromium/chromium/src/+/main:components/url_formatter/elide_url.h;l=40;drc=9bf53ab9128027a4a3df5cc10485e7962ddfad4d;bpv=1;bpt=1?q=url_formatter::ElideUrl&sq=&ss=chromium%2Fchromium%2Fsrc)\n\n* When the full hostname cannot be displayed, elide labels starting from the front. (Right-to-Left character support means that the *front* of the string may not appear at the *left*). (Note that Chrome's omnibox behavior on desktop is currently [buggy](https://bugs.chromium.org/p/chromium/issues/detail?id=527638) in this respect.)\n\n* Ensure that at least the registrable domain can be shown, to avoid showing **...paypal.com** when loading `https://not-really-paypal.com`.\n\n ![URL elision examples](elision.jpg)\n\n### Highlighting\n\nIf showing the full URL is deemed necessary, consider highlighting to help the user focus on the most security relevant information in the URL.\n\n* Ideally, consider highlighting only the registrable domain component, to increase awareness of situations where a malicious site uses a misleading subdomain in an attempt to fool the user:\n\n![URL highlighting example](highlighting.png)\n\n(Note that Chrome's omnibox does [not](https://bugs.chromium.org/p/chromium/issues/detail?id=527638#c6) currently do this style of highlighting.)\n\n### URL Length\n\nIn general, the *web platform* does not have limits on the length of URLs (although 2^31 is a common limit). *Chrome* limits URLs to a maximum length of **2MB** for practical reasons and to avoid causing denial-of-service problems in inter-process communication.\n\nOn most platforms, Chrome’s omnibox limits URL display to **32kB** (`kMaxURLDisplayChars`) although a **1kB** limit is used on VR platforms.\n\nEnsure that the client behaves reasonably if the length of the URL exceeds any limits:\n\n* Origin information appears at the start of the URL, so truncating the end is typically harmless.\n\n* Rendering a URL as an empty string in edge cases is not ideal, but truncating poorly (or crashing unexpectedly and insecurely) could be worse.\n\n* Attackers may use long URLs to abuse other parts of the system. [DNS syntax](https://en.wikipedia.org/wiki/Domain_Name_System#Domain_name_syntax) limits fully-qualified hostnames to **253 characters** and each [label](#label) in the hostname to **63 characters**, but Chromium's GURL class does not enforce this limit.\n\n### Display Font\n\nSelection of fonts can have a meaningful impact on the security of a URL display. Relevant factors include:\n\n* Visual distinctiveness for visually-similar characters and sequences (**0** vs **o**, **1** vs **l**, **w** vs. **vv**, etc.), especially in the ASCII character set:\n\n![Visually similar characters in a URL](visually_similar.png)\n\n* Glyphs for all necessary characters, including those used in International Domain Names\n\nIf a font is missing a glyph for a character, it may render as a blank, leading to the possibility of a spoofing vulnerability like [714196](https://crbug.com/714196).\n\n![Spoofing vulnerability from a missing glyph](blank_spoofing.png)\n\nDepending on the platform, font fallback may be used.\n\n### Display of International Characters\n\nNon-English characters present a huge attack surface for spoofing attacks but are today *relatively* uncommon in the host component of URLs worldwide. As global use of the web continues to grow, use of International Domain Names is expected to grow and URL displays should not be overly English-centric.\n\nThe primary threat of non-ASCII text in URLs is a [homoglyph attack](https://en.wikipedia.org/wiki/Homoglyph) in which ASCII characters are swapped with visually-identical (or near-identical) characters.\n\n#### IDN Display Restrictions in Chrome\n\nTo mitigate URL spoofing attacks, Chrome follows a set of [rules](https://www.chromium.org/developers/design-documents/idn-in-google-chrome) that determine whether a given International Domain Name is shown in Unicode characters or degrades to show the ASCII [Punycode](https://en.wikipedia.org/wiki/Punycode) string (starting with \"xn--\"). The Punycode string is not intended to be human-readable: display of Punycode is instead only intended to defeat homoglyph attacks.\n\nChrome’s [`FormatURLForSecurityDisplay`](https://cs.chromium.org/chromium/src/components/url_formatter/elide_url.h?q=FormatURLForS&sq=package:chromium&l=108) function encapsulates this and other behaviors.\n\n* Follow Chrome’s rules for [IDN Display](https://www.chromium.org/developers/design-documents/idn-in-google-chrome) to avoid homograph attacks.\n\n* For the benefit of technical users, *consider* displaying an indicator when a URL contains non-ASCII characters. To date, Chrome has not implemented such an indicator. By way of comparison, Internet Explorer shows an icon in the address bar which opens a bubble that provides more information. An international URL indicator generally *should not* be positioned as a warning, but should allow the user to get more information about the content of the URL. Security-savvy users may find this UI useful in detecting spoofs and escalating reports to more effective security mechanisms (e.g., SafeBrowsing).\n\n#### RTL\n\n* Ensure that URLs are sanitized to prevent abuse by [Right-to-Left override characters](https://unicode.org/reports/tr9/#Explicit_Directional_Embeddings).\n\n    * In Chrome Views code, a view that solely contains a URL should use `DIRECTIONALITY_AS_URL`, which handles everything correctly and will be updated if the spec changes.\n\n    * Follow the advice in [Sneaky Unicode Characters](#sneakyunicode) below to remove all explicit overrides.\n\n    * Wrap the URL in U+202A ... U+202C, so that it is forced to appear in a LTR paragraph. The URL standard (both [RFC 3987](https://www.ietf.org/rfc/rfc3987.txt) and https://url.spec.whatwg.org) mandate that URLs are displayed in LTR paragraphs, no matter what they contain.\n\n<a name=\"sneakyunicode\"></a>\n#### Sneaky Unicode Characters\n\nAn attacker may abuse whitespace and line-wrapping characters in order to push the display of their true origin out of view. Such characters should be banned or displayed in %-escaped form.\n\n* Non-breaking spaces (U+00A0) and other invisible characters (e.g., U+2000 to U+200A inclusive).\n\n* Unicode line terminators (e.g., U+2028, U+2029, and U+0085).\n\n* Unicode explicit directional formatting commands (e.g., U+200E--U+200F, U+202A--U+202E, see the full [list](https://unicode.org/reports/tr9/#Directional_Formatting_Characters) and [`ShouldUnescapeCodePoint`](https://cs.chromium.org/chromium/src/net/base/escape.cc?l=172&rcl=dc22553340d5c4dda162f17a07d706748be44042)).\n\n* Characters that look like security UI (e.g., U+1F512 🔒).\n    - Emoji may be confusing because users are not accustomed to seeing graphics in URL displays and may be misled into believing that they represent claims on the browser’s part (e.g., the Lock emoji). See issue [746350](https://bugs.chromium.org/p/chromium/issues/detail?id=746350).\n\n* Use of [Combining characters](https://blog.emojipedia.org/fun-emoji-hacks/) to create look-alikes.\n\nIn Chromium, this is handled by routines in `base/strings/escape.h`. Outside of Chromium, consult `ShouldUnescapeCodePoint` in `net/base/escape.cc` for guidance.\n\n### Literal IP Addresses\n\nA URL may contain an IPv4 or IPv6 literal in the host component instead of a fully-qualified hostname:\n\n![URL with an IP literal hostname](ip_example.png)\n\n#### IPv4\n\nThe canonical representation of an IPv4 literal is [dotted-decimal](https://en.wikipedia.org/wiki/Dot-decimal_notation) (aka \"dotted quad\") notation, but many surfaces will accept a 32-bit decimal integer (or even [octal](https://bugs.chromium.org/p/chromium/issues/detail?id=787361) or hex) IP representation:\n\n![URL with a decimal IP literal hostname](decimal_ip.png)\n![URL with a hex IP literal hostname](hex_ip.png)\n\nIPv4 literals should be converted to canonical form for display. If your code makes any sort of formatting or security decision based on the [absence of dots within a hostname](https://blogs.msdn.microsoft.com/ieinternals/2014/03/06/browser-arcana-ip-literals-in-urls/), be sure that it isn’t fooled by undotted IP literals.\n\n##### IPv6\n\nIPv6 literals use colons as a delimiter and the literal is wrapped in square brackets (this allows disambiguating the colons in the literal from the colon which begins the port component of the URL).\n\n![URL with an IPv6 literal hostname](ipv6.png)\n\nIf an IPv6 Literal contains a Zone component, the % delimiter must be [escaped to %25](https://en.wikipedia.org/wiki/IPv6_address#Use_of_zone_indices_in_URIs).\n\nhttps://[fe80::1ff:fe23:4567:890a%25eth0]/\n\nMost surfaces will accept non-canonical forms of IPv6 literals:\n\n![URL with a non-canonical IPv6 literal hostname](noncanonical_ipv6.png)\n\n....but these should be converted to canonical form for display.\n\n### Uncommon Schemes and Virtual URLs\n\nWhile many users have at least *some* familiarity with HTTP and HTTPS URLs, the browser offers a number of more exotic URL schemes with which users have no experience.\n\nWhere possible, limit acceptance of URLs outside of the most common schemes (e.g., HTTP/HTTPS) to reduce risk.\n\n[data:// URIs](https://en.wikipedia.org/wiki/Data_URI_scheme) carry the response data directly within the URL, meaning that they do not need to hit the network. Attackers have tried to spoof users by putting what *looks like* an origin at the front of the URL string, so Chrome no longer allows websites to navigate directly to Data URIs in the top frame and also labels Data URIs as \"Not secure\" in the event that a user loads one via other means. filesystem URIs (a Chrome-specific scheme) receive the same treatment.\n\n![data:// URI with a Not Secure label](notsecure_data_uri.png)\n\nfilesystem URIs and blob URIs embed the origin from which they originated as the first component after the URL scheme.\n\n![blob URI with a Not Secure label](notsecure_blob_uri.png)\n\n![filesystem URI with a Not Secure label](notsecure_filesystem_uri.png)\n\n![filesystem URI](filesystem_uri.png)\n\nIn Chrome, file:// schemed URIs do not contain a host component; be sure that your UI accounts for this possibility.\n\n**view-source:** is a special URL scheme which wraps another scheme (e.g., `view-source:https://example.com`) and displays the document in a special Blink \"view-source\" mode. For security reasons, web content cannot navigate directly to view-source URLs.\n\nIn Chrome, a **virtual URL** is roughly the URL displayed in the omnibox even though the real url is something different. An example is `chrome://newtab/` -- under the hood it is either a local version or a remote one but that shouldn’t matter to the user who sees it as the New Tab Page. URL spoofs can easily result if the URL display surface fails to update the URL upon a navigation to a resource which is not intended to be rendered under the virtual URL (e.g., [750298](https://bugs.chromium.org/p/chromium/issues/detail?id=750298)).\n\n### %-Escaping\n\nThe [URL Standard](https://url.spec.whatwg.org/#url-rendering) suggests that *the path, query, and fragment components of the URL should have their sequences of percent-encoded bytes replaced with code points resulting from percent decoding those sequences converted to bytes, unless that renders those sequences invisible.*\n\nThis is generally a user-experience feature (some sites strive to use human-readable URLs and %-escaped characters are not human readable) but could lead to spoofing attacks if performed incorrectly.\n\nChrome’s [`FormatUrl`](https://cs.chromium.org/chromium/src/components/url_formatter/url_formatter.h?l=100&rcl=1deab0dd75a1659e44b8159d60de9cf26dc3dbf0) function takes an [`UnescapeRule`](https://cs.chromium.org/chromium/src/base/strings/escape.h?l=64&rcl=ecba19472b9290092745e9846edd0d6fd8dcc48b) parameter that determines what components should be decoded for display. As of Chrome 65, we unescape path, query, and fragment components for display.\n\nSpace and other invisible characters should be displayed in encoded form.\n\nExample URL with space and emoji:\n\n     https://example.com/ 🍌/? 🍌# 🍌\n\nRenders as:\n\n![Rendering of Emojis and spaces in Chrome's omnibox](encode_for_display.png)\n\n## Glossary\n\nA **homograph** (or **homoglyph**) **attack** occurs when an attacker uses lookalike characters to make one URL resemble another. This can occur purely in ASCII (e.g., **1** looks like **l**, **vv** looks like **w**), or when using Unicode characters via the **International Domain Names** mechanism.\n\n**International Domain Names** is a mechanism for using Unicode characters for hostnames. Under the covers, the Unicode labels are encoded using **[punycode](https://tools.ietf.org/html/rfc3492)** and prefixed with **xn--**. The browser may display the label in Unicode, or in the underlying punycode form. Users are not expected to be able to decode the punycode form: display in this form is intended to foil spoofing attempts.\n\n<a name=\"label\"></a>\nA **[label](https://en.wikipedia.org/wiki/DNS_label#Parts_of_a_domain_name)** is a single component of a [domain name](https://en.wikipedia.org/wiki/Domain_name) string, delimited by periods. For instance, \"*www*\", “*microsoft*”, and “*com*” are the three labels in the domain name “*www.microsoft.com*”.\n\nA **plain hostname** is an unqualified, single-label hostname like \"*payroll*\", which typically refers to a server on a local intranet.\n\nA **[Public Suffix](https://publicsuffix.org/)** is the suffix portion of a FQDN under which independent entities may register subdomains. For example, *ltd.co.im* is a Public Suffix. A Public Suffix contains one or more labels. Sometimes the term \"[effective TLD](https://wiki.mozilla.org/Gecko:Effective_TLD_List)\" is used as a synonym.\n\n<a name=\"registrabledomain\"></a>\nThe **registrable domain** is the public suffix plus one additional label. Sometimes eTLD+1 is used as a synonym.\n\n## A Caveat on Security Sensitive Surfaces\n\nIt should be noted that *many* displays of URLs in the web platform occur on surfaces [not deemed securable](https://docs.google.com/document/d/11-SXwzCGBlk8q1cNtb7peZjb2UjRPrKSFhOfZhTOz24/edit#).\n\nAny URL displayed below the browser’s \"[line of death](https://textslashplain.com/2017/01/14/the-line-of-death/)\" is usually *inherently* spoofable (insofar as web content can usually fake the entire UI). In particular, this means that UI helpers like the [Status Bubble](https://dev.chromium.org/user-experience/status-bubble#TOC-Lack-of-Security) are inherently untrustworthy.\n\n## Out-of-Scope for this Document\n\n### Dynamic Displays\n\nThis document focuses on best practices for \"static\" display of URLs: that is, the display of a single URL.\n\nURL spoofing vulnerabilities also include \"dynamic\" spoofing attacks that rely upon the browser failing to update a URL display (or failing to do so in a timely manner) based on an asynchronous operation.\n\nIf your UI allows for asynchronous operations (e.g., user can interact with one context while another loads), care must be taken to avoid mismatch between the \"current\" and the “loading” context.\n\nThe most common dynamic attacks rely upon the fact that navigation and script execution are typically asynchronous operations.\n\n* An attacker starts on a page with a trusted URL and then navigates that context to malicious content. The user sees the outdated trusted URL and interacts with the malicious content, believing that it is under the control of the trusted site.\n\n* Alternatively, the attack starts on a malicious page which begins a navigation to a trusted URL. The user sees the pending trusted URL and interacts with the malicious content believing it is under the control of the trusted site.\n\nDynamic attacks are particularly prevalent and challenging to mitigate in UI surfaces used both for URL input *and* display, such as the omnibox.\n\nAn additional complexity arises from the fact that some forms of navigation (those which result in a HTTP/204 response or a file download) are not expected to update the URL shown in the omnibox. As a consequence, the precise timing of updates to the URL shown in the omnibox is a critical factor in the security of the system and a common source of vulnerability. Chrome has been discussing possible UI treatments to more clearly differentiate pending and committed URLs in [crbug.com/719856](https://crbug.com/719856).\n\nHowever, because dynamic attacks have little in common with static attacks and are generally very tightly tied to the behavior of the URL display surface, they are out of scope for this document.\n\n### Misleading Domain Names\n\nThe web security model expects the user to recognize which hosts are backed by legitimate organizations (`google.com`) and which are not (`google-update.com`).\n\nFor the purposes of this document, we do not concern ourselves with the ability of a website to obtain a misleading domain name that is visually distinctive from a legitimate domain name.\n\nThere exist features, like Safe Browsing and EV certificate display, which may act as a supplement to origin display, but these features are not available in all situations and are often \"best effort.\" Replacing or augmenting human analysis of origins remains an active topic of research and [brainstorming](https://medium.com/@owencm/rethinking-url-bars-as-primary-browser-ui-e2118339d2c0).\n\n### \"Proxy\" Domain Names\n\nIn some cases, a domain owner is willing to supply content from a third-party within their own address space, leading to potential confusion about the ownership and source of the content displayed.\n\nThis is, generally, outside of the client threat-model, although in some cases (e.g., AMP), the client platform may attempt to introduce new UI to clarify the situation.\n\n## Testing URL Displays\n\nChromium's open-source [Trickuri](https://github.com/chromium/trickuri) tool is a Go-based proxy server designed to enable manual testing of URL display behavior.\n\n## Further Reading\n\n* [The Trouble with URLs](https://docs.google.com/presentation/d/1Nr47m1qlLjV8xZfw03jVKU3GQpSwbC0PWLHhXcyMcQM/) (LocoMoco Sec Conference talk)\n\n* [Chris Palmer’s Problems of URLs](https://noncombatant.org/2017/11/07/problems-of-urls/)\n\n* [Rethinking URL bars](https://medium.com/@owencm/rethinking-url-bars-as-primary-browser-ui-e2118339d2c0)\n\n* [Chromium’s URL Formatter Component (C++)](https://cs.chromium.org/chromium/src/components/url_formatter/url_formatter.cc)\n\n* [URL interop issues across specs](https://github.com/bagder/docs/blob/master/URL-interop.md)\n"
  },
  {
    "path": "security/safe_browsing/safe_browsing_navigation",
    "title": "How Safe Browsing Lookup Interacts with Navigation",
    "content": "# How Safe Browsing Lookup Interacts with Navigation\n\n## Overview\n\nDuring navigation, Chrome checks the Safe Browsing reputation of each URL and\ndecides whether to show a warning to the user. This document describes how Safe\nBrowsing lookup interacts with navigation and how Safe Browsing lookups affect\nthe speed of navigation.\n\n## Background\n\nWhen a user navigates to a URL, Chrome checks the Safe Browsing reputation of\nthe URL before the URL is loaded. If Safe Browsing believes that the URL is\ndangerous, Chrome shows a warning to the user:\n\n![warning page](warning_screenshot.png)\n\nChrome can perform three types of Safe Browsing checks during navigation:\n\n*   The hash-prefix database check\n    ([HPD](https://developers.google.com/safe-browsing/v4/update-api)).\n*   The URL real-time check\n    ([URT](https://source.chromium.org/chromium/chromium/src/+/main:components/safe_browsing/core/browser/realtime/)).\n*   The hash-prefix real-time check\n    ([HPRT](https://developers.google.com/safe-browsing/reference)).\n\nOnly the HPD check is on the blocking path of navigation. Before the check is\ncompleted, the navigation is not committed, the page body is not read by the\nrenderer, and the user won’t see any page content in their browser.\n\nReal-time checks (URT and HPRT) take longer than HPD checks. To ensure smooth\nuser experience, they don't delay navigation. This means the navigation can\nproceed before the check is completed and the user may see the page before the\nwarning is shown. When real-time checks are enabled, an additional HPD check is\nadded to ensure users are still protected against threats like exploits against\nthe browser.\n\nNOTE: There is another type of Safe Browsing check called Client Side Phishing\nDetection (CSD). It also checks the reputation of the page. However, this check\nis performed after the navigation is committed and it doesn’t block the\nnavigation, so it is out-of-scope for this doc.\n\n## Navigation Basics\n\n[Life of a Navigation](https://chromium.googlesource.com/chromium/src/+/main/docs/navigation.md)\ngives a high level overview of a navigation from the time a URL is typed in the\nURL bar to the time the web page is completely loaded. It breaks down a frame\nnavigation into two phases:\n\n*   **Navigation phase**: From the time the network request is sent to the time\n    the a navigation is committed. Note that at this point, nothing is rendered\n    on the page.\n    *   Any of the three Safe Browsing checks above may be performed in this\n        phase, depending on user consent. The URT check is only performed if the\n        user has agreed to share URLs with Google. The HPRT check is used in\n        most other scenarios, but in incognito mode or other cases when the\n        real-time checks are unavailable, the HPD check will be performed\n        instead.\n*   **Loading phase**: Consists of reading the response body from the server,\n    parsing it, rendering the document so it is visible to the user, executing\n    any script, and loading any subresources (images, scripts, CSS files)\n    specified by the document.\n    *   Safe Browsing doesn't check subresources, iframes or websocket\n        connection.\n\n[Navigation Concepts](https://chromium.googlesource.com/chromium/src/+/refs/heads/main/docs/navigation_concepts.md)\ncovers a set of important topics to understand navigation, such as:\n\n*   Same-document and cross-document navigation. Same-document navigation keeps\n    the same document and changes states associated with it. Some examples of\n    same-document navigation are fragment navigation\n    (`https://foo.com/1.html#fragment`) and using the\n    [history.pushState](https://developer.mozilla.org/en-US/docs/Web/API/History/pushState)\n    API.\n    *   Same-document navigation can change the URL, but Safe Browsing doesn’t\n        check these navigation. Same-document navigation doesn’t pose a security\n        risk because no new content is loaded from the network.\n*   Server redirects and client redirects. A server redirect happens when the\n    browser receives a 300-level HTTP\n    [response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#redirection_messages)\n    before the document commits, telling it to request a different URL, possibly\n    cross-origin. A client redirect happens after a document has been committed,\n    when the HTML in the document instructs the browser to request a new\n    document (e.g., via\n    [meta tags](https://www.w3schools.com/tags/att_meta_http_equiv.asp) or\n    [JavaScript](https://www.w3schools.com/howto/howto_js_redirect_webpage.asp)).\n    *   Redirect URLs are all checked by Safe Browsing. Server redirects are\n        checked in the navigation phase and client redirects are checked after a\n        document is committed.\n\n## Workflow\n\n![workflow](safe_browsing_navigation_flowchart.png)\n\nAs illustrated above, only the HPD check delays navigation in the navigation\nphase. It blocks the navigation before it is committed. The check needs to\nfinish checking all URLs (including redirect URLs) before committing the\nnavigation. If one of the URLs (initial URL or redirect URLs) is classified as\ndangerous, a warning page will be shown and the navigation will be canceled.\n\nFor URT and HPRT checks, they don't delay navigation. If they find any URL to be\ndangerous, a warning will be triggered immediately.\n\nAll three checks are initiated from the browser process.\n\n## Speed\n\nSafe Browsing checks and network requests are performed in parallel. Performing\na Safe Browsing check doesn’t block the start of network requests or the fetch\nof response header and body. It doesn’t block redirects either.\n\nHowever, completion of the HPD check does block the browser from reading or\nparsing the response body. When the response header is received, the HPD check\nwill block the navigation if the check is not completed.\n\nHPD check won’t slow down the navigation if it is completed before the response\nheader is received. If the HPD check is not completed at this point, the\nresponse body will still be fetched but the renderer won’t read or parse it.\n\nSafeBrowsing.BrowserThrottle.TotalDelay2 is the metric to measure the speed of\nSafe Browsing checks. 0 means that the Safe Browsing check is completed before\nthe response header is received -- it doesn't delay the navigation.\n\nReal-time checks, on the other hand, don't affect the loading speed.\n\n## Implementation Details\n\nSafe Browsing blocks navigation by implementing the\n[URLLoaderThrottle](https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/public/common/loader/url_loader_throttle.h;l=43;drc=0e45c020c43b1a9f6d2870ff7f92b30a2f03a458)\ninterface. This interface provides several phases to defer URL loading:\n\n*   `WillStartRequest(request, defer)`\n*   `WillRedirectRequest(request, defer)`\n*   `WillProcessResponse(request, defer)`\n\nThe throttle can mark `defer` as true if it wants to defer the navigation and\ncan call Resume to resume the navigation.\n\nThe throttle class is\n[BrowserUrlLoaderThrottle](https://source.chromium.org/chromium/chromium/src/+/main:components/safe_browsing/content/browser/browser_url_loader_throttle.h;drc=67847e1d488161fcc71acdfd3b77e1654f6e6121).\nThe throttle only marks `defer` as true in\n[WillProcessResponse](https://source.chromium.org/chromium/chromium/src/+/main:components/safe_browsing/content/browser/browser_url_loader_throttle.cc;l=489;drc=710d8b1f851677cbdafb8f14d0a5bba26066aebe).\n\nSafe Browsing doesn’t defer navigation forever. The current timeout is set to 5\nseconds. If the check is not completed in\n[5 seconds](https://source.chromium.org/chromium/chromium/src/+/main:components/safe_browsing/core/browser/safe_browsing_lookup_mechanism_runner.cc;l=14;drc=615be57df122442ad3c09558b7d7a7b8495c2360),\nthe navigation will resume.\n"
  },
  {
    "path": "security/research/README",
    "title": "Security Research Notes",
    "content": "# Security Research Notes\n\nThis directory contains security research notes about parts of Chromium of particular interest to attackers.\n\nThe notes represent our understanding of a particular area of functionality at time of publishing, which we know is often incomplete, can become stale as code evolves, and may accidentally contain inaccuracies.\n\nWe publish these notes to\n1. Preserve our understanding of areas of interest to security so we can refresh our memory of complex features after visiting other topics.\n2. Give new team members a learning resource.\n3. Boost productivity for external researchers making contributions to the [Chrome Vulnerability Rewards Program](https://www.chromium.org/Home/chromium-security/vulnerability-rewards-program/).\n"
  },
  {
    "path": "security/research/graphics/webgpu_technical_report",
    "title": "WebGPU Technical Report",
    "content": "# WebGPU Technical Report\n\nAuthors: [tiszka@chromium.org](mailto:tiszka@chromium.org),\n[bookholt@chromium.org](mailto:bookholt@chromium.org),\n[mattdr@chromium.org](mailto:mattdr@chromium.org)\n\n## Chrome Graphics as Seen By Attackers\n\nIn this document we outline how WebGPU works through the mind of an attacker,\nour vulnerability research methodologies, and our thought processes in some of\nthe more difficult research areas. There are many interesting portions of Chrome\ngraphics that we omitted from review to keep scope manageable. While our primary\nfocus was WebGPU, we did explore a few attack surfaces shared by other graphics\nfeatures. We will interleave background information on WebGPU with descriptions\nof the important bugs we found. We hope this report will give the security\ncommunity a deeper understanding of the shape of vulnerabilities we may come to\nexpect with the addition of WebGPU, along with a lens into the vulnerabilities\nwe might encounter in the future.\n\nThe graphics stack has long been an area of interest for Chrome Security. Before\nwe dive into WebGPU internals, consider the diagram below showing a simplified\nview of the Chrome graphics architecture.\n\n![image](resources/chromeoffensiv--nb3icxsvqik.png)\n\nShow above: Attackers' perspective of Chrome graphics.\n\nThe Chrome process model uses sandboxing to create layered security boundaries\nbetween untrusted content from the web and protected user data. However, the\nrapid evolution and high complexity of Chrome's accelerated graphics features\ncoupled with their need to interface directly with drivers in the kernel, as\nwell as their implementation in memory-unsafe languages mean bugs in graphics\ncode are especially useful for bypassing Chrome sandbox boundaries. Furthermore,\nalthough Chrome sets the industry standard for rapidly fixing security bugs and\nquickly shipping updates to users, the presence and exposure of code supported\nby third parties creates challenges to getting fixes to users rapidly that can\nlengthen the period when a vulnerability may be viable for exploitation,\nreducing the cost attackers must bear to sustain a capability.\n\n## Enter WebGPU\n\nWebGPU entered Origin Trial in mid-2022 marking the first time web developers\nand users got to experience the new features. Coincidentally, the Chrome\nOffensive Security team decided to look into WebGPU as our first major\nresearch target.\n\nAccording to the [WebGPU spec](https://www.w3.org/TR/webgpu/), \"WebGPU exposes\nan API for performing operations, such as rendering and computation, on a\nGraphics Processing Unit\". Unlike WebGL, its predecessor that set out with\nsimilar goals, WebGPU isn't an existing native API ported to the Web; WebGPU is\na new API designed to surface the functionality of existing graphics APIs like\nVulkan, Metal, and Direct3D. In the context of this document we will only be\ndiscussing Vulkan as it is ubiquitously reachable on every platform that WebGPU\nsupports either through the GPU rendering pipeline or the software rendering\npipeline.\n\nWebGPU introduces two unique attack surfaces to Chrome that will come with their\nown challenges:\n\n+   the WebGPU API Implementation which was added to the GPU process & renderer\n    process; and\n+   the WGSL shader compiler added to the GPU process\n\nWhile they are related and shader compilation is accessible via web-exposed\nAPIs, they pose two unique challenges so we will dig into both attack surfaces\nseparately.\n\nTo give you the big picture first, the diagram below shows the slice of the\nChrome graphics stack required for WebGPU. While WebGPU has many pieces and\ninter-connections, we omitted a great many notable portions of Chrome's graphics\nattack surface, including WebGL, Skia, Canvas2D, Widevine DRM, and video\ndecoding for the sake of avoiding complexity explosion.\n\n![image](resources/chromeoffensiv--25zv8cth637j.png)\n\nShown above: The full Chrome WebGPU stack.\n\n## WebGPU API\n\nThe **WebGPU** API is exposed via JavaScript which calls into **Dawn**, the\nlibrary within Chrome that implements WebGPU.\n\n**Dawn** is separated into two different libraries: **Dawn Wire** and **Dawn\nNative**. **Dawn Wire** is a client-server implementation of **WebGPU**. When a\nWebGPU API call is made from JavaScript the request is serialized in the\nrenderer process using the **Dawn Wire Client**, the serialized blob is passed\nto the GPU process using WebGPU extensions to the Chrome GPU Command Buffer\n([`WebGPUDecoderImpl`](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/service/webgpu_decoder_impl.cc;l=1768;drc=34ba1d95a41c614308175e932a2b121018891bbf))\n, and then deserialized in the GPU process by **Dawn Wire Server**. **Dawn Wire\nServer** then calls into **Dawn Native** which is the \"native\" implementation of\nWebGPU that wraps the underlying platform's GPU APIs.\n\nThis portion of the review focused on the WebGPU API implementation from\n**Blink** to **Dawn Backends**. We also chose to scope our review to Dawn's\n**Vulkan Backend** because it is reachable on every WebGPU platform and it is\nthe only platform that's fuzzable with ClusterFuzz since most of the Vulkan\nBackend code can be exercised without a physical GPU.\n\n![image](resources/chromeoffensiv--6mue8ablsri.png)\n\nShown above: The subset of the Chrome WebGPU stack we focused on during this\nportion of the review, with out-of-scope portions de-emphasized in white.\n\n### Finding: Incorrect State Tracking in Dawn Native leads to UAF\n\n> _**tl;dr - Systemic Concerns**_\n>\n> Dawn has a pattern where objects hold a raw pointer to reference counted\n> objects, assuming a reference is held elsewhere. This assumption can easily\n> break with future changes to the code as we've seen in the browser process\n> with Mojo handlers. Dawn should discourage this pattern to reduce\n> use-after-free bugs.\n\nInteracting with WebGPU begins with requesting an `adapter` which is an object\nwrapping a single instance of WebGPU and then a `device` which is a logical\ninstantiation of the `adapter`.\n\n```js\nconst gpuAdapter = await navigator.gpu.requestAdapter();\nconst gpuDevice = await gpuAdapter.requestDevice();\n\n/* Call WebGPU APIs */\nlet buffer = gpuDevice.createBuffer();\n```\n\nAs shown in the picture below, under the covers, `gpuDevice.createBuffer`\ncreates an Oilpan managed **WebGPU Buffer** object in Blink that holds a raw\npointer and a reference to a **Dawn Wire Client Object**.\n\nThis **Dawn Wire Client Object**, which lives in the renderer process, holds a\nreference to a **Dawn Wire Server Object**, which lives in the GPU process,\nimplicitly incrementing and decrementing the reference count by sending a\n`wgpuCreateObject` on construction and `wgpuDestroyObject` on destruction over\nIPC to the GPU process.\n\nThis **Dawn Wire Server Object** holds a reference to the **Dawn Native\nObject**. Finally, the **Dawn Native Object** holds a raw pointer to the\nunderlying Vulkan Object (or other graphics API platform object on non-Vulkan\nplatforms.)\n\n![image](resources/chromeoffensiv--dxtg69vpyxl.jpg)\n\nThrough this long chain of reference counted objects we hold a pointer to a\nresource in the **Usermode Graphics Driver (UMD)** through our Oilpan managed\n`gpuBuffer` object in JavaScript. This is a lot of state to track!\n\nInterestingly, this means that it's possible to drop references and free objects\nin the GPU process from an uncompromised renderer by garbage collecting the\ncorresponding WebGPU object in the renderer process.\n\n```js\nconst gpuAdapter = await navigator.gpu.requestAdapter();\nconst gpuDevice = await gpuAdapter.requestDevice();\n\nlet buffer = gpuDevice.createBuffer();\nbuffer = null;\ngc();\n```\n\nUnder the covers, the destruction of an Oilpan object drops a reference to its\nDawn Wire Client object which when destructed sends a `wgpuDestroyObject` IPC\ncommand to the GPU process.\n\n![image](resources/chromeoffensiv--y3qar9s40vd.jpg)\n\nSituations can arise where multiple objects within Dawn Native hold references\nto the same object, so this destruction won't actually free the Dawn Native\nBuffer.\n\n![image](resources/chromeoffensiv--m5a1m5mf2h.jpg)\n\nWhen we began auditing these references we checked for many of the \"classic\"\nreference counting implementation issues. For example, sending multiple\n`wgpuDestroyObject` commands from a compromised renderer does not allow the\ncompromised renderer to decrement the reference indefinitely. Reference counted\nobjects use [64 bit integers\n](https://source.chromium.org/chromium/chromium/src/+/refs/heads/main:third_party/dawn/src/dawn/common/RefCounted.cpp;l=69;drc=76be2f9f117654f3fe4faa477b0445114fccedda)for\ntracking on all architectures which prevents integer overflow style bugs.\nHowever, we did come across instances where raw pointers were being held without\ntaking a reference to the reference counted pointer.\n\n![image](resources/chromeoffensiv--x4oroth0usm.jpg)\n\n#### What's happening inside WebGPU?\n\nWebGPU gives developers an API to queue up operations and then run them in\nbatches using modern graphics APIs. Under the hood, a lot goes on to make this\nwork. The diagram below shows the simplified life cycle of creating and running\na compute shader.\n\n![image](resources/chromeoffensiv--gms7nqwczp.png)\n\nThe Dawn Native `GPUCommandBuffer` object, created by the step highlighted in\n<span style=\"background-color:blue\">Blue</span>, holds a pre-recorded set of\ncommands that can then be executed at an arbitrary time. Herein lies the magic\nof WebGPU! It's possible to queue up thousands of GPU compute jobs and execute\nthem asynchronously.\n\n> **_Note_**: The WebGPU\n[`GPUCommandBuffer`](https://www.w3.org/TR/webgpu/#gpucommandbuffer) is\ncompletely unrelated to the Chrome [GPU Command\nBuffer](https://www.chromium.org/developers/design-documents/gpu-command-buffer).\nThis is an unfortunate name collision. The `GPUCommandBuffer` is a WebGPU object\nand the Chrome GPU Command Buffer is a mechanism for communicating over shared\nmemory with the GPU process.\n\n```js\nconst commandEncoder = device.createCommandEncoder();\n\n// Encode commands for copying buffer to buffer.\ncommandEncoder.copyBufferToBuffer(\n  source_buffer, /* source buffer */\n  0, /* source offset */\n  dest_buffer, /* destination buffer */\n  0, /* destination offset */\n  10 /* size */\n);\n\n// Create a GPUCommandBuffer\nconst gpuCommandBuffer = commandEncoder.finish();\n...\n// Execute the GPU commands asynchronously\ndevice.queue.submit([gpuCommandBuffer, gpuCommandBuffer]);\n```\n\nThe same interface is used to create **compute pipelines**. These pipelines\nfacilitate shader execution and create `GPUComputePassEncoder` objects which\nhold references to objects - `GPUBuffer`s, `GPUTexture`s, etc - that the GPU\ncompute shaders will be modifying during execution.\n\n```js\nconst commandEncoder = device.createCommandEncoder();\n\nconst passEncoder = commandEncoder.beginComputePass();\npassEncoder.setPipeline(computePipeline);\npassEncoder.dispatchWorkgroups(1, 1);\npassEncoder.end();\n\n\nconst gpuCommand = commandEncoder.finish();\n...\n// Execute the GPU commands asynchronously\ndevice.queue.submit([gpuCommand, gpuCommand]);\n```\n\nUnder the covers, the `GPUCommandBuffer` holds references to **Dawn Native**\nobjects (in the example above the `source_buffer` and `dest_buffer`). A lot can\nhappen during execution of a sequence of commands within the `GPUCommandBuffer`\n- `wgpuDispatchWorkGroups` is used to execute shaders, `wgpuCopyBufferToBuffer`\nis used to copy one GPU buffer's content to another, `wgpuSetBindGroup` can be\nused to change the bindings that a compute job is executing on - so it's very\nimportant that the objects the `GPUCommandBuffer` holds references to are not\nde-allocated until after the execution of the compute pipeline.\n\nHowever, there are areas in **Dawn** where the code holds raw pointers with the\nassumption that a reference is already held to an object such as at [1] in the\nexcerpt below.\n\n```cpp\n// Used to track operations that are handled after recording.\n// Currently only tracks semaphores, but may be used to do barrier coalescing in the future.\nstruct CommandRecordingContext {\n    ...\n    // External textures that will be eagerly transitioned just before VkSubmit.\n    // The textures are kept alive by the CommandBuffer so they don't need to be Ref-ed.\n    std::set<Texture*> externalTexturesForEagerTransition;\n\n    std::set<Buffer*> mappableBuffersForEagerTransition; // [1]\n    ...\n};\n```\n\n#### The Bug\n\nHerein lies a bug, and likely a bug pattern that could cause issues in the\nfuture. An assumption was made that raw pointers could not be added to\n`mappableBuffersForEagerTransition` outside of `GPUCommandBuffer` execution. The\ncode also assumes that references would not be dropped within `GPUCommandBuffer`\nexecution.\n\nWithin Buffer initialization, there was a branch that called the function\n`ClearBuffer` [1] if the size of the buffer being created was unaligned.\n\n```cpp\nMaybeError Buffer::Initialize(bool mappedAtCreation) {\n  if (device->IsToggleEnabled(Toggle::LazyClearResourceOnFirstUse) && !mappedAtCreation) {\n    uint32_t paddingBytes = GetAllocatedSize() - GetSize();\n    if (paddingBytes > 0) {\n      CommandRecordingContext* recordingContext = device->GetPendingRecordingContext();\n      // [1]\n      ClearBuffer(recordingContext, 0, clearOffset, clearSize);\n    }\n  }\n}\n```\n\nThe `ClearBuffer` call leads to many other state changing effects and function\ncalls. One of those code paths adds a Buffer's raw pointer to\n`mappableBuffersForEagerTransition`.\n\n![image](resources/chromeoffensiv--umacwcr8c1p.jpg)\n\nThis `TrackResourceAndGetResourceBarrier` call occurs outside of WebGPU\n`GPUCommandBuffer` command execution, which is unexpected, so the only other\nreference to the **Dawn Native Buffer** is the reference from the renderer\nprocess.\n\nFrom here it was possible to drop all other references to the **Dawn Native\nBuffer** object in the GPU process held from the renderer process by garbage\ncollecting the WebGPU JavaScript buffer object, leading to a use-after-free the\nnext time `mappableBuffersForEagerTransition` was iterated.\n\nPointer lifetimes are difficult to get right. Taking a closer look at this\nvulnerability we see that there are other raw pointers. These _appeared_ to be\nsafe, but they could easily be turned into vulnerabilities by future changes to\n**Dawn**.\n\n```cpp\n// Used to track operations that are handled after recording.\n// Currently only tracks semaphores, but may be used to do barrier coalescing in the future.\nstruct CommandRecordingContext {\n    ...\n    // External textures that will be eagerly transitioned just before VkSubmit.\n    // The textures are kept alive by the CommandBuffer so they don't need to be Ref-ed.\n    std::set<Texture*> externalTexturesForEagerTransition;\n\n-    std::set<Buffer*> mappableBuffersForEagerTransition;\n+    std::set<Ref<Buffer>> mappableBuffersForEagerTransition;\n\n    ...\n};\n```\n\nAs the diff above shows, the fix was to add reference counting to accurately\ntrack the Buffer life cycle. It appears that this vulnerability was introduced\nbecause assumptions were made about `Buffer` lifetimes based on the earlier\ncomment about `GPUTexture` lifetimes. This shows us a problem: even when this\npattern is used <ins>correctly</ins>, it may too easily encourage other\n<ins>incorrect</ins> uses. It is hard to verify that the raw pointers in\n`externalTexturesForEagerTransition` aren't vulnerable in a similar way. It is\nprobably safer to avoid raw pointers altogether when working with **Dawn Native\nObjects**.\n\n### Finding: Unexpected State Change Before Callback leads to UAF\n\n> _**tl;dr - Systemic Concerns**_\n>\n> WebGPU implements callbacks in the GPU process.Similar patterns in Mojo and\n> JavaScript have consistently caused high severity issues in Chrome over the\n> years. We believe a high bar of scrutiny should be applied to changes within\n> existing **Dawn** callback handlers and for any new callback handlers being\n> added to **Dawn**. Increasing complexity in this area would likely have a high\n> cost to Chrome Security.\n\nWebGPU was built to offload work from the CPU to the GPU. GPU execution is\nasynchronous, so WebGPU was built to be entirely asynchronous. In the bug above\nwe learned that Dawn `GPUCommandBuffer` execution can execute\n`GPUComputePipelines`. For example, `GPUComputePipelines` contain shader\nprograms that have no guarantees on when they terminate.\n\n```\n// WGSL Script\nfn main() {\n  loop {}\n}\n```\n\nGPU Drivers implement Fences to signal the completion of GPU work. These Fences\nare polled on every logical `wgpuTick` within **Dawn**. Once the work on the GPU\ncompletes, **Dawn** will execute a callback in the GPU process that will then\nchange state within the GPU process and send any results to the renderer process\nusing **Dawn Wire**.\n\n![image](resources/chromeoffensiv--vcq2rype4h7.jpg)\n\nThis creates a point of reentrancy during callback execution in `wgpuTick` when\nthe pending callbacks are executed. State can change in unexpected ways during\ncallback execution within `wgpuTick` and state can change in unexpected ways\nbefore callback execution. This creates room for bugs similar to the classic\nJavascript engine callback bugs that we've seen in the\n[browser](https://googleprojectzero.blogspot.com/2019/04/virtually-unlimited-memory-escaping.html)\nand [renderer](https://tiszka.com/blog/CVE_2021_21225.html) processes.\n\n![image](resources/chromeoffensiv--594gh2o328e.jpg)\n\nLuckily, as of May 2023, there aren't that many asynchronous calls in WebGPU and\nthese callbacks do not introduce unbounded re-entrancy (i.e. it is not possible\nto call `ApiTick` within an `ApiTick`).\n\n![image](resources/chromeoffensiv--suavcw9636c.jpg)\n\n#### The Bug\n\nThe bug we're looking at occurred because of an unexpected state change between\ncallback registration and callback execution. WebGPU registers a callback\nhandler that executes whenever an error is encountered.\n\n```cpp\nvoid Server::SetForwardingDeviceCallbacks(ObjectData<WGPUDevice>* deviceObject) {\n    ...\n    mProcs.deviceSetUncapturedErrorCallback(\n        deviceObject->handle,\n        [](WGPUErrorType type, const char* message, void* userdata) {\n            DeviceInfo* info = static_cast<DeviceInfo*>(userdata);\n            info->server->OnUncapturedError(info->self, type, message);\n        },\n        deviceObject->info.get()); // [1.a]\n    ...\n}\n```\n\nA raw pointer to the `WGPUDevice`'s **Object's** `userdata` is fetched and\npassed to the callback [1.a], which later stores the saved pointer into\n`mUncapturedErrorUserdata` [1.b].\n\n```cpp\nvoid DeviceBase::APISetUncapturedErrorCallback(wgpu::ErrorCallback callback, void* userdata) {\n    if (IsLost()) { // [2]\n        return;\n    }\n    FlushCallbackTaskQueue();\n    mUncapturedErrorCallback = callback;\n    mUncapturedErrorUserdata = userdata; // [1.b]\n}\n```\n\nWhen a Dawn Wire Server `GPUDevice` object is freed, `mUncapturedErrorCallback`\nis set to null.\n\n```cpp\nvoid Server::ClearDeviceCallbacks(WGPUDevice device) {\n    ...\n    mProcs.deviceSetUncapturedErrorCallback(device, nullptr, nullptr);\n    ...\n}\n```\n\nHowever if the Device is put into a \"Lost\" state [2] after 1.a and before 1.b\nwhen the `ClearDeviceCallbacks` is called it will not be nulled out, leading to\na dangling pointer. This creates room for an attacker to send a\n`wgpuBufferDestroy` command to Dawn Wire Server before the callback is executed.\n\n```cpp\nvoid DeviceBase::APISetUncapturedErrorCallback(wgpu::ErrorCallback callback, void* userdata) {\n    if (IsLost()) { // [2]\n        return;\n    }\n    FlushCallbackTaskQueue();\n    mUncapturedErrorCallback = callback;\n    mUncapturedErrorUserdata = userdata; // [1.b]\n}\n```\n\nAfter that the attacker can clear all references to the `WGPUDevice`, freeing\nthe userdata leading to a dangling pointer. On the next `wgpuTick`, if an error\ncallback is invoked it will lead to `mUncapturedErrorUserdata` being\ndereferenced, causing a use-after-free (UAF).\n\nThis leads to the proof of concept below that uses the trick we mentioned\nearlier where Garbage Collected objects created from JavaScript in the renderer\nprocess can be used to drop a single reference to a Dawn Wire Server object in\nthe GPU process, opening the door for the use-after-free.\n\n```js\nasync function trigger() {\n    let adapter1 = await self.navigator.gpu.requestAdapter({\n        forceFallbackAdapter: true\n    });\n    let device1 = await adapter1.requestDevice();\n\n    // Request a second device.\n    let adapter2 = await self.navigator.gpu.requestAdapter({\n        forceFallbackAdapter: true\n    });\n\n    let buffer1 = device1.createBuffer(\n        { mappedAtCreation: false,\n          size: 128, usage:\n          GPUBufferUsage.UNIFORM });\n\n    // Set Device::mState to State::kDestroyed.\n    device1.destroy();\n\n    // Trigger an error by unmapping a buffer on a destroyed device,\n    // which queues up an error callback\n    buffer1.unmap();\n\n    // Trigger GC to drop the renderer's reference to device, and free it\n    buffer1 = null;\n    adapter1 = null;\n    device1 = null;\n    try { new ArrayBuffer(31 * 1024 * 1024 * 1024); } catch(e) {}\n\n    // Flush. Trigger UAF.\n    await adapter2.requestDevice();\n}\n```\n\n### Finding: Multiple vulnerabilities in WebGPU use of GPU Command Buffer\n\n> _**Our Concerns - The Short Version**_\n>\n>The Chrome Command Buffer is prone to input validation issues, has many legacy\n>undocumented footguns, and is difficult to fuzz effectively. Manual auditing is\n>currently the best way to discover bugs in this area of the codebase. Snapshot\n>fuzzing could help solve this problem.\n\n**Dawn Wire** is a serialization/deserialization library. **Dawn Wire** does not\nimplement IPC mechanisms that can be used to transfer data between processes in\nChrome. Instead, within Chrome, **Dawn Wire** is built on top of the existing\n[Chrome Command\nBuffer](https://www.chromium.org/developers/design-documents/gpu-command-buffer/)\narchitecture to facilitate inter-process communication between the Renderer and\nGPU processes. One of the WebGPU-specific GPU Command Buffer [IPC\nhandlers](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/service/webgpu_decoder_impl.cc;l=1768;drc=34ba1d95a41c614308175e932a2b121018891bbf)\nreceives serialized **Dawn Wire** data over shared memory and deserializes and\nexecutes it using **Dawn Wire Server.**\n\n```cpp\nerror::Error WebGPUDecoderImpl::HandleDawnCommands(...) {\n  if (!wire_server_->HandleCommands(shm_commands, size)) {\n    return error::kLostContext;\n  }\n  ...\n}\n```\n\n![image](resources/chromeoffensiv--ttzz54pb83l.jpg)\n\nWebGPU improved on the `GLES2CommandBuffer` implementation in many ways. For\nexample, the `GLES2CommandBuffer` has been plagued with\ntime-of-check/time-of-use (TOCTOU) [vulnerabilities](https://crbug.com/1422594)\n[that](https://crbug.com/597636) [come](https://crbug.com/597625)\n[with](https://crbug.com/468936) working directly on shared memory that can be\nconcurrently modified by a compromised renderer process. In direct response to\nthis bug class, the WebGPU usage of the Chrome GPU Command Buffer and Dawn Wire\nServer always copy shared memory passed from the renderer process into a static\nheap-allocated buffer within the deserializer in the GPU process, before calling\ninto **Dawn Native**.\n\nThere are still a few other footguns to avoid when building on top of the Chrome\nGPU Command Buffer abstraction. [The](https://crbug.com/1373314)\n[vulnerabilities](https://crbug.com/1340654)\n[discovered](https://crbug.com/1393177) [in](https://crbug.com/1314754)\n[the](https://crbug.com/1406115) WebGPU usage of the **Chrome GPU Command\nBuffer** so far are good examples; such as not holding a `scoped_ptr` reference\nto a `TransferBuffer` while holding a raw_ptr to its shared memory and not\nvalidating buffer offsets/sizes received from a compromised renderer process.\n\nWhile these vulnerabilities are in WebGPU's implementation within Chrome, they\nare not unique to WebGPU.  The **Chrome GPU Command Buffer** had similar issues\nin 2013, and it is notoriously difficult to fuzz effectively, so we will likely\nintroduce similar bugs that reach stable with future abstractions that build on\nthe **Chrome GPU Command Buffer**.\n\n### More Bugs and Notes on WebGPU Implementation Complexity\n\n+   **WebGPU** was the first web-exposed user to back an `ArrayBuffer` with a\n    raw pointer. This led to [some](https://crbug.com/1336014)\n    [issues](https://crbug.com/1326210).\n\n+   The **WebGPU** specification states the `getMappedRange()` method returns an\n    `ArrayBuffer`. Within Chrome, this `ArrayBuffer` is backed by shared memory.\n    Concurrent modification of `ArrayBuffer` backing stores has led to\n    [multiple](https://crbug.com/1174582) security vulnerabilities. Fortunately,\n    it is not possible to modify the shared memory in the GPU process after the\n    `ArrayBuffer` is created. However, if that ever becomes possible in the\n    future it will be a security vulnerability.\n\n    +   Interestingly, this also means that we have a well-defined way to\n        compromise an uncompromised renderer that is colluding with a\n        compromised GPU process.\n\n+   Google do not control the underlying **Vulkan** implementation in the\n    various third party **Usermode Graphics Driver** that **Dawn** calls into.\n    Usermode Graphics Driver complexity could reach a point where it becomes\n    indefensible.\n\n+   Vulkan, Metal, and D3d are inherently insecure APIs. Dawn has the hefty\n    responsibility of validating user input before calling into these APIs.\n\n+   The current **Dawn** fuzzers -\n    [DawnWireServerFuzzer](https://source.chromium.org/chromium/chromium/src/+/main:third_party/dawn/src/dawn/fuzzers/DawnWireServerFuzzer.cpp)\n    and\n    [DawnLPMFuzzer](https://source.chromium.org/chromium/chromium/src/+/main:third_party/dawn/src/dawn/fuzzers/lpmfuzz/)\n    - fuzz the **Dawn** wire byte stream, and therefore all of the validation\n    and everything the validation is protecting.\n\n+   **Dawn** will one day be multithreaded, first as a standalone library and\n    then within Chrome. This will increase its complexity.\n\n## WebGPU Shaders\n\nThis section focuses on the portions of WebGPU that ingest and process shaders.\nRefer again to the high level picture below for an illustration of the\ncomponents of interest in this section.\n\n![image](resources/chromeoffensiv--fci4atgmk2e.png)\n\nShow above: The subset of the Chrome WebGPU stack we focused on during this\nportion of the review, with out-of-scope portions de-emphasized in white.\n\nThere is not much information out there about threats facing Chrome's existing\nshader compilers for **WebGL** shaders, or how Chrome currently defends against\nthem. **WebGPU** introduced a new shader compiler pipeline that is defended in a\nsimilar manner.\n\nWebGPU moves away from WebGL's GLSL shader language entirely and implements\n**WGSL**, a re-imagined high level shading language for the web.\n**[Tint](https://dawn.googlesource.com/tint)** is Google's translator for\n**WGSL**. **Tint** compiles **WGSL** into a platform dependent intermediate\nlanguage - **SPIR-V**, **HLSL**, **MSL** -  that the underlying Usermode\nGraphics Drivers will further compile.\n\n![image](resources/dawn_wgsl_pipeline.png)\n\nWith the addition of **WebGPU**, Chrome now has two front-end compilers in the\nGPU process that can compile some high-level language into **SPIR-V**: the\n**ANGLE Translator** for WebGL shaders (not discussed here) and **Tint** for\nWebGPU shaders. Interestingly, the **SPIR-V** emitted by **Tint** is not the\nsame subset of **SPIR-V** emitted by the **ANGLE Translator**. However, both\ncompilers end up passing their emitted **SPIR-V** to the same underlying\n**Usermode Graphics Drivers** for further backend compilation.\n\n![image](resources/chromeoffensiv--ct2njhd1x14.png)\n\n### Integer Overflow in SwiftShader JIT leads to out-of-bounds read/write\n\n> _**tl;dr - Systemic Concerns**_\n>\n> Vulnerabilities in the **SwiftShader JIT** compiler aren't being fixed in the\n> **SwiftShader** codebase. Instead they are fixed by translating away code\n> patterns using the higher-level front end compilers like the **ANGLE\n> Translator**. This has led to bug variants. Furthermore, ANGLE and Tint\n> sanitization happens on a representation of shaders that is distinct from the\n> representation used by SwiftShader and Usermode Graphics Drivers, creating\n> gaps in protection coverage. Finally, Chrome now has two front-end compilers\n> that pass compiled code to **SwiftShader** for further compilation making this\n> even more precarious.\n\nWe did dig into **SwiftShader's** shader execution pipeline. **SwiftShader**\nemulates an entire GPU stack - the **Vulkan** Implementation within the\n**Usermode Graphics Drivers**, shader compiler within the **Usermode Graphics\nDrivers**, and the GPU hardware these call into - all on the CPU.\n\nGPUs make heavy use of parallel shader computation. **SwiftShader** implemented\na **SPIR-V JIT compiler** to reach _near-GPU_ speeds that compiles to various\narchitectures (x86, x64, arm, arm64). After shader compilation, the JITTed code\nis executed on multiple threads to emulate a GPU executing shaders.\n\n#### SwiftShader's JIT\n\n**SwiftShader's** JIT compiler is built on the Reactor API which acts as a\ndomain specific language and interface to the underlying JIT compiler. Reactor\nemits LLVM-like IR which is then ingested by the JIT compiler backend for\nReactor,\n[Subzero](https://swiftshader.googlesource.com/SwiftShader/+/refs/heads/master/docs/Subzero.md).\n\n#### The Bug\n\nThe vulnerability is a classic integer overflow within a **SubZero**\noptimization that collates multiple\n[`alloca`](https://llvm.org/docs/LangRef.html#alloca-instruction) instructions\ninto a single `alloca` instruction.\n\n```cpp\nvoid Cfg::sortAndCombineAllocas(CfgVector<InstAlloca *> &Allocas,\n                                uint32_t CombinedAlignment, InstList &Insts,\n                                AllocaBaseVariableType BaseVariableType) {\n uint32_t CurrentOffset = 0; // [1]\n for (Inst *Instr : Allocas) {\n   auto *Alloca = llvm::cast<InstAlloca>(Instr);\n   uint32_t Alignment = std::max(Alloca->getAlignInBytes(), 1u);\n   auto *ConstSize =\n       llvm::dyn_cast<ConstantInteger32>(Alloca->getSizeInBytes());\n   uint32_t Size = Utils::applyAlignment(ConstSize->getValue(), Alignment);\n   CurrentOffset += Size; // [2]\n }\n uint32_t TotalSize =\n    Utils::applyAlignment(CurrentOffset, CombinedAlignment);\n\n Operand *AllocaSize = Ctx->getConstantInt32(TotalSize);\n InstAlloca *CombinedAlloca =\n InstAlloca::create(\n    this,\n    BaseVariable,\n    AllocaSize,\n    CombinedAlignment\n ); // [3]\n ...\n}\n```\n\n`CurrentOffset` is a 32 bit unsigned integer declared at [1]. By supplying a\nSPIR-V shader that generates enough large `alloca` nodes, it's possible for the\nrepeated addition at [2] to overflow the 32-bit unsigned integer, leading to an\nundersized `alloca` node being generated at [3].\n\n`alloca` instructions are later lowered to stack allocations for the actual\nvariables in the shader program. Reading and writing into an undersized stack\nallocation will lead to out-of-bounds reads/writes.\n\n#### SwiftShader JIT Bugs: Reachable from WebGPU and WebGL\n\nAs we mentioned earlier, both **WebGPU** and **WebGL** shaders are compiled to\n**SPIR-V** in **Vulkan** environments. SwiftShader implements the **Vulkan**\nGraphics API.\n\n![image](resources/chromeoffensiv--utbsiqyq43e.jpg)\n\nWe found a bug, but there are many many layers to dig through to figure out if\nthe bug is reachable. The **ANGLE Translator** will emit an\n[`spv::Op::OpVariable`](https://source.chromium.org/chromium/chromium/src/+/main:third_party/angle/src/common/spirv/spirv_instruction_builder_autogen.cpp;l=567;drc=bec40d7684688eaf8a5ca4747341dcea4243c996)\nSPIR-V instruction whenever it encounters a variable declaration within the\nWebGL SL it is compiling. **Tint** will also emit an\n[`spv::Op::OpVariable`](https://source.chromium.org/chromium/chromium/src/+/main:third_party/dawn/src/tint/writer/spirv/builder.cc;l=835;drc=9543f74739118a853dd5e5a46297f5442c3352f8)\n**SPIR-V** instruction whenever it encounters a variable declaration within the\nWGSL it is compiling.\n\n![image](resources/chromeoffensiv--hummy4droud.png)\n\nWhen the **SwiftShader SPIR-V compiler** encounters the\n[`spv::Op::OpVariable`](https://source.chromium.org/chromium/chromium/src/+/refs/heads/main:third_party/swiftshader/src/Pipeline/SpirvShader.cpp;l=1768;drc=004227a1fc7355a9080146c2621d072bd2327701)\ninstruction it will generate a Variable IR.\n\n![image](resources/chromeoffensiv--xef89t24uli.jpg)\n\nWhenever this Variable IR is being converted from Reactor IR into Subzero IR it\ncalls into\n[`allocateStackVariable()`](https://source.chromium.org/chromium/chromium/src/+/refs/heads/main:third_party/swiftshader/src/Reactor/Reactor.cpp;l=106;drc=004227a1fc7355a9080146c2621d072bd2327701)\nwhich emits a SubZero InstAlloca instruction.\n\n![image](resources/chromeoffensiv--i5ooq44jbrq.jpg)\n\n```cpp\nValue *Nucleus::allocateStackVariable(Type *t, int arraySize)\n{\n\tIce::Type type = T(t);\n\tint typeSize = Ice::typeWidthInBytes(type);\n\tint totalSize = typeSize * (arraySize ? arraySize : 1);\n\n\tauto bytes = Ice::ConstantInteger32::create(\n             ::context, Ice::IceType_i32, totalSize);\n\tauto address = ::function->makeVariable(T(getPointerType(t)));\n\tauto alloca =\n          Ice::InstAlloca::create(::function, address, bytes, typeSize); // [4]\n\t::function->getEntryNode()->getInsts().push_front(alloca);\n\n\treturn V(address);\n}\n```\n\n`allocateStackVariable()` generates the **SubZero** `InstAlloca` IR instruction\nthat `sortAndCombineAllocas` incorrectly optimizes.\n\n![image](resources/chromeoffensiv--86bdvm4zaei.jpg)\n\nWhen the assembly emitted by **SubZero** is executed on the CPU and the\nundersized allocation is read/written to, it leads to out-of-bounds memory\naccesses.\n\n#### The Fix\n\nSimilar to other bugs in shader compilers, this vulnerability is prevented by\nthe front-end compilers and no changes were made to **SwiftShader**. For those\nwho don't follow the bug tracker closely, looking closer at the\n[fix](https://chromium-review.googlesource.com/c/angle/angle/+/4377639) this is\na [variant](https://chromium-review.googlesource.com/c/angle/angle/+/4377639) of\na [variant](https://chromium-review.googlesource.com/c/angle/angle/+/3023033).\nInteger overflows keep popping up in shader compilers and\n[`ValidateTypeSizeLimitations()`](https://source.chromium.org/chromium/chromium/src/+/main:third_party/angle/src/compiler/translator/ValidateTypeSizeLimitations.cpp;l=34;drc=d0ee0197ddff25fe1a9876511c07542ac483702d)\nis being used to further restrict the maximum size of variables within shaders\nto prevent these vulnerabilities. It's unclear if this strategy will prevent\nmore variants from popping up in **SwiftShader**; [especially now that\n**WebGPU** will also need to make similar fixes in their front-end\ncompiler.](https://bugs.chromium.org/p/chromium/issues/detail?id=1431761#c14)\n\n> _**Note**_: When **Tint** emits an `OpVariable` it also emits an\n[`OpConstantNull`](https://registry.khronos.org/SPIR-V/specs/unified1/SPIRV.html#OpConstantNull)\nSPIR-V instruction. The `OpConstantNull` instruction causes SwiftShader, and any\nother **SPIR-V** compiler, to zero-initialize variables allocations. [As noted\nin the bug](https://crbug.com/1431761), it prevents the bug from triggering in a\nconvenient amount of time on WebGPU. This is an interesting inconsistency\nbetween the two front-end compilers. We are also actively investigating if the\n**ANGLE Translator's** lack of `OpConstantNull` leads to infoleaks. The\n**WebGPU** team is considering a [separate\nfix](https://bugs.chromium.org/p/chromium/issues/detail?id=1431761#c14) for this\nbug.\n\n### More Bugs and Notes on Shader Compiler Complexity\n\n+   The front-end shader compilers - ANGLE Translator and Tint - break Chrome's\n    [Rule of\n    Two](https://chromium.googlesource.com/chromium/src/+/master/docs/security/rule-of-2.md)\n    on platforms like Android, where the GPU process is un-sandboxed **and**\n    parses complex attacker-controlled shaders as input. In addition, backend\n    shader compilers in the Usermode Graphics Drivers have a high complexity,\n    are closed source, and are evolving targets that are continuously adding new\n    optimizations and functionality.\n\n+   **WGSL Shader Compilers** are more expressive in general than **WebGL SL\n    shader compilers**. Notably, **WGSL** supports both dynamic sized arrays and\n    runtime-sized arrays which introduces complexity when handling. There is\n    state tracking within Dawn to ensure that object types don't change between\n    executions of the JIT compiler. However as complexity increases in both\n    **Dawn** and **Tint** this could become harder to manage and lead to bugs.\n\n+   We are currently fixing bugs in SwiftShader by making fixes in the front-end\n    compilers. [This is likely a risky way to fix these\n    vulnerabilities](https://bugs.chromium.org/p/chromium/issues/detail?id=1431761#c14)\n    and leads to situations where variants can easily slip through the cracks.\n\n+   We believe that Chrome owning the entire front-end compilation component in\n    **Tint** is a net-positive win for security. The less attack surface we pass\n    on to the Usermode Graphics Drivers the better.\n\n+   We did not spend time digging into speculative execution vulnerabilities.\n    However, we would be surprised if there are no Spectre gadgets in\n    SwiftShader.\n\n+   SwiftShader unifies the GPU process attack surface, and enables exploits\n    that are reachable through the **Vulkan** API on all platforms. We\n    encouraged the **WebGPU** team to consider shipping the\n    `forceFallbackAdapter` adapter option behind a runtime flag.\n\n+   We have not yet audited what any of this means at the Kernel level. We don't\n    know what shader compiler execution looks like on a GPU and what the shape\n    of a vulnerability in that area would look like.\n\n## Summary of Findings\n\nWebGPU introduces a significant amount of attack surface to Chrome's GPU process\nboth through the core WebGPU implementation which lives in **Dawn**, the\n**WebGPU extensions to the GPU Command Buffer**, and transitively through the\nthird party Usermode Graphics Drivers and everything below.\n\nThe vulnerabilities in the document are meant to showcase attack surfaces and\npatterns that demonstrate further complexity will likely lead to more\nvulnerabilities.\n\nWebGPU invested a significant amount of effort on validating renderer supplied\ninput before calling into drivers and reference counting pointers. This\ninvestment paid off – we found precisely zero \"low-hanging\" vulnerabilities in\nDawn.\n\nWebGPU also introduces a large amount of attack surface through the compilation\nand execution of shader compilers in Chrome's privileged GPU process in\n**Tint**, third party **Usermode Graphics Drivers**, and **SwiftShader**.\n\nWebGPU has invested a significant amount of effort on fuzzing **Tint**. However\nthe fuzzing only targets the parsers and lexers within **Tint** and doesn't\nexercise the code in SwiftShader or on **Usermode Graphics Drivers**. There is\nroom for Chrome to invest in fuzzing shader compilers with syntactically and\nsemantically correct code in the same way that we fuzz V8 with Fuzzilli to\nexercise code in **SwiftShader's** **JIT** compiler. Like V8, shader compilers\nwill have bugs that are unfuzzable. Chrome Security will need to continue\nmanually auditing shader compiler implementations to correctly assess risk and\nreduce bug density.  Furthermore, where we lack access to source code, such as\nthird party **Usermode Graphics Drivers**, expanding fuzzing support is our only\nfeasibly scalable approach to mitigating the risk of third party code within the\nChrome GPU process.\n\n### Systemic Concerns\n\nWe found many one-off vulnerabilities in WebGPU during this exercise, and we\nfound some bugs that hinted at future problem areas:\n\n+   **Dawn use-after-frees**: Dawn has a pattern where objects hold a raw\n    pointer to reference counted objects, assuming a reference is held\n    elsewhere. This assumption can easily break with future changes to the code\n    as we've seen in the browser process with Mojo handlers. Dawn should\n    discourage this pattern to reduce use-after-free bugs.\n\n+   **Dawn Callbacks**: WebGPU implements callbacks in the GPU process. Similar\n    patterns in Mojo and JavaScript have consistently caused high severity\n    issues in Chrome over the years. We believe a high bar of scrutiny should be\n    applied to changes within existing **WebGPU** callback handlers and for any\n    new callback handlers being added to **Dawn**. Increasing complexity in this\n    area would likely have a high cost to Chrome Security.\n\n+   **Chrome Command Buffer**: The Chrome Command Buffer is prone to input\n    validation issues, has many undocumented legacy footguns, and is difficult\n    to fuzz effectively because feature coverage requires (a) a harness that\n    supports Chrome in multi-process mode, (b) a stateful generator that can\n    leverage context across test cases, and (c) can sometimes also require\n    execution on a host with a physical GPU. Snapshot fuzzing may be useful to\n    address some of these challenges, although manual auditing is currently the\n    best way to discover bugs in this area of the codebase.\n\n+   **SwiftShader JIT**: Vulnerabilities in the **SwiftShader JIT compiler**\n    aren't being fixed in the **SwiftShader** codebase. Instead they are fixed\n    by translating away code patterns using the higher-level front end compilers\n    like the **ANGLE Translator**. This has led to bug variants. Furthermore,\n    **ANGLE** and **Tint** sanitization happens on a representation of shaders\n    that is distinct from the representation used by **SwiftShader** and\n    **Usermode Graphics Drivers**, creating gaps in protection coverage.\n    Finally, Chrome now has two front-end compilers that pass compiled code to\n    **SwiftShader** for further compilation making this strategy more\n    precarious.\n\n## Glossary: Chrome Security GPU Terminology\n\nThe security relevance of GPU terms is hard to track. Here are a lot of them in\none place.\n\n+   **Dawn Wire**: Client-Server implementation of\n    [`webgpu.h`](https://github.com/webgpu-native/webgpu-headers/blob/main/webgpu.h).\n\n    +   **Dawn Wire Client**: Lives in the renderer process.\n    +   **Dawn Wire Server**: Lives in the GPU process\n\n+   **Dawn Native**: Core implementation of WebGPU that calls into the Dawn\n    backends.\n+   **Dawn Backends**: Wrappers around the System Graphics Apis that Dawn Native\n    needs to call into (Vulkan, Metal, & DirectX3D).\n+   **Tint:** Google's OSS implementation of WGSL. Compiles WGSL to SPIRV, MSL,\n    HLSL, & DXIL. Mostly a front-end compiler as of May 23, 2023.\n+   **ANGLE**: Google's OSS implementation of OpenGL.\n+   **ANGLE Translator**: Google's OSS implementation of WebGL SL. Compiles\n    WebGL SL to GLSL or SPIR-V.\n+   **SwiftShader**: Vulkan implementation and SPIR-V compiler built to run\n    directly on the CPU. Emulates an entire GPU as well. Does so with JIT\n    compiled SIMD shader compiler execution.\n+   **SwiftShader JIT Compiler**: SwiftShader compiles SPIR-V shaders to\n    X86/Arm/aarch64/etc using PNACL's old JIT compiler, SubZero.\n+   **D3D12**:  Direct3D 12, Microsoft's newest System Graphics API. Implemented\n    in Usermode Graphics Driver.\n+   **OpenGL**:  WebGL is built on OpenGL. Implemented in Usermode Graphics\n    Driver. SwiftShader no longer\n+   **Vulkan**: Systems Graphics API on Linux (and some Windows devices™).\n    WebGPU is built on top of Vulkan.\n\n    +   WebGL can be run with a Vulkan backend natively. Currently enabled on\n        50%  is built on top of Vulkan on 50% of Linux Desktop devices through a\n        finch experiment.\n    +   WebGL on SwiftShader uses the Vulkan backend on every platform.\n    +   WebGPU on Linux uses Vulkan for 100% of Linux Desktop and Android\n        devices\n    +   WebGPU on SwiftShader uses the Vulkan backend on every platform.\n\n+   **Metal**: Systems Graphics API on Mac.\n+   **DXIL**:  DirectX Intermediate Language, essentially LLVM IR for shaders\n    for D3D12\n+   **HLSL**:  High Level Shading Language, Direct3D's shading language\n    (including D3D12).\n+   **MSL**:  Metal Shading Language (shading language that runs on apple\n    hardware).\n+   **SPIR-V**: Standard Portable Intermediate Representation - Vulkan. An SSA\n    form bytecode shading language used for Vulkan. Both WebGL and WebGPU\n    compile to SPIR-V on Vulkan.\n+   **WGSL**: WebGPU Shading Language. WebGL's successor.\n+   **GLSL**: OpenGL Shading Language.\n+   **WebGL SL**: WebGL Shader language. A subset of GLSL that is safe for the\n    web. Compiled and sanitized by the ANGLE translator.\n+   **Usermode Graphics Driver (UMD)**: A shared library that ships with a\n    kernel graphics driver (think Arm, Nvidia, AMD, Qualcomm). This is where\n    shader compilation happens. This is where the system graphics APIs are\n    implemented. SwiftShader emulates an entire GPU, so it is a Usermode\n    Graphics Driver and more.\n+   **GPU Command Buffer**: High level abstraction for transferring data over\n    shared memory to the GPU process. Both the renderer process and browser\n    process use various command buffers to do GPU operations in Chrome.\n+   **WebGPU use of GPU Command Buffer (`WebGPUDecoderImpl`)**: An extension of\n    the Chrome GPU Command Buffer abstraction that is used for transferring Dawn\n    Wire data between the Renderer and GPU processes.\n+   **Dawn Native `GPUCommandBuffer`**: An object within Dawn that has a name\n    collision with the legacy Chrome GPU Command Buffer abstraction. They are\n    not related.\n"
  },
  {
    "path": "security/research/graphics/README",
    "title": "",
    "content": "## Why Graphics?\n\nThe GPU process is interesting from an attacker perspective for several reasons.\n\n1. Many of its features are reachable directly from web content by default,\n   which creates an opportunity for malicious websites to attack Chromium users.\n2. It processes complex data in (mostly) C++ native code, which is difficult to\n   do safely.\n3. It needs the privilege to interact with GPU drivers in the kernel, so our\n   ability to sandbox the process is limited.\n4. It loads third party native code into its address space to interact with\n   platform specific graphics features.\n\nCollectively these properties make the GPU process particularly attractive for\nboth remote code execution and privilege escalation.\n"
  },
  {
    "path": "security/research/graphics/overview",
    "title": "Chromium Graphics",
    "content": "# Chromium Graphics\n\nAuthors: chrome-offsec-team@google.com<br/>\nLast updated: April 7, 2023<br/>\n\n## Overview\n\nChromium graphics features are among the most complex parts of the browser. This\ndocument is a snapshot of our evolving understanding of graphics features from\nan attacker perspective. Relevant and important pieces of the graphics stack are\nomitted, but the intent is to give VRP contributors a boost when getting started\nwith unfamiliar graphics features.\n\n## In Pictures: WebGL and WebGPU\n\nThe diagram below is a simplified view of WebGL and WebGPU with a focus on\ncomponents of particular interest to attackers.\n\n![Chrome Graphics Architecture Overview](resources/chrome_gfx_overview.png)\n\nNote: Skia and Canvas APIs are omitted and may be incorporated in a future\niteration.\n"
  },
  {
    "path": "security/research/graphics/gpu_command_buffer",
    "title": "GPU Command Buffer",
    "content": "# GPU Command Buffer\n\nAuthors: chrome-offsec-team@google.com<br/>\nLast updated: April 10, 2023<br/>\n\n## Overview\nThe GPU Command Buffer is used to create and manage high throughput\ncommunication paths using a combination of Mojo IPC and shared memory regions.\nIt is currently used by Chrome to support three distinct methods of\nGPU-accelerated graphics: WebGL2, Skia rasterization, and WebGPU. Mojo IPC\nmessages are used to communicate the metadata necessary to establish new shared\nmemory regions and coordinate state among endpoints producing and consuming from\nthe memory region. Communication paths are between two endpoints: one client and\none service. Typically the service endpoint resides in the GPU process and\nclients reside in the renderer process, but there is support for exceptions such\nas when GPU functionality is forced to run as a thread in the browser process,\nor unit tests where everything runs in a single process. A single renderer\nprocess may host multiple clients, such as when web content utilizes both WebGL\nand WebGPU features, where each context has an independent dedicated Command\nBuffer.\n\nThe structure of Command Buffer data can be thought of as a layering of\nprotocols. Common functionality is shared by all client-service types and\nspecialized application-specific functionality is built on top. Core\nfunctionality provides the mechanisms to establish and manage the communication\npath, such as creating new shared memory regions and providing synchronization\nprimitives. Specialized functionality pertains to application-specific features,\nsuch as shader configuration and execution.\n\n### Security Investment Rationale\nIsolating untrustworthy content within a sandboxed process is a cornerstone of\nthe Chrome security model. Communication paths that bridge the sandbox to more\ntrustworthy processes are a key part of the inter-process security boundary. The\nseparation of the signaling mechanism (i.e. Mojo IPC) from the data transfer\nmechanism (i.e. shared memory) creates the potential for state inconsistencies.\nThe layered nature of communication creates the potential for nuanced,\ncross-protocol dependencies. The use of shared memory creates the potential for\ntime-of-check-time-of-use issues and other state inconsistencies.\n\n## Research Scope & Outcomes\nThe Command Buffer investigation was part of a broader effort to identify areas\nof interest to attackers within the Chrome GPU acceleration stack. After\nnarrowing our focus to code supporting the WebGPU subsystem, we still found the\nstack to be far larger and more complex than we could comprehensively audit in\none pass. To further narrow scope we identified discrete components of\nfunctionality underpinning the WebGPU stack. The Command Buffer is one such\ncomponent. Investigation of the Command Buffer proceeded in parallel to analysis\nof other GPU subsystems.\n\nCommand Buffer communication is structured in layers. The foundational or common\nlayer is shared by all higher layers. We chose to start analysis with the common\nfeatures because of their applicability to all higher layers. The Command Buffer\nalso supports application-specific features that behave like higher level\nprotocols. Examples of higher level layers include support for WebGL and WebGPU.\nIt is these higher level features - specifically the implications of\ncross-protocol interactions with the low level features - that we believe\nrepresent the bulk of complexity and attack surface. However, these are not yet\nexplored and give motivation to revisit this subsystem.\n\n### Command Buffer Usage in Chrome\nThe Command Buffer is used in four different scenarios:\n\n- [Proxy](https://source.chromium.org/chromium/chromium/src/+/main:gpu/ipc/client/command_buffer_proxy_impl.h;l=70;drc=93a273dd903e50a36011ea159fd9dc70c7000d87):\n  Used by sandboxed processes to communicate with the CommandBuffer service in\n  the GPU process.\n- [Direct](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/service/command_buffer_direct.h;l=19;drc=93a273dd903e50a36011ea159fd9dc70c7000d87):\n  Used by the CommandBuffer service within the GPU process.\n- [In-process](https://source.chromium.org/chromium/chromium/src/+/main:gpu/ipc/in_process_command_buffer.h;l=88;drc=b8743ff466a0d0208841a8c1bd906f481fc7f9ec):\n  Used when clients and services are all in the same process, such as unit tests\n  and when Chrome is run in single-process mode.\n- [Pepper](https://source.chromium.org/chromium/chromium/src/+/main:ppapi/proxy/ppapi_command_buffer_proxy.h;l=31;drc=821ca309ed94810aa52df2b31fc916806e207e0e)\n  (deprecated): Used by plugins - and possibly also extensions and apps - to\n  communicate with the CommandBuffer service in the GPU process.\n\nEach client type implements at least one of two additional classes that each\nprovide a means of IPC signaling:\n\n1. CommandBufferClient: Used by the service to send messages to clients.\n   Implemented via\n   [Mojo](https://source.chromium.org/chromium/chromium/src/+/main:gpu/ipc/common/gpu_channel.mojom;l=282;drc=0a3ae731632f6d414e0460ab6bc0bb6e452adfda)\n   for multi-process use cases; the CommandBufferServiceClient is an equivalent\n   variation to support single-process operation via direct\n   [C++](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/service/command_buffer_service.h;l=50;drc=0a3ae731632f6d414e0460ab6bc0bb6e452adfda)\n   API calls.\n2. [GPUControl](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/client/gpu_control.h;l=37;drc=0a3ae731632f6d414e0460ab6bc0bb6e452adfda):\n   Used by clients to send messages to the service. Implemented via\n   [Mojo](https://source.chromium.org/chromium/chromium/src/+/main:gpu/ipc/common/gpu_channel.mojom;l=240;drc=0a3ae731632f6d414e0460ab6bc0bb6e452adfda)\n   for multi-process clients and C++ for in-process use cases.\n\nThe Proxy use case is most interesting because it is the attack surface\navailable from a sandboxed renderer process as deployed in real Chrome\ninstances. However, we intended to pursue fuzzing and Chrome's primary fuzzer\nframework is based on single-process unit tests, so it was necessary to instead\ntarget the in-process implementation supported by unit tests even though it\nstubs-out or emulates interesting features.\n\n## Fuzzing\nExisting fuzzers for the Command Buffer were developed by the GPU team and\npredate our analysis. We studied these fuzzers - in particular how they\nbootstrap the graphics subsystem for testing - and developed new fuzzers with\ndifferent generation strategies, finding one new high severity security\n[bug](https://crbug.com/1406115) in the same narrow feature set already covered\nby existing fuzzers. Much like our fuzzers from this first pass, the existing\nfuzzers targeted specific portions of functionality. An emerging theme for\nimproving fuzzing Chrome at large applies here as well: layering and integration\nof fuzzers is expected to increase reachability of complex state and therefore\nincrease aggregate fuzzer effectiveness.\n\nIn other words, the coverage resulting from a combination of individual fuzzers\nis greater than the sum of its parts. Consequently, we intend to incrementally\nextend and integrate fuzzers in order to exercise cross-feature complexity\nduring future work in the graphics subsystem. The next step is integration with\nnew and complementary fuzzer targeting the WebGPU Dawn Wire protocol.\n\nWe developed [one new fuzzer](https://crrev.com/c/4261996) tailored for the\nCommand Buffer. Its design is similar to an [existing\nfuzzer](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/tests/fuzzer_main.cc;l=537;drc=c098bef2b9cb022ef1a037f28d3e3ee845c7a91a),\nbut the new fuzzer differs in a few ways:\n- It uses [libprotobuf-mutator](https://github.com/google/libprotobuf-mutator)\n  (LPM) for generation.\n- The LPM\n  [grammar](https://source.chromium.org/chromium/chromium/src/+/main:testing/libfuzzer/fuzzers/command_buffer_lpm_fuzzer/cmd_buf_lpm_fuzz.proto)\n  targets\n  [CommandBuffer](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/common/command_buffer.h;l=18;drc=e20bf9387f49c3a0b208bad26dc8efc0dc214e96)\n  features that are common to all client-service types, making it suitable for\n  combination with other LPM fuzzers targeting higher level client-service\n  types, such as WebGPU.\n\nIt so happens that most of the CommandBuffer features targeted by the new fuzzer\nalso involve Mojo IPC.\n\n## Higher Layers\nThree types of CommandBuffer clients make direct use of its features: WebGL2,\nSkia Raster, and WebGPU. This section characterizes how each client type makes\nuse of the CommandBuffer.\n\nOne renderer may act as many clients. For example, if web content makes use of\nboth WebGL2 and WebGPU, the renderer would have at least two separate\nCommandBuffer sessions.\n\n### CommandBuffer Protocol Structure\nCommandBuffer commands all start with a header containing two fields: an 11-bit\ncommand identifier and a 21-bit size. The header is followed by zero or more\ndata fields.\n\n![Command Buffer Structure](resources/cmdbuf_command_structure.png)\n\nThe structure allows for 11 bits of unique commands, each with customizable data\npayloads. This is the mechanism used to implement the [common\ncommands](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/common/cmd_buffer_common.h;l=171;drc=05dfbc82b07e06f610b8f2fecaa4d272430cc451).\n\n### WebGL2 and Skia Raster\nJust like the common commands, Skia [Raster\ncommands](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/common/raster_cmd_ids_autogen.h)\nand [WebGL2/GLES2\ncommands](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/common/gles2_cmd_ids_autogen.h;l=14;drc=4c9af88c669bd724a999193c8ffcff57f0cf6bbe)\nare also implemented as native CommandBuffer commands; each new command is\nassigned an identifier and their parameters are defined using CommandBuffer\nconventions. All native CommandBuffer commands are validated at the\nCommandBuffer level.\n\n### WebGPU\nWebGPU takes a different approach: a [few native CommandBuffer\ncommands](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/common/webgpu_cmd_ids_autogen.h;l=14;drc=b4bc946c63b2b95e1f05dec4e84adcadd10499c6)\nsupport [many new Dawn\nAPIs](https://source.chromium.org/chromium/chromium/src/+/main:out/Debug/gen/third_party/dawn/include/dawn/dawn_proc_table.h;l=8;drc=b4bc946c63b2b95e1f05dec4e84adcadd10499c6).\nIn particular, a single CommandBuffer command called `DawnCommands` (code\npointer:\n[client](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/client/webgpu_cmd_helper_autogen.h;l=14;drc=b4bc946c63b2b95e1f05dec4e84adcadd10499c6)\n/\n[service](https://source.chromium.org/chromium/chromium/src/+/main:gpu/command_buffer/service/webgpu_decoder_impl.cc;l=1710;drc=48f518a9bb701c5ef7315b5affad8987517eb234))\nimplements the bulk of the Dawn Wire protocol.\n\nUnlike native CommandBuffer commands, which each have a unique id at the\nCommandBuffer level, Dawn Wire commands are nested inside the data portion of\nthe `DawnCommands` CommandBuffer command. In the GPU process, the CommandBuffer\ncommand handler reads the command data, which includes an identifier for the\nDawn command, then uses a switch statement to determine which Dawn command to\nexecute and how to further decode the data. Consequently, Dawn commands use an\nentirely discrete set of serialization, deserialization and validation logic.\n\nNotably, the [command\nhandler](https://source.chromium.org/chromium/chromium/src/+/refs/heads/main:third_party/dawn/src/dawn/wire/ChunkedCommandHandler.h;l=59;drc=2dfb6431910db3004672ccb94df3ce09d13e8770)\nis platform specific,\n[auto-generated](https://source.chromium.org/chromium/chromium/src/+/main:third_party/dawn/src/dawn/wire/BUILD.gn;l=46;drc=0ebe86d1ca46dd203e7c55577df6115fa527c1c3),\nand its design allows individual handlers for Dawn commands (i.e. function\npointers) to be overridden when the WebGPU session is established.\n\nSpecific Dawn commands of interest are described in a dedicated document.\n\n## Features of Particular Interest\n\nThis section describes high level feature operation to introduce the concepts.\nLater documents go into more detail about specific use cases.\n\n### `TransferBuffer`: Efficient Bulk Data Transfer\n\nEnabling efficient bulk data transfer is a core CommandBuffer feature. GPU\nclients - namely a renderer process - create a `gpu::TransferBuffer` that\nincludes a pointer to a memory region the client intends to share with the GPU\nprocess. However, a pointer is only meaningful within a single process's address\nspace and the goal is cross-process sharing, so `gpu::TransferBuffer` also\ncontains a unique numeric identifier, `buffer_id_`, that will be consistent\nacross processes.\n\nWhen a `gpu::TransferBuffer` is allocated and initialized, an important step is\n\"registration\" with the GPU process. Registration is a Mojo IPC message\ncontaining the `buffer_id_` of the newly created `gpu::TransferBuffer`, which\nallows the GPU process to record an association between the `buffer_id_` and a\npointer to the shared buffer within its own address space. After registration\nboth client and service can refer to the same memory by its ID rather than a\npointer.\n\nThe `gpu::TransferBuffer` includes data structures that allow the client and\nservice to treat the shared memory region as a ring buffer. These data\nstructures include alignments and offsets within the shared buffer, as well as\nfeatures to let client and service indicate their respective positions of\nproduction and consumption within the buffer. These structures must be kept in\nsync across both the client and service to achieve intended operation, which\nrequires cooperation of both client and service. Notably, since the client\ncreates the shared memory it may also unilaterally reorganize or altogether\nde-allocate the memory; this means the service must guard against a compromised\nrenderer that has many opportunities to manipulate state and shared resources.\n\n### `SyncToken`: Foundation for Synchronization\n\nThe `SyncToken` is a CommandBuffer synchronization primitive. A token is\ninserted into the command stream as a shared point of reference for both\nproducers and consumers. The shared point of reference allows building higher\nlevel synchronization features. For example, client and server can communicate\nexpectations about the ordering of operations in the stream in relation to a\ntoken.\n\nHigher level several synchronization features such as `CHROMIUM_sync_point` and\n`CHROMIUM_ordering_barrier` build on the `SyncToken` and are implemented as\n[extensions](https://source.chromium.org/chromium/chromium/src/+/main:gpu/GLES2/extensions/CHROMIUM/)\nto the CommandBuffer protocol.\n\nDedicated\n[documentation](https://source.chromium.org/chromium/chromium/src/+/main:docs/design/gpu_synchronization.md)\ngoes into more detail about Chromium GPU synchronization features.\n\n### `Mailbox`: A cross-process identity for shared resources\n\nA `gpu::Mailbox` is a 16-byte random value that can be shared across GPU\ncontexts and processes. The `gpu::Mailbox` name is a single shared identifier\nused to refer to the same object. Similar to a `gpu::TransferBuffer` ID, the\ncommon identifier gives GPU clients and services in different processes a name\nto refer to data across address spaces.\n\nThe key feature of the `gpu::Mailbox` is allowing producers and consumers to\nassociate *multiple* resources with a single name. After establishing a mailbox,\ncommunicating endpoints can reference a collection of related resources using a\ncommon name rather than managing many separate resources.\n\n### `SharedImage`: Allowing many endpoints to operate on shared binary data\n\n`SharedImage` is a complex collection of features to pass binary data between\nproducers and consumers. It's key feature is allowing multiple contexts of\npotentially different types - e.g. Skia Raster, WebGL, and WebGPU - to operate\non the same object.\n\n## Observations & Lessons Learned\n\nLayering and integration of fuzzers is expected to increase reachable state and\ntherefore increase aggregate fuzzer effectiveness. Consequently, we intended to\nincrementally extend and integrate fuzzers in order to exercise cross-protocol\ncomplexity during future work in the graphics subsystem. Modest changes to\nexisting fuzzers can yield new bugs.\n\nLayering Dawn on top of an independent communication mechanism has been a source\nof security bugs (e.g. [1](crbug.com/1314754), [2](crbug.com/1393177),\n[3](crbug.com/1373314), [4](crbug.com/1340654)) because operations at the lower\nCommandBuffer level can violate assumptions made at the higher level.\n\n### Prior Work\n- [2020 Q2 WebGPU Security design\n  doc](https://docs.google.com/document/d/1rrNBF8Ft8WZU3VzAgAUycrwlaesWWw6hvmIqJPiGRII)\n  (Google only)\n- [SwiftShader - A Chrome Security\n  Perspective](https://docs.google.com/presentation/d/1nVmbrJikT_qfhMls4rK0txtPXLuRhrhGs-TICLyU3Wc/)\n  (Google only)\n"
  },
  {
    "path": "security/research/graphics/vulnerabilities/README",
    "title": "Vulnerability Discovery: Chromium Graphics",
    "content": "# Vulnerability Discovery: Chromium Graphics\n\nThis directory contains notes on vulnerabilities found during the study of\nChromium graphics subsystems.\n\nNotes about vulnerabilities are kept separate from general descriptions of\nfeatures so we can publish general information without exposing sensitive\ndetails about vulnerabilities that may not yet be fixed.\n\nAside from the issue of sensitivity, it's useful to talk about vulnerabilities\nseparately so we can go into detail about the *process* of vulnerability\ndiscovery, including what worked and what didn't.\n"
  },
  {
    "path": "security/lookalikes/lookalike-domains",
    "title": "\"Lookalike\" Warnings in Google Chrome",
    "content": "# \"Lookalike\" Warnings in Google Chrome\n\n[TOC]\n\n## What are lookalike warnings?\n\n\"Lookalike\" domains are domains that are crafted to impersonate the URLs of\nother sites in order to trick users into believing they're on a different site.\nThese domains are used in social engineering attacks, from phishing to retail\nfraud.\n\nIn addition to [Google Safe Browsing](https://safebrowsing.google.com/)\nprotections, Chrome attempts to detect these lookalike domains by comparing the\nURL you visited with other URLs that are either very popular, or that you have\nvisited previously. These checks all happen within Chrome -- Chrome does not\ncommunicate with Google to perform these checks.\n\nWhen Chrome detects a potential lookalike domain, it may block the page and show\na full-page warning, or it may show a pop-up warning, depending on how certain\nChrome is that the site is a spoof. These warnings typically have a \"Did you\nmean ...?\" message.\n\n| High-confidence warnings               | Low-confidence warning        |\n|:--------------------------------------:|:-----------------------------:|\n| ![Interstitial page](interstitial.png) | ![Safety Tip bubble](tip.png) |\n\nThese warnings do not indicate that the site the user has visited is malicious.\nThe warnings indicate that the site looks like another site, and that the user\nshould make sure that they are visiting the site that they expected.\n\n## Examples of lookalike domains\n\nChrome's checks are designed to detect spoofing techniques in the wild. Some\nexample \"lookalike\" patterns that trigger warnings include:\n\n * Domains that are a small edit-distance away from other domains, such as\n   `goog0le.com`.\n * Domains that embed other domain names within their own hostname, such as\n   `google.com.example.com`.\n * Domains that use IDN\n   [homographs](https://chromium.googlesource.com/chromium/src/+/main/docs/idn.md),\n   such as `goögle.com`.\n\nThis list is not exhaustive, and developers are encouraged to avoid using\ndomains that users without technical backgrounds may confuse for another site.\n\n\n## Lookalike checks are imperfect\n\nChrome's lookalike checks are not always right. Chrome can not detect all\nlookalike domains, and often lookalike domains are not malicious. Our\nintent with Chrome's lookalike warnings is not to make spoofing impossible,\nbut to force attackers to use less convincing lookalikes, allowing users to\nnotice spoofs more easily.\n\nWhile Chrome's checks sometimes label some benign pages as lookalikes, we use\nseveral approaches to minimize mistakes:\n\n * Checks are tuned to minimize warnings on legitimate pages.\n * Users are never prohibited from visiting the site requested, and the warnings\n   shown are designed to be helpful and informative, rather than scary.\n * We monitor what sites trigger the most warnings on a regular basis, and\n   disable warnings when we identify mistakes.\n * For domains used in company environments, we provide an [Enterprise\n   Policy](https://cloud.google.com/docs/chrome-enterprise/policies/?policy=LookalikeWarningAllowlistDomains)\n   allowing businesses to selectively disable warnings as needed for their\n   users.\n * For several months following the roll-out of new lookalike checks, we accept\n   review requests from site operators whose sites have been flagged.\n * New lookalike checks launching in Chrome 88 or later will trigger a console\n   message informing site owners of the issue for at least one release prior to\n   triggering user-visible warnings.\n\n\n## Not all users see all warnings\n\nChrome shows warnings in part based on a users' browsing history. This allows\nChrome to be both more helpful (by providing better recommendations) and make\nfewer mistakes (by not flagging lookalikes for irrelevant sites).\n\nChrome only shows warnings on sites that the user has not used frequently.\nFurther, Chrome will only recommend sites that are either well-known (i.e. top)\nsites, or the user has an established relationship.\n\nSites that show a warning to you may not show for another user, unless that user\nhas visited the same sites that you have.\n\n## Removing Lookalike Warnings from a site\n\nIt is possible to remove warnings on sites where Chrome is incorrectly showing\na warning.\n * If you are the owner of both the site showing the warning and the site that\n   Chrome thinks users should visit, you can use our\n   [**automated warning removal process**](#automated-warning-removal).\n * If you are not the owner of both sites, or you can't follow the automated\n   process, you can [**request a manual review**](#requesting-a-manual-review).\n\n\n### Automated warning removal\n\nIf you own both the site where Chrome is showing a warning, as well as the site\nthat Chrome is recommending, you can suppress these warnings by proving that you\ncontrol both sites using a special form of \n[Digital Asset Links](https://developers.google.com/digital-asset-links).\n\n#### Instructions\n1.  Create a file named `assetlinks.json` containing the following:\n```\n[{\n  \"relation\": [\"lookalikes/allowlist\"],\n  \"target\" : { \"namespace\": \"web\", \"site\": \"https://example.com\"}\n},{\n  \"relation\": [\"lookalikes/allowlist\"],\n  \"target\" : { \"namespace\": \"web\", \"site\": \"https://example.net\"}\n}]\n```\n2. Replace `example.com` and `example.net` with the domain where the warning is\n   shown, and with the domain that Chrome recommends users visit. Do not use\n   subdomains (e.g. use \"example.com\", not \"www.example.com\").\n3. Upload this file to both of your sites at `/.well-known/assetlinks.json`. For\n   instance, in our example, you would upload the files at both\n   `https://example.com/.well-known/assetlinks.json` and\n   `https://example.net/.well-known/assetlinks.json`.\n4. **Fill out an automated verification [request](https://forms.gle/DsoM64EmSZ5H4bNd8)**.\n   Please provide an email address with a **valid Google account**, otherwise\n   you won't get updates.\n\nOnce you submit the [request](https://forms.gle/DsoM64EmSZ5H4bNd8), please allow\na few days for all warnings to stop. If verification fails, you should be\nnotified via email within a few hours. If you don't get an email indicating\nverification failure and your sites still show a warning after a week, please\nsubmit a manual review using the process below.\n\nImportant notes:\n * You only need to submit the verification request form once with a single\n   domain. All other domains are discovered automatically, since you should have\n   listed them in the first domain's `assetlinks.json` file.\n * You must keep the `assetlinks.json` file in place so long as you wish to\n   suppress the warnings. If you remove either file, Chrome may resume showing\n   warnings.\n * You can extend the example `assetlinks.json` to support more than two\n   domains, or to support additional Digital Asset Links entries, if needed.\n   Please note that Chrome does not support `include` statements in\n   `assetlinks.json` files.\n\n### Requesting a manual review\n\nIf a site triggers erroneous lookalike warnings in Chrome,\nyou can ask for a manual review. Please only use this process if you are unable\nto use the [Automated Process](#automated-warning-removal) above. In some\ncases, we may require that you use the automated process to demonstrate that\nyou control both sites.\n\nRequests for manual review are generally considered for six months following\nafter that warning would have started (i.e. after Chrome introduces the check).\nAfter that time, we encourage developers to test their new sites in Chrome to\nensure that their new domain does not trigger warnings.\n\nIf you are unable to use the automated process above, and would like to request\na manual review, please fill out a manual review\n[request](https://forms.gle/BxV3JGbCbRjucDxq6).\nPlease provide an email address with a **valid Google account**, otherwise\nyou won't get updates.\n\n#### Reasons an appeal might be denied\n\nThere are several reasons that may lead us to deny your appeal. The following\nare some of the most common reasons that don't qualify for manual appeals:\n\n  * **Domains that are only used internally**, such as for testing or in\n    enterprise settings. We recommend using the [Enterprise\n    Policy](https://cloud.google.com/docs/chrome-enterprise/policies/?policy=LookalikeWarningAllowlistDomains)\n    in this case.\n  * **For new sites or where very few users are impacted**. We encourage domain\n    owners to choose domains that do not look like domains used by other sites\n    commonly visited by your users.\n  * **For high-risk sites**. In some cases, some domains might be particularly\n    vulnerable to spoofing attacks (for instance, where money exchange is\n    involved). In these cases, we deny appeals that aren't from the site owner.\n\nPlease note that the [automated process](#automated-warning-removal) is not\nsubject to these restrictions.\n"
  },
  {
    "path": "platforms/README",
    "title": "Platform-Specific Documentation",
    "content": "# Platform-Specific Documentation\r\n\r\nThis section contains documentation specific to each platform that Chromium supports.\r\n\r\n## Platforms\r\n\r\n### [Android](android/)\r\nMobile platform documentation including build instructions, JNI best practices, and Android-specific features.\r\n\r\n- [Build Instructions](android/android_build_instructions.md)\r\n- [Debugging Instructions](android/android_debugging_instructions.md)\r\n- [JNI Ownership Best Practices](android/android_jni_ownership_best_practices.md)\r\n- [Android Studio Setup](android/android_studio.md)\r\n\r\n### [iOS](ios/)\r\niOS platform documentation including build instructions and platform-specific considerations.\r\n\r\n- [Build Instructions](ios/ios_build_instructions.md)\r\n- [iOS Infrastructure](ios/ios_infra.md)\r\n- [VoiceOver Support](ios/ios_voiceover.md)\r\n\r\n### [ChromeOS](chromeos/)\r\nChromeOS-specific documentation including build instructions and platform features.\r\n\r\n- [Build Instructions](chromeos/chromeos_build_instructions.md)\r\n- [ChromeOS Glossary](chromeos/chromeos_glossary.md)\r\n\r\n### [Windows](windows/)\r\nWindows platform documentation including build instructions and Windows-specific features.\r\n\r\n- [Build Instructions](windows/windows_build_instructions.md)\r\n- [Native Window Occlusion Tracking](windows/windows_native_window_occlusion_tracking.md)\r\n- [PWA Integration](windows/windows_pwa_integration.md)\r\n\r\n### [macOS](mac/)\r\nmacOS platform documentation including build instructions and macOS-specific features.\r\n\r\n- [Build Instructions](mac/mac_build_instructions.md)\r\n- [ARM64 Support](mac/mac_arm64.md)\r\n- [LLD Linker](mac/mac_lld.md)\r\n\r\n## Cross-Platform Considerations\r\n\r\nWhen developing for multiple platforms, consider:\r\n\r\n- **API Availability**: Not all APIs are available on all platforms\r\n- **Performance Characteristics**: Different platforms have different performance profiles\r\n- **Security Models**: Platform-specific security requirements and capabilities\r\n- **UI Guidelines**: Each platform has its own design guidelines\r\n\r\n## Related Documentation\r\n\r\n- [Architecture Overview](../architecture/overview.md)\r\n- [Development Guidelines](../development/)\r\n- [Security Considerations](../security/)\r\n"
  },
  {
    "path": "platforms/overview",
    "title": "Platform-Specific Development Overview",
    "content": "# Platform-Specific Development Overview\r\n\r\nChromium runs on multiple platforms, each with unique characteristics, capabilities, and constraints. This section provides platform-specific guidance for developing and optimizing Chromium across different operating systems and device types.\r\n\r\n## 🎯 What You'll Learn\r\n\r\n- **Platform Architecture**: Understanding platform-specific implementations\r\n- **Development Setup**: Platform-specific build and development environments\r\n- **Optimization Techniques**: Platform-specific performance and feature optimization\r\n- **Testing Strategies**: Platform-specific testing approaches\r\n\r\n## 🖥️ Supported Platforms\r\n\r\n### **Desktop Platforms**\r\n- [**macOS Development**](mac/) - Apple ecosystem development\r\n- [**Windows Development**](windows/) - Microsoft Windows ecosystem\r\n- [**Linux Development**](../development/testing/web_tests_linux) - Linux distributions\r\n\r\n### **Mobile Platforms**  \r\n- [**Android Development**](android/) - Android mobile and tablet development\r\n- [**iOS Development**](ios/) - Apple iOS mobile development\r\n\r\n### **Specialized Platforms**\r\n- [**Chrome OS Development**](chromeos/) - Google's Chrome operating system\r\n\r\n## 📱 Platform Characteristics\r\n\r\n### **Android**\r\n- **Unique Features**: Touch interface, limited resources, Java integration\r\n- **Key Challenges**: Performance on diverse hardware, battery optimization\r\n- **Development Focus**: Mobile UX, background processing, memory management\r\n\r\n### **iOS** \r\n- **Unique Features**: Apple ecosystem integration, strict app store guidelines\r\n- **Key Challenges**: iOS-specific APIs, memory constraints, review process\r\n- **Development Focus**: Native iOS integration, performance optimization\r\n\r\n### **macOS**\r\n- **Unique Features**: macOS native APIs, Metal graphics, Apple Silicon\r\n- **Key Challenges**: System integration, security model, hardware diversity\r\n- **Development Focus**: Native macOS experience, Apple ecosystem features\r\n\r\n### **Windows**\r\n- **Unique Features**: Windows APIs, DirectX graphics, diverse hardware\r\n- **Key Challenges**: Multiple Windows versions, hardware compatibility\r\n- **Development Focus**: Windows integration, accessibility, enterprise features\r\n\r\n### **Chrome OS**\r\n- **Unique Features**: Linux-based, web-first, container support\r\n- **Key Challenges**: Security model, resource constraints, update system\r\n- **Development Focus**: Web platform, Android app support, Linux compatibility\r\n\r\n## 🛠️ Platform-Specific Development\r\n\r\n### **Build Systems**\r\nEach platform has specific build requirements and optimization:\r\n- **Android**: Gradle integration, APK packaging\r\n- **iOS**: Xcode integration, app bundle creation  \r\n- **macOS**: Xcode tools, framework linking\r\n- **Windows**: Visual Studio integration, MSI packaging\r\n- **Chrome OS**: Portage build system, security hardening\r\n\r\n### **Testing Approaches**\r\nPlatform-specific testing considerations:\r\n- **Hardware Testing**: Testing on real devices and configurations\r\n- **Emulator Testing**: Using platform emulators for development\r\n- **Cloud Testing**: Using cloud-based device farms\r\n- **Performance Testing**: Platform-specific benchmarks\r\n\r\n### **Distribution Models**\r\nHow Chromium is distributed on each platform:\r\n- **Android**: Google Play Store, OEM integration\r\n- **iOS**: App Store distribution\r\n- **macOS**: Direct download, enterprise deployment\r\n- **Windows**: Direct download, enterprise MSI\r\n- **Chrome OS**: Integrated into OS updates\r\n\r\n## 🎯 Development Strategies\r\n\r\n### **Cross-Platform Development**\r\n- **Shared Codebase**: Common code across platforms\r\n- **Platform Abstraction**: Abstracting platform differences\r\n- **Feature Parity**: Maintaining consistent features\r\n- **Performance Consistency**: Similar performance across platforms\r\n\r\n### **Platform-Specific Optimization**\r\n- **Native Integration**: Using platform-specific APIs\r\n- **Performance Tuning**: Optimizing for platform characteristics\r\n- **User Experience**: Platform-appropriate UX patterns\r\n- **Security Model**: Following platform security practices\r\n\r\n## 🚀 Getting Started\r\n\r\n1. **Choose Your Platform**: Start with the platform most relevant to your work\r\n2. **Setup Environment**: Follow platform-specific build instructions\r\n3. **Understand Differences**: Learn platform-specific characteristics\r\n4. **Development Workflow**: Master platform-specific development tools\r\n\r\n## 📋 Platform Comparison\r\n\r\n| Platform | Complexity | Resources | Unique Features |\r\n|----------|------------|-----------|-----------------|\r\n| **Android** | High | Limited | Touch, Java, Diverse HW |\r\n| **iOS** | High | Limited | App Store, Metal, Ecosystem |\r\n| **macOS** | Medium | Abundant | Native APIs, Apple Silicon |\r\n| **Windows** | Medium | Variable | DirectX, Enterprise, Legacy |\r\n| **Chrome OS** | Medium | Limited | Security, Web-first, Updates |\r\n\r\n## 🔗 Related Sections\r\n\r\n- [🚀 Getting Started](../getting-started/setup-build) - General build instructions\r\n- [🧪 Testing & QA](../development/testing/android_test_instructions) - Platform-specific testing\r\n- [⚡ Performance](../performance/profiling_content_shell_on_android) - Platform performance\r\n- [🏗️ Core Architecture](../architecture/process-model) - How architecture varies by platform\r\n"
  },
  {
    "path": "platforms/windows/windows_virtual_desktop_handling",
    "title": "Windows 10 Virtual Desktop support",
    "content": "#  Windows 10 Virtual Desktop support\n\nWindows 10 introduced Virtual Desktop support. Virtual Desktops are similar to\nChrome OS and Mac workspaces. A virtual desktop is a collection of windows.\nEvery window belongs to a virtual desktop.  When a virtual desktop is selected\nto be active, the windows associated with that virtual desktop are displayed on\nthe screen. When a virtual desktop is hidden, all of its windows are also\nhidden. This enables the user to create multiple working environments and to\nswitch between them. An app (e.g., Chromium) can have windows open on\ndifferent virtual desktops, and thus may need to be Virtual Desktop-aware.\n\nThe user-facing Chromium support for virtual desktops consists of two things:\n\n  * When launching the browser with session restore, browser windows are moved\n  to the virtual desktop they were on when the browser shutdown.\n  * When opening a URL with the browser, either open it in a window on the\n  current virtual desktop, or open a new window on the current virtual desktop.\n  Don't open it in a tab in a window on another virtual desktop.\n\nThe core UI principles are that windows should be restored to the desktop they\nwere shut down on, and opening an app window shouldn't change the current\nvirtual desktop. Only the user should be able to change virtual desktops, or\nmove windows between virtual desktops.\n\nWindows 10 exposes the COM interface\n[IVirtualDesktopManager](https://docs.microsoft.com/en-us/windows/win32/api/shobjidl_core/nn-shobjidl_core-ivirtualdesktopmanager)\nto access the Virtual Desktop functionality. To make sure that opening a URL\nstays on the current virtual desktop,\n[BrowserView::IsOnCurrentWorkspace](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/ui/views/frame/browser_view.cc?q=%20BrowserView::IsOnCurrentWorkspace)\nuses the IVirtualDesktopManager method\n[IsWindowOnCurrentVirtualDesktop](https://docs.microsoft.com/en-us/windows/win32/api/shobjidl_core/nf-shobjidl_core-ivirtualdesktopmanager-iswindowoncurrentvirtualdesktop).\n[BrowserMatches](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/ui/browser_finder.cc?q=BrowserMatches)\nin browser_finder.cc only returns a browser window on the current desktop.\n\nTo restore browser windows to the desktop they were last open on,\n[BrowserDesktopWindowTreeHostWin](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/ui/views/frame/browser_desktop_window_tree_host_win.cc)\nimplements GetWorkspace by using the\n[GetWindowDesktopId](https://docs.microsoft.com/en-us/windows/win32/api/shobjidl_core/nf-shobjidl_core-ivirtualdesktopmanager-getwindowdesktopid) method on\nIVirtualDesktopManager, and restores the workspace using\n[MoveWindowToDesktop](https://docs.microsoft.com/en-us/windows/win32/api/shobjidl_core/nf-shobjidl_core-ivirtualdesktopmanager-movewindowtodesktop),\nin its ::Init method.\n\nThe actual implementation is a bit more complicated in order to avoid\ncalling COM methods on the UI thread, or destroying COM objects on the UI\nthread, since doing so can cause nested message loops and re-entrant calls,\nleading to blocked UI threads and crashes. The\n[VirtualDesktopHelper](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/ui/views/frame/browser_desktop_window_tree_host_win.cc?q=VirtualDesktopHelper&sq=&ss=chromium%2Fchromium%2Fsrc)\nclass does the workspace handling for BrowserDesktopWindowTreeHostWin, including\ndoing all the COM operations on a separate COM task runner. The GetWorkspace\nmethod is synchronous so VirtualDesktopHelper has to remember and return the\nmost recent virtual desktop. Windows has no notification of a window changing\nvirtual desktops, so BrowserDesktopWindowTreeHostWin updates the virtual desktop\nof a window whenever it gets focus, and if it has changed, calls\nWindowTreeHost::OnHostWorkspaceChanged. This means that if a window is moved\nto a different virtual desktop, but doesn't get focus before the browser is shut\ndown, the browser window will be restored to the previous virtual desktop.\n\nWindows on different virtual desktops share the same coordinate system, so code\nthat iterates over windows generally needs to be Virtual Desktop-aware.\nFor example, the following places in code ignore windows not on the current\nvirtual desktop:\n\n * [LocalProcessWindowFinder::GetProcessWindowAtPoint](https://source.chromium.org/chromium/chromium/src/+/main:ui/display/win/local_process_window_finder_win.cc?q=LocalProcessWindowFinder::ShouldStopIterating&ss=chromium%2Fchromium%2Fsrc)\n * third_party/webrtc/modules/desktop_capture/win/window_capture_utils.cc\n * [Native Window occlusion tracker](https://source.chromium.org/chromium/chromium/src/+/main:ui/aura/native_window_occlusion_tracker_win.cc?q=WindowCanOccludeOtherWindowsOnCurrentVirtualDesktop&ss=chromium%2Fchromium%2Fsrc),\n when determining if a Chromium window is occluded/covered by other windows.\n Windows not on the current virtual desktop are considered occluded.\n\n\n"
  },
  {
    "path": "platforms/windows/windows_split_dll",
    "title": "Windows Split DLLs",
    "content": "# Windows Split DLLs\n\nA build mode where chrome.dll is split into two separate DLLs. This was\nundertaken as one possible workaround for toolchain limitations on Windows.\n\nWe removed support for this again after the toolchain limitations were fixed,\nsee https://crbug.com/726150.\n\n## How\n\nSplit DLL used to be controlled by the `is_multi_dll_chrome` gn variable.\n\n## Details\n\nThis forcible split was implemented by putting .lib files in either one DLL or\nthe other, and causing unresolved externals that result during linking to be\nforcibly exported from the other DLL. This works relatively cleanly for function\nimport/export, however it cannot work for data export.\n\nSome more details can be found on the initial commit of the `split_link` script\nhttps://src.chromium.org/viewvc/chrome?revision=200049&view=revision and the\nassociated bugs: https://crbug.com/237249 https://crbug.com/237267.\n"
  },
  {
    "path": "platforms/windows/windows_shortcut_and_taskbar_handling",
    "title": "Windows Shortcut and Pinned Taskbar Icon handling",
    "content": "# Windows Shortcut and Pinned Taskbar Icon handling\n\nWhen Chrome is installed on Windows, it creates a shortcut on the desktop that\nlaunches Chrome. It also adds the same shortcut to the start menu. These\nshortcuts do not specify a profile, so they launch Chrome with the most recently\nused profile.\n\nWindows allows users to pin applications to the taskbar. When a user\npins an application to the taskbar, Windows looks for a desktop shortcut that\nmatches the application, and if it finds one, it creates a .lnk file in the\ndirectory\n`<user dir>\\AppData\\Roaming\\Microsoft\\Internet Explorer\\Quick Launch\\User Pinned\\TaskBar.`\nIf it does not find a matching desktop shortcut, it creates an 8-hex-digit\nsub-directory of\n`<user dir>\\AppData\\Roaming\\Microsoft\\Internet Explorer\\Quick Launch\\ImplicitAppShortcuts\\`\nand puts the .lnk file in that directory. For example, 3ffff1b1b170b31e.\n\nApp windows on Windows have an\n[App User Model ID (AUMI)](https://docs.microsoft.com/en-us/windows/win32/shell/appids)\nproperty. For Chrome windows, this is set in\n[BrowserWindowPropertyManager::UpdateWindowProperties](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/ui/views/frame/browser_window_property_manager_win.cc?q=BrowserWindowPropertyManager::UpdateWindowProperties),\nwhen a window is opened. Windows desktop shortcuts have an app model property,\nand this should match the open window's AUMI. Windows groups open windows with\nthe same AUMI to a taskbar icon.\n\nThere are two kinds of Chrome windows with AUMI's: browser windows, and app\nwindows, which include web apps, and extensions, i.e., windows opened via\n--app-id or --app.\n\n[GetAppUserModelIdForBrowser](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/shell_integration_win.cc?q=GetAppUserModelIdForBrowser)\nconstructs an AUMI for a browser window and\n[GetAppUserModelIdForApp](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/shell_integration_win.cc?q=GetAppUserModelIdForApp)\nconstructs an AUMI for an app window. Each calls\n[ShellUtil::BuildAppUserModelId](https://source.chromium.org/chromium/chromium/src/+/main:chrome/installer/util/shell_util.cc;q=ShellUtil::BuildAppUserModelId)\nto construct the AUMI out of component strings.\n\nAll AUMI's start with the base app id,\n[install_static::GetBaseAppId](https://source.chromium.org/chromium/chromium/src/+/main:chrome/install_static/install_util.cc?q=install_static::GetBaseAppId).\nThis varies for different Chrome channels (e.g., Canary vs. Stable) and\ndifferent Chromium-based browsers (e.g., Chrome vs. Chromium).\n\nThe AUMI for a browser app has the format:\n`<BaseAppId>.<app_name>[.<profile_name>]`.\nprofile_name is only appended when it's not the default profile.\n\nThe AUMI for a Chrome browser window has the format:\n`<BaseAppId>[browser_suffix][.profile_name]`.\nprofile_name is only appended when it's not the default profile.\nbrowser_suffix is only appended to the BaseAppId if the installer\nhas set the kRegisterChromeBrowserSuffix command line switch, e.g.,\non user-level installs.\n\nSince AUMI's for browser and app windows include the profile_name, each\nprofile's windows will be grouped together on the taskbar.\n\nshell_integration_win.cc has a function [GetExpectedAppId](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/shell_integration_win.cc?q=GetExpectedAppid)\nto determine what the AUMI for a shortcut should be. It also has a function\n[MigrateTaskbarPins](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/shell_integration_win.cc?q=MigrateTaskbarPins)\nto migrate pinned taskbar icons if the AUMI's need to change.\n\n## Multi-profile Support\nWhen the user has more than one profile, the shortcuts are renamed to include\nthe profile name, e.g., `Chrome.lnk` becomes `<profile name> - Chrome`.  The\nshortcut icons, both desktop and taskbar, are badged with their profile icon.\nThis badged icon is also used in the tab preview for a Chrome window.\n\n## Diagnosing Issues\nTo dump a taskbar icon's properties, run this command:\n\n```\nvpython3 \\src\\chromium\\src\\chrome\\installer\\tools\\shortcut_properties.py \\\n    --dump-all \\\n    \"%APPDATA%\\Microsoft\\Internet Explorer\\Quick Launch\\User Pinned\\TaskBar\"\n```\n\nThis shows you the properties of all the taskbar pinned icons. If the taskbar\nicon is in a subdirectory of ImplicitApps, pass that directory to\nshortcut_properties.py.\n"
  },
  {
    "path": "platforms/windows/windows_pwa_integration",
    "title": "Windows Progressive Web App integration",
    "content": "# Windows Progressive Web App integration\n\n## Desktop Shortcuts\nWhen a Progressive Web App (PWA) is installed on Windows, Chrome creates a\ndesktop shortcut to the PWA, with the PWA icon. The shortcut launches a small\nChrome binary chrome_proxy.exe with the app id of the PWA, and the Chrome\nprofile the PWA is installed in. When chrome_proxy.exe runs, it launches Chrome\nwith the same command line options. The shortcut links to chrome_proxy.exe\ninstead of chrome.exe because of\n [a bug in Windows 10 start menu pinning](https://source.chromium.org/chromium/chromium/src/+/main:chrome/chrome_proxy/chrome_proxy_main_win.cc;l=23).\n\n## File handling support\nIn order to make Progressive Web Apps (PWA's) more like traditional apps, PWA's\nsupport opening files on the user's desktop. On Windows, when a PWA is\ninstalled, if the PWA's manifest lists one or more file extension types that it\nsupports opening, Chrome registers the PWA as a handler for the file\nextension(s), in the Windows registry. When the user right clicks on a file with\na registered extension, the PWA name and custom icon appears in the list of\napplications that can open the file. The user can also set the PWA as the\ndefault handler for the file extension.\n\nBecause of a limitation of the Windows shell, Chrome registers a per-PWA install \n[launcher app](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/web_applications/chrome_pwa_launcher/README.md;l=1) \n as a handler for the file extension. Chrome ships with a canonical launcher app\n called chrome_pwa_launcher.exe, which lives in the version sub-directory of the\n Chrome install dir. When a PWA is installed, we create a hard link from the\n PWA install dir `<profile_dir>/Web Applications/<app_id>` to the canonical launcher\n app. If the hard link fails (e.g., Chrome install dir is on a different drive\n than the profile dir), we copy the launcher app to the PWA install dir. In either\n case, the name of the launcher app in the PWA install dir is a sanitized version\n of the PWA name.\n\nRegistration starts in [web_app::RegisterFileHandlersWithOS](https://source.chromium.org/search?q=RegisterFileHandlersWithOS%20file:_win.cc&sq=),\n and works as follows: we create a unique\n [ProgID](https://docs.microsoft.com/en-us/windows/win32/com/-progid--key)\n for the PWA installation with the following format:\n`<BaseAppId>.<hash(Profile + AppID)>`\nWe use the hash due to 32-character limit for ProgID's. The registry work is\ndone in\n[ShellUtil::AddFileAssociations](https://source.chromium.org/chromium/chromium/src/+/main:chrome/installer/util/shell_util.cc?q=%20ShellUtil::AddFileAssociations):\n\n* Register the ProgID by adding key `HKCU\\Software\\Classes\\<progID>` to the registry.\n* Set the application name and icon for the PWA with these two keys:\n    * `HKCU\\Software\\Classes\\<progID>\\Application::ApplicationIcon = <path to icon in PWA install dir>,0`\n    * `HKCU\\Software\\Classes\\<progID>\\Application::ApplicationName = <PWA name>`\n* Hook up the command to launch the launcher app\n    * `HKCU\\Software\\Classes\\<progID>\\shell\\open\\command = <launcher_app_path_in_profile> --app-id=<app_id> --profile-directory=<profile_dir>`\n* Add a key to keep track of the file extensions registered for a progId,\nfor ease of uninstallation:\n    * `HKCU\\Software\\Classes\\<progID>\\File Extensions = <semicolon delimited list of extensions>`\n\nWhen Chrome is launched, it writes its path into the \"Last Browser\" file in\nthe User Data dir.\nWhen the launcher app is run, it launches Chrome using the path written into the\n\"Last Browser\" file. Because the launcher app is in a sub-directory of the profile\ndirectory, the \"Last Browser\" file is in its great grandparent directory.\n\nWhen a new version of Chrome is installed, we need to update the hard links\nto and copies of the installed launcher apps to use the newly installed canonical\nlauncher app. This is done by having the launcher app pass its version to Chrome, when\nlaunching Chrome. If the launcher app is out of date, Chrome updates all the\nlauncher apps in the current user data dir.\n\nWhen a PWA is uninstalled, we unregister the PWA as a handler for the file\nextensions it was registered for. When a PWA changes the file extensions it can\nhandle, we update the registry.\n\n## Miscellaneous\n * If the same PWA is registered in multiple profiles, we distinguish them by\nadding the profile name in parentheses to the PWA name, e.g,\n\"Example PWA (profile1)\". If a PWA is uninstalled from a  profile, and there is\none remaining install in another profile, we remove the profile name from the\napplication name.\n * Windows 7 does not support some of the registry entries needed to set the\n name and icon for a PWA. So, the file open context menu item for a PWA on\n Windows 7 gets its name from the launcher app created for the PWA, and uses a\n generic PWA icon.\n"
  },
  {
    "path": "platforms/windows/windows_native_window_occlusion_tracking",
    "title": "Windows Native Window Occlusion Detection",
    "content": "# Windows Native Window Occlusion Detection\n\n## Background\n\nUi::aura has an\n[API](https://source.chromium.org/chromium/chromium/src/+/main:ui/aura/window_occlusion_tracker.h)\nto track which aura windows are occluded, i.e., covered by\none or more other windows. If a window is occluded, Chromium treats foreground\ntabs as if they were background tabs; rendering stops, and js is throttled. On\nChromeOS, since all windows are aura windows, this is sufficient to determine\nif a Chromium window is covered by other windows. On Windows, we need to\nconsider native app windows when determining if a Chromium window is occluded.\nThis is implemented in\n[native_window_occlusion_tracker_win.cc](https://source.chromium.org/chromium/chromium/src/+/main:ui/aura/native_window_occlusion_tracker_win.cc).\n\n## Implementation\nWhen the core WindowOcclusionTracker decides to track a WindowTreeHost, it\ncalls EnableNativeWindowOcclusionTracking. On non-Windows platforms, this\ndoes nothing. On Windows, it calls ::Enable on the singleton\nNativeWindowOcclusionTrackerWin object, creating it first, if it hasn't already\nbeen created.\n\nWhen NativeWindowOcclusionTrackerWin starts tracking a WindowTreeHost, it adds\nthe HWND of the host's root window to a map of Windows HWNDs it is tracking,\nand its corresponding aura Window. It also starts observing the window to know\nwhen its visibility changes, or it is destroyed.\n\nThe main work of occlusion calculation is done by a helper class,\nWindowOcclusionCalculator, which runs on a separate COM task runner, in order\nto not block the UI thread. If the WindowOcclusionCalculator is tracking any\nwindows, it \n[registers](https://source.chromium.org/chromium/chromium/src/+/main:ui/aura/native_window_occlusion_tracker_win.cc?q=WindowOcclusionCalculator::RegisterEventHooks)\na set of\n[event](https://docs.microsoft.com/en-us/windows/win32/winauto/event-constants)\nhooks with Windows, in order to know when\nthe occlusion state might need to be recalculated. These events include window\nmove/resize, minimize/restore, foreground window changing, etc. Most of these\nare global event hooks, so that we get notified of events for all Windows\nwindows. For windows that could possibly\nocclude Chromium windows, (i.e., fully visible windows on the current virtual\ndesktop), we register for EVENT_OBJECT_LOCATIONCHANGE events for the window's\nprocess. pids_for_location_change_hook_ keeps track of which pids are hooked,\nand is\n[used to remove the hook](https://source.chromium.org/chromium/chromium/src/+/main:ui/aura/native_window_occlusion_tracker_win.cc;drc=eeee643ae963e1d78c7457184f8af93f48bba9d3;l=443)\nif the process no longer has any windows open.\n\nWhen the event handler gets notified of an event, it usually kicks off new\nocclusion calculation, which runs after a 16ms timer. It doesn't do a new\nocclusion calculation if the timer is currently running. 16ms corresponds to the\ninterval between frames when displaying 60 frames per second(FPS). There's\nno point in doing occlusion calculations more frequently than frames are\ndisplayed. If the user is in the middle of moving a window around, occlusion\nisn't calculated until the window stops moving, because moving a window is\nessentially modal, and there's no point in recalculating occlusion over and\nover again for each incremental move event.\n\nTo calculate occlusion, we first mark minimized Chromium windows as hidden, and\nChromium windows on a different virtual desktop as occluded.  We compute the\nSKRegion for the virtual screen, which takes multiple monitor configurations\ninto account, and set the initial unoccluded_desktop_region_ to the screen\nregion. Then, we enumerate all the HWNDs, in z-order (topmost window first).\nFor each occluding window (visible, not transparent, etc), we save the current\nunoccluded_desktop_region_, and subtract the window's window_rect from the\nunoccluded_desktop_region_ . If the hwnd is not a root Chromium window, we\ncontinue to the next hwnd. If it is a root Chromium window, then we have seen\nall the windows above it, and know whether it is occluded or not. We determine\nthis by checking if subtracting its window_rect from the\nunoccluded_desktop_region_ actually changed the unoccluded_desktop_region_. If\nnot, that means previous windows occluded the current window's window_rect, and\nit is occluded, otherwise, not.\nOnce the occlusion state of all root Chromium windows has been determined, the\nWindowOcclusionTracker posts a task to the ui thread to run a callback on the\nNativeWindowOcclusionTrackerWin object. That callback is\n[NativeWindowOcclusionTrackerWin::UpdateOcclusionState](https://source.chromium.org/chromium/chromium/src/+/main:ui/aura/native_window_occlusion_tracker_win.cc;l=226?q=NativeWindowOcclusionTrackerWin::UpdateOcclusionState)\n, and is passed\nroot_window_hwnds_occlusion_state_, which is a map between root window HWNDs\nand their calculated occlusion state.\nNativeWindowOcclusionTrackerWin::UpdateOcclusionState iterates over those HWNDs,\nfinds the corresponding root window, and calls SetNativeWindowOcclusionState on\nits WindowTreeHost, with the corresponding HWND's occlusion state from the map.\nIf the screen is locked, however, it sets the occlusion state to OCCLUDED.\n\n## Miscellaneous\n\n * If a window is falsely determined to be occluded, the content area will be\nwhite.\n * When the screen is locked, all Chromium windows are considered occluded.\n * Windows on other virtual desktops are considered occluded.\n * Transparent windows, cloaked windows, floating windows, non-rectangular\n windows, etc, are not considered occluding.\n"
  },
  {
    "path": "platforms/windows/windows_build_instructions",
    "title": "Checking out and Building Chromium for Windows",
    "content": "# Checking out and Building Chromium for Windows\n\nThere are instructions for other platforms linked from the\n[get the code](get_the_code.md) page.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-chrome-win](https://goto.google.com/building-chrome-win) instead.\n\n[TOC]\n\n## System requirements\n\n* An x86-64 machine with at least 8GB of RAM. More than 16GB is highly\n  recommended.\n* At least 100GB of free disk space on an NTFS-formatted hard drive. FAT32\n  will not work, as some of the Git packfiles are larger than 4GB.\n* An appropriate version of Visual Studio, as described below.\n* Windows 10 or newer.\n\n## Setting up Windows\n\n### Visual Studio\n\nChromium requires [Visual Studio 2022](https://learn.microsoft.com/en-us/visualstudio/releases/2022/release-notes)\n(>=17.0.0) to build. Visual Studio can also be used to debug Chromium.\nThe clang-cl compiler is used but Visual Studio's header files, libraries, and\nsome tools are required. Visual Studio Community Edition should work if its\nlicense is appropriate for you. You must install the \"Desktop development with\nC++\" component and the \"MFC/ATL support\" sub-components. This can be done from\nthe command line by passing these arguments to the Visual Studio installer (see\nbelow for ARM64 instructions):\n```shell\n$ PATH_TO_INSTALLER.EXE ^\n--add Microsoft.VisualStudio.Workload.NativeDesktop ^\n--add Microsoft.VisualStudio.Component.VC.ATLMFC ^\n--includeRecommended\n```\n\nIf you want to build for ARM64 Win32 then some extra arguments are needed. The\nfull set for that case is:\n```shell\n$ PATH_TO_INSTALLER.EXE ^\n--add Microsoft.VisualStudio.Workload.NativeDesktop ^\n--add Microsoft.VisualStudio.Component.VC.ATLMFC ^\n--add Microsoft.VisualStudio.Component.VC.Tools.ARM64 ^\n--add Microsoft.VisualStudio.Component.VC.MFC.ARM64 ^\n--includeRecommended\n```\n\nRequired\n\n* [Windows 11 SDK](https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/)\nversion 10.0.22621.2428. This can be installed separately or by checking the\nappropriate box in the Visual Studio Installer.\n* (Windows 11) SDK Debugging Tools 10.0.22621.755 or higher. This version of the\nDebugging tools is needed in order to support reading the large-page PDBs that\nChrome uses to allow greater-than 4 GiB PDBs. This can be installed after the\nmatching Windows SDK version is installed, from: Control Panel -> Programs and\nFeatures -> Windows Software Development Kit [version] -> Change -> Debugging Tools for\nWindows. If building on ARM64 Windows then you will need to manually copy the\nDebuggers\\x64 directory from another machine because it does not get installed\non ARM64 and is needed, whether you are building Chromium for x64 or ARM64 on\nARM64.\n\nWARNING: On sufficiently old versions of Windows (1909 or earlier), dawn (or\nrelated components) may fail with a D3d-related error when using the 26100 SDK.\nThis is because the d3dcompiler_47.dll file in the new SDK attempts to\ndynamically link versions of the Universal C Runtime which are not present by\ndefault on older systems. If you experience these errors, you can either update\nthe UCRT on your system, or install the 22612 SDK and use the d3dcompiler_47.dll\nfile included there, which statically links the UCRT.\n\nThis problem may also manifest as a DLL failure to load `__CxxFrameHandler4`.\n\n## git installation\n\n### Install git\n\nIf you haven't installed `git` directly before, you can download a standalone\ninstaller for the latest version of Git For Windows from the Git website at\nhttps://git-scm.com/download/win.\n\nFor more information on Git for Windows (which is a separate project from Git),\nsee https://gitforwindows.org.\n\nNote: if you are a Google employee, see git installation instructions at\n[go/building-chrome-win](https://goto.google.com/building-chrome-win#install-updates-and-required-software).\n\n### Update git\n\nNote: this section is about updating a direct installation of `git` because\n`depot_tools` will soon stop bundling `git`.\n\nUpdating to the latest version of `git` will depend on which version you\ncurrently have installed. First, check your `git` version. From a cmd.exe shell,\nrun:\n```shell\n$ git version\n```\n\n| Current version | How to update to latest |\n| --- | --- |\n| `2.14.1` or earlier | You will need to manually uninstall Git, then follow the instructions above to [install git](#install-git) |\n| `2.14.2` to `2.16.1` | In a cmd.exe shell, run: `git update` |\n| `2.16.1(2)` and later | In a cmd.exe shell, run: `git update-git-for-windows` |\n\n## Install `depot_tools`\n\n***\n**Warning:** `depot_tools` will stop bundling Git for Windows from Sep 23, 2024\nonwards. To prepare for this change, Windows users should\n[install Git](#git-installation) directly before then.\n***\n\nDownload the\n[depot_tools bundle](https://storage.googleapis.com/chrome-infra/depot_tools.zip)\nand extract it somewhere (eg: C:\\src\\depot_tools).\n\n***\n**Warning:** **DO NOT** use drag-n-drop or copy-n-paste extract from Explorer,\nthis will not extract the hidden “.git” folder which is necessary for\ndepot_tools to autoupdate itself. You can use “Extract all…” from the\ncontext menu though.\n***\n\nAdd depot_tools to the start of your PATH (must be ahead of any installs of\nPython. Note that environment variable names are case insensitive).\n* Assuming you unzipped the bundle to `C:\\src\\depot_tools`, open:\n  Control Panel → System and Security → System\n* Select which PATH variable to edit.\n  * If you have Administrator access, you can edit the **system** PATH. Click\n  Advanced system settings → Environment Variables. Under \"System variables\",\n  select the Path variable for editing.\n  * If you don't have Administrator access, you can edit your **user-level**\n  PATH. Search for \"Edit environment variables for your account\". Under \"User\n  variables for %USER%\", select the Path variable for editing.\n* Modify the Path variable by adding `C:\\src\\depot_tools` at the front (or at\n  least in front of any directory that might already have a copy of Python).\n  Note: If you can only modify your user-level PATH and the system PATH has a\n  Python in it, you will be out of luck.\n\nAlso, add a DEPOT_TOOLS_WIN_TOOLCHAIN environment variable in the same way, and\nset it to 0. This tells depot_tools to use your locally installed version of\nVisual Studio (by default, depot_tools will try to use a google-internal\nversion).\n\nYou may also have to set variable `vs2022_install` to your installation path of\nVisual Studio 2022, like\n`set vs2022_install=C:\\Program Files\\Microsoft Visual Studio\\2022\\Professional`.\n\nFrom a cmd.exe shell, run:\n```shell\n$ gclient\n```\n\nOn first run, gclient will install all the Windows-specific bits needed to work\nwith the code, including msysgit and python.\n\n* If you run gclient from a non-cmd shell (e.g., cygwin, PowerShell),\n  it may appear to run properly, but msysgit, python, and other tools\n  may not get installed correctly.\n* If you see strange errors with the file system on the first run of gclient,\n  you may want to [disable Windows Indexing](https://tortoisesvn.net/faq.html#cantmove2).\n\n## Check python install\n\nAfter running gclient open a command prompt and type `where python3` and\nconfirm that the depot_tools `python3.bat` comes ahead of any copies of\npython3.exe. Failing to ensure this can lead to overbuilding when\nusing gn - see [crbug.com/611087](https://crbug.com/611087).\n\n[App Execution Aliases](https://docs.microsoft.com/en-us/windows/apps/desktop/modernize/desktop-to-uwp-extensions#alias)\ncan conflict with other installations of python on the system so disable\nthese for 'python.exe' and 'python3.exe' by opening 'App execution aliases'\nsection of Control Panel and unticking the boxes next to both of these\nthat point to 'App Installer'.\n\n## Get the code\n\nFirst, configure Git:\n\n```shell\n$ git config --global user.name \"My Name\"\n$ git config --global user.email \"my-name@chromium.org\"\n$ git config --global core.autocrlf false\n$ git config --global core.filemode false\n$ git config --global core.preloadindex true\n$ git config --global core.fscache true\n$ git config --global branch.autosetuprebase always\n```\n\nWhile not necessarily required it can be helpful to configure git to allow long\npath support (beyond the Windows MAX_PATH limit):\n\n```shell\ngit config --global core.longpaths true\n```\n\nCreate a `chromium` directory for the checkout and change to it. You can call\nthis whatever you like and put it wherever you like, as long as the full path\nhas no spaces. However there are some performance benefits for Googlers in\nplacing the directory under `C:\\src\\`\n(See [Why is my build slow?](https://chromium.googlesource.com/chromium/src/+/main/docs/windows_build_instructions.md#why-is-my-build-slow)).\n\n```shell\n$ mkdir chromium && cd chromium\n```\n\nRun the `fetch` tool from `depot_tools` to check out the code and its\ndependencies.\n\n```shell\n$ fetch chromium\n```\n\nIf you don't want the full repo history, you can save a lot of time by\nadding the `--no-history` flag to `fetch`.\n\nExpect the command to take over an hour on even a fast connection, and many\nhours on slower ones. You should configure your PC so that it doesn't sleep\nor hibernate during the fetch or else errors may occur. If errors occur while\nfetching sub-repos then you can start over, or you may be able to correct them\nby going to the chromium/src directory and running this command:\n\n```shell\n$ gclient sync\n```\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\n$ cd src\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory:\n\n```shell\n$ gn gen out\\Default\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings or using an alternate\n  version of Visual Studio, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build matching the current host\n  operating system and CPU.\n* For more info on GN, run `gn help` on the command line or read the [quick\n  start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n\n### Faster builds\n\n* Reduce file system overhead by excluding build directories from\n  antivirus and indexing software.\n* Store the build tree on a fast disk (preferably SSD).\n* The more cores the better (20+ is not excessive) and lots of RAM is needed\n(64 GB is not excessive).\n\nThere are some gn flags that can improve build speeds. You can specify these\nin the editor that appears when you create your output directory\n(`gn args out\\Default`) or on the gn gen command line\n(`gn gen out\\Default --args=\"is_component_build = true is_debug = true\"`).\nSome helpful settings to consider using include:\n* `is_component_build = true` - this uses more, smaller DLLs, and may avoid\nhaving to relink chrome.dll after every change.\n* `enable_nacl = false` - this disables Native Client which is usually not\nneeded for local builds.\n* `target_cpu = \"x86\"` - x86 builds may be slightly faster than x64 builds. Note\nthat if you set this but don't set `enable_nacl = false` then build times may\nget worse.\n* `blink_symbol_level = 0` - turn off source-level debugging for\nblink to reduce build times, appropriate if you don't plan to debug blink.\n* `v8_symbol_level = 0` - turn off source-level debugging for v8 to reduce\nbuild times, appropriate if you don't plan to debug v8.\n\nIn order to speed up linking you can set `symbol_level = 1` or\n`symbol_level = 0` - these options reduce the work the compiler and linker have\nto do. With `symbol_level = 1` the compiler emits file name and line number\ninformation so you can still do source-level debugging but there will be no\nlocal variable or type information. With `symbol_level = 0` there is no\nsource-level debugging but call stacks still have function names. Changing\n`symbol_level` requires recompiling everything.\n\nWhen invoking ninja, specify 'chrome' as the target to avoid building all test\nbinaries as well.\n\n#### Use Reclient\n\nIn addition, Google employees should use Reclient, a distributed compilation\nsystem. Detailed information is available internally but the relevant gn arg is:\n* `use_remoteexec = true`\n\nGoogle employees can visit\n[go/building-chrome-win#setup-remote-execution](https://goto.google.com/building-chrome-win#setup-remote-execution)\nfor more information. For external contributors, Reclient does not support\nWindows builds.\n\n#### Use SCCACHE\n\nYou might be able to use [sccache](https://github.com/mozilla/sccache) for the\nbuild process by enabling the following arguments:\n\n* `cc_wrapper = \"sccache\"` - assuming the `sccache` binary is in your `%PATH%`\n\n### Why is my build slow?\n\nMany things can make builds slow, with Windows Defender slowing process startups\nbeing a frequent culprit. Have you ensured that the entire Chromium src\ndirectory is excluded from antivirus scanning (on Google machines this means\nputting it in a ``src`` directory in the root of a drive)? Have you tried the\ndifferent settings listed above, including different link settings and -j\nvalues? Have you asked on the chromium-dev mailing list to see if your build is\nslower than expected for your machine's specifications?\n\nIf you suspect that Defender is slowing your build then you can try Microsoft's\n[Performance analyzer for Microsoft Defender Antivirus](https://learn.microsoft.com/en-us/microsoft-365/security/defender-endpoint/tune-performance-defender-antivirus?view=o365-worldwide)\nto investigate in detail.\n\nThe next step is to gather some data. If you set the ``NINJA_SUMMARIZE_BUILD``\nenvironment variable to 1 then ``autoninja`` will do three things. First, it\nwill set the [NINJA_STATUS](https://ninja-build.org/manual.html#_environment_variables)\nenvironment variable so that ninja will print additional information while\nbuilding Chrome. It will show how many build processes are running at any given\ntime, how many build steps have completed, how many build steps have completed\nper second, and how long the build has been running, as shown here:\n\n```shell\n$ set NINJA_SUMMARIZE_BUILD=1\n$ autoninja -C out\\Default base\nninja: Entering directory `out\\Default'\n[1 processes, 86/86 @ 2.7/s : 31.785s ] LINK(DLL) base.dll base.dll.lib base.dll.pdb\n```\n\nThis makes slow process creation immediately obvious and lets you tell quickly\nif a build is running more slowly than normal.\n\nIn addition, setting ``NINJA_SUMMARIZE_BUILD=1`` tells ``autoninja`` to print a\nbuild performance summary when the build completes, showing the slowest build\nsteps and slowest build-step types, as shown here:\n\n```shell\n$ set NINJA_SUMMARIZE_BUILD=1\n$ autoninja -C out\\Default base\nLongest build steps:\n       0.1 weighted s to build obj/base/base/trace_log.obj (6.7 s elapsed time)\n       0.2 weighted s to build nasm.exe, nasm.exe.pdb (0.2 s elapsed time)\n       0.3 weighted s to build obj/base/base/win_util.obj (12.4 s elapsed time)\n       1.2 weighted s to build base.dll, base.dll.lib (1.2 s elapsed time)\nTime by build-step type:\n       0.0 s weighted time to generate 6 .lib files (0.3 s elapsed time sum)\n       0.1 s weighted time to generate 25 .stamp files (1.2 s elapsed time sum)\n       0.2 s weighted time to generate 20 .o files (2.8 s elapsed time sum)\n       1.7 s weighted time to generate 4 PEFile (linking) files (2.0 s elapsed\ntime sum)\n      23.9 s weighted time to generate 770 .obj files (974.8 s elapsed time sum)\n26.1 s weighted time (982.9 s elapsed time sum, 37.7x parallelism)\n839 build steps completed, average of 32.17/s\n```\n\nThe \"weighted\" time is the elapsed time of each build step divided by the number\nof tasks that were running in parallel. This makes it an excellent approximation\nof how \"important\" a slow step was. A link that is entirely or mostly serialized\nwill have a weighted time that is the same or similar to its elapsed time. A\ncompile that runs in parallel with 999 other compiles will have a weighted time\nthat is tiny.\n\nYou can also generate these reports by manually running the script after a\nbuild:\n\n```shell\n$ python depot_tools\\post_build_ninja_summary.py -C out\\Default\n```\n\nFinally, setting ``NINJA_SUMMARIZE_BUILD=1`` tells autoninja to tell Ninja to\nreport on its own overhead by passing \"-d stats\". This can be helpful if, for\ninstance, process creation (which shows up in the StartEdge metric) is making\nbuilds slow, perhaps due to antivirus interference due to clang-cl not being in\nan excluded directory:\n\n```shell\n$ set NINJA_SUMMARIZE_BUILD=1\n$ autoninja -C out\\Default base\nmetric                  count   avg (us)        total (ms)\n.ninja parse            3555    1539.4          5472.6\ncanonicalize str        1383032 0.0             12.7\ncanonicalize path       1402349 0.0             11.2\nlookup node             1398245 0.0             8.1\n.ninja_log load         2       118.0           0.2\n.ninja_deps load        2       67.5            0.1\nnode stat               2516    29.6            74.4\ndepfile load            2       1132.0          2.3\nStartEdge               88      3508.1          308.7\nFinishCommand           87      1670.9          145.4\nCLParser::Parse         45      1889.1          85.0\n```\n\nYou can also get a visual report of the build performance with\n[ninjatracing](https://github.com/nico/ninjatracing). This converts the\n.ninja_log file into a .json file which can be loaded into [chrome://tracing](chrome://tracing):\n\n```shell\n$ python ninjatracing out\\Default\\.ninja_log >build.json\n```\n\n## Build Chromium\n\nBuild Chromium (the \"chrome\" target) with Ninja using the command:\n\n```shell\n$ autoninja -C out\\Default chrome\n```\n\n`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.\n\nYou can get a list of all of the other build targets from GN by running\n`gn ls out\\Default` from the command line. To compile one, pass to Ninja\nthe GN label with no preceding \"//\" (so for `//chrome/test:unit_tests`\nuse `autoninja -C out\\Default chrome/test:unit_tests`).\n\n## Compile a single file\n\nNinja supports a special [syntax `^`][ninja hat syntax] to compile a single\nobject file specifying the source file. For example, `ninja -C\nout/Default ../../base/logging.cc^` compiles `obj/base/base/logging.o`.\n\n[ninja hat syntax]: https://ninja-build.org/manual.html#:~:text=There%20is%20also%20a%20special%20syntax%20target%5E%20for%20specifying%20a%20target%20as%20the%20first%20output%20of%20some%20rule%20containing%20the%20source%20you%20put%20in%20the%20command%20line%2C%20if%20one%20exists.%20For%20example%2C%20if%20you%20specify%20target%20as%20foo.c%5E%20then%20foo.o%20will%20get%20built%20(assuming%20you%20have%20those%20targets%20in%20your%20build%20files)\n\nWith autoninja, you need to add  `^^` to preserve the trailing `^`.\n\n```shell\n$ autoninja -C out\\Default ..\\..\\base\\logging.cc^^\n```\n\nIn addition to `foo.cc^^`, Siso also supports `foo.h^^` syntax to compile\nthe corresponding `foo.o` if it exists.\n\nIf you run a `bash` shell, you can use the following script to ease invocation:\n\n```shell\n#!/bin/sh\nfiles=(\"${@/#/..\\/..\\/}\")\nautoninja -C out/Default ${files[@]/%/^^}\n```\n\nThis script assumes it is run from `src` and your output dir is `out/Default`;\nit invokes `autoninja` to compile all given files. If you place it in your\n`$PATH` and name it e.g. `compile`, you can invoke like this:\n\n```shell\n$ pwd  # Just to illustrate where this is run from\n/c/src\n$ compile base/time/time.cc base/time/time_unittest.cc\n...\n[0/47] 5.56s S CXX obj/base/base/time.obj\n...\n[2/3] 9.27s S CXX obj/base/base_unittests/time_unittest.obj\n...\n```\n\n## Run Chromium\n\nOnce it is built, you can simply run the browser:\n\n```shell\n$ out\\Default\\chrome.exe\n```\n\n(The \".exe\" suffix in the command is actually optional).\n\n## Running test targets\n\nTests are split into multiple test targets based on their type and where they\nexist in the directory structure. To see what target a given unit test or\nbrowser test file corresponds to, the following command can be used:\n\n```shell\n$ gn refs out\\Default --testonly=true --type=executable --all chrome\\browser\\ui\\browser_list_unittest.cc\n//chrome/test:unit_tests\n```\n\nIn the example above, the target is unit_tests. The unit_tests binary can be\nbuilt by running the following command:\n\n```shell\n$ autoninja -C out\\Default unit_tests\n```\n\nYou can run the tests by running the unit_tests binary. You can also limit which\ntests are run using the `--gtest_filter` arg, e.g.:\n\n```shell\n$ out\\Default\\unit_tests.exe --gtest_filter=\"BrowserListUnitTest.*\"\n```\n\nYou can find out more about GoogleTest at its\n[GitHub page](https://github.com/google/googletest).\n\n## Build an Installer\n\nBuild the `mini_installer` target to create a self-contained installer. This\nhas everything needed to install your browser on a machine.\n\n```shell\n$ autoninja -C out\\Default mini_installer\n```\n\nSee [//chrome/installer/setup/README.md](../chrome/installer/setup/README.md)\nand [//chrome/installer/mini_installer/README.md](../chrome/installer/mini_installer/README.md)\nfor more information.\n\n## Update your checkout\n\nTo update an existing checkout, you can run\n\n```shell\n$ git rebase-update\n$ gclient sync -D\n```\n\nThe first command updates the primary Chromium source repository and rebases\nany of your local branches on top of tip-of-tree (aka the Git branch\n`origin/main`). If you don't want to use this script, you can also just use\n`git pull` or other common Git commands to update the repo.\n\nThe second command syncs the subrepositories to the appropriate versions,\ndeleting those that are no longer needed, and re-runs the hooks as needed.\n\n### Editing and Debugging With the Visual Studio IDE\n\nYou can use the Visual Studio IDE to edit and debug Chrome, with or without\nIntellisense support.\n\n#### Using Visual Studio Intellisense\n\nIf you want to use Visual Studio Intellisense when developing Chromium, use the\n`--ide` command line argument to `gn gen` when you generate your output\ndirectory (as described on the [get the code](https://dev.chromium.org/developers/how-tos/get-the-code)\npage). This is an example when your checkout is `C:\\src\\chromium` and your\noutput directory is `out\\Default`:\n\n```shell\n$ gn gen --ide=vs --ninja-executable=C:\\src\\chromium\\src\\third_party\\ninja\\ninja.exe out\\Default\n$ devenv out\\Default\\all.sln\n```\n\nGN will produce a file `all.sln` in your build directory. It will internally\nuse Ninja to compile while still allowing most IDE functions to work (there is\nno native Visual Studio compilation mode). If you manually run \"gen\" again you\nwill need to resupply this argument, but normally GN will keep the build and\nIDE files up to date automatically when you build.\n\nThe generated solution will contain several thousand projects and will be very\nslow to load. Use the `--filters` argument to restrict generating project files\nfor only the code you're interested in. Although this will also limit what\nfiles appear in the project explorer, debugging will still work and you can\nset breakpoints in files that you open manually. A minimal solution that will\nlet you compile and run Chrome in the IDE but will not show any source files\nis:\n\n```\n$ gn gen --ide=vs --ninja-executable=C:\\src\\chromium\\src\\third_party\\ninja\\ninja.exe --filters=//chrome --no-deps out\\Default\n```\n\nYou can selectively add other directories you care about to the filter like so:\n`--filters=//chrome;//third_party/WebKit/*;//gpu/*`.\n\nThere are other options for controlling how the solution is generated, run `gn\nhelp gen` for the current documentation.\n\n#### Using Visual Studio without Intellisense\n\nIt is also possible to debug and develop Chrome in Visual Studio without the\noverhead of a multi-project solution file. Simply \"open\" your chrome.exe binary\nwith `File->Open->Project/Solution`, or from a Visual Studio command prompt like\nso: `devenv /debugexe out\\Debug\\chrome.exe <your arguments>`. Many of Visual\nStudio's code exploration features will not work in this configuration, but by\ninstalling the [VsChromium Visual Studio Extension](https://chromium.github.io/vs-chromium/)\nyou can get the source code to appear in the solution explorer window along\nwith other useful features such as code search. You can add multiple executables\nof interest (base_unittests.exe, browser_tests.exe) to your solution with\n`File->Add->Existing Project...` and change which one will be debugged by\nright-clicking on them in `Solution Explorer` and selecting `Set as Startup\nProject`. You can also change their properties, including command line\narguments, by right-clicking on them in `Solution Explorer` and selecting\n`Properties`.\n\nBy default when you start debugging in Visual Studio the debugger will only\nattach to the main browser process. To debug all of Chrome, install\n[Microsoft's Child Process Debugging Power Tool](https://blogs.msdn.microsoft.com/devops/2014/11/24/introducing-the-child-process-debugging-power-tool/).\nYou will also need to run Visual Studio as administrator, or it will silently\nfail to attach to some of Chrome's child processes.\n\n### Improving performance of git commands\n\n#### Configure git to use an untracked cache\n\nTry running\n\n```shell\n$ git update-index --test-untracked-cache\n```\n\nIf the output ends with `OK`, then the following may also improve performance of\n`git status`:\n\n```shell\n$ git config core.untrackedCache true\n```\n\n#### Configure git to use fsmonitor\n\nYou can significantly speed up git by using [fsmonitor.](https://github.blog/2022-06-29-improve-git-monorepo-performance-with-a-file-system-monitor/)\nYou should enable fsmonitor in large repos, such as Chromium and v8. Enabling\nit globally will launch many processes and consume excess commit/memory and\nprobably isn't worthwhile. The command to enable fsmonitor in the current repo\nis:\n\n```shell\n$ git config core.fsmonitor true\n```\n"
  },
  {
    "path": "platforms/mac/mac_lld",
    "title": "LLD for Mac builds",
    "content": "# LLD for Mac builds\n\nLike on other platforms, Chromium uses the LLD linker on iOS and macOS.\n\n## Background\n\nChromium uses [LLD](https://lld.llvm.org/) as linker on all platforms.\nLLD is faster than other ELF linkers (ELF\nis the executable file format used on most OSs, including Linux, Android,\nChrome OS, Fuchsia), and it's faster than other COFF linkers (the executable\nfile format on Windows).\n\nLLD is currently twice as fast as ld64, the macOS system linker, at linking\nChromium Framework in symbol\\_level=0 release builds, despite ld64 being already\nfast. (Before Xcode 14.1, LLD was 6x as fast as ld64.)\n\nLLD has advantages unrelated to speed, too:\n\n- It's developed in the LLVM repository, and we ship it in our clang package.\n  We can fix issues upstream and quickly deploy fixed versions, instead of\n  having to wait for Xcode releases (which is where ld64 ships).\n\n- For the same reason, it has a much simpler LTO setup: Clang and LLD both link\n  in the same LLVM libraries that are built at the same revision, and compiler\n  and linker bitcodes are interopable for that reason. With ld64, the LTO code\n  has to be built as a plugin that's loaded by the linker.\n\n- LLD/Mach-O supports \"LLVM-y\" features that the ELF and COFF LLDs support as\n  well, such as thin archives, colored diagnostics, and response files\n  (ld64 supports this too as of Xcode 12, but we had to wait many years for it,\n  and it's currently [too crashy](https://crbug.com/1147968) to be usable).\n\n- While LLD for ELF, LLD for COFF, and LLD for MachO are mostly independent\n  codebases, they all use LLVM libraries. That gives them similar behavior.\n  Using LLD unifies the build across platforms somewhat.\n\nFor that reason, we moved to LLD for iOS and macOS builds.\n\nJust like the LLD ELF port tries to be commandline-compatible with other ELF\nlinkers and the LLD COFF port tries to be commandline-compatible with the\nVisual Studio linker link.exe, the LLD Mach-O port tries to be\ncommandline-compatible with ld64. This means LLD accepts different flags on\ndifferent platforms.\n\n## Current status and known issues\n\nLLD is used by default in all build configurations.\nAll tests on all bots are passing, both Intel and Arm.\nMost things even work.\n\n## Hacking on LLD\n\nIf you want to work on LLD, follow [this paragraph](clang.md#Using-a-custom-clang-binary).\n\n## Creating stand-alone repros for bugs\n\nFor simple cases, LLD's `--reproduce=foo.tar` flag / `LLD_REPRODUCE=foo.tar`\nenv var is sufficient.\n\nSee \"Note to self:\" [here](https://bugs.llvm.org/show_bug.cgi?id=48657#c0) for\nmaking a repro file that involved the full app and framework bundles.\n"
  },
  {
    "path": "platforms/mac/mac_build_instructions",
    "title": "Checking out and building Chromium for Mac",
    "content": "# Checking out and building Chromium for Mac\n\nThere are instructions for other platforms linked from the\n[get the code](get_the_code.md) page.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-chrome](https://goto.google.com/building-chrome) instead.\n\n[TOC]\n\n## System requirements\n\n*   A Mac, Intel or Arm.\n    ([More details about Arm Macs](https://chromium.googlesource.com/chromium/src.git/+/main/docs/mac_arm64.md).)\n*   [Xcode](https://developer.apple.com/xcode/). Xcode comes with...\n*   The macOS SDK. Run\n\n    ```shell\n    $ ls `xcode-select -p`/Platforms/MacOSX.platform/Developer/SDKs\n    ```\n\n    to check whether you have it, and what version you have.\n    `mac_sdk_official_version` in [mac_sdk.gni](../build/config/mac/mac_sdk.gni)\n    is the SDK version used on all the bots and for\n    [official builds](https://source.chromium.org/search?q=MAC_BINARIES_LABEL&ss=chromium),\n    so that version is guaranteed to work. Building with a newer SDK usually\n    works too (please fix or file a bug if it doesn't).\n\n    Building with an older SDK might also work, but if it doesn't then we won't\n    accept changes for making it work.\n\n    The easiest way to get the newest SDK is to use the newest version of Xcode,\n    which often requires using the newest version of macOS. We don't use Xcode\n    itself much, so if you're know what you're doing, you can likely get the\n    build working with an older version of macOS as long as you get a new\n    version of the macOS SDK on it.\n*   An APFS-formatted volume (this is the default format for macOS volumes).\n\n## Install `depot_tools`\n\nClone the `depot_tools` repository:\n\n```shell\n$ git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\n```\n\nAdd `depot_tools` to the end of your PATH (you will probably want to put this in\nyour `~/.bash_profile` or `~/.zshrc`). Assuming you cloned `depot_tools` to\n`/path/to/depot_tools` (note: you **must** use the absolute path or Python will\nnot be able to find infra tools):\n\n```shell\n$ export PATH=\"$PATH:/path/to/depot_tools\"\n```\n\n## Get the code\n\nCreate a `chromium` directory for the checkout and change to it (you can call\nthis whatever you like and put it wherever you like, as long as the full path\nhas no spaces):\n\n```shell\n$ mkdir chromium && cd chromium\n```\n\nRun the `fetch` tool from `depot_tools` to check out the code and its\ndependencies.\n\n```shell\n$ caffeinate fetch chromium\n```\n\nRunning the `fetch` with `caffeinate` is optional, but it will prevent the\nsystem from sleeping for the duration of the `fetch` command, which may run for\na considerable amount of time.\n\nIf you don't need the full repo history, you can save time by using\n`fetch --no-history chromium`. You can call `git fetch --unshallow` to retrieve\nthe full history later.\n\nExpect the command to take 30 minutes on even a fast connection, and many\nhours on slower ones.\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\n$ cd src\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory:\n\n```shell\n$ gn gen out/Default\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build matching the current host\n  operating system and CPU.\n* For more info on GN, run `gn help` on the command line or read the\n  [quick start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n* Building Chromium for arm Macs requires [additional setup](mac_arm64.md).\n\n\n### Faster builds\n\nFull rebuilds are about the same speed in Debug and Release, but linking is a\nlot faster in Release builds.\n\nPut\n\n```\nis_debug = false\n```\n\nin your `args.gn` to do a release build.\n\nPut\n\n```\nis_component_build = true\n```\n\nin your `args.gn` to build many small dylibs instead of a single large\nexecutable. This makes incremental builds much faster, at the cost of producing\na binary that opens less quickly. Component builds work in both debug and\nrelease.\n\nPut\n\n```\nsymbol_level = 0\n```\n\nin your args.gn to disable debug symbols altogether.  This makes both full\nrebuilds and linking faster (at the cost of not getting symbolized backtraces\nin gdb).\n\n#### Use Reclient\n\nIn addition, Google employees should use Reclient, a distributed compilation system.\nDetailed information is available internally but the relevant gn arg is:\n* `use_remoteexec = true`\n\nGoogle employees can visit\n[go/building-chrome-mac#using-remote-execution](https://goto.google.com/building-chrome-mac#using-remote-execution)\nfor more information. For external contributors, Reclient does not support Mac\nbuilds.\n\n#### CCache\n\nYou might also want to [install ccache](ccache_mac.md) to speed up the build.\n\n## Build Chromium\n\nBuild Chromium (the \"chrome\" target) with Ninja using the command:\n\n```shell\n$ autoninja -C out/Default chrome\n```\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.)\n\nYou can get a list of all of the other build targets from GN by running `gn ls\nout/Default` from the command line. To compile one, pass the GN label to Ninja\nwith no preceding \"//\" (so, for `//chrome/test:unit_tests` use `autoninja -C\nout/Default chrome/test:unit_tests`).\n\n## Run Chromium\n\nOnce it is built, you can simply run the browser:\n\n```shell\n$ out/Default/Chromium.app/Contents/MacOS/Chromium\n```\n\n## Avoiding system permissions dialogs after each build\n\nEvery time you start a new developer build, you may get two system dialogs:\n`Chromium wants to use your confidential information stored in \"Chromium Safe\nStorage\" in your keychain.`, and `Do you want the application \"Chromium.app\" to\naccept incoming network connections?`.\n\nTo avoid them, you can run Chromium with these command-line flags (but of\ncourse beware that they will change the behavior of certain subsystems):\n\n```shell\n--use-mock-keychain --disable-features=DialMediaRouteProvider\n```\n\n## Build and run test targets\n\nTests are split into multiple test targets based on their type and where they\nexist in the directory structure. To see what target a given unit test or\nbrowser test file corresponds to, the following command can be used:\n\n```shell\n$ gn refs out/Default --testonly=true --type=executable --all chrome/browser/ui/browser_list_unittest.cc\n//chrome/test:unit_tests\n```\n\nIn the example above, the target is unit_tests. The unit_tests binary can be\nbuilt by running the following command:\n\n```shell\n$ autoninja -C out/Default unit_tests\n```\n\nYou can run the tests by running the unit_tests binary. You can also limit which\ntests are run using the `--gtest_filter` arg, e.g.:\n\n```shell\n$ out/Default/unit_tests --gtest_filter=\"BrowserListUnitTest.*\"\n```\n\nYou can find out more about GoogleTest at its\n[GitHub page](https://github.com/google/googletest).\n\n## Debugging\n\nGood debugging tips can be found [here](mac/debugging.md).\n\n## Update your checkout\n\nTo update an existing checkout, you can run\n\n```shell\n$ git rebase-update\n$ gclient sync\n```\n\nThe first command updates the primary Chromium source repository and rebases\nany of your local branches on top of tip-of-tree (aka the Git branch\n`origin/main`). If you don't want to use this script, you can also just use\n`git pull` or other common Git commands to update the repo.\n\nThe second command syncs dependencies to the appropriate versions and re-runs\nhooks as needed.\n\n## Tips, tricks, and troubleshooting\n\n### Using Xcode-Ninja Hybrid\n\nWhile using Xcode is unsupported, GN supports a hybrid approach of using Ninja\nfor building, but Xcode for editing and driving compilation.  Xcode is still\nslow, but it runs fairly well even **with indexing enabled**.  Most people\nbuild in the Terminal and write code with a text editor, though.\n\nWith hybrid builds, compilation is still handled by Ninja, and can be run from\nthe command line (e.g. `autoninja -C out/gn chrome`) or by choosing the `chrome`\ntarget in the hybrid project and choosing Build.\n\nTo use Xcode-Ninja Hybrid pass `--ide=xcode` to `gn gen`:\n\n```shell\n$ gn gen out/gn --ide=xcode\n```\n\nOpen it:\n\n```shell\n$ open out/gn/all.xcodeproj\n```\n\nYou may run into a problem where http://YES is opened as a new tab every time\nyou launch Chrome. To fix this, open the scheme editor for the Run scheme,\nchoose the Options tab, and uncheck \"Allow debugging when using document\nVersions Browser\". When this option is checked, Xcode adds\n`--NSDocumentRevisionsDebugMode YES` to the launch arguments, and the `YES`\ngets interpreted as a URL to open.\n\nIf you have problems building, join us in `#chromium` on `irc.freenode.net` and\nask there. Be sure that the\n[waterfall](https://build.chromium.org/buildbot/waterfall/) is green and the\ntree is open before checking out. This will increase your chances of success.\n\n### Improving performance of git commands\n\n#### Increase the vnode cache size\n\n`git status` is used frequently to determine the status of your checkout.  Due\nto the large number of files in Chromium's checkout, `git status` performance\ncan be quite variable.  Increasing the system's vnode cache appears to help. By\ndefault, this command:\n\n```shell\n$ sysctl -a | egrep 'kern\\..*vnodes'\n```\n\nOutputs `kern.maxvnodes: 263168` (263168 is 257 * 1024).  To increase this\nsetting:\n\n```shell\n$ sudo sysctl kern.maxvnodes=$((512*1024))\n```\n\nHigher values may be appropriate if you routinely move between different\nChromium checkouts.  This setting will reset on reboot.  To apply it at startup:\n\n```shell\n$ sudo tee /Library/LaunchDaemons/kern.maxvnodes.plist > /dev/null <<EOF\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n  <dict>\n    <key>Label</key>\n      <string>kern.maxvnodes</string>\n    <key>ProgramArguments</key>\n      <array>\n        <string>sysctl</string>\n        <string>kern.maxvnodes=524288</string>\n      </array>\n    <key>RunAtLoad</key>\n      <true/>\n  </dict>\n</plist>\nEOF\n```\n\nOr edit the file directly.\n\n#### Configure git to use an untracked cache\n\nTry running\n\n```shell\n$ git update-index --test-untracked-cache\n```\n\nIf the output ends with `OK`, then the following may also improve performance of\n`git status`:\n\n```shell\n$ git config core.untrackedCache true\n```\n\n#### Configure git to use fsmonitor\n\nYou can significantly speed up git by using [fsmonitor.](https://github.blog/2022-06-29-improve-git-monorepo-performance-with-a-file-system-monitor/)\nYou should enable fsmonitor in large repos, such as Chromium and v8. Enabling\nit globally will launch many processes and probably isn't worthwhile. Be sure\nyou have at least version 2.43 (fsmonitor on the Mac is broken before then). The\ncommand to enable fsmonitor in the current repo is:\n\n```shell\n$ git config core.fsmonitor true\n```\n\n### Xcode license agreement\n\nIf you're getting the error\n\n> Agreeing to the Xcode/iOS license requires admin privileges, please re-run as\n> root via sudo.\n\nthe Xcode license hasn't been accepted yet which (contrary to the message) any\nuser can do by running:\n\n```shell\n$ xcodebuild -license\n```\n\nOnly accepting for all users of the machine requires root:\n\n```shell\n$ sudo xcodebuild -license\n```\n\n### Exclude checkout from Spotlight indexing\n\nChromium's checkout contains a lot of files, and building generates many more.\nSpotlight will try to index all of those files, and uses a lot of CPU time\ndoing so, especially during a build, which can slow things down.\n\nTo prevent the Chromium checkout from being indexed by Spotlight, open System\nPreferences, go to \"Spotlight\" -> \"Privacy\" and add your Chromium checkout\ndirectory to the list of excluded locations.\n"
  },
  {
    "path": "platforms/mac/mac_arm64",
    "title": "Chromium for Arm Macs",
    "content": "# Chromium for Arm Macs\n\nThis document describes the state of Chromium on Apple Silicon Macs.\nThe short summary is that almost everything works, without needing Rosetta.\n\nThere's a [main waterfall\nbot](https://ci.chromium.org/p/chromium/builders/ci/mac-arm64-rel)\nthat builds for Arm. It cross-builds on an x86-64 machine.\n\nThere's a [main waterfall\nbot](https://ci.chromium.org/p/chromium/builders/ci/mac-arm64-on-arm64-rel)\nthat builds for Arm on an Arm bot as well. This bot does not have Rosetta\ninstalled.\n\nThere's also a [tester\nbot](https://ci.chromium.org/p/chromium/builders/ci/mac12-arm64-rel-tests)\nthat continuously runs tests. Most tests pass. The tester bots don't\nhave Rosetta installed.\n\nASan builds do not yet work ([tracking bug](https://crbug.com/1271140))\n\n## Building _for_ Arm Macs\n\nIf you are on an Intel Mac, all that's required to build Chromium for arm64\nis to add a `target_cpu = \"arm64\"` line to your `args.gn`. Then build normally.\nIf you are on an Arm Mac, your build will by default be an Arm build, though\nplease see the section below about building _on_ Arm Macs for specific things\nto keep in mind.\n\nA note about copying a Chromium build to your Arm Mac. If you don't do a\ncomponent build (e.g. a regular `is_debug=false` build), you can just copy\nover Chromium.app from your build directory. If you copy it using\nmacOS's \"Shared Folder\" feature and Finder, Chromium.app should be directly\nrunnable. If you zip, upload Chromium.app to some web service and download\nit to an Arm Mac, browsers will set the `com.apple.quarantine` bit, which will\ncause the Finder to say `\"Chromium\" is damanged and can't be opened. You should\nmove it to the Trash.\"`. In Console.app, the kernel will log\n`kernel: Security policy would not allow process: 2204,\n/Users/you/Downloads/Chromium.app/Contents/MacOS/Chromium` and amfid will log\n`amfid: /Users/you/Downloads/Chromium.app/Contents/MacOS/Chromium signature not\nvalid: -67050`. To fix this, open a terminal and run\n\n    % cd ~/Downloads && xattr -rc Chromium.app\n\nAfter that, it should start fine.\n\nAs an alternative to building locally, changes can be submitted to the opt-in\n[mac12-arm64-rel\ntrybot](https://ci.chromium.org/p/chromium/builders/try/mac12-arm64-rel). A small\nnumber of [swarming bots](https://goto.corp.google.com/run-on-dtk) are also\navailable for Googlers to run tests on.\n\nArm Mac-specific bugs are tagged with the\n[Mac-ARM64 label](https://crbug.com/?q=label%3Amac-arm64).\n\n### Universal Builds\n\nA “universal” (or “fat”) `.app` can be created from distinct x86\\_64 and arm64\nbuilds produced from the same source version. Chromium has a `universalizer.py`\ntool that can then be used to merge the two builds into a single universal\n`.app`.\n\n    % ninja -C out/release_x86_64 chrome\n    % ninja -C out/release_arm64 chrome\n    % mkdir out/release_universal\n    % chrome/installer/mac/universalizer.py \\\n          out/release_x86_64/Chromium.app \\\n          out/release_arm64/Chromium.app \\\n          out/release_universal/Chromium.app\n\nThe universal build is produced in this way rather than having a single\nall-encompassing `gn` configuration because:\n\n - Chromium builds tend to take a long time, even maximizing the parallelism\n   capabilities of a single machine. This split allows an additional dimension\n   of parallelism by delegating the x86\\_64 and arm64 build tasks to different\n   machines.\n - During the mac-arm64 bring-up, the x86\\_64 and arm64 versions were built\n   using different SDK and toolchain versions. When using the hermetic SDK and\n   toolchain, a single version of this package must be shared by an entire\n   source tree, because it’s managed by `gclient`, not `gn`. However, as of\n   November 2020, Chromium builds for the two architectures converged and are\n   expected to remain on the same version indefinitely, so this is now more of a\n   historical artifact.\n\n## Building _on_ arm Macs\n\nIt's possible to build _on_ an arm Mac, without Rosetta. This\nconfiguration is covered by a [main waterfall\nbot](https://ci.chromium.org/p/chromium/builders/ci/mac-arm64-on-arm64-rel).\n\nChecking out and building (with reclient too) should just work.\nYou should be able to run `fetch chromium` normally, and then build, using\n`gn`, `ninja` etc like normal.\n\nBuilding Chrome/Mac/Intel on an arm Mac currently needs a small local tweak\nto work, see [tracking bug](https://crbug.com/1280968).\n\nAll tests should build, run, and mostly pass.\n"
  },
  {
    "path": "platforms/ios/ios_voiceover",
    "title": "This document has moved",
    "content": "# This document has moved\n\nNOTE: Please update your link to this file!\n\n[The new file location is //docs/ios/voiceover.md](ios/voiceover.md)"
  },
  {
    "path": "platforms/ios/ios_infra",
    "title": "This document has moved",
    "content": "# This document has moved\n\nNOTE: Please update your link to this file!\n\nThe new file location is [//docs/ios/infra.md](ios/infra.md)"
  },
  {
    "path": "platforms/ios/ios_build_instructions",
    "title": "This document has moved",
    "content": "# This document has moved\n\nNOTE: Please update your link to this file!\n\nThe new file location is [//docs/ios/build_instructions.md](ios/build_instructions.md)"
  },
  {
    "path": "platforms/chromeos/chromeos_glossary",
    "title": "",
    "content": "See [Chromium OS Glossary](https://chromium.googlesource.com/chromiumos/docs/+/main/glossary.md)\n"
  },
  {
    "path": "platforms/chromeos/chromeos_build_instructions",
    "title": "Chrome OS Build Instructions",
    "content": "# Chrome OS Build Instructions\n\nChrome for Chromium OS can be built in a couple different ways. After following\nthe [initial setup](#common-setup), you'll need to choose one of the following\nbuild configurations:\n\n- If you're interested in testing Chrome OS code in Chrome, but not interactions\n  with Chrome OS services, you can build for\n  [linux-chromeos](#Chromium-OS-on-Linux-linux_chromeos) using just a Linux\n  workstation.\n- Otherwise, Chrome's full integration can be covered by building for a real\n  Chrome OS device or VM using [Simple Chrome](#Chromium-OS-Device-Simple-Chrome).\n- Use `is_chromeos_device` in GN and `BUILDFLAG(IS_CHROMEOS_DEVICE)` in C++ code\n  to differentiate between these two modes.\n\n[TOC]\n\n## Common setup\n\nFirst, follow the [normal Linux build\ninstructions](https://chromium.googlesource.com/chromium/src/+/main/docs/linux/build_instructions.md)\nas usual to get a Chromium checkout.\n\nYou'll also need to add `'chromeos'` to the `target_os` list in your `.gclient`\nconfiguration, which will fetch the additional build dependencies required for\nCrOS. This file is located one level up from your Chromium checkout's `src`.\n\nIf you don't already have a `target_os` line present, simply add this to the\nend of the `.gclient` file:\n\n    target_os = ['chromeos']\n\nIf you already have a `target_os` line present in your `.gclient file`, you can\nsimply append `'chromeos'` to the existing list there. For example:\n\n    target_os = ['android', 'chromeos']\n\nOnce your `.gclient` file is updated, you will need to run `gclient sync` once\nbefore proceeding with the rest of these instructions.\n\n## Chromium OS on Linux (linux-chromeos)\n\nChromium on Chromium OS uses Linux Chromium as a base, but adds a large number\nof Chrome OS-specific features to the code. For example, the login UI, window\nmanager and system UI are part of the Chromium code base and built into the\nchrome binary.\n\nFortunately, most Chromium changes that affect Chromium OS can be built and\ntested on a Linux workstation. This build is called \"linux-chromeos\". In this\nconfiguration most system services (like the power manager, bluetooth daemon,\netc.) are stubbed out. The entire system UI runs in a single X11 window on your\ndesktop.\n\nYou can test sign-in/sync in this mode by adding the --login-manager flag, see\nthe [Login notes](#Login-notes) section.\n\n### Building and running Chromium with Chromium OS UI on your local machine\n\nRun the following in your chromium checkout:\n\n    $ gn gen out/Default --args='target_os=\"chromeos\"'\n    $ autoninja -C out/Default chrome\n    $ out/Default/chrome --use-system-clipboard\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`).\n\nSome additional options you may wish to set by passing in `--args` to `gn gen`\nor running `gn args out/Default`:\n\n    # Googlers: Reclient is a distributed compiler service.\n    use_remoteexec = true\n\n    is_component_build = true  # Links faster.\n    is_debug = false           # Release build, runs faster.\n    dcheck_always_on = true    # Enables DCHECK despite release build.\n    enable_nacl = false        # Skips native client build, compiles faster.\n\n    # Builds Chrome instead of Chromium. This requires a src-internal\n    # checkout. Adds internal features and branded art assets.\n    is_chrome_branded = true\n\n    # Enables many optimizations, leading to much slower compiles, links,\n    # and no runtime stack traces.\n    #\n    # Note: not compatible with `is_component_build = true`.\n    is_official_build = true\n\nNOTE: You may wish to replace 'Default' with something like 'Cros' if\nyou switch back and forth between Linux and Chromium OS builds, or 'Debug'\nif you want to differentiate between Debug and Release builds (see below).\n\nSee [GN Build Configuration](https://www.chromium.org/developers/gn-build-configuration)\nfor more information about configuring your build.\n\nYou can also build and run test targets like `unit_tests`, `browser_tests`, etc.\n\n### Flags\n\nSome useful flags:\n\n*    `--ash-debug-shortcuts`: Enable shortcuts such as Ctl+Alt+Shift+T to toggle\n     tablet mode.\n*    `--ash-host-window-bounds=\"0+0-800x600,800+0-800x600\"`: Specify one or more\n     virtual screens, by display position and size.\n*    `--enable-features=Feature1,OtherFeature2`: Enable specified features.\n     Features are often listed in chrome://flags, or in source files such as\n     [chrome_features.cc](https://source.chromium.org/chromium/chromium/src/+/main:chrome/common/chrome_features.cc)\n     or [ash_features.cc](https://source.chromium.org/chromium/chromium/src/+/main:ash/constants/ash_features.cc).\n     Note that changing values in chrome://flags does not work for\n     linux-chromeos, and this flag must be used.\n*    `--enable-ui-devtools[=9223]`: Allow debugging of the system UI through\n     devtools either within linux-chromeos at chrome://inspect, or from a remote\n     browser at\n     devtools://devtools/bundled/devtools_app.html?uiDevTools=true&ws=127.0.0.1:9223/0\n*    `--remote-debugging-port=9222`: Allow debugging through devtools at\n     http://localhost:9222\n*    `--use-system-clipboard`: Integrate clipboard with the host X11 system.\n\n### Login notes\n\nBy default this build signs in with a stub user. To specify a real user:\n\n*   For first run, add the following options to chrome's command line:\n    `--user-data-dir=/tmp/chrome --login-manager`\n*   Go through the out-of-the-box UX and sign in with a real Gmail account.\n*   For subsequent runs, if you want to skip the login manager page, add:\n    `--user-data-dir=/tmp/chrome --login-user=username@gmail.com\n    --login-profile=username@gmail.com-hash`. It's also fine to just keep\n    --login-manager instead.\n*   To run in guest mode instantly, add:\n    `--user-data-dir=/tmp/chrome --bwsi --incognito --login-user='$guest'\n    --login-profile=user`\n\nSigning in as a specific user is useful for debugging features like sync\nthat require a logged in user.\n\n### Graphics notes\n\nThe Chromium OS build requires a functioning GL so if you plan on\ntesting it through Chromium Remote Desktop you might face drawing\nproblems (e.g. Aura window not painting anything). Possible remedies:\n\n*   `--ui-enable-software-compositing --ui-disable-threaded-compositing`\n*   `--use-gl=angle --use-angle=swiftshader`, but it's slow.\n\nTo more closely match the UI used on devices, you can install fonts used\nby Chrome OS, such as Roboto, on your Linux distro.\n\n## Chromium OS Device (Simple Chrome)\n\nThis configuration allows you to build a fully functional Chrome for a real\nChrome OS device or VM. Since Chrome OS uses a different toolchain for each\ndevice model, you'll first need to know the name of the model (or \"board\") you\nwant to build for. For most boards, `amd64-generic` and `arm-generic` will\nproduce a functional binary, though it won't be optimized and may be missing\nfunctionality.\n\n### Additional gclient setup\n\nEach board has its own toolchain and misc. build dependencies. To fetch these,\nlist the board under the `\"cros_boards\"` gclient custom var. If you were using\nthe `amd64-generic` board, your `.gclient` file would look like:\n```\nsolutions = [\n  {\n    \"url\": \"https://chromium.googlesource.com/chromium/src.git\",\n    \"name\": \"src\",\n    \"custom_deps\": {},\n    \"custom_vars\" : {\n        \"cros_boards\": \"amd64-generic\",\n    },\n  },\n]\ntarget_os = [\"chromeos\"]\n```\nOnce your .gclient file is updated, you will need to run `gclient sync` again\nto fetch the toolchain.\n\nNOTE:\n - If you'd like a VM image additionally downloaded for the board, add it to the\n   `\"cros_boards_with_qemu_images\"` gclient custom var. That var downloads the\n   SDK along with a VM image. `cros_boards` downloads only the SDK.\n - If you'd like to fetch multiple boards, add a `:` between each board in the\n   gclient var. For example: `\"cros_boards\": \"amd64-generic:arm-generic\"`.\n\n### Building for the board\n\nAfter the needed toolchain has been downloaded for your ${BOARD}, a build dir\nwill have been conveniently created for you at `out_$BOARD/Release`, which can\nthen be used to build Chrome. For the `amd64-generic` board, this would\nlook like:\n\n    $ gn gen out_amd64-generic/Release\n    $ autoninja -C out_$BOARD/Release chrome\n\nOr if you prefer to use your own build dir, simply add the following line to the\ntop of your GN args: `import(\"//build/args/chromeos/amd64-generic.gni\")`. eg:\n\n    $ gn gen out/Default --args='import(\"//build/args/chromeos/amd64-generic.gni\")'\n    $ autoninja -C out/Default chrome\n\nThat will produce a Chrome OS build of Chrome very similar to what is shipped\nfor that device. You can also supply additional args or even overwrite ones\nsupplied in the imported .gni file after the `import()` line.\n\n### Additional notes\n\nFor more information (like copying the locally-built Chrome to a device, or\nrunning Tast tests), consult Simple Chrome's\n[full documentation](https://chromium.googlesource.com/chromiumos/docs/+/main/simple_chrome_workflow.md).\n"
  },
  {
    "path": "platforms/android/android_studio",
    "title": "Android Studio",
    "content": "# Android Studio\n\n[TOC]\n\n## Usage\n\nMake sure you have followed\n[android build instructions](android_build_instructions.md) already.\n\n```shell\nbuild/android/gradle/generate_gradle.py --output-directory out/Debug\n```\n\nThe above commands create a project dir `gradle` under your output directory.\nUse `--project-dir <project-dir>` to change this.\n\nTo import the project:\n* Use \"Import Project\", and select the directory containing the generated\n  project, e.g. `out/Debug/gradle`.\n\nSee [android_test_instructions.md](testing/android_test_instructions.md#Using-Emulators)\nfor more information about building and running emulators.\n\nFeel free to accept Android Studio's recommended actions. `generate_gradle.py`\nshould have already set up a working version of the gradle wrapper and the\nandroid gradle plugin, as well as a default Android SDK location at\n`~/Android/Sdk`. Since the same script needs to support various versions of\nAndroid Studio, the defaults may have lower version than the one recommended by\nyour version of Android Studio. After you accept Android Studio's update actions\nthe `generate_gradle.py` script will try to keep the newer versions when it is\nre-run.\n\nYou'll need to re-run `generate_gradle.py` whenever new directories containing\nsource files are added.\n\n* After regenerating, Android Studio should prompt you to \"Sync\". If it\n  doesn't, try some of the following options:\n    * File -&gt; \"Sync Project with Gradle Files\"\n    * Button with two arrows on the right side of the top strip.\n    * Help -&gt; Find Action -&gt; \"Sync Project with Gradle Files\"\n    * After `gn clean` you may need to restart Android Studio.\n    * File -&gt; \"Invalidate Caches / Restart...\"\n\n## How It Works\n\nBy default, only an `_all` module containing all java apk targets is generated.\nIf just one apk target is explicitly specified, then a single apk module is\ngenerated.\n\nIf you really prefer a more detailed structure of gn targets, the deprecated\n`--split-projects` flag can be used. This will generate one module for every gn\ntarget in the dependency graph. This can be very slow and is no longer\nsupported.\n\n### Generated files\n\nMost generated .java files in GN are stored as `.srcjars`. Android Studio does\nnot support them. Our build will automatically extract them to a\n`generated_java` directory in the output directory during the build. Thus if a\ngenerated file is missing in Android Studio, build it with ninja first and it\nshould show up in Android Studio afterwards.\n\n### Native Files\n\nThis option is deprecated and no longer supported since Android Studio is very\nslow when editing in a code base with a large number of C++ files, and Chromium\nhas a lot of C++ code. It is recommended to use [VS Code](vscode.md) to edit\nnative files and stick to just editing java files in Android Studio.\n\nIf you still want to enable editing native C/C++ files with Android Studio, pass\nin any number of `--native-target [target name]` flags in order to use it. The\ntarget must be the full path and name of a valid gn target (no short-forms).\nThis will require you to install `cmake` and `ndk` when prompted. Accept Android\nStudio's prompts for these SDK packages.\n\nYou need to disable a new gradle option in order to edit native files:\nFile -&gt; Settings -&gt; Experimental\n-&gt; Gradle and uncheck \"Only resolve selected variants\".\n\nThis is not necessary, but to avoid \"This file is not part of the project...\",\nyou can either add an extra `--native-target` flag or simply copy and paste the\nabsolute path to that file into the CMakeLists.txt file alongside the existing\nfile paths. Note that changes to CMakeLists.txt will be overwritten on your next\ninvocation of `generate_gradle.py`.\n\nExample:\n\n```shell\nbuild/android/gradle/generate_gradle.py --native-target //chrome/android:libchrome\n```\n\n## Tips\n\n* Use environment variables to avoid having to specify `--output-directory`.\n    * Example: Append `export CHROMIUM_OUT_DIR=out; export BUILDTYPE=Debug` to\n      your `~/.bashrc` to always default to `out/Debug`.\n* Using the Java debugger is documented\n  [here](android_debugging_instructions.md#android-studio).\n* Configuration instructions can be found\n  [here](http://tools.android.com/tech-docs/configuration). One suggestions:\n    * Launch it with more RAM:\n      `STUDIO_VM_OPTIONS=-Xmx2048m /opt/android-studio-stable/bin/studio-launcher.sh`\n* If you ever need to reset it: `rm -r ~/.config/Google/AndroidStudio*/`\n* Import Chromium-specific style and inspections settings:\n    * Help -&gt; Find Action -&gt; \"Code Style\" (settings) -&gt; Java -&gt;\n      Scheme -&gt; Import Scheme\n        * Select `tools/android/android_studio/ChromiumStyle.xml` -&gt; OK\n    * Help -&gt; Find Action -&gt; \"Inspections\" (settings) -&gt;\n      Profile -&gt; Import profile\n        * Select `tools/android/android_studio/ChromiumInspections.xml` -&gt; OK\n* Turn on automatic import:\n    * Help -&gt; Find Action -&gt; \"Auto Import\"\n        * Tick all the boxes under \"Java\" and change the dropdown to \"All\".\n* Turn on documentation on mouse hover:\n    * Help -&gt; Find Action -&gt; \"Show quick documentation on mouse move\"\n* Turn on line numbers:\n    * Help -&gt; Find Action -&gt; \"Show line numbers\"\n* Turn off indent notification:\n    * Help -&gt; Find Action -&gt; \"Show notifications about detected indents\"\n* Format changed files (Useful for changes made by running code inspection):\n    * Set up version control\n        * File -&gt; Settings -&gt; Version Control\n        * Add src directories\n    * Commit changes and reformat\n        * Help -&gt; Find Action -&gt; \"Commit Changes\"\n        * Check \"Reformat code\" & \"Optimize imports\" and commit\n* Change theme from GTK+ to another one to avoid invisible menus.\n    * Help -&gt; Find Action -&gt; \"Theme: Settings > Appearance\"\n\n### Useful Shortcuts\n\n* `Shift - Shift`: Search to open file or perform IDE action\n* `Ctrl + N`: Jump to class\n* `Ctrl + Shift + T`: Jump to test\n* `Ctrl + Shift + N`: Jump to file\n* `Ctrl + F12`: Jump to method\n* `Ctrl + G`: Jump to line\n* `Shift + F6`: Rename variable\n* `Ctrl + Alt + O`: Organize imports\n* `Alt + Enter`: Quick Fix (use on underlined errors)\n* `F2`: Find next error\n\n### Building with Gradle\n\nGradle builds are not supported. Only editing is supported in Android Studio.\nUse ninja to build as usual.\n\n## Status\n\n### What works\n\n* Android Studio v2021~v2023.\n* Java editing.\n    * Application code in `main` sourceset.\n    * Instrumentation test code in `androidTest` sourceset.\n* Native code editing (deprecated, use [VS Code](vscode.md) instead).\n* Symlinks to existing .so files in jniLibs (doesn't generate them).\n* Editing resource xml files\n* Layout editor (limited functionality).\n* Java debugging (see\n[here](/docs/android_debugging_instructions.md#Android-Studio)).\n* Import resolution and refactoring across java files.\n* Separate Android SDK for Android Studio.\n\n### What doesn't work\n\n* Building with Gradle.\n* The \"Make Project\" button doesn't work.\n    * Stick to using `autoninja` to build targets and just use Android Studio\n      for editing java source files.\n* No active work is underway or planned to expand Android Studio support."
  },
  {
    "path": "platforms/android/android_native_libraries",
    "title": "Shared Libraries on Android",
    "content": "# Shared Libraries on Android\nThis doc outlines some tricks / gotchas / features of how we ship native code in\nChrome on Android.\n\n[TOC]\n\n## Library Packaging\n * Android N, O & P (MonochromePublic.aab):\n   * `libmonochrome.so` is stored uncompressed within the apk (an\n     AndroidManifest.xml attribute disables extraction).\n   * It is loaded directly from the apk by the system linker.\n   * It exports all JNI symbols and does not use explicit JNI registration.\n   * It is not loaded by `libchromium_android_linker.so` and relies on the\n     system's webview zygote for RELRO sharing.\n * Android Q (TrichromeChrome.aab + TrichromeLibrary.apk):\n   * Trichrome uses the exact same native library as Monochrome:\n     `libmonochrome.so`.\n   * `libmonochrome.so` is stored in the shared APK (TrichromeLibrary.apk)\n     so that it can be shared with TrichromeWebView.\n   * It is loaded by `libchromium_android_linker.so` using\n     `android_dlopen_ext()` to enable RELRO sharing.\n\n## Build Variants (eg. monochrome_64_32_apk)\nThe packaging above extends to cover both 32-bit and 64-bit device\nconfigurations.\n\nChrome support 64-bit builds, but these do not ship to Stable.\nThe system WebView APK that ships to those devices contains a 32-bit library,\nand for 64-bit devices, a 64-bit library as well (32-bit WebView client apps\nwill use the 32-bit library, and vice-versa).\n\n### Monochrome\nMonochrome's intent was to eliminate the duplication between the 32-bit Chrome\nand WebView libraries (most of the library is identical). In 32-bit Monochrome,\na single combined library serves both Chrome and WebView needs. The 64-bit\nversion adds an extra WebView-only library.\n\nMore recently, additional Monochrome permutations have arrived. First, Google\nPlay will eventually require that apps offer a 64-bit version to compatible\ndevices. In Monochrome, this implies swapping the architecture of the Chrome and\nWebView libraries (64-bit combined lib, and extra 32-bit WebView lib). Further\ndown the road, silicon vendors may drop 32-bit support from their chips, after\nwhich a pure 64-bit version of Monochrome will apply. In each of these cases,\nthe library name of the combined and WebView-only libraries must match (an\nAndroid platform requirement), so both libs are named libmonochrome.so (or\nlibmonochrome_64.so in the 64-bit browser case).\n\nSince 3 of these variations require a 64-bit build config, it makes sense to\nalso support the 4th variant on 64-bit, thus allowing a single builder to build\nall variants (if desired). Further, a naming scheme must exist to disambiguate\nthe various targets:\n\n**monochrome_(browser ABI)_(extra_webview ABI)**\n\nFor example, the 64-bit browser version with extra 32-bit WebView is\n**monochrome_64_32_apk**. The combinations are as follows:\n\nBuilds on | Variant | Description\n--- | --- | ---\n32-bit | monochrome | The original 32-bit-only version\n64-bit | monochrome | The original 64-bit version, with 32-bit combined lib and 64-bit WebView. This would be named monochrome_32_64_apk if not for legacy naming.\n64-bit | monochrome_64_32 | 64-bit combined lib with 32-bit WebView library.\n64-bit | monochrome_64 | 64-bit combined lib only, for eventual pure 64-bit hardware.\n64-bit | monochrome_32 | A mirror of the original 32-bit-only version on 64-bit, to allow building all products on one builder. The result won't be bit-identical to the original, since there are subtle compilation differences.\n\n### Trichrome\nTrichrome has the same 4 permutations as Monochrome, but adds another dimension.\nTrichrome returns to separate apps for Chrome and WebView, but places shared\nresources in a third shared-library APK. The table below shows which native\nlibraries are packaged where. Note that **dummy** placeholder libraries are\ninserted where needed, since Android determines supported ABIs from the presence\nof native libraries, and the ABIs of a shared library APK must match its client\napp.\n\nBuilds on | Variant | Chrome | Library | WebView\n--- | --- | --- | --- | ---\n32-bit | trichrome | `32/dummy` | `32/combined` | `32/dummy`\n64-bit | trichrome | `32/dummy`, `64/dummy` | `32/combined`, `64/dummy` | `32/dummy`, `64/webview`\n64-bit | trichrome_64_32 | `32/dummy`, `64/dummy` | `32/dummy`, `64/combined` | `32/webview`, `64/dummy`\n64-bit | trichrome_64 | `64/dummy` | `64/combined` | `64/dummy`\n64-bit | trichrome_32 | `32/dummy` | `32/combined` | `32/dummy`\n\n## Crashpad Packaging\n * Crashpad is a native library providing out-of-process crash dumping. When a\n   dump is requested (e.g. after a crash), a Crashpad handler process is started\n   to produce a dump.\n * Chrome (Android L through M):\n   * libchrome_crashpad_handler.so is a standalone executable containing all of\n     the crash dumping code. It is stored compressed and extracted automatically\n     by the system, allowing it to be directly executed to produce a crash dump.\n * Monochrome (N through P) and SystemWebView (L through P):\n    * All of the Crashpad code is linked into the package's main native library\n      (e.g. libmonochrome.so). When a dump is requested, /system/bin/app_process\n      is executed, loading CrashpadMain.java which in turn uses JNI to call into\n      the native crash dumping code. This approach requires building CLASSPATH\n      and LD_LIBRARY_PATH variables to ensure app_process can locate\n      CrashpadMain.java and any native libraries (e.g. system libraries, shared\n      libraries, split apks, etc.) the package's main native library depends on.\n * Monochrome, Trichrome, and SystemWebView (Q+):\n    * All of the Crashpad handler code is linked into the package's native\n      library. libcrashpad_handler_trampoline.so is a minimal executable\n      packaged with the main native library, stored uncompressed and left\n      unextracted. When a dump is requested, /system/bin/linker is executed to\n      load the trampoline from the APK, which in turn `dlopen()`s the main\n      native library to load the remaining Crashpad handler code. A trampoline\n      is used to de-duplicate shared code between Crashpad and the main native\n      library packaged with it. This approach isn't used for P- because the\n      linker doesn't support loading executables on its command line until Q.\n      This approach also requires building a suitable LD_LIBRARY_PATH to locate\n      any shared libraries Chrome/WebView depends on.\n\n## Debug Information\n**What is it?**\n * Sections of an ELF that provide debugging and symbolization information (e.g. ability convert addresses to function & line numbers).\n\n**How we use it:**\n * ELF debug information is too big to push to devices, even for local development.\n * All of our APKs include `.so` files with debug information removed via `strip`.\n * Unstripped libraries are stored at `out/Default/lib.unstripped`.\n   * Many of our scripts are hardcoded to look for them there.\n\n## Unwind Info & Frame Pointers\n**What are they:**\n * Unwind info is data that describes how to unwind the stack. It is:\n   * It is required to support C++ exceptions (which Chrome doesn't use).\n   * It can also be used to produce stack traces.\n   * It is generally stored in an ELF section called `.eh_frame` & `.eh_frame_hdr`, but arm32 stores it in `.ARM.exidx` and `.ARM.extab`.\n     * You can see these sections via: `readelf -S libchrome.so`\n * \"Frame Pointers\" is a calling convention that ensures every function call has the return address pushed onto the stack.\n   * Frame Pointers can also be used to produce stack traces (but without entries for inlined functions).\n\n**How we use them:**\n * We disable unwind information (search for [`exclude_unwind_tables`](https://cs.chromium.org/search/?q=exclude_unwind_tables+file:%5C.gn&type=cs)).\n * For all architectures except arm64, we disable frame pointers in order to reduce binary size (search for [`enable_frame_pointers`](https://cs.chromium.org/search/?q=enable_frame_pointers+file:%5C.gn&type=cs)).\n * Crashes are unwound offline using `minidump_stackwalk`, which can create a stack trace given a snapshot of stack memory and the unstripped library (see [//docs/testing/using_breakpad_with_content_shell.md](testing/using_breakpad_with_content_shell.md))\n * To facilitate heap profiling, we ship unwind information to arm32 canary & dev channels as a separate file: `assets/unwind_cfi_32`\n\n## JNI Native Methods Resolution\n * For ChromePublic.apk:\n   * `JNI_OnLoad()` is the only exported symbol (enforced by a linker script).\n   * Native methods registered explicitly during start-up by generated code.\n * For MonochromePublic.apk and TrichromeChrome.aab:\n   * `JNI_OnLoad()` and `Java_*` symbols are exported by linker script.\n   * No manual JNI registration is done. Symbols are resolved lazily by the runtime.\n\n## Packed Relocations\n * All flavors of `lib(mono)chrome.so` enable \"packed relocations\", or \"APS2 relocations\" in order to save binary size.\n   * Refer to [this source file](https://android.googlesource.com/platform/bionic/+/refs/heads/master/tools/relocation_packer/src/delta_encoder.h) for an explanation of the format.\n * To process these relocations:\n   * Pre-M Android: Our custom linker must be used.\n   * M+ Android: The system linker understands the format.\n * To see if relocations are packed, look for `LOOS+#` when running: `readelf -S libchrome.so`\n * Android P+ [supports an even better format](https://android.googlesource.com/platform/bionic/+/8b14256/linker/linker.cpp#2620) known as RELR.\n   * We'll likely switch non-Monochrome apks over to using it once it is implemented in `lld`.\n\n## RELRO Sharing\n**What is it?**\n * RELRO refers to the ELF segment `GNU_RELRO`. It contains data that the linker marks as read-only after it applies relocations.\n   * To inspect the size of the segment: `readelf --segments libchrome.so`\n   * For `lib(mono)chrome.so` the region occupies about 2.4MiB on arm32 and 4.7 MiB on arm64\n * If two processes map this segment to the same virtual address space, then pages of memory within the segment which contain only relative relocations (99% of them) will be byte-for-byte identical.\n * \"RELRO sharing\" is when this segment is moved into shared memory and shared by multiple processes.\n * Processes `fork()`ed from the app zygote (where the library is loaded) share RELRO (via `fork()`'s copy-on-write semantics), but this region is not shared with other process types (privileged, utility, GPU)\n\n**How does it work?**\n * For a more detailed description, refer to comments in [Linker.java](https://cs.chromium.org/chromium/src/base/android/java/src/org/chromium/base/library_loader/Linker.java).\n * For Android N-P:\n   * The OS maintains a RELRO file on disk with the contents of the GNU_RELRO segment.\n   * All Android apps that contain a WebView load `libmonochrome.so` at the same virtual address and apply RELRO sharing against the memory-mapped RELRO file.\n   * Chrome uses `WebViewLibraryPreloader` to call into the same WebView library loading code.\n     * When Monochrome is the WebView provider, `libmonochrome.so` is loaded with the system's cached RELRO's applied.\n   * `System.loadLibrary()` is called afterwards.\n     * When Monochrome is the WebView provider, this only calls JNI_OnLoad, since the library is already loaded. Otherwise, this loads the library and no RELRO sharing occurs.\n * For non-low-end Android O-P (where there's a WebView zygote):\n   * For non-renderer processes, the above Android N+ logic applies.\n   * For renderer processes, the OS starts all Monochrome renderer processes by `fork()`ing the WebView zygote rather than the normal application zygote.\n     * In this case, RELRO sharing would be redundant since the entire process' memory is shared with the zygote with copy-on-write semantics.\n * For Android Q+ (Trichrome):\n   * TrichromeWebView works the same way as on Android N-P.\n   * TrichromeChrome uses `android_dlopen_ext()` and `ASharedMemory_create()` to\n     perform RELRO sharing, and then relies on a subsequent call to\n     `System.loadLibrary()` to enable JNI method resolution without loading the\n     library a second time.\n   * For renderer processes, TrichromeChrome `fork()`s from a chrome-specific\n     app zygote. `libmonochrome.so` is loaded in the zygote before `fork()`.\n     * Similar to O-P, app zygote provides copy-on-write memory semantics so\n       RELRO sharing is redundant.\n * For Android R+ (still Trichrome)\n   * The RELRO region is created in the App Zygote, picked up by the Browser\n     process, which then redistributes the region to all other processes. The\n     receiving of the region and remapping it on top of the non-shared RELRO\n     happens asynchronously after the library has been loaded. Native code is\n     generally already running at this point. Hence the replacement must be\n     atomic.\n\n## Partitioned libraries\nSome Chrome code is placed in feature-specific libraries and delivered via\n[Dynamic Feature Modules](android_dynamic_feature_modules.md).\n\nA linker-assisted partitioning system automates the placement of code into\neither the main Chrome library or feature-specific .so libraries. Feature code\nmay continue to make use of core Chrome code (eg. base::) without modification,\nbut Chrome must call feature code through a virtual interface.\n\n**How partitioning works**\n\nThe lld linker is now capable of producing a [partitioned\nlibrary](https://lld.llvm.org/Partitions.html), which is effectively an\nintermediate single file containing multiple libraries. A separate tool\n*(llvm-objcopy)* then splits the file into standalone .so files, invoked through\na [partitioned shared library](https://cs.chromium.org/chromium/src/build/partitioned_shared_library.gni)\nGN template.\n\nThe primary partition is Chrome's main library (eg. libchrome.so), and other\npartitions may contain feature code (eg. libvr.so). By specifying a list of\nC/C++ symbols to use as entrypoints, the linker can collect all code used only\nthrough these entrypoints, and place it in a particular partition.\n\nTo facilitate partitioning, all references from Chrome to the feature\nentrypoints must be indirect. That is, Chrome must obtain a symbol from the\nfeature library through dlsym(), cast the pointer to its actual type, and call\nthrough the resulting pointer.\n\nFeature code retains the ability to freely call back into Chrome's core code.\nWhen loading the library, the feature module system uses the feature name to\nlook up a partition name *(libfoo.so)* in an address offset table built into the\nmain library. The resulting offset is supplied to android_dlopen_ext(), which\ninstructs Android to load the library in a particular reserved address region.\nThis allows the feature library's relative references back to the main library\nto work, as if the feature code had been linked into the main library\noriginally. No dynamic symbol resolution is required here.\n\n**Implications on code placement**\n\n* Any symbol referenced by multiple partitions ends up in the main library (even\n  if all calling libraries are feature partitions).\n* Symbols that aren't feature code (eg. base::) will be pulled into the\n  feature's library if only that feature uses the code. This is a benefit, but\n  can be unexpected.\n\n**Builds that support partitioned libraries**\n\nPartitioned libraries are usable when all of the following are true:\n* Component build is disabled (component build splits code across GN component\n  target boundaries instead).\n* The compiler is Clang.\n* The linker is lld.\n\n## Library Prefetching\n * During start-up, we `fork()` a process that reads a byte from each page of the library's memory (or just the ordered range of the library).\n   * See [//base/android/library_loader/](../base/android/library_loader/).\n\n## Historical Tidbits\n * We used to use the system linker on M (`ModernLinker.java`).\n   * This was removed due to [poor performance](https://bugs.chromium.org/p/chromium/issues/detail?id=719977).\n * We used to use `relocation_packer` to pack relocations after linking, which complicated our build system and caused many problems for our tools because it caused logical addresses to differ from physical addresses.\n   * We now link with `lld`, which supports packed relocations natively and doesn't have these problems.\n * We used to use the Crazy Linker until Android M was deprecated\n   * It allowed storing `libchrome.so` uncompressed within the apk before the\n     system linker allowed it (with the name `crazy.libchrome.so` to avoid extraction).\n   * It was loaded directly from the apk via `libchromium_android_linker.so`.\n   * Only JNI_OnLoad was exported. Explicit JNI registration was required\n     because the Android runtime uses the system's `dlsym()`, which doesn't know\n     about Crazy-Linker-opened libraries. (see [JNI README](/third_party/jni_zero/README.md)).\n\n## See Also\n * [//docs/android_build_instructions.md#Multiple-Chrome-APK-Targets](android_build_instructions.md#Multiple-Chrome-APK-Targets)\n * [//third_party/android_crazy_linker/README.chromium](../third_party/android_crazy_linker/README.chromium)\n * [//base/android/linker/BUILD.gn](../base/android/linker/BUILD.gn)\n"
  },
  {
    "path": "platforms/android/android_logging",
    "title": "Logging",
    "content": "# Logging\n\n[TOC]\n\n\n## Overview\n\nLogging used to be done using Android's\n[android.util.Log](https://developer.android.com/reference/android/util/Log.html).\n\nA wrapper on that is now available:\n[org.chromium.base.Log](/base/android/java/src/org/chromium/base/Log.java). It\nis designed to write logs as belonging to logical groups going beyond single\nclasses, and to make it easy to switch logging on or off for individual groups.\n\nUsage:\n\n```java\nprivate static final String TAG = \"YourModuleTag\";\n...\nLog.i(TAG, \"Logged INFO message.\");\nLog.d(TAG, \"Some DEBUG info: %s\", data);\n```\n\nOutput:\n\n```\nI/cr_YourModuleTag: ( 999): Logged INFO message\nD/cr_YourModuleTag: ( 999): [MyClass.java:42] Some DEBUG info: data.toString\n```\n\nHere, **TAG** will be a feature or package name, \"MediaRemote\" or \"NFC\" for\nexample. In most cases, the class name is not needed. It will be prepended by\nthe \"cr\\_\" prefix to make obvious which logs are coming from Chrome.\n\n### Verbose and Debug logs have special handling\n\n*   `Log.v` and `Log.d` Calls made using `org.chromium.base.Log` are stripped\n    out of production binaries using Proguard. There is no way to get those logs\n    in release builds.\n\n*   The file name and line number will be prepended to the log message.\n    For higher priority logs, those are not added for performance concerns.\n\n### An exception trace is printed when the exception is the last parameter\n\nAs with `android.util.Log`, putting a throwable as last parameter will dump the\ncorresponding stack trace:\n\n```java\nLog.i(TAG, \"An error happened: %s\", e)\n```\n\n```\nI/cr_YourModuleTag: ( 999): An error happened: This is the exception's message\nI/cr_YourModuleTag: ( 999): java.lang.Exception: This is the exception's message\nI/cr_YourModuleTag: ( 999):     at foo.bar.MyClass.test(MyClass.java:42)\nI/cr_YourModuleTag: ( 999):     ...\n```\n\nHaving the exception as last parameter doesn't prevent it from being used for\nstring formatting.\n\n## Logging Best Practices\n\n### Rule #1: Never log user data or PII (Personal Identification Information)\n\nThis is a huge concern, because other applications can access the log and\nextract a lot of data from your own by doing so. Even if JellyBean restricted\nthis, people are going to run your application on rooted devices and allow some\napps to access it. Also anyone with USB access to the device can use ADB to get\nthe full logcat and get the same data right now.\n\nIf you really need to print something, print a series of Xs instead\n(e.g. \"XXXXXX\"), or print a truncated hash of the data instead. Truncation is\nrequired to make it harder for an attacker to recover the full data through\nrainbow tables and similar methods.\n\nSimilarly, avoid dumping API keys, cookies, IP addresses, URLs, page content,\netc...\n\n### Rule #2: Do not build debug logs in production code\n\nThe log methods are removed in release builds using Proguard. Because log\nmessages might not be written, the cost of creating them should also be avoided.\nThis can be done using three complementary ways:\n\n#### Use string formatting instead of concatenations\n\n```java\n// BAD\nLog.d(TAG, \"I \" + preference + \" writing logs.\");\n\n// BETTER\nLog.d(TAG, \"I %s writing logs.\", preference);\n```\n\nProguard removes the method call itself, but doesn't do anything about the\narguments. The method's arguments will still be computed and provided as\ninput. The first call above will always lead to the creation of a\n`StringBuilder` and a few concatenations, while the second just passes the\narguments and won't need that.\n\n#### Guard expensive calls\n\nSometimes the values to log aren't readily available and need to be computed\nspecially. This should be avoided when logging is disabled.\n\n```java\n...\nif (Log.isLoggable(TAG, Log.INFO)) {\n  Log.i(TAG, createThatExpensiveLogMessage(activity))\n}\n```\n\nBecause the variable is a `static final` that can be evaluated at compile\ntime, the Java compiler will optimize out all guarded calls from the\ngenerated `.class` file. Changing it however requires editing each of the\nfiles for which debug should be enabled and recompiling.\n\n### Rule #3: Favor small log messages\n\nThis is still related to the global fixed-sized kernel buffer used to keep all\nlogs. Try to make your log information as terse as possible. This reduces the\nrisk of pushing interesting log data out of the buffer when something really\nnasty happens. It's really better to have a single-line log message, than\nseveral ones. I.e. don't use:\n\n```java\nLog.GROUP.d(TAG, \"field1 = %s\", value1);\nLog.GROUP.d(TAG, \"field2 = %s\", value2);\nLog.GROUP.d(TAG, \"field3 = %s\", value3);\n```\n\nInstead, write this as:\n\n```java\nLog.d(TAG, \"field1 = %s, field2 = %s, field3 = %s\", value1, value2, value3);\n```\n\nThat doesn't seem to be much different if you count overall character counts,\nbut each independent log entry also implies a small, but non-trivial header, in\nthe kernel log buffer. And since every byte count, you can also try something\neven shorter, as in:\n\n```java\nLog.d(TAG, \"fields [%s,%s,%s]\", value1, value2, value3);\n```\n\n## Filtering logs\n\nLogcat allows filtering by specifying tags and the associated level:\n\n```shell\nadb logcat [TAG_EXPR:LEVEL]...\nadb logcat cr_YourModuleTag:D *:S\n```\n\nThis shows only logs having a level higher or equal to DEBUG for\n`cr_YourModuleTag`, and SILENT (nothing is logged at this level or higher, so it\nsilences the tags) for everything else. You can persist a filter by setting an\nenvironment variable:\n\n```shell\nexport ANDROID_LOG_TAGS=\"cr_YourModuleTag:D *:S\"\n```\n\nThe syntax does not support tag expansion or regular expressions other than `*`\nfor all tags. Please use `grep` or a similar tool to refine your filters\nfurther.\n\nFor more, see the [related page on developer.android.com]\n(https://developer.android.com/tools/debugging/debugging-log.html#filteringOutput)\n\n## Logs in JUnit tests\n\nWe use [robolectric](http://robolectric.org/) to run our JUnit tests. It\nreplaces some of the Android framework classes with \"Shadow\" classes\nto ensure that we can run our code in a regular JVM. `android.util.Log` is one\nof those replaced classes, and by default calling `Log` methods doesn't print\nanything.\n\nThat default is not changed in the normal configuration, but if you need to\nenable logging locally or for a specific test, just add those few lines to your\ntest:\n\n```java\n@Before\npublic void setUp() {\n  ShadowLog.stream = System.out;\n  // Your other setup here\n}\n```\n"
  },
  {
    "path": "platforms/android/android_jni_ownership_best_practices",
    "title": "Android Java <-> C++ Ownership Best Practices",
    "content": "# Android Java <-> C++ Ownership Best Practices\n\nAims to provide best practices for maintaining the ownership and lifecycle of\nlogically paired Java / C++ objects (e.g. if you have a Foo object in Java and\none in C++ that are meant to represent the same concept).\n\n[TOC]\n\n## Establish clear ownership\nEither the Java or C++ object should own the lifecycle of the corresponding\nobject in the other language. The initially created object (either in C++ or\nJava) should be the object that creates and owns the corresponding other\nobject.\n\nIt is the responsibility of the initial object to handle destruction / teardown\nof the corresponding other object.\n\n### [Option #1] Java owns the C++ counterpart\nBecause Java objects are garbage collected and finalizers are prohibited in\nChromium ([link](/styleguide/java/java.md#finalizers)), an explicit\ndestroy / teardown method on the Java object is required to prevent leaking the\ncorresponding C++ object. The destroy / teardown method on the Java object\nwould call an appropriate function on the C++ object (via JNI) to trigger the\ndeletion of the C++ object. At this point, the Java object should reset its\npointer reference to the C++ object to prevent any calls to the now destroyed\nC++ instance.\n\n### [Option #2] C++ owns the Java counterpart\nFor C++ objects, utilizing the appropriate smart java references\n([link](/third_party/jni_zero/README.md#java-objects-and-garbage-collection),\n[code ref](/base/android/scoped_java_ref.h)) will ensure corresponding Java\nobjects can be garbage collected. But if the Java object requires cleaning up\ndependencies, the C++ object should call a corresponding teardown method on the\nJava object in its destructor.\n\nEven in cases where the Java object does not have dependencies requiring clean\nup, the C++ object should notify the Java object that is has gone away. Then the\nJava object can reset its pointer reference to the C++ object and prevent any\ncalls to the already destroyed object.\n\n## Enforce relationship cardinality\nThere should be one Java object per native object (and vice versa) to keep the\nlifecycle simple and easily understood.\n\nFor example, there is one BookmarkModel per Chrome profile in C++, and\ntherefore, there should only be one BookmarkModel instance per Profile in Java.\n\n## Pick a side for your business logic\nWhere possible, keep the business logic in either C++ or Java, and have the\nother object simply act as a shim to the other.\n\nTo facilitate cross-platform development, C++ is the preferred place for\nbusiness logic that could be shared in the future.\n\n## Prefer colocation\nThe code of the Java and C++ object should be colocated to ensure consistent\nlayering and dependencies..\n\nIf the C++ object is in //components/[foo], then the corresponding Java object\nshould also reside in //components/[foo].\n\n## Keep your C++ code close and your Java code closer\nThe C++ code shared across platforms and the corresponding Java class should be\nas close as possible in the code.\n\nFor cases where there are just a few Java <-> C++ calls, try to simply inline\nthose into the same C++ file to minimize indirection.\n\n**Example:**\n\n//components/[foo]/foo_factory.cc\n```c++\n<...> cross platform includes\n\n#if BUILDFLAG(IS_ANDROID)\n#include “base/android/scoped_java_ref.h”\n#include “components/[foo]/android/jni_headers/FooFactory_jni.h”\n#endif  // BUILDFLAG(IS_ANDROID)\n\n<...> shared functions\n\n#if BUILDFLAG(IS_ANDROID)\nstatic ScopedJavaLocalRef<jobject> JNI_FooFactory_Get(JNIEnv* env) {\n    return FooFactory::Get()->GetJavaObject();\n}\n#endif  // BUILDFLAG(IS_ANDROID)\n```\n\nFor cases where the Java <-> C++ API surface is substantial (e.g. if you have a\nC++ object with a large public API and you want to expose all those functions to\nJava), you can split out a JNI methods to a separate class that is owned by the\nprimary C++ object. This approach is suitable when we want to minimize the JNI\nboilerplate in the C++ class.\n\n**Example:**\n\n//components/[foo]/foo.h\n```c++\nclass Foo {\n public:\n  <...>\n\n#if BUILDFLAG(IS_ANDROID)\n  void DoSomething();\n#endif  // BUILDFLAG(IS_ANDROID)\n\n private:\n#if BUILDFLAG(IS_ANDROID)\n  std::unique_ptr<FooAndroid> foo_android_;\n#endif  // BUILDFLAG(IS_ANDROID)\n}\n```\n\n//components/[foo]/foo.cc\n```c++\n<...>\n\n#if BUILDFLAG(IS_ANDROID)\nvoid Foo::DoSomething() {\n  if (!foo_android_) {\n    foo_android_ = std::make_unique<FooAndroid>(this);\n  }\n  foo_android_->DoSomething();\n}\n#endif  // BUILDFLAG(IS_ANDROID)\n```\n\n//components/[foo]/android/foo_android.h\n```c++\nclass FooAndroid {\n public:\n  void DoAThing();\n\n  // JNI methods called from Java.\n  void SomethingElse(JNIEnv* env);\n  jboolean AndABooleanToo(JNIEnv* env);\n  <...>\n\n private:\n  const raw_ptr<Foo> foo_;\n  base::android::ScopedJavaGlobalRef<jobject> java_ref_;\n}\n```\n\n//components/[foo]/android/foo_android.cc\n```c++\nFooAndroid::FooAndroid(Foo* foo) : foo_(foo) {}\n\nFooAndroid::DoAThing() {\n  Java_Foo_DoAThing(base::android::AttachCurrentThread(), java_ref_);\n}\n\nvoid FooAndroid::SomethingElse(JNIEnv* env) {\n  foo_->SomethingElse();\n}\n\njboolean FooAndroid::AndABooleanToo(JNIEnv* env) {\n  return foo->AndABooleanToo();\n}\n```\n\n## When Lifetime is Hard\nWe do not allow the [use of finalizers](/styleguide/java/java.md#Finalizers),\nbut there are a couple of other tricks that have been used to clean up objects\nbesides explicit lifetimes:\n1. Destroy and re-create the native object every time you need it\n   ([GURL does this](/url/android/java/src/org/chromium/url/Parsed.java)).\n2. Use a reference queue that is flushed every once in a while\n   ([example](https://source.chromium.org/search?q=symbol:TaskRunnerImpl.destroyGarbageCollectedTaskRunners)).\n"
  },
  {
    "path": "platforms/android/android_isolated_splits",
    "title": "Isolated Splits",
    "content": "# Isolated Splits\n\nThis doc aims to explain the ins and outs of using Isolated Splits on Android.\n\nFor an overview of apk splits and how to use them in Chrome, see\n[android_dynamic_feature_modules.md].\n\n[TOC]\n\n## About\n\n### What are Isolated Splits?\n\nIsolated Splits is an opt-in feature (via [android:isolatedSplits] manifest\nentry) that cause all feature splits in an application to have separate\n`Context` objects, rather than being merged together into a single Application\n`Context`. The `Context` objects have distict `ClassLoader` and `Resources`\ninstances. They are loaded on-demand instead of eagerly on launch.\n\nWith Isolated Splits, each feature split is loaded in its own ClassLoader, with\nthe parent split set as the parent ClassLoader.\n\n[android:isolatedSplits]: https://developer.android.com/reference/android/R.attr#isolatedSplits\n\n### Why use Isolated Splits?\n\nThe more DEX that is loaded on start-up, the more RAM and time it takes for\napplication code to start running. Loading less code on start-up is particularly\nhelpful for Chrome, since Chrome tends to spawn a lot of processes, and because\nrenderer processes require almost no DEX.\n\n### What Splits Exist in Chrome?\n\nChrome's splits look like:\n\n```\nbase.apk <-- chrome.apk <-- image_editor.apk\n                        <-- feedv2.apk\n                        <-- ...\n```\n\n* The browser process loads the `chrome` split on start-up, and other splits are\n  loaded on-demand.\n* Renderer and GPU processes do not load any feature splits.\n  * The `chrome` split exists to minimize the amount of DEX loaded by renderer\n    processes. However, it also enables faster browser process start-up by\n    allowing DEX to be loaded concurrently with other start-up tasks.\n\n### How are Isolated Splits Loaded?\n\nThere are two ways:\n1) They can be loaded by Android Framework when handling an intent.\n   * E.g.: If a feature split defines an Activity in its manifest, Android\n     will create the split's Context and associate the Activity with it.\n2) They can be loaded explicitly via [BundleUtils.createIsolatedSplitContext()].\n   * The most common way to load in this way is through declaring a\n     `ModuleInterface`, as described in [android_dynamic_feature_modules.md].\n\n[BundleUtils.createIsolatedSplitContext()]: https://source.chromium.org/search?q=func:createIsolatedSplitContext&ss=chromium\n[android_dynamic_feature_modules.md]: android_dynamic_feature_modules.md\n\n## OS Support for Isolated Splits\n\nInitial support was added in Android O. On earlier Android versions, all\nfeature splits are loaded during process start-up and merged into the\nApplication Context.\n\n## OS Bugs\n\n### Base ClassLoader used for Services in Splits (Android Pre-S)\n\nService Contexts are created with the base split's ClassLoader rather than the\nsplit's ClassLoader.\n\nFixed in Android S. Bug: [b/169196314] (Googler only).\n\n**Work-around:**\n\nWe use [SplitCompatService] (and siblings) to put a minimal service class in the\nbase split. They forward all calls to an implementation class, which can live\nin the `chrome` split (or other splits). We also have a [compile-time check] to\nenforce that no Service subclasses exist outside of the base split.\n\n[b/169196314]: https://issuetracker.google.com/169196314\n[SplitCompatService]: https://source.chromium.org/search?q=symbol:SplitCompatService&ss=chromium\n[compile-time check]: https://source.chromium.org/chromium/chromium/src/+/main:build/android/gyp/create_app_bundle.py;l=446;drc=c4dd266492ad1e242161b415ac5a1d9fccd7a041\n\n### Corrupted .odex (Android O MR1)\n\nAndroid O MR1 has a bug where `bg-dexopt-job` (runs during maintenance windows)\nbreaks optimized dex files for Isolated Splits. The corrupt `.odex` files cause\nextremely slow startup times.\n\n**Work-around:**\n\nWe [preemptively run] `dexopt` so that `bg-dexopt-job` decides there is no work\nto do. We trigger this from [PackageReplacedBroadcastReceiver] so that it\nhappens whenever Chrome is updated rather than when the user launches Chrome.\n\n[preemptively run]: https://source.chromium.org/search?q=symbol:DexFixer.needsDexCompile&ss=chromium\n[PackageReplacedBroadcastReceiver]: https://source.chromium.org/search?q=symbol:PackageReplacedBroadcastReceiver&ss=chromium\n\n### Conflicting ClassLoaders #1\n\nTracked by [b/172602571], sometimes a split's parent ClassLoader is different\nfrom the Application's ClassLoader. This manifests as odd-looking\n`ClassCastExceptions` where `\"TypeA cannot be cast to TypeA\"` (since the two\n`TypeAs` are from different ClassLoaders).\n\nTracked by UMA `Android.IsolatedSplits.ClassLoaderReplaced`. Occurs < 0.05% of\nthe time.\n\n**Work-around:**\n\nOn Android O, there is no work-around. We just [detect and crash early].\n\nAndroid P added [AppComponentFactory], which offers a hook that we use to\n[detect and fix] ClassLoader mixups. The ClassLoader mixup also needs to be\ncorrected for `ContextImpl` instances, which we do via\n[ChromeBaseAppCompatActivity.attachBaseContext()].\n\n[b/172602571]: https://issuetracker.google.com/172602571\n[detect and crash early]: https://source.chromium.org/search?q=crbug.com%2F1146745&ss=chromium\n[AppComponentFactory]: https://developer.android.com/reference/android/app/AppComponentFactory\n[detect and fix]: https://source.chromium.org/search?q=f:splitcompatappcomponentfactory&ss=chromium\n[ChromeBaseAppCompatActivity.attachBaseContext()]: https://source.chromium.org/search?q=BundleUtils\\.checkContextClassLoader&ss=chromium\n\n### Conflicting ClassLoaders #2\n\nTracked by [b/172602571], when a new split language split or feature split is\ninstalled, the ClassLoaders for non-base splits are recreated. Any reference to\na class from the previous ClassLoader (e.g. due to native code holding\nreferences to them) will result in `ClassCastExceptions` where\n`\"TypeA cannot be cast to TypeA\"`.\n\n**Work-around:**\n\nThere is no work-around. This is a source of crashes. We could potentially\nmitigate by restarting chrome when a split is installed.\n\n### System.loadLibrary() Broken for Libraries in Splits\n\nTracked by [b/171269960], Android is not adding the apk split to the associated\nClassLoader's `nativeSearchPath`.  This means that `libfoo.so` within an\nisolated split is not found by a call to `System.loadLibrary(\"foo\")`.\n\n**Work-around:**\n\nLoad libraries via `System.load()` instead.\n\n```java\nSystem.load(BundleUtils.getNativeLibraryPath(\"foo\", \"mysplitsname\"));\n```\n\n[b/171269960]: https://issuetracker.google.com/171269960\n\n### System.loadLibrary() Unusable from Split if Library depends on Another Loaded by Base Split\n\nAlso tracked by [b/171269960], maybe related to linker namespaces. If a split\ntries to load `libfeature.so`, and `libfeature.so` has a `DT_NEEDED` entry for\n`libbase.so`, and `libbase.so` is loaded by the base split, then the load will\nfail.\n\n**Work-around:**\n\nHave base split load libraries from within splits. Proxy all JNI calls through\na class that exists in the base split.\n\n### System.loadLibrary() Broken for Libraries in Splits on System Image\n\nAlso tracked by [b/171269960], Android's linker config (`ld.config.txt`) sets\n`permitted_paths=\"/data:/mnt/expand\"`, and then adds the app's `.apk` to an\nallowlist. This allowlist does not contain apk splits, so library loading is\nblocked by `permitted_paths` when the splits live on the `/system` partition.\n\n**Work-around:**\n\nUse compressed system image stubs (`.apk.gz` and `-Stub.apk`) so that Chrome is\nextracted to the `/data` partition upon boot.\n\n### Too Many Splits Break App Zygote\n\nStarting with Android Q / TriChrome, Chrome uses an [Application Zygote]. As\npart of initialization, Chrome's `ApplicationInfo` object is serialized into a\nfixed size buffer. Each installed split increases the size of the\n`ApplicationInfo` object, and can push it over the buffer's limit.\n\n**Work-around:**\n\nDo not add too many splits, and monitor the size of our `ApplicationInfo` object\n([crbug/1298496]).\n\n[crbug/1298496]: https://bugs.chromium.org/p/chromium/issues/detail?id=1298496\n[Application Zygote]: https://developer.android.com/reference/android/app/ZygotePreload\n\n### AppComponentFactory does not Hook Split ClassLoaders\n\n`AppComponentFactory#instantiateClassLoader()` is meant to allow apps to hook\n`ClassLoader` creation. The hook is called for the base split, but not for other\nisolated splits. Tracked by [b/265583114]. There is no work-around.\n\n[b/265583114]: https://issuetracker.google.com/265583114\n\n### Incorrect Handling of Shared Libraries\n\nTracked by [b/265589431]. If an APK split has `<uses-library>` in its manifest,\nthe classloader for the split is meant to have that library added to it by the\nframework. However, Android does not add the library to the classpath when a\nsplit is dynamically installed, but instead adds it to the classpath of the base\nsplit's classloader upon subsequent app launches.\n\n**Work-around:**\n\n * Always add `<uses-library>` to the base split.\n\n[b/265589431]: https://issuetracker.google.com/265589431\n\n## Other Quirks & Subtleties\n\n### System Image APKs\n\nWhen distributing Chrome on Android system images, we generate a single `.apk`\nfile that contains all splits merged together (or rather, all splits whose\n`AndroidManifest.xml` contain `<dist:fusing dist:include=\"true\" />`). We do this\nfor simplicity; Android supports apk splits on the system image.\n\nYou can build Chrome's system `.apk` via:\n```sh\nout/Release/bin/trichrome_chrome_bundle build-bundle-apks --output-apks SystemChrome.apks --build-mode system\nunzip SystemChrome.apks system/system.apk\n```\n\nShipping a single `.apk` file simplifies distribution, but eliminates all the\nbenefits of Isolated Splits.\n\n### Chrome's Application ClassLoader\n\nA lot of Chrome's code uses the `ContextUtils.getApplicationContext()` as a\nContext object. Rather than auditing all usages and replacing applicable ones\nwith the `chrome` split's Context, we [use reflection] to change the\nApplication instance's ClassLoader to point to the `chrome` split's ClassLoader.\n\n[use reflection]: https://source.chromium.org/search?q=f:SplitChromeApplication%20replaceClassLoader&ss=chromium\n\n### ContentProviders\n\nUnlike other application components, ContentProviders are created on start-up\neven when they are not the reason the process is being created. If a\nContentProvider were to be declared in a split, its split's Context would need\nto be loaded during process creation, eliminating any benefit.\n\n**Work-around:**\n\nWe declare all ContentProviders in the base split's `AndroidManifest.xml` and\nenforce this with a [compile-time check]. ContentProviders that would pull in\nsignificant amounts of code use [SplitCompatContentProvider] to delegate to a\nhelper class living within a split.\n\n[compile-time check]: https://source.chromium.org/search?q=symbol:_MaybeCheckServicesAndProvidersPresentInBase&ss=chromium\n[SplitCompatContentProvider]: https://source.chromium.org/search?q=symbol:SplitCompatContentProvider&ss=chromium\n\n### JNI and ClassLoaders\n\nWhen you call from native-&gt;Java (via `@CalledByNative`), there are two APIs\nthat Chrome could use to resolve the target class:\n\n1) JNI API: [JNIEnv::FindClass()]\n2) Java Reflection API:`ClassLoader.loadClass())`\n\nChrome uses #2. For methods within feature splits, `generate_jni()` targets\nuse `split_name = \"foo\"` to make the generated JNI code use the split's\nClassLoader.\n\n[JNIEnv::FindClass()]: https://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/functions.html#wp16027\n\n### Accessing Android Resources\n\nWhen resources live in a split, they must be accessed through a Context object\nassociated with that split. However:\n\n* Bug: Chrome's build system [improperly handles ID conflicts] between splits.\n* Bug: Splash screens [fail to load] for activities in Isolated Splits (unless\n  associated resources are defined in the base split).\n* Quirk: `RemoteViews`, notification icons, and other Android features that\n  access resources by Package ID require resources to be in the base split when\n  Isolated Splits are enabled.\n\n**Work-around:**\n\nChrome [stores all Android resources in the base split]. There is [a crbug] to\ntrack moving resources into splits, but it may prove too challenging.\n\n[stores all Android resources in the base split]: https://source.chromium.org/search?q=recursive_resource_deps%5C%20%3D%5C%20true\n[improperly handles ID conflicts]: https://crbug.com/1133898\n[fail to load]: https://issuetracker.google.com/171743801\n[a crbug]: https://crbug.com/1165782\n\n### Inflating Layouts\n\nLayouts should be inflated with an Activity Context so that\nconfiguration-specific resources and themes are used. If layouts contain\nreferences to View classes from different feature splits than the Activity's,\nthen the views' split ClassLoaders must be used.\n\n**Work-around:**\n\nUse the `ContextWrapper` created via: [BundleUtils.createContextForInflation()]\n\n[BundleUtils.createContextForInflation()]: https://source.chromium.org/search?q=symbol:BundleUtils.createContextForInflation&ss=chromium\n\n### onRestoreInstanceState with Classes From Splits\n\nWhen Android kills an app, it normally calls `onSaveInstanceState()` to allow\nthe app to first save state. The saved state includes the class names of active\nFragments, RecyclerViews, and potentially other classes from splits. Upon\nre-launch, these class names are used to reflectively instantiate instances.\n`FragmentManager` uses the ClassLoader of the Activity to instantiate them,\nand `RecyclerView` uses the ClassLoader associated with the `Bundle` object.\nThe reflection fails if the active Activity resides in a different spilt from\nthe reflectively instantiated classes.\n\n**Work-around:**\n\nChrome stores the list of all splits that have been used for inflation during\n[`onSaveInstanceState`] and then uses [a custom ClassLoader] to look within them\nfor classes that do not exist in the application's ClassLoader. The custom\nClassLoader is passed to `Bundle` instances in\n`ChromeBaseAppCompatActivity.onRestoreInstanceState()`.\n\nHaving Android Framework call `Bundle.setClassLoader()` is tracked in\n[b/260574161].\n\n[`onSaveInstanceState`]: https://source.chromium.org/search?q=symbol:ChromeBaseAppCompatActivity.onSaveInstanceState&ss=chromium\n[a custom ClassLoader]: https://source.chromium.org/search?q=symbol:ChromeBaseAppCompatActivity.getClassLoader&ss=chromium\n[b/260574161]: https://issuetracker.google.com/260574161\n\n### Calling Methods Across a Split Boundary\n\nDue to having different ClassLoaders, package-private methods don't work across\nthe boundary, even though they will compile.\n\n**Work around:**\n\nMake any method public that you wish to call in another module, even if it's in\nthe same package.\n\n### Proguarding Splits\n\n\"Proguarding\" is the build step that performs whole-program optimization of Java\ncode, and \"R8\" is the program Chrome uses to do this. R8 currently supports\nmapping input `.jar` files to output feature splits. If two feature splits share\na common GN `dep`, then its associated `.jar` will be promoted to the parent\nsplit (or to the base split) by our [proguard.py] wrapper script.\n\nThis scheme means that if a single class from a large library is needed by, or\npromoted to, the base split, then every class needed from that library by\nfeature splits will also remain in the base split. The feature request to have\nR8 move code into deeper splits on a per-class basis is [b/225876019] (Googler\nonly).\n\n[proguard.py]: https://source.chromium.org/search?q=symbol:_DeDupeInputJars%20f:proguard.py&ss=chromium\n[b/225876019]: https://issuetracker.google.com/225876019\n\n### Metadata in Splits\n\nMetadata is queried on a per-app basis (not a per-split basis). E.g.:\n\n```java\nApplicationInfo ai = context.getPackageManager().getApplicationInfo(context.getPackageName(), PackageManager.GET_META_DATA);\nBundle b = ai.metaData;\n```\n\nThis bundle contains merged values from all fully-installed apk splits.\n\n## Other Resources\n\n * [go/isolated-splits-dev-guide] (Googlers only).\n * [go/clank-isolated-splits-architecture] (Googlers only).\n\n[go/isolated-splits-dev-guide]: http://go/isolated-splits-dev-guide\n[go/clank-isolated-splits-architecture]: http://go/clank-isolated-splits-architecture\n"
  },
  {
    "path": "platforms/android/android_emulator",
    "title": "Using an Android Emulator",
    "content": "# Using an Android Emulator\nAlways use x86 emulators (or x86\\_64 for testing 64-bit APKs). Although arm\nemulators exist, they are so slow that they are not worth your time.\n\n[TOC]\n\n## Building for Emulation\nYou need to target the correct architecture via GN args:\n```gn\ntarget_cpu = \"x86\"  # or \"x64\" if you have an x86_64 emulator\n```\n\n## Running an Emulator\n\n### Googler-only Emulator Instructions\n\nSee http://go/clank-emulator/\n\n### Using Prebuilt AVD configurations.\n\nChromium has a set of prebuilt AVD configurations stored as CIPD packages.\nThese are used by various builders to run tests on the emulator. Their\nconfigurations are currently stored in [`//tools/android/avd/proto`](../tools/android/avd/proto/).\nYou can run this command to list them:\n```sh\ntools/android/avd/avd.py list\n```\n\n| Configurations | Android Version | Form Factor | Builder |\n|:-------------- |:--------------- |:------- |:---------- |:------- |\n| `generic_android26.textpb` | 8.0 (O) | Phone | [android-oreo-x86-rel][android-oreo-x86-rel] |\n| `generic_android27.textpb` | 8.1 (O_MR1) | Phone | N/A |\n| `android_28_google_apis_x86.textpb` | 9 (P) | Phone | [android-pie-x86-rel][android-pie-x86-rel] |\n| `android_29_google_apis_x86.textpb` | 10 (Q) | Phone | N/A |\n| `android_30_google_apis_x86.textpb` | 11 (R) | Phone | [android-11-x86-rel][android-11-x86-rel] |\n| `android_31_google_apis_x64.textpb` | 12 (S) | Phone | [android-12-x64-rel][android-12-x64-rel] |\n| `android_32_google_apis_x64_foldable.textpb` | 12L (S_V2) | Foldable Phone | [android-12l-x64-dbg-tests][android-12l-x64-dbg-tests] |\n| `android_32_google_apis_x64_foldable_landscape.textpb` | 12L (S_V2) | Foldable Phone (Landscape) | [android-12l-landscape-x64-dbg-tests][android-12l-landscape-x64-dbg-tests] |\n| `android_33_google_apis_x64.textpb` | 13 (T) | Phone | [android-13-x64-rel][android-13-x64-rel] |\n| `android_34_google_apis_x64.textpb` | 14 (U) | Phone | [android-14-x64-rel][android-14-x64-rel] |\n| `android_35_google_apis_x64.textpb` | 15 (V) | Phone | [android-15-x64-rel][android-15-x64-rel] |\n| `android_35_google_apis_x64_tablet.textpb` | 15 (V) | Tablet | [android-15-tablet-x64-dbg-tests][android-15-tablet-x64-dbg-tests] |\n| `android_35_google_apis_x64_tablet_landscape.textpb` | 15 (V) | Tablet (Landscape) | [android-15-tablet-landscape-x64-dbg-tests][android-15-tablet-landscape-x64-dbg-tests] |\n\nYou can use these configuration files to run the same emulator images locally.\n\n[android-oreo-x86-rel]: https://ci.chromium.org/p/chromium/builders/ci/android-oreo-x86-rel\n[android-pie-x86-rel]: https://ci.chromium.org/p/chromium/builders/ci/android-pie-x86-rel\n[android-11-x86-rel]: https://ci.chromium.org/p/chromium/builders/ci/android-11-x86-rel\n[android-12-x64-rel]: https://ci.chromium.org/p/chromium/builders/ci/android-12-x64-rel\n[android-12l-x64-dbg-tests]: https://ci.chromium.org/p/chromium/builders/ci/android-12l-x64-dbg-tests\n[android-12l-landscape-x64-dbg-tests]: https://ci.chromium.org/p/chromium/builders/ci/android-12l-landscape-x64-dbg-tests\n[android-13-x64-rel]: https://ci.chromium.org/p/chromium/builders/ci/android-13-x64-rel\n[android-14-x64-rel]: https://ci.chromium.org/p/chromium/builders/ci/android-14-x64-rel\n[android-15-x64-rel]: https://ci.chromium.org/p/chromium/builders/ci/android-15-x64-rel\n[android-15-tablet-x64-dbg-tests]: https://ci.chromium.org/ui/p/chromium/builders/ci/android-15-tablet-x64-dbg-tests\n[android-15-tablet-landscape-x64-dbg-tests]: https://ci.chromium.org/ui/p/chromium/builders/ci/android-15-tablet-landscape-x64-dbg-tests\n\n> Note: New AVD configurations for LOCAL development are also available now,\n> whose file names end with `_local`, i.e. `*_local.textpb`.\n>\n> The only difference is that these configs has 12GB storage space instead of\n> 4GB on the non-local ones. Larger storage space makes it easier for developers\n> to install large APKs without hitting the space issue.\n\n#### Prerequisite\n\n * Make sure KVM (Kernel-based Virtual Machine) is enabled.\n   See this\n   [link](https://developer.android.com/studio/run/emulator-acceleration#vm-linux)\n   from android studio for more details and instructions.\n\n * You need to have the permissions to use KVM.\n   Use the following command to see if you are in group `kvm`:\n\n   ```\n     $ grep kvm /etc/group\n   ```\n\n   If your username is not shown in the group, add yourself to the group:\n\n   ```\n     $ sudo adduser $USER kvm\n     $ newgrp kvm\n   ```\n\n   You need to log out and log back in so the new groups take effect.\n\n#### Running via the test runner\n\nThe android test runner can run emulator instances on its own. In doing so, it\nstarts the emulator instances, runs tests against them, and then shuts them\ndown. This is how builders run the emulator.\n\n##### Options\n\n * `--avd-config`\n\n    To have the test runner run an emulator instance, use `--avd-config`:\n\n    ```\n      $ out/Debug/bin/run_base_unittests \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb\n    ```\n\n * `--emulator-count`\n\n    The test runner will launch one instance by default. To have it run multiple\n    instances, use `--emulator-count`:\n\n    ```\n      $ out/Debug/bin/run_base_unittests \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --emulator-count 4\n    ```\n\n * `--emulator-enable-network`\n\n    The test runner runs the emulator without network access by default. To have\n    it run with network access, use `--emulator-enable-network`:\n\n    ```\n    $ out/Debug/bin/run_base_unittests \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --emulator-enable-network\n    ```\n\n * `--emulator-window`\n\n    The test runner runs the emulator in headless mode by default. To have it run\n    with a window, use `--emulator-window`:\n\n    ```\n      $ out/Debug/bin/run_base_unittests \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --emulator-window\n    ```\n\n#### Running standalone\n\nThe test runner will set up and tear down the emulator on each invocation.\nTo manage emulator lifetime independently, use `tools/android/avd/avd.py`.\n\n##### Options\n\n * `--avd-config`\n\n    This behaves the same as it does for the test runner.\n\n    ```\n      $ tools/android/avd/avd.py start \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb\n    ```\n\n    > Note: `avd.py start` will start an emulator instance and then terminate.\n    > To shut down the emulator, use `adb emu kill`.\n\n * `--enable-network`\n\n    Like the test runner, `avd.py` runs the emulator without network access by\n    default. To enable network access, use `--enable-network`:\n\n    ```\n      $ tools/android/avd/avd.py start \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --enable-network\n    ```\n\n * `--emulator-window`\n\n    Like the test runner, `avd.py` runs the emulator in headless mode by default.\n    To have it run with a window, use `--emulator-window`:\n\n    ```\n      $ tools/android/avd/avd.py start \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --emulator-window\n    ```\n\n * `--gpu-mode GPU_MODE`\n\n    Override the mode of hardware OpenGL ES emulation indicated by the AVD.\n    See \"emulator -help-gpu\" for a full list of modes.\n\n * `--no-read-only`\n\n    `avd.py` runs the emulator in read-only mode by default. To run a modifiable\n    emulator, use `--no-read-only`:\n\n    ```\n      $ tools/android/avd/avd.py start \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --no-read-only\n    ```\n\n * `--wipe-data`\n\n    Reset the /data partition to the factory defaults. This removes all user\n    settings from the AVD.\n\n    ```\n      $ tools/android/avd/avd.py start \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --wipe-data\n    ```\n\n * `--writable-system`\n\n    Makes system & vendor image writable. It's necessary to run\n    ```\n    adb root\n    adb remount\n    ```\n    after the emulator starts.\n\n * `--debug-tags`\n\n    `avd.py` disables the emulator log by default. When this option is used,\n    emulator log will be enabled. It is useful when the emulator cannot be\n    launched correctly. See `emulator -help-debug-tags` for a full list of tags.\n    Use `--debug-tags=all` if you want to output all logs (warning: it is quite\n    verbose).\n\n    ```\n      $ tools/android/avd/avd.py start \\\n          --avd-config tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n          --debug-tags init,snapshot\n    ```\n\n### Using Your Own Emulator Image\n\nBy far the easiest way to set up emulator images is to use Android Studio.\nIf you don't have an [Android Studio project](android_studio.md) already, you\ncan create a blank one to be able to reach the Virtual Device Manager screen.\n\nRefer to: https://developer.android.com/studio/run/managing-avds.html\n\nWhere files live:\n * System partition images are stored within the sdk directory.\n * Emulator configs and data partition images are stored within\n   `~/.android/avd/`.\n\n#### Creating an Image\n\n##### Choosing a Skin\n\nChoose a skin with a small screen for better performance (unless you care about\ntesting large screens).\n\n##### Choosing an Image\n\nAndroid Studio's image labels roughly translate to the following:\n\n| AVD \"Target\" | Virtual Device Configuration tab | GMS? | Build Properties |\n| --- | --- | --- | --- |\n| Google Play | \"Recommended\" (the default tab) | This has GMS | `user`/`release-keys` |\n| Google APIs | \"x86 Images\" | This has GMS | `userdebug`/`dev-keys` |\n| No label | \"x86 Images\" | AOSP image, does not have GMS | `eng`/`test-keys` |\n\n*** promo\n**Tip:** if you're not sure which to use, choose **Google APIs** under the **x86\nImages** tab in the Virtual Device Configuration wizard.\n***\n\n##### Configuration\n\n\"Show Advanced Settings\" > scroll down:\n* Set internal storage to 4000MB (component builds are really big).\n* Set SD card to 1000MB (our tests push a lot of files to /sdcard).\n\n##### Known Issues\n\n * Our test & installer scripts do not work with pre-MR1 Jelly Bean.\n * Component builds do not work on pre-KitKat (due to the OS having a max\n   number of shared libraries).\n * Jelly Bean and KitKat images sometimes forget to mount /sdcard :(.\n   * This causes tests to fail.\n   * To ensure it's there: `adb -s emulator-5554 shell mount` (look for /sdcard)\n   * Can often be fixed by editing `~/.android/avd/YOUR_DEVICE/config.ini`.\n     * Look for `hw.sdCard=no` and set it to `yes`\n * The \"Google APIs\" Android L and M emulator images are configured to expect\n   the \"AOSP\" WebView package (`com.android.webview`). This does not resemble\n   production devices with GMS, which expect the [\"Google WebView\"\n   configuration](/android_webview/docs/webview-providers.md#webview-provider-options)\n   (`com.google.android.webview` on L and M). See [Removing preinstalled\n   WebView](/android_webview/docs/build-instructions.md#Removing-preinstalled-WebView)\n   if you need to install a local build or official build.\n\n\n#### Starting an Emulator from the Command Line\n\nRefer to: https://developer.android.com/studio/run/emulator-commandline.html.\n\n*** promo\nCtrl-C will gracefully close an emulator.\n***\n\n*** promo\n**Tip:** zsh users can add https://github.com/zsh-users/zsh-completions to\nprovide tab completion for the `emulator` command line tool.\n***\n\n#### Basic Command Line Use\n\n```shell\n$ # List virtual devices that you've created:\n$ ~/Android/Sdk/emulator/emulator -list-avds\n$ # Start a named device:\n$ ~/Android/Sdk/emulator/emulator @EMULATOR_ID\n```\n\n#### Running a Headless Emulator\n\nYou can run an emulator without creating a window on your desktop (useful for\n`ssh`):\n```shell\n$ ~/Android/Sdk/emulator/emulator -no-window @EMULATOR_ID\n$ # This also works for new enough emulator builds:\n$ ~/Android/Sdk/emulator/emulator-headless @EMULATOR_ID\n```\n\n#### Running Multiple Emulators\n\nTests are automatically sharded amongst available devices. If you run multiple\nemulators, then running test suites becomes much faster. Refer to the\n\"Multiple AVD instances\" section of these [emulator release notes](\nhttps://androidstudio.googleblog.com/2018/11/emulator-28016-stable.html)\nfor more about how this works.\n```shell\n$ # Start 8 emulators. Press Ctrl-C to stop them all.\n$ ( for i in $(seq 8); do ~/Android/Sdk/emulator/emulator @EMULATOR_ID -read-only & done; wait )\n$ # Start 12 emulators. More than 10 requires disabling audio on some OS's. Reducing cores increases parallelism.\n$ ( for i in $(seq 12); do ~/Android/Sdk/emulator/emulator @EMULATOR_ID -read-only -no-audio -cores 2 & done; wait )\n```\n\n#### Writable system partition\n\nUnlike physical devices, an emulator's `/system` partition cannot be modified by\ndefault (even on rooted devices). If you need to do so (such as to remove a\nsystem app), you can start your emulator like so:\n```shell\n$ ~/Android/Sdk/emulator/emulator -writable-system @EMULATOR_ID\n```\n\n## Using an Emulator\n * Emulators show up just like devices via `adb devices`\n   * Device serials will look like \"emulator-5554\", \"emulator-5556\", etc.\n\n## Emulator pros and cons\n\n### Pros\n * **Compiles are faster.** Many physical devices are arm64, whereas emulators\n   are typically x86 (32-bit). 64-bit builds may require 2 copies of the native\n   library (32-bit and 64-bit), so compiling for an arm64 phone is ~twice as\n   much work as for an emulator (for targets which support WebView).\n * **APKs install faster.** Since emulators run on your workstation, adb can\n   push the APK onto the emulator without being [bandwidth-constrained by\n   USB](https://youtu.be/Mzop8bXZI3E).\n * Emulators can be nice for working remotely. Physical devices usually require\n   `scp` or ssh port forwarding to copy the APK from your workstation and\n   install on a local device. Emulators run on your workstation, so there's **no\n   ssh slow-down**.\n\n### Cons\n * If you're investigating a hardware-specific bug report, you'll need a\n   physical device with the actual hardware to repro that issue.\n * x86 emulators need a separate out directory, so building for both physical\n   devices and emulators takes up more disk space (not a problem if you build\n   exclusively for the emulator).\n * `userdebug`/`eng` emulators don't come with the Play Store installed, so you\n   can't install third party applications. Sideloading is tricky, as not all\n   third-party apps support x86.\n"
  },
  {
    "path": "platforms/android/android_dynamic_feature_modules",
    "title": "App Bundles and Dynamic Feature Modules (DFMs)",
    "content": "# App Bundles and Dynamic Feature Modules (DFMs)\n\n[TOC]\n\n## About Bundles\n[Android App bundles] is a Play Store feature that allows packaging an app as\nmultiple `.apk` files, known as \"splits\". Bundles are zip files with an `.aab`\nextension. See [android_build_instructions.md#multiple-chrome-targets] for a\nlist of buildable bundle targets.\n\nBundles provide three main advantages over monolithic `.apk` files:\n1. Language resources are split into language-specific `.apk` files, known as\n   \"resource splits\". Delivering only the active languages reduces the overhead\n   of UI strings.\n   * Resource splits can also be made on a per-screen-density basis (for drawables),\n     but Chrome has not taken advantage of this (yet).\n2. Features can be packaged into lazily loaded `.apk` files, known as\n   \"feature splits\". Chrome enables [isolated splits], which means feature\n   splits have no performance overhead until used (on Android O+ at least).\n3. Feature splits can be downloaded on-demand, saving disk space for users that\n   do not need the functionality they provide. These are known as\n   \"Dynamic feature modules\", or \"DFMs\".\n   * **The install experience for DFMs is quite poor (5-30 seconds install times,\n     sometimes fails, sometimes [triggers a crash]).**\n\nYou can inspect which `.apk` files are produced by a bundle target via:\n```\nout/Default/bin/${target_name} build-bundle-apks --output-apks foo.apks\nunzip -l foo.apks\n```\n\n*** note\nAdding new features via feature splits is highly encouraged when it makes sense\nto do so:\n * Has a non-trivial amount of Java code (after optimization). E.g. >150kb\n * Not needed on startup\n * Has a small integration surface (calls into it must be done with reflection)\n * Not used by WebView\n***\n\n[android_build_instructions.md#multiple-chrome-targets]: android_build_instructions.md#multiple-chrome-targets\n[Android App Bundles]: https://developer.android.com/guide/app-bundle\n[isolated splits]: android_isolated_splits.md\n[triggers a crash]: https://chromium.googlesource.com/chromium/src/+/main/docs/android_isolated_splits.md#Conflicting-ClassLoaders-2\n\n### Declaring App Bundles with GN Templates\n\nHere's an example that shows how to declare a simple bundle that contains a\nsingle base module, which enables language-based splits:\n\n```gn\n  android_app_bundle_module(\"foo_base_module\") {\n    # Declaration are similar to android_apk here.\n    ...\n  }\n\n  android_app_bundle(\"foo_bundle\") {\n    base_module_target = \":foo_base_module\"\n\n    # The name of our bundle file (without any suffix).\n    bundle_name = \"FooBundle\"\n\n    # Enable language-based splits for this bundle. Which means that\n    # resources and assets specific to a given language will be placed\n    # into their own split APK in the final .apks archive.\n    enable_language_splits = true\n\n    # Proguard settings must be passed at the bundle, not module, target.\n    proguard_enabled = !is_java_debug\n  }\n```\n\nWhen generating the `foo_bundle` target with Ninja, you will end up with\nthe following:\n\n  * The bundle file under `out/Release/apks/FooBundle.aab`\n\n  * A helper script called `out/Release/bin/foo_bundle`, which can be used\n    to install / launch / uninstall the bundle on local devices.\n\n    This works like an APK wrapper script (e.g. `foo_apk`). Use `--help`\n    to see all possible commands supported by the script.\n\n\nThe remainder of this doc focuses on DFMs.\n\n## Declaring Dynamic Feature Modules (DFMs)\n\nThis guide walks you through the steps to create a DFM called _Foo_ and add it\nto the Chrome bundles.\n\n*** note\n**Note:** To make your own module you'll essentially have to replace every\ninstance of `foo`/`Foo`/`FOO` with `your_feature_name`/`YourFeatureName`/\n`YOUR_FEATURE_NAME`.\n***\n\n### Reference DFM\n\nIn addition to this guide, the\n[Test Dummy](https://cs.chromium.org/chromium/src/chrome/android/modules/test_dummy/test_dummy_module.gni)\nmodule serves as an actively-maintained reference DFM. Test Dummy is used in\nautomated bundle testing, and covers both Java and native code and resource\nusage.\n\n### Create DFM target\n\nDFMs are APKs. They have a manifest and can contain Java and native code as well\nas resources. This section walks you through creating the module target in our\nbuild system.\n\nFirst, create the file\n`//chrome/android/modules/foo/internal/java/AndroidManifest.xml` and add:\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:dist=\"http://schemas.android.com/apk/distribution\"\n    featureSplit=\"foo\">\n\n    <!-- dist:onDemand=\"true\" makes this a separately installed module.\n         dist:onDemand=\"false\" would always install the module alongside the\n         rest of Chrome. -->\n    <dist:module\n        dist:onDemand=\"true\"\n        dist:title=\"@string/foo_module_title\">\n        <!-- This will fuse the module into the base APK if a system image\n             APK is built from this bundle. -->\n        <dist:fusing dist:include=\"true\" />\n    </dist:module>\n\n    <!-- Remove android:hasCode=\"false\" when adding Java code. -->\n    <application android:hasCode=\"false\" />\n</manifest>\n```\n\nNext, create a descriptor configuring the Foo module. To do this, create\n`//chrome/android/modules/foo/foo_module.gni` and add the following:\n\n```gn\nfoo_module_desc = {\n  name = \"foo\"\n  android_manifest =\n      \"//chrome/android/modules/foo/internal/java/AndroidManifest.xml\"\n}\n```\n\nThen, add the module descriptor to the appropriate descriptor list in\n//chrome/android/modules/chrome_feature_modules.gni, e.g. the Chrome list:\n\n```gn\nimport(\"//chrome/android/modules/foo/foo_module.gni\")\n...\nchrome_module_descs += [ foo_module_desc ]\n```\n\nThe next step is to add Foo to the list of feature modules for UMA recording.\nFor this, add `foo` to the `AndroidFeatureModuleName` in\n`//tools/metrics/histograms/metadata/histogram_suffixes_list.xml`:\n\n```xml\n<histogram_suffixes name=\"AndroidFeatureModuleName\" ...>\n  ...\n  <suffix name=\"foo\" label=\"Super Duper Foo Module\" />\n  ...\n</histogram_suffixes>\n```\n\nLastly, give your module a title that Chrome and Play can use for the install\nUI. To do this, add a string to\n`//chrome/browser/ui/android/strings/android_chrome_strings.grd`:\n\n```xml\n...\n<message name=\"IDS_FOO_MODULE_TITLE\"\n  desc=\"Text shown when the Foo module is referenced in install start, success,\n        failure UI (e.g. in IDS_MODULE_INSTALL_START_TEXT, which will expand to\n        'Installing Foo for Chrome…').\">\n  Foo\n</message>\n...\n```\n\n*** note\n**Note:** This is for module title only. Other strings specific to the module\nshould go in the module, not here (in the base module).\n***\n\nCongrats! You added the DFM Foo to Chrome. That is a big step but not very\nuseful so far. In the next sections you'll learn how to add code and resources\nto it.\n\n\n### Building and installing modules\n\nBefore we are going to jump into adding content to Foo, let's take a look on how\nto build and deploy the Monochrome bundle with the Foo DFM. The remainder of\nthis guide assumes the environment variable `OUTDIR` is set to a properly\nconfigured GN build directory (e.g. `out/Debug`).\n\nTo build and install the Monochrome bundle to your connected device, run:\n\n```shell\n$ autoninja -C $OUTDIR monochrome_public_bundle\n$ $OUTDIR/bin/monochrome_public_bundle install -m foo\n```\n\nThis will install the `Foo` module, the `base` module, and all modules with an\n`AndroidManifest.xml` that:\n * Sets `<module dist:onDemand=\"false\">`, or\n * Has `<dist:delivery>` conditions that are satisfied by the device being\n   installed to.\n\n*** note\n**Note:** The install script may install more modules than you specify, e.g.\nwhen there are default or conditionally installed modules (see\n[below](#conditional-install) for details).\n***\n\nYou can then check that the install worked with:\n\n```shell\n$ adb shell dumpsys package org.chromium.chrome | grep splits\n>   splits=[base, config.en, foo]\n```\n\nThen try installing the Monochrome bundle without your module and print the\ninstalled modules:\n\n```shell\n$ $OUTDIR/bin/monochrome_public_bundle install\n$ adb shell dumpsys package org.chromium.chrome | grep splits\n>   splits=[base, config.en]\n```\n\n*** note\nThe wrapper script's `install` command does approximately:\n```sh\njava -jar third_party/android_build_tools/bundletool/cipd/bundletool.jar build-apks --output tmp.apks ...\njava -jar third_party/android_build_tools/bundletool/cipd/bundletool.jar install-apks --apks tmp.apks\n```\n\nThe `install-apks` command uses `adb install-multiple` under-the-hood.\n***\n\n### Adding Java code\n\nTo make Foo useful, let's add some Java code to it. This section will walk you\nthrough the required steps.\n\nFirst, define a module interface for Foo. This is accomplished by adding the\n`@ModuleInterface` annotation to the Foo interface. This annotation\nautomatically creates a `FooModule` class that can be used later to install and\naccess the module. To do this, add the following in the new file\n`//chrome/browser/foo/android/java/src/org/chromium/chrome/browser/foo/Foo.java`:\n\n```java\npackage org.chromium.chrome.browser.foo;\n\nimport org.chromium.components.module_installer.builder.ModuleInterface;\n\n/** Interface to call into Foo feature. */\n@ModuleInterface(module = \"foo\", impl = \"org.chromium.chrome.browser.FooImpl\")\npublic interface Foo {\n    /** Magical function. */\n    void bar();\n}\n```\n\nNext, define an implementation that goes into the module in the new file\n`//chrome/browser/foo/internal/android/java/src/org/chromium/chrome/browser/foo/FooImpl.java`:\n\n```java\npackage org.chromium.chrome.browser.foo;\n\nimport org.chromium.base.Log;\n\npublic class FooImpl implements Foo {\n    @Override\n    public void bar() {\n        Log.i(\"FOO\", \"bar in module\");\n    }\n}\n```\n\nYou can then use this provider to access the module if it is installed. To test\nthat, instantiate Foo and call `bar()` somewhere in Chrome:\n\n```java\nif (FooModule.isInstalled()) {\n    FooModule.getImpl().bar();\n} else {\n    Log.i(\"FOO\", \"module not installed\");\n}\n```\n\nThe interface has to be available regardless of whether the Foo DFM is present.\nTherefore, put those classes into the base module, creating a new public\nbuild target in: `//chrome/browser/foo/BUILD.gn`:\n\n```gn\nimport(\"//build/config/android/rules.gni\")\n\nandroid_library(\"java\") {\n  sources = [\n    \"android/java/src/org/chromium/chrome/browser/foo/Foo.java\",\n  ]\n  deps = [\n    \"//components/module_installer/android:module_installer_java\",\n    \"//components/module_installer/android:module_interface_java\",\n  ]\n  annotation_processor_deps =\n    [ \"//components/module_installer/android:module_interface_processor\" ]\n}\n```\n\nThen, depend on this target from where it is used as usual. For example, if the\ncaller is in `chrome_java in //chrome/android/BUILD.gn`:\n\n```gn\n...\nandroid_library(\"chrome_java\") {\n  deps =[\n    ...\n    \"//chrome/browser/foo:java\",\n    ...\n  ]\n}\n...\n```\n\nThe actual implementation, however, should go into the Foo DFM. For this\npurpose, create a new file `//chrome/browser/foo/internal/BUILD.gn` and\nmake a library with the module Java code in it:\n\n```gn\nimport(\"//build/config/android/rules.gni\")\n\nandroid_library(\"java\") {\n  # Define like ordinary Java Android library.\n  sources = [\n    \"android/java/src/org/chromium/chrome/browser/foo/FooImpl.java\",\n    # Add other Java classes that should go into the Foo DFM here.\n  ]\n  deps = [\n    \"//base:base_java\",\n    # Put other Chrome libs into the classpath so that you can call into them\n    # from the Foo DFM.\n    \"//chrome/browser/bar:java\",\n    # The module can depend even on `chrome_java` due to factory magic, but this\n    # is discouraged. Consider passing a delegate interface in instead.\n    \"//chrome/android:chrome_java\",\n    # Also, you'll need to depend on any //third_party or //components code you\n    # are using in the module code.\n  ]\n}\n```\n\nThen, add this new library as a dependency of the Foo module descriptor in\n`//chrome/android/modules/foo/foo_module.gni`:\n\n```gn\nfoo_module_desc = {\n  ...\n  java_deps = [\n    \"//chrome/browser/foo/internal:java\",\n  ]\n}\n```\n\nFinally, tell Android that your module is now containing code. Do that by\nremoving the `android:hasCode=\"false\"` attribute from the `<application>` tag in\n`//chrome/android/modules/foo/internal/java/AndroidManifest.xml`. You should be\nleft with an empty tag like so:\n\n```xml\n...\n    <application />\n...\n```\n\nRebuild and install `monochrome_public_bundle`. Start Chrome and run through a\nflow that tries to executes `bar()`. Depending on whether you installed your\nmodule (`-m foo`) \"`bar in module`\" or \"`module not installed`\" is printed to\nlogcat. Yay!\n\n### Adding pre-built native libraries\n\nYou can add a third-party native library (or any standalone library that doesn't\ndepend on Chrome code) by adding it as a loadable module to the module descriptor in\n`//chrome/android/moduiles/foo/foo_module.gni`:\n\n```gn\nfoo_module_desc = {\n  ...\n  loadable_modules_32_bit = [ \"//path/to/32/bit/lib.so\" ]\n  loadable_modules_64_bit = [ \"//path/to/64/bit/lib.so\" ]\n}\n```\n\n### Adding Chrome native code\n\nChrome native code may be placed in a DFM. The easiest way to access native\nfeature code is by calling it from Java via JNI. When a module is first\naccessed, its native library (or potentially libraries, if using a component\nbuild), are automatically opened by the DFM framework, and a feature-specific\nJNI method (supplied by the feature's implementation) is invoked. Hence, a\nmodule's Java code may freely use JNI to call module native code.\n\nUsing the module framework and JNI to access the native code eliminates concerns\nwith DFM library file names (which vary across build variants),\n`android_dlopen_ext()` (needed to open feature libraries), and use of dlsym().\n\nThis mechanism can be extended if necessary by DFM implementers to facilitate\nsubsequent native-native calls, by having a JNI-called initialization method\ncreate instance of a object or factory, and register it through a call to the\nbase module's native code (DFM native code can call base module code directly).\n\n#### JNI\n\nRead the `jni_generator` [docs](../third_party/jni_zero/README.md) before\nreading this section.\n\nThere are some subtleties to how JNI registration works with DFMs:\n\n* Generated wrapper `ClassNameJni` classes are packaged into the DFM's dex file\n* The class containing the actual native definitions,\n  `<module_name>_GEN_JNI.java`, is currently stored in the base module, but\n  could be moved out\n* The `Natives` interface you provide will need to be annotated with your module\n  name as an argument to `NativeMethods`, eg. `@NativeMethods(\"foo\")`, resulting\n  in a uniquely named `foo_GEN_JNI.java`\n* The DFM will need to provide a `generate_jni_registration` target\n  that will generate all of the native registration functions\n\n#### Calling DFM native code via JNI\n\nA linker-assisted partitioning system automates the placement of code into\neither the main Chrome library or feature-specific .so libraries. Feature code\nmay continue to make use of core Chrome code (eg. base::) without modification,\nbut Chrome must call feature code through a virtual interface (any \"direct\"\ncalls to the feature code from the main library will cause the feature code to\nbe pulled back into the main library).\n\nPartitioning is explained in [Android Native\nLibraries](android_native_libraries.md#partitioned-libraries).\n\nFirst, build a module native interface. Supply a JNI method named\n`JNI_OnLoad_foo` for the module framework to call, in\n`//chrome/android/modules/foo/internal/entrypoints.cc`. This method is invoked\non all Chrome build variants, including Monochrome (unlike base module JNI).\n\n```c++\n#include \"third_party/jni_zero/jni_zero_helper.h\"\n#include \"base/android/jni_utils.h\"\n#include \"chrome/android/modules/foo/internal/jni_registration.h\"\n\nextern \"C\" {\n// This JNI registration method is found and called by module framework code.\nJNI_ZERO_BOUNDARY_EXPORT bool JNI_OnLoad_foo(JNIEnv* env) {\n  if (!foo::RegisterNatives(env)) {\n    return false;\n  }\n  return true;\n}\n}  // extern \"C\"\n```\n\nNext, include the module entrypoint and related pieces in the build config at\n`//chrome/android/modules/foo/internal/BUILD.gn`:\n\n```gn\nimport(\"//build/config/android/rules.gni\")\nimport(\"//chrome/android/modules/buildflags.gni\")\n...\n\n# Put the JNI entrypoint in a component, so that the component build has a\n# library to include in the foo module. This makes things feel consistent with\n# a release build.\ncomponent(\"foo\") {\n  sources = [\n    \"entrypoints.cc\",\n  ]\n  deps = [\n    \":jni_registration\",\n    \"//base\",\n    \"//chrome/browser/foo/internal:native\",\n  ]\n\n  # Instruct the compiler to flag exported entrypoint function as belonging in\n  # foo's library. The linker will use this information when creating the\n  # native libraries. The partition name must be <feature>_partition.\n  if (use_native_partitions) {\n    cflags = [ \"-fsymbol-partition=foo_partition\" ]\n  }\n}\n\n# Generate JNI registration for the methods called by the Java side. Note the\n# no_transitive_deps argument, which ensures that JNI is generated for only the\n# specified Java target, and not all its transitive deps (which could include\n# the base module).\ngenerate_jni_registration(\"jni_registration\") {\n  targets = [ \"//chrome/browser/foo/internal:java\" ]\n  namespace = \"foo\"\n  no_transitive_deps = true\n  manual_jni_registration = true\n}\n\n# This group is a convenience alias representing the module's native code,\n# allowing it to be named \"native\" for clarity in module descriptors.\ngroup(\"native\") {\n  deps = [\n    \":foo\",\n  ]\n}\n```\n\nNow, over to the implementation of the module. These are the parts that\nshouldn't know or care whether they're living in a module or not.\n\nAdd a stub implementation in\n`//chrome/browser/foo/internal/android/foo_impl.cc`:\n\n```c++\n#include \"base/logging.h\"\n#include \"chrome/browser/foo/internal/jni_headers/FooImpl_jni.h\"\n\nstatic int JNI_FooImpl_Execute(JNIEnv* env) {\n  LOG(INFO) << \"Running foo feature code!\";\n  return 123;\n}\n```\n\nAnd, the associated build config in\n`//chrome/browser/foo/internal/BUILD.gn`:\n\n```gn\nimport(\"//build/config/android/rules.gni\")\n\n...\n\nsource_set(\"native\") {\n  sources = [\n    \"android/foo_impl.cc\",\n  ]\n\n  deps = [\n    \":jni_headers\",\n    \"//base\",\n  ]\n}\n\ngenerate_jni(\"jni_headers\") {\n  sources = [\n    \"android/java/src/org/chromium/chrome/browser/foo/FooImpl.java\",\n  ]\n}\n```\n\nWith a declaration of the native method on the Java side:\n\n```java\npublic class FooImpl implements Foo {\n    ...\n\n    @NativeMethods(\"foo\")\n    interface Natives {\n        int execute();\n    }\n}\n```\n\nFinally, augment the module descriptor in\n`//chrome/android/modules/foo/foo_module.gni` with the native dependencies:\n\n```gn\nfoo_module_desc = {\n  ...\n  native_deps = [\n    \"//chrome/android/modules/foo/internal:native\",\n    \"//chrome/browser/foo/internal:native\",\n  ]\n  load_native_on_get_impl = true\n}\n```\n\nIf `load_native_on_get_impl` is set to `true` then Chrome automatically loads\nFoo DFM's native libraries and PAK file resources when `FooModule.getImpl()` is\ncalled for the first time. The loading requires Chrome's main native libraries\nto be loaded. If you wish to call `FooModule.getImpl()` earlier than that, then\nyou'd need to set `load_native_on_get_impl` to `false`, and manage native\nlibraries / resources loading yourself (potentially, on start-up and on install,\nor on use).\n\n#### Calling feature module native code from base the module\n\nIf planning to use direct native-native calls into DFM code, then the module\nshould have a purely virtual interface available. The main module can obtain a\npointer to a DFM-created object or factory (implemented by the feature), and\ncall its virtual methods.\n\nIdeally, the interface to the feature will avoid feature-specific types. If a\nfeature defines complex data types, and uses them in its own interface, then it's\nlikely the main library will utilize the code backing these types. That code,\nand anything it references, will in turn be pulled back into the main library,\nnegating the intent to house code in the DFM.\n\nTherefore, designing the feature interface to use C types, C++ standard types,\nor classes that aren't expected to move out of Chrome's main library is ideal.\nIf feature-specific classes are needed, they simply need to avoid referencing\nfeature library internals.\n\n### Adding Android resources\n\nIn this section we will add the required build targets to add Android resources\nto the Foo DFM.\n\nFirst, add a resources target to\n`//chrome/browser/foo/internal/BUILD.gn` and add it as a dependency on\nFoo's `java` target in the same file:\n\n```gn\n...\nandroid_resources(\"java_resources\") {\n  # Define like ordinary Android resources target.\n  ...\n  custom_package = \"org.chromium.chrome.browser.foo\"\n}\n...\nandroid_library(\"java\") {\n  ...\n  deps = [\n    \":java_resources\",\n  ]\n}\n```\n\nTo add strings follow steps\n[here](http://dev.chromium.org/developers/design-documents/ui-localization) to\nadd new Java GRD file. Then create\n`//chrome/browser/foo/internal/android/resources/strings/android_foo_strings.grd` as\nfollows:\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<grit current_release=\"1\" latest_public_release=\"0\">\n  <outputs>\n    <output\n        filename=\"values-am/android_foo_strings.xml\"\n        lang=\"am\"\n        type=\"android\" />\n    <!-- List output file for all other supported languages. See\n         //chrome/browser/ui/android/strings/android_chrome_strings.grd for the\n         full list. -->\n    ...\n  </outputs>\n  <translations>\n    <file lang=\"am\" path=\"vr_translations/android_foo_strings_am.xtb\" />\n    <!-- Here, too, list XTB files for all other supported languages. -->\n    ...\n  </translations>\n  <release seq=\"1\">\n    <messages fallback_to_english=\"true\">\n      <message name=\"IDS_BAR_IMPL_TEXT\" desc=\"Magical string.\">\n        impl\n      </message>\n    </messages>\n  </release>\n</grit>\n```\n\nThen, create a new GRD target and add it as a dependency on `java_resources` in\n`//chrome/browser/foo/internal/BUILD.gn`:\n\n```gn\n...\njava_strings_grd(\"java_strings_grd\") {\n  defines = chrome_grit_defines\n  grd_file = \"android/resources/strings/android_foo_strings.grd\"\n  outputs = [\n    \"values-am/android_foo_strings.xml\",\n    # Here, too, list output files for other supported languages.\n    ...\n  ]\n}\n...\nandroid_resources(\"java_resources\") {\n  ...\n  deps = [\":java_strings_grd\"]\n  custom_package = \"org.chromium.chrome.browser.foo\"\n}\n...\n```\n\nYou can then access Foo's resources using the\n`org.chromium.chrome.browser.foo.R` class. To do this change\n`//chrome/browser/foo/internal/android/java/src/org/chromium/chrome/browser/foo/FooImpl.java`\nto:\n\n```java\npackage org.chromium.chrome.browser.foo;\n\nimport org.chromium.base.ContextUtils;\nimport org.chromium.base.Log;\nimport org.chromium.chrome.browser.foo.R;\n\npublic class FooImpl implements Foo {\n    @Override\n    public void bar() {\n        Log.i(\"FOO\", ContextUtils.getApplicationContext().getString(\n                R.string.bar_impl_text));\n    }\n}\n```\n\n### Adding non-string native resources\n\nThis section describes how to add non-string native resources to Foo DFM.\nKey ideas:\n\n* The compiled resource file shipped with the DFM is `foo_resourcess.pak`.\n* At run time, native resources need to be loaded before use. Also, DFM native\n  resources can only be used from the Browser process.\n\n#### Creating PAK file\n\nTwo ways to create `foo_resourcess.pak` (using GRIT) are:\n\n1. (Preferred) Use `foo_resourcess.grd` to refer to individual files (e.g.,\n  images, HTML, JS, or CSS) and assigns resource text IDs. `foo_resourcess.pak`\n  must have an entry in `/tools/gritsettings/resource_ids.spec`.\n1. Combine existing .pak files via `repack` rules in GN build files. This is\n  done by the DevUI DFM, which aggregates resources from many DevUI pages.\n\n#### Loading PAK file\n\nAt runtime, `foo_resources.pak` needs to be loaded (memory-mapped) before any of\nits resource gets used. Alternatives to do this are:\n\n1. (Simplest) Specify native resources (with native libraries if any exist) to\n  be automatically loaded on first call to `FooModule.getImpl()`. This behavior\n  is specified via `load_native_on_get_impl = true` in `foo_module_desc`.\n1. In Java code, call `FooModule.ensureNativeLoaded()`.\n1. In C++ code, use JNI to call `FooModule.ensureNativeLoaded()`. The code to do\n  this can be placed in a helper class, which can also have JNI calls to\n  `FooModule.isInstalled()` and `FooModule.installModule()`.\n\n#### Cautionary notes\n\nCompiling `foo_resources.pak` auto-generates `foo_resources.h`, which defines\ntextual resource IDs, e.g., `IDR_FOO_HTML`. C++ code then uses these IDs to get\nresource bytes. Unfortunately, this behavior is fragile: If `IDR_FOO_HTML` is\naccessed before the Foo DFM is (a) installed, or (b) loaded, then runtime error\nensues! Some mitigation strategies are as follows:\n\n* (Ideal) Access Foo DFM's native resources only from code in Foo DFM's native\n  libraries. So by the time that `IDR_FOO_HTML` is accessed, everything is\n  already in place! This isn't always possible; henceforth we assume that\n  `IDR_FOO_HTML` is accessed by code in the base DFM.\n* Before accessing IDR_FOO_HTML, ensure Foo DFM is installed and loaded. The\n  latter can use `FooModule.ensureNativeLoaded()` (needs to be called from\n  Browser thread).\n* Use inclusion of `foo_resources.h` to restrict availability of `IDR_FOO_HTML`.\n  Only C++ files dedicated to \"DFM-gated code\" (code that runs only when its DFM\n  is installed and loaded) should include `foo_resources.h`.\n\n#### Associating native resources with DFM\n\nHere are the main GN changes to specify PAK files and default loading behavior\nfor a DFM's native resources:\n\n```gn\nfoo_module_desc = {\n  ...\n  paks = [ \"$root_gen_dir/chrome/browser/foo/internal/foo_resourcess.pak\" ]\n  pak_deps = [ \"//chrome/browser/foo/internal:foo_paks\" ]\n  load_native_on_get_impl = true\n}\n```\n\nNote that `load_native_on_get_impl` specifies both native libraries and native\nresources.\n\n\n### Module install\n\nSo far, we have installed the Foo DFM as a true split (`-m foo` option on the\ninstall script). In production, however, we have to explicitly install the Foo\nDFM for users to get it. There are three install options: _on-demand_,\n_deferred_ and _conditional_.\n\n#### On-demand install\n\nOn-demand requesting a module will try to download and install the\nmodule as soon as possible regardless of whether the user is on a metered\nconnection or whether they have turned updates off in the Play Store app.\n\nYou can use the autogenerated module class to on-demand install the module like\nso:\n\n```java\nFooModule.install((success) -> {\n    if (success) {\n        FooModule.getImpl().bar();\n    }\n});\n```\n\n**Optionally**, you can show UI telling the user about the install flow. For\nthis, add a function like the one below. Note, it is possible\nto only show either one of the  install, failure and success UI or any\ncombination of the three.\n\n```java\npublic static void installModuleWithUi(\n        Tab tab, OnModuleInstallFinishedListener onFinishedListener) {\n    ModuleInstallUi ui =\n            new ModuleInstallUi(\n                    tab,\n                    R.string.foo_module_title,\n                    new ModuleInstallUi.FailureUiListener() {\n                        @Override\n                        public void onFailureUiResponse(retry) {\n                            if (retry) {\n                                installModuleWithUi(tab, onFinishedListener);\n                            } else {\n                                onFinishedListener.onFinished(false);\n                            }\n                        }\n                    });\n    // At the time of writing, shows toast informing user about install start.\n    ui.showInstallStartUi();\n    FooModule.install(\n            (success) -> {\n                if (!success) {\n                    // At the time of writing, shows infobar allowing user\n                    // to retry install.\n                    ui.showInstallFailureUi();\n                    return;\n                }\n                // At the time of writing, shows toast informing user about\n                // install success.\n                ui.showInstallSuccessUi();\n                onFinishedListener.onFinished(true);\n            });\n}\n```\n\nTo test on-demand install, \"fake-install\" the DFM. It's fake because\nthe DFM is not installed as a true split. Instead it will be emulated by play\ncore's `--local-testing` [mode][play-core-local-testing].\nFake-install and launch Chrome with the following command:\n\n```shell\n$ $OUTDIR/bin/monochrome_public_bundle install -f foo\n$ $OUTDIR/bin/monochrome_public_bundle launch\n```\n\nWhen running the install code, the Foo DFM module will be emulated.\nThis will be the case in production right after installing the module. Emulation\nwill last until Play Store has a chance to install your module as a true split.\nThis usually takes about a day. After it has been installed, it will be updated\natomically alongside Chrome. Always check that it is installed and available\nbefore invoking code within the DFM.\n\n*** note\n**Warning:** There are subtle differences between emulating a module and\ninstalling it as a true split. We therefore recommend that you always test both\ninstall methods.\n***\n\n*** note\nTo simplify development, the DevUI DFM (dev_ui) is installed by default, i.e.,\n`-m dev_ui` is implied by default. This is overridden by:\n* `--no-module dev_ui`, to test error from missing DevUI,\n* `-f dev_ui`, for fake module install.\n***\n\n#### Deferred install\n\nDeferred install means that the DFM is installed in the background when the\ndevice is on an unmetered connection and charging. The DFM will only be\navailable after Chrome restarts. When deferred installing a module it will\nnot be faked installed.\n\nTo defer install Foo do the following:\n\n```java\nFooModule.installDeferred();\n```\n\n#### Conditional install\n\nConditional install means the DFM will be installed automatically upon first\ninstalling or updating Chrome if the device supports a particular feature.\nConditional install is configured in the module's manifest. To install your\nmodule on all Daydream-ready devices for instance, your\n`//chrome/android/modules/foo/internal/java/AndroidManifest.xml` should look\nlike this:\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:dist=\"http://schemas.android.com/apk/distribution\"\n    featureSplit=\"foo\">\n\n    <dist:module\n      dist:instant=\"false\"\n      dist:title=\"@string/foo_module_title\">\n      <dist:fusing dist:include=\"true\" />\n      <dist:delivery>\n        <dist:install-time>\n          <dist:conditions>\n            <dist:device-feature\n              dist:name=\"android.hardware.vr.high_performance\" />\n          </dist:conditions>\n        </dist:install-time>\n        <!-- Allows on-demand or deferred install on non-Daydream-ready\n             devices. -->\n        <dist:on-demand />\n      </dist:delivery>\n    </dist:module>\n\n    <application />\n</manifest>\n```\n\nYou can also specify no conditions to have your module always installed.\nYou might want to do this in order to delay the performance implications\nof loading your module until its first use (true only on Android O+ where\n[android:isolatedSplits](https://developer.android.com/reference/android/R.attr#isolatedSplits)\nis supported. See [go/isolated-splits-dev-guide](http://go/isolated-splits-dev-guide)\n(googlers only).\n\n### chrome_public_apk and Integration Tests\n\nTo make the Foo feature available in the non-bundle `chrome_public_apk`\ntarget, add the `java` target to the template in\n`//chrome/android/chrome_public_apk_tmpl.gni` like so:\n\n```gn\n  # Add to where \"chrome_all_java\" is added:\n  if (!_is_bundle) {\n    deps += [ \"//chrome/browser/foo/internal:java\" ]\n  }\n}\n```\n\nYou may also have to add `java` as a dependency of\n`//chrome/android/javatests/chrome_test_java_org.chromium.chrome.browser.foo`\nif you want to call into Foo from test code.\n\n[play-core-local-testing]: https://developer.android.com/guide/playcore/feature-delivery/on-demand#local-testing\n"
  },
  {
    "path": "platforms/android/android_debugging_instructions",
    "title": "Android Debugging Instructions",
    "content": "# Android Debugging Instructions\nChrome on Android has java and c/c++ code. Each \"side\" have its own set of tools\nfor debugging. Here's some tips.\n\n[TOC]\n\n## Instructions for Google Employees\n\nSee also\n[go/clankium/06-debugging-clank](https://goto.google.com/clankium/06-debugging-clank).\n\n## Launching\nYou can run the app by using one of the wrappers.\n\n```shell\n# Installs, launches, and enters logcat.\nout/Default/bin/content_shell_apk run --args='--disable-fre' 'data:text/html;utf-8,<html>Hello World!</html>'\n# Launches without first installing. Does not show logcat.\nout/Default/bin/chrome_public_apk launch --args='--disable-fre' 'data:text/html;utf-8,<html>Hello World!</html>'\n```\n\n## Logging\n[Chromium logging from LOG(INFO)](https://chromium.googlesource.com/chromium/src/+/main/docs/android_logging.md)\netc., is directed to the Android logcat logging facility. You can filter the\nmessages, e.g. view chromium verbose logging, everything else at warning level\nwith:\n\n```shell\n# Shows a coloured & filtered logcat.\nout/Default/bin/chrome_public_apk logcat [-v]  # Use -v to show logs for other processes\n```\n\nIf this doesn't display the logs you're looking for, try `adb logcat` with your system `adb`\nor the one in `//third_party/android_sdk/`.\n\n### Warnings for Blink developers\n*   **Do not use fprintf or printf debugging!** This does not\n    redirect to adb logcat. Use `LOG(ERROR)` etc. instead.\n    See also the \"Get Blink code to output to the adb log\" section.\n\n*   Redirecting stdio to logcat, as documented\n    [here](https://developer.android.com/studio/command-line/logcat.html#viewingStd),\n    has a bad side-effect in that it breaks `adb_install.py`. See\n    [here for details](http://stackoverflow.com/questions/28539676/android-adb-fails-to-install-apk-to-nexus-5-on-windows-8-1).\n\n## Take a Screenshot\n```shell\nbuild/android/screenshot.py /tmp/screenshot.png\n```\n\n## Inspecting the View Hierarchy\nGenerate an [Android Studio](android_studio.md) project, and then use\n[Layout Inspector](https://developer.android.com/studio/debug/layout-inspector).\n\n## Debugging Java\nFor both apk and test targets, pass `--wait-for-java-debugger` to the wrapper\nscripts.\n\nExamples:\n\n```shell\n# Install, launch, and wait:\nout/Default/bin/chrome_public_apk run --wait-for-java-debugger\n\n# Launch, and have GPU process wait rather than Browser process:\nout/Default/bin/chrome_public_apk launch --wait-for-java-debugger --debug-process-name privileged_process0\n\n# Have Renderers wait:\nout/Default/bin/chrome_public_apk launch --args=\"--renderer-wait-for-java-debugger\"\n\n# Have tests wait:\nout/Default/bin/run_chrome_public_test_apk --wait-for-java-debugger\nout/Default/bin/run_chrome_junit_tests --wait-for-java-debugger  # Specify custom port via --debug-socket=9999\n```\n\n### Android Studio\n*   Open Android Studio ([instructions](android_studio.md))\n*   Click \"Run\"->\"Attach debugger to Android process\" (see\n[here](https://developer.android.com/studio/debug/index.html) for more).\n*   Click \"Run\"->\"Attach to Local Process...\" for Robolectric junit tests.\n    * If this fails, you likely need to follow [these instructions](https://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process).\n\n### Eclipse\n*   In Eclipse, make a debug configuration of type \"Remote Java Application\".\n    Choose a \"Name\" and set \"Port\" to `8700`.\n\n*   Make sure Eclipse Preferences > Run/Debug > Launching > \"Build (if required)\n    before launching\" is unchecked.\n\n*   Run Android Device Monitor:\n\n    ```shell\n    third_party/android_sdk/public/tools/monitor\n    ```\n\n*   Now select the process you want to debug in Device Monitor (the port column\n    should now mention 8700 or xxxx/8700).\n\n*   Run your debug configuration, and switch to the Debug perspective.\n\n## Debugging C/C++\nWhile the app is running, use the wrapper script's `lldb` command to enter into a\nlldb shell.\n\nWhen running with `lldb` attached, the app runs **extremely slowly**.\n\n```shell\n# Attaches to browser process.\nout/Default/bin/content_shell_apk lldb\nout/Default/bin/chrome_public_apk lldb\n\n# Attaches to gpu process.\nout/Default/bin/chrome_public_apk lldb --debug-process-name privileged_process0\n\n# Attach to other processes (\"chrome_public_apk ps\" to show pids).\nout/Default/bin/chrome_public_apk lldb --pid $PID\n```\n\n### Using Visual Studio Code\n\n**NOT WORKING**\n\nThis used to work with GDB, but the LLDB instructions have not been written. If\nyou would like to take this on, please use:\n[crbug/1266055](https://bugs.chromium.org/p/chromium/issues/detail?id=1266055).\n\n### Waiting for Debugger on Early Startup\n```shell\n# Install, launch, and wait:\nout/Default/bin/chrome_public_apk run --args=\"--wait-for-debugger\"\n# Launch, and have GPU process wait rather than Browser process:\nout/Default/bin/chrome_public_apk launch --args=\"--wait-for-debugger-children=gpu-process\"\n# Or for renderers:\nout/Default/bin/chrome_public_apk launch --args=\"--wait-for-debugger-children=renderer\"\n```\n\n#### With Command-line LLDB\nOnce attached, `lldb` will drop into a prompt. Set your breakpoints and run \"c\" to\ncontinue.\n\n## Symbolizing Crash Stacks and Tombstones (C++)\n\nIf a crash has generated a tombstone in your device, use:\n\n```shell\nbuild/android/tombstones.py --output-directory out/Default\n```\n\nIf you have a stack trace (from `adb logcat`) that needs to be symbolized, copy\nit into a text file and symbolize with the following command (run from\n`${CHROME_SRC}`):\n\n```shell\nthird_party/android_platform/development/scripts/stack --output-directory out/Default [tombstone file | dump file]\n```\n\n`stack` can also take its input from `stdin`:\n\n```shell\nadb logcat -d | third_party/android_platform/development/scripts/stack --output-directory out/Default\n```\n\nExample:\n\n```shell\nthird_party/android_platform/development/scripts/stack --output-directory out/Default ~/crashlogs/tombstone_07-build231.txt\n```\n\n## Deobfuscating Stack Traces (Java)\n\nYou will need the ProGuard mapping file that was generated when the application\nthat crashed was built. When building locally, these are found in:\n\n```shell\nout/Default/apks/ChromePublic.apk.mapping\netc.\n```\n\nWhen debugging a failing test on the build waterfall, you can find the mapping\nfile as follows:\n\n1. Open buildbot page for the failing build (e.g.,\n   https://ci.chromium.org/p/chrome/builders/ci/android-go-perf/1234).\n2. Open the swarming page for the failing shard (e.g., shard #3).\n3. Click on \"Isolated Inputs\" to locate the files the shard used to run the\n   test.\n4. Download the `.mapping` file for the APK used by the test (e.g.,\n   `ChromePublic.apk.mapping`). Note that you may need to use the\n   `tools/luci-go/isolated` to download the mapping file if it's too big. The\n   viewer will provide instructions for this.\n\n**Googlers Only**: For official build mapping files, see\n[go/chromejavadeobfuscation](https://goto.google.com/chromejavadeobfuscation).\n\nOnce you have a .mapping file:\n\n```shell\n# For a file:\nbuild/android/stacktrace/java_deobfuscate.py PROGUARD_MAPPING_FILE.mapping < FILE\n# For logcat:\nadb logcat | build/android/stacktrace/java_deobfuscate.py PROGUARD_MAPPING_FILE.mapping\n```\n\n## Get Blink code to output to the adb log\n\nIn your build environment:\n\n```shell\nadb root\nadb shell stop\nadb shell setprop log.redirect-stdio true\nadb shell start\n```\n\nIn the source itself, use `LOG(ERROR),` `LOG(INFO)`, etc. whenever you need to\noutput a message, and it will be automatically redirected to adb logcat.\nRunning `adb logcat chromium:E`, for example, will show all log lines from\n`LOG(ERROR)` (plus others that match \"chromium\").\n\n## Debug unit tests with LLDB\n\nTo run unit tests use the following command:\n\n```shell\nout/Debug/bin/run_test_name -f <test_filter_if_any> --wait-for-debugger -t 6000\n```\n\nThat command will cause the test process to wait until a debugger is attached.\n\nTo attach a debugger:\n\n```shell\nbuild/android/connect_lldb.sh --output-directory=out/Default --package-name=org.chromium.native_test\n```\n\n## Examine app data on a non-rooted device\n\nIf you're developing on a non-rooted device such as a retail phone, security restrictions\nwill prevent directly accessing the application's data. However, as long as the app is\nbuilt with debugging enabled, you can use `adb shell run-as PACKAGENAME` to execute\nshell commands using the app's authorization, roughly equivalent to `su $user`.\n\nNon-Play-Store builds with `is_official_build=false` will by default set\n`android:debuggable=\"true\"` in the app's manifest to allow debugging.\n\nFor exammple, for a Chromium build, run the following:\n\n```\nadb shell run-as org.chromium.chrome\n```\n\nIf successful, this will silently wait for input without printing anything.\nIt acts as a simple shell despite not showing the usual `$ ` shell prompt.\nJust type commands and press RETURN to execute them.\n\nThe starting directory is the app's user data directory where user preferences and other\nprofile data are stored.\n\n```\npwd\n/data/user/0/org.chromium.chrome\n\nfind -type f\n./files/rList\n./shared_prefs/org.chromium.chrome_preferences.xml\n```\n\nIf you need to access the app's application data directory, you need to look up the\nobfuscated installation path since you don't have read access to the */data/app/* directory.\nFor example:\n\n```\npm list packages -f org.chromium.chrome\npackage:/data/app/~~ybTygSP5u72F9GN-3TMKXA==/org.chromium.chrome-zYY5mcB7YgB5pa3vfS3CBQ==/base.apk=org.chromium.chrome\n\nls -l /data/app/~~ybTygSP5u72F9GN-3TMKXA==/org.chromium.chrome-zYY5mcB7YgB5pa3vfS3CBQ==/\ntotal 389079\n-rw-r--r-- 1 system system 369634375 2022-11-05 01:49 base.apk\ndrwxr-xr-x 3 system system      3452 2022-11-05 01:49 lib\n-rw-r--r-- 1 system system    786666 2022-11-05 01:49 split_cablev2_authenticator.apk\n-rw-r--r-- 1 system system  21258500 2022-11-05 01:49 split_chrome.apk\n-rw-r--r-- 1 system system   1298934 2022-11-05 01:49 split_config.en.apk\n-rw-r--r-- 1 system system    413913 2022-11-05 01:49 split_dev_ui.apk\n-rw-r--r-- 1 system system     12432 2022-11-05 01:49 split_weblayer.apk\n```\n"
  },
  {
    "path": "platforms/android/android_cast_build_instructions",
    "title": "Checking out and building Cast for Android",
    "content": "# Checking out and building Cast for Android\n\n**Note**: it is **not possible** to build a binary functionally\nequivalent to a Chromecast. This is to build a single-page content\nembedder with similar functionality to Cast products.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-android-cast](https://goto.google.com/building-android-cast) instead.\n\n[TOC]\n\n## System requirements\n\n* An x86-64 machine running Linux with at least 8GB of RAM. More than 16GB is\n  highly recommended.\n* At least 100GB of free disk space.\n* You must have Git and Python installed already.\n\nMost development is done on Ubuntu. Other distros may or may not work;\nsee the [Linux instructions](linux/build_instructions.md) for some suggestions.\n\nBuilding the Android client on Windows or Mac is not supported and doesn't work.\n\n## Install `depot_tools`\n\nClone the `depot_tools` repository:\n\n```shell\n$ git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\n```\n\nAdd `depot_tools` to the end of your PATH (you will probably want to put this\nin your `~/.bashrc` or `~/.zshrc`). Assuming you cloned `depot_tools`\nto `/path/to/depot_tools`:\n\n```shell\n$ export PATH=\"$PATH:/path/to/depot_tools\"\n```\n\n## Get the code\n\nCreate a `chromium` directory for the checkout and change to it (you can call\nthis whatever you like and put it wherever you like, as\nlong as the full path has no spaces):\n\n```shell\n$ mkdir ~/chromium && cd ~/chromium\n$ fetch --nohooks android\n```\n\nIf you don't want the full repo history, you can save a lot of time by\nadding the `--no-history` flag to `fetch`.\n\nExpect the command to take 30 minutes on even a fast connection, and many\nhours on slower ones.\n\nIf you've already installed the build dependencies on the machine (from another\ncheckout, for example), you can omit the `--nohooks` flag and `fetch`\nwill automatically execute `gclient runhooks` at the end.\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\n$ cd src\n```\n\n### Converting an existing Linux checkout\n\nIf you have an existing Linux checkout, you can add Android support by\nappending `target_os = ['android']` to your `.gclient` file (in the\ndirectory above `src`):\n\n```shell\n$ echo \"target_os = [ 'android' ]\" >> ../.gclient\n```\n\nThen run `gclient sync` to pull the new Android dependencies:\n\n```shell\n$ gclient sync\n```\n\n(This is the only difference between `fetch android` and `fetch chromium`.)\n\n### Install additional build dependencies\n\nOnce you have checked out the code, run\n\n```shell\n$ build/install-build-deps.sh --android\n```\n\nto get all of the dependencies you need to build on Linux, *plus* all of the\nAndroid-specific dependencies (you need some of the regular Linux dependencies\nbecause an Android build includes a bunch of the Linux tools and utilities).\n\n### Run the hooks\n\nOnce you've run `install-build-deps` at least once, you can now run the\nChromium-specific hooks, which will download additional binaries and other\nthings you might need:\n\n```shell\n$ gclient runhooks\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory which builds Chrome\nfor Android, run:\n\n```shell\n$ gn gen --args='target_os=\"android\" is_cast_android=true' out/Default\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build matching the current host\n  operating system and CPU.\n* For more info on GN, run `gn help` on the command line or read the\n  [quick start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n\nAlso be aware that some scripts (e.g. `tombstones.py`, `adb_gdb.py`)\nrequire you to set `CHROMIUM_OUTPUT_DIR=out/Default`.\n\n### Faster builds\n\nThis section contains some things you can change to speed up your builds,\nsorted so that the things that make the biggest difference are first.\n\n#### Use Reclient\n\n*** note\n**Warning:** If you are a Google employee, do not follow the instructions below.\nSee\n[go/building-android-chrome#initialize-remote-execution-distributed-builds](https://goto.google.com/building-android-chrome#initialize-remote-execution-distributed-builds)\ninstead.\n***\n\nChromium's build can be sped up significantly by using a remote execution system\ncompatible with [REAPI](https://github.com/bazelbuild/remote-apis). This allows\nyou to benefit from remote caching and executing many build actions in parallel\non a shared cluster of workers.\n\nTo use Reclient, follow the corresponding\n[Linux build instructions](linux/build_instructions.md#use-reclient).\n\n## Build cast\\_shell\\_apk\n\nBuild `cast_browser_apk` with Ninja using the command:\n\n```shell\n$ autoninja -C out/Default cast_browser_apk\n```\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.)\n\n## Installing and Running `cast_browser_apk` on a device\n\n### Plug in your Android device\n\nMake sure your Android device is plugged in via USB, and USB Debugging\nis enabled.\n\nTo enable USB Debugging:\n\n*   Navigate to Settings \\> About Phone \\> Build number\n*   Click 'Build number' 7 times\n*   Now navigate back to Settings \\> Developer Options\n*   Enable 'USB Debugging' and follow the prompts\n\nYou may also be prompted to allow access to your PC once your device is\nplugged in.\n\nYou can check if the device is connected by running:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb devices\n```\n\nWhich prints a list of connected devices. If not connected, try\nunplugging and reattaching your device.\n\n### Build the APK\n\n```shell\nautoninja -C out/Release cast_browser_apk\n```\n\nAnd deploy it to your Android device:\n\n```shell\nout/Default/bin/cast_browser_apk install\n# Or to install and run:\nout/Default/bin/cast_browser_apk run \"http://google.com\"\n```\n\nThe app will appear on the device as \"Chromium\".\n\n### Testing\n\nFor information on running tests, see\n[Android Test Instructions](testing/android_test_instructions.md).\n"
  },
  {
    "path": "platforms/android/android_build_instructions",
    "title": "Checking out and building Chromium for Android",
    "content": "# Checking out and building Chromium for Android\n\nThere are instructions for other platforms linked from the\n[get the code](get_the_code.md) page.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-android-chrome](https://goto.google.com/building-android-chrome)\ninstead.\n\n[TOC]\n\n## System requirements\n\n* An x86-64 machine running Linux with at least 8GB of RAM. More than 16GB is\n  highly recommended.\n* At least 100GB of free disk space.\n* You must have Git and Python installed already.\n\nMost development is done on Ubuntu. Other distros may or may not work;\nsee the [Linux instructions](linux/build_instructions.md) for some suggestions.\n\nBuilding the Android client on Windows or Mac is not supported and doesn't work.\n\n## Install depot\\_tools\n\nClone the `depot_tools` repository:\n\n```shell\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\n```\n\nAdd `depot_tools` to the end of your PATH (you will probably want to put this\nin your `~/.bashrc` or `~/.zshrc`). Assuming you cloned `depot_tools`\nto `/path/to/depot_tools`:\n\n```shell\nexport PATH=\"$PATH:/path/to/depot_tools\"\n```\n\n## Get the code\n\nCreate a `chromium` directory for the checkout and change to it (you can call\nthis whatever you like and put it wherever you like, as\nlong as the full path has no spaces):\n\n```shell\nmkdir ~/chromium && cd ~/chromium\nfetch --nohooks android\n```\n\nIf you don't want the full repo history, you can save a lot of time by\nadding the `--no-history` flag to `fetch`.\n\nExpect the command to take 30 minutes on even a fast connection, and many\nhours on slower ones.\n\nIf you've already installed the build dependencies on the machine (from another\ncheckout, for example), you can omit the `--nohooks` flag and `fetch`\nwill automatically execute `gclient runhooks` at the end.\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\ncd src\n```\n\n### Converting an existing Linux checkout\n\nIf you have an existing Linux checkout, you can add Android support by\nappending `target_os = ['linux', 'android']` to your `.gclient` file (in the\ndirectory above `src`):\n\n```shell\necho \"target_os = [ 'linux', 'android' ]\" >> ../.gclient\n```\n\nThen run `gclient sync` to pull the new Android dependencies:\n\n```shell\ngclient sync\n```\n\n(This is the only difference between `fetch android` and `fetch chromium`.)\n\n### Install additional build dependencies\n\nOnce you have checked out the code, run\n\n```shell\nbuild/install-build-deps.sh\n```\n\nto get all of the dependencies you need to build on Linux, *plus* all of the\nAndroid-specific dependencies (you need some of the regular Linux dependencies\nbecause an Android build includes a bunch of the Linux tools and utilities).\n\nNOTE: For 32-bit builds, the `--lib32` command line switch could be used.\nYou may run into issues where `gperf` or `pkgconf` don't get installed,\nwithout it. To remedy this, and potentially other missing packages, you will\nhave to install them manually using:\n\n```shell\nsudo apt-get install {missing_pkg}\n```\n\n### Run the hooks\n\nOnce you've run `install-build-deps` at least once, you can now run the\nChromium-specific hooks, which will download additional binaries and other\nthings you might need:\n\n```shell\ngclient runhooks\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory which builds Chrome\nfor Android, run `gn args out/Default` and edit the file to contain the\nfollowing arguments:\n\n```gn\ntarget_os = \"android\"\ntarget_cpu = \"arm64\"  # See \"Figuring out target_cpu\" below\nuse_remoteexec = true  # Enables distributed builds. See \"Faster Builds\".\nandroid_static_analysis = \"build_server\"  # Does static checks in background. See \"Faster Builds\".\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build.\n* For more info on GN, run `gn help` on the command line or read the\n  [quick start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n\nAlso be aware that some scripts (e.g. `tombstones.py`, `adb_gdb.py`)\nrequire you to set `CHROMIUM_OUTPUT_DIR=out/Default`.\n\n### Figuring out target\\_cpu\n\nThe value of\n[`target_cpu`](https://gn.googlesource.com/gn/+/main/docs/reference.md#var_target_cpu)\ndetermines what instruction set to use for native code. Given a device (or\nemulator), you can determine the correct instruction set with `adb shell getprop\nro.product.cpu.abi`:\n\n| `getprop ro.product.cpu.abi` output | `target_cpu` value |\n|-------------------------------------|--------------------|\n| `arm64-v8a`                         | `arm64`            |\n| `armeabi-v7a`                       | `arm`              |\n| `x86`                               | `x86`              |\n| `x86_64`                            | `x64`              |\n\n*** promo\n`arm` and `x86` may optionally be used instead of `arm64` and `x64` for\nnon-WebView targets. This is also allowed for Monochrome, but only when not set\nas the WebView provider.\n***\n\n## Build Chromium\n\nBuild Chromium with Ninja using the command:\n\n```shell\nautoninja -C out/Default chrome_public_apk\n```\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.)\n\nYou can get a list of all of the other build targets from GN by running `gn ls\nout/Default` from the command line. To compile one, pass the GN label to Ninja\nwith no preceding \"//\" (so, for `//chrome/test:unit_tests` use `autoninja -C\nout/Default chrome/test:unit_tests`).\n\n### Multiple Chrome Targets\n\nThe Google Play Store allows apps to send customized bundles (`.aab` files)\ndepending on the version of Android running on a device. Chrome uses this\nfeature to package optimized versions for different OS versions.\n\n1. `monochrome_public_bundle` (`MonochromePublic.aab`)\n   * `minSdkVersion=26` (Oreo).\n   * Contains both Chrome and WebView (to save disk space).\n2. `trichrome_chrome_bundle` (`TrichromeChrome.aab`)\n   * `minSdkVersion=29` (Android 10).\n   * Native code shared with WebView through a \"Static Shared Library APK\": `trichrome_library_apk`\n   * Corresponding WebView target: `trichrome_webview_bundle`\n3. `chrome_public_bundle` & `chrome_public_apk` (`ChromePublic.aab`, `ChromePublic.apk`)\n   * `minSdkVersion=26` (Oreo).\n   * Used for local development (to avoid building WebView).\n   * WebView packaged independently (`system_webview_bundle` / `system_webview_apk`).\n\n*** note\n**Notes:**\n* These instructions use `chrome_public_apk`, but any of the other targets can\n  be substituted.\n* For more about bundles, see [android_dynamic feature modules.md](android_dynamic_feature_modules.md).\n* For more about native library packaging & loading, see [android_native_libraries.md](android_native_libraries.md).\n* There are closed-source equivalents to these targets (for Googlers), which\n  are identical but link in some extra code.\n***\n\n## Updating your checkout\n\nTo update an existing checkout, you can run\n\n```shell\n$ git rebase-update\n$ gclient sync\n```\n\nThe first command updates the primary Chromium source repository and rebases\nany of your local branches on top of tip-of-tree (aka the Git branch\n`origin/main`). If you don't want to use this script, you can also just use\n`git pull` or other common Git commands to update the repo.\n\nThe second command syncs dependencies to the appropriate versions and re-runs\nhooks as needed.\n\n## Installing and Running Chromium on a device\n\n### Plug in your Android device\n\nMake sure your Android device is plugged in via USB, and USB Debugging\nis enabled.\n\nTo enable USB Debugging:\n\n*   Navigate to Settings \\> About Phone \\> Build number\n*   Click 'Build number' 7 times\n*   Now navigate back to Settings \\> Developer Options\n*   Enable 'USB Debugging' and follow the prompts\n\nYou may also be prompted to allow access to your PC once your device is\nplugged in.\n\nYou can check if the device is connected by running:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb devices\n```\n\nWhich prints a list of connected devices. If not connected, try\nunplugging and reattaching your device.\n\n### Enable apps from unknown sources\n\nAllow Android to run APKs that haven't been signed through the Play Store:\n\n*   Enable 'Unknown sources' under Settings \\> Security\n\nIn case that setting isn't present, it may be possible to configure it via\n`adb shell` instead:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb shell settings put global verifier_verify_adb_installs 0\n```\n\n### Build the full browser\n\n```shell\nautoninja -C out/Default chrome_public_apk\n```\n\nAnd deploy it to your Android device:\n\n```shell\nout/Default/bin/chrome_public_apk install\n```\n\nThe app will appear on the device as \"Chromium\".\n\n### Build Content shell\n\nWraps the content module (but not the /chrome embedder). See\n[https://www.chromium.org/developers/content-module](https://www.chromium.org/developers/content-module)\nfor details on the content module and content shell.\n\n```shell\nautoninja -C out/Default content_shell_apk\nout/Default/bin/content_shell_apk install\n```\n\nthis will build and install an Android apk under\n`out/Default/apks/ContentShell.apk`.\n\n### Build WebView\n\n[Android WebView](https://developer.android.com/reference/android/webkit/WebView.html)\nis a system framework component. Since Android KitKat, it is implemented using\nChromium code (based off the [content module](https://dev.chromium.org/developers/content-module)).\n\nIf you want to build the complete Android WebView framework component and test\nthe effect of your chromium changes in Android apps using WebView, you should\nfollow the [Android AOSP + chromium WebView\ninstructions](https://www.chromium.org/developers/how-tos/build-instructions-android-webview)\n\n### Running\n\nFor Content shell:\n\n```shell\nout/Default/bin/content_shell_apk launch [--args='--foo --bar'] http://example.com\n```\n\nFor Chrome public:\n\n```shell\nout/Default/bin/chrome_public_apk launch [--args='--foo --bar'] http://example.com\n```\n\n### Logging and debugging\n\nLogging is often the easiest way to understand code flow. In C++ you can print\nlog statements using the LOG macro. In Java, refer to\n[android_logging.md](android_logging.md).\n\nYou can see these log via `adb logcat`, or:\n\n```shell\nout/Default/bin/chrome_public_apk logcat\n```\n\nLogcat supports an additional feature of filtering and highlighting user-defined patterns. To use\nthis mechanism, define a shell variable: `CHROMIUM_LOGCAT_HIGHLIGHT` and assign your desired\npattern. The pattern will be used to search for any substring (ie. no need to prefix or suffix it\nwith `.*`), eg:\n\n```shell\nexport CHROMIUM_LOGCAT_HIGHLIGHT='(WARNING|cr_Child)'\nout/Default/bin/chrome_public_apk logcat\n# Highlights messages/tags containing WARNING and cr_Child strings.\n```\n\nNote: both _Message_ and _Tag_ portion of logcat are matched against the pattern.\n\nTo debug C++ code, use one of the following commands:\n\n```shell\nout/Default/bin/content_shell_apk gdb\nout/Default/bin/chrome_public_apk gdb\n```\n\nSee [Android Debugging Instructions](android_debugging_instructions.md)\nfor more on debugging, including how to debug Java code.\n\n### Testing\n\nFor information on running tests, see\n[Android Test Instructions](/docs/testing/android_test_instructions.md)\n\n## Faster Builds\n\n### Use Reclient\n\n*** note\n**Warning:** If you are a Google employee, do not follow the instructions below.\nSee\n[go/building-android-chrome#initialize-remote-execution-distributed-builds](https://goto.google.com/building-android-chrome#initialize-remote-execution-distributed-builds)\ninstead.\n***\n\nChromium's build can be sped up significantly by using a remote execution system\ncompatible with [REAPI](https://github.com/bazelbuild/remote-apis). This allows\nyou to benefit from remote caching and executing many build actions in parallel\non a shared cluster of workers.\n\nTo use Reclient, follow the corresponding\n[Linux build instructions](linux/build_instructions.md#use-reclient).\n\n### GN Args\n\nArgs that affect build speed:\n * `use_remoteexec = true` *(default=false)*\n   * What it does: Enables distributed builds via Reclient\n * `symbol_level = 0` *(default=1)*\n   * What it does: Disables debug information in native code.\n   * Use this when doing primarily Java development.\n   * To disable symbols only in Blink / V8: `blink_symbol_level = 0`, `v8_symbol_level = 0`\n * `is_component_build = true` *(default=`is_debug`)*\n   * What it does: Uses multiple `.so` files instead of just one (faster links)\n * `is_java_debug = true` *(default=`is_debug`)*\n   * What it does: Disables R8 (whole-program Java optimizer)\n * `treat_warnings_as_errors = false` *(default=`true`)*\n   * Causes any compiler warnings or lint checks to not fail the build.\n   * Allows you to iterate without needing to satisfy static analysis checks.\n * `android_static_analysis = \"build_server\"` *(default=`\"on\"`)*\n   * Offloads static analysis steps to the build server. Explained below.\n   * Set this to `\"off\"` if you want to turn off static analysis altogether.\n * `incremental_install = true` *(default=`false`)*\n   * Makes build and install quite a bit faster. Explained in a later section.\n * `enable_chrome_android_internal = false` *(Googlers only)*\n   * Disables non-public code, which exists even when building public targets.\n   * Use this is you do not need to test internal-only things.\n\n### Asynchronous Static Analysis\n\nNormally analysis build steps like Lint and Error Prone will run as normal build\nsteps. The build will then wait for all analysis steps to complete successfully.\nBy offloading analysis build steps to a separate build server to be run lazily at\na low priority, the actual build can complete much faster.\n\n**Note**: Since the build completes before the analysis checks finish, the build\nwill not fail if an analysis check fails.\n\nTo enable this mode, add the gn args:\n\n```gn\nandroid_static_analysis = \"build_server\"\n```\n\nCommand output will show up on the terminal that ran the build, as well as in\n`out/Debug/buildserver.log.0`.\n\nSee the status of the server at any time via:\n```\nbuild/android/fast_local_dev_server.py --print-status-all\n```\n\n### Incremental Install\n[Incremental Install](/build/android/incremental_install/README.md) uses\nreflection and sideloading to speed up the edit & deploy cycle (normally < 10\nseconds). The initial launch of the apk will be a lot slower on older Android\nversions (pre-N) where the OS needs to pre-optimize the side-loaded files, but\nthen be only marginally slower after the first launch.\n\nTo enable Incremental Install, add the gn args:\n\n```gn\nincremental_install = true\n```\n\nSome APKs (e.g. WebView) do not work with `incremental install = true` and are\nalways built as normal APKs. This behavior is controlled via\n`never_incremental = true`.\n\n## Installing and Running Chromium on an Emulator\n\nRunning on an emulator is the same as on a device. Refer to\n[android_emulator.md](android_emulator.md) for setting up emulators.\n\n## Tips, tricks, and troubleshooting\n\n### Rebuilding libchrome.so for a particular release\n\nThese instructions are only necessary for Chrome 51 and earlier.\n\nIn the case where you want to modify the native code for an existing\nrelease of Chrome for Android (v25+) you can do the following steps.\nNote that in order to get your changes into the official release, you'll\nneed to send your change for a codereview using the regular process for\ncommitting code to chromium.\n\n1.  Open Chrome on your Android device and visit chrome://version\n2.  Copy down the id listed next to \"Build ID:\"\n3.  Go to\n    [http://storage.googleapis.com/chrome-browser-components/BUILD\\_ID\\_FROM\\_STEP\\_2/index.html](http://storage.googleapis.com/chrome-browser-components/BUILD_ID_FROM_STEP_2/index.html)\n4.  Download the listed files and follow the steps in the README.\n\n### Building with Docker\n\nTo build Chromium for Android using Docker, please follow the\ninstructions in the [Docker in Linux build instructions](/docs/linux/build_instructions.md#docker).\n\n*** note\n**Note:** You need install the [Android dependencies](#install-additional-build-dependencies) after setting up the [Build dependencies](/docs/linux/build_instructions.md#install-additional-build-dependencies).\n***\n"
  },
  {
    "path": "platforms/android/android_accessing_cpp_switches_in_java",
    "title": "Accessing C++ Switches In Java",
    "content": "# Accessing C++ Switches In Java\n\n[TOC]\n\n## Introduction\n\nAccessing C++ switches in Java is implemented via a Python script which analyzes\nthe C++ switches file and generates the corresponding Java class, based on a\ntemplate file. The template file must be specified in the GN target.\n\n## Usage\n\n1. Create a template file (ex. `FooSwitches.java.tmpl`). Change \"Copyright\n   2020\" to be whatever the year is at the time of writing (as you would for any\n   other file).\n   ```java\n    // Copyright 2020 The Chromium Authors\n    // Use of this source code is governed by a BSD-style license that can be\n    // found in the LICENSE file.\n\n    package org.chromium.foo;\n\n    // Be sure to escape any curly braces in your template by doubling as\n    // follows.\n    /**\n     * Contains command line switches that are specific to the foo project.\n     */\n    public final class FooSwitches {{\n\n    {NATIVE_STRINGS}\n\n        // Prevents instantiation.\n        private FooSwitches() {{}}\n    }}\n   ```\n\n2. Add a new build target and add it to the `srcjar_deps` of an\n   `android_library` target:\n\n    ```gn\n    if (is_android) {\n      import(\"//build/config/android/rules.gni\")\n    }\n\n    if (is_android) {\n      java_cpp_strings(\"java_switches_srcjar\") {\n        # External code should depend on \":foo_java\" instead.\n        visibility = [ \":*\" ]\n        sources = [\n          \"//base/android/foo_switches.cc\",\n        ]\n        template = \"//base/android/java_templates/FooSwitches.java.tmpl\"\n      }\n\n      # If there's already an android_library target, you can add\n      # java_switches_srcjar to that target's srcjar_deps. Otherwise, the best\n      # practice is to create a new android_library just for this target.\n      android_library(\"foo_java\") {\n        srcjar_deps = [ \":java_switches_srcjar\" ]\n      }\n    }\n    ```\n\n3. The generated file `out/Default/gen/.../org/chromium/foo/FooSwitches.java`\n   would contain:\n\n    ```java\n    // Copyright $YEAR The Chromium Authors\n    // Use of this source code is governed by a BSD-style license that can be\n    // found in the LICENSE file.\n\n    package org.chromium.foo;\n\n    /**\n     * Contains command line switches that are specific to the foo project.\n     */\n    public final class FooSwitches {\n\n        // ...snip...\n\n        // This following string constants were inserted by\n        //     java_cpp_strings.py\n        // From\n        //     ../../base/android/foo_switches.cc\n        // Into\n        //     ../../base/android/java_templates/FooSwitches.java.tmpl\n\n        // Documentation for the C++ switch is copied here.\n        public static final String SOME_SWITCH = \"some-switch\";\n\n        // ...snip...\n\n        // Prevents instantiation.\n        private FooSwitches() {}\n    }\n    ```\n\n## See also\n* [Accessing C++ Enums In Java](android_accessing_cpp_enums_in_java.md)\n* [Accessing C++ Features In Java](android_accessing_cpp_features_in_java.md)\n\n## Code\n* [Generator\ncode](https://cs.chromium.org/chromium/src/build/android/gyp/java_cpp_strings.py?dr=C&sq=package:chromium)\nand\n[Tests](https://cs.chromium.org/chromium/src/build/android/gyp/java_cpp_strings_tests.py?dr=C&sq=package:chromium)\n* [GN\ntemplate](https://cs.chromium.org/chromium/src/build/config/android/rules.gni?sq=package:chromium&dr=C)\n"
  },
  {
    "path": "platforms/android/android_accessing_cpp_features_in_java",
    "title": "Accessing C++ Features In Java",
    "content": "# Accessing C++ Features In Java\n\n[TOC]\n\n# Checking if a Feature is enabled\n\nIn C++, add your `base::Feature` to an existing `base::android::FeatureMap` in the appropriate layer/component. Then, you can check the\nenabled state like so:\n\n```java\n// FooFeatureMap can check FooFeatures.MY_FEATURE as long as foo_feature_map.cc\n// adds `kMyFeature` to its `base::android::FeatureMap`.\nif (FooFeatureMap.getInstance().isEnabled(FooFeatures.MY_FEATURE)) {\n    // ...\n}\n```\n\nIf the components or layer does not have a FeatureMap, create a new one:\n\n1. In C++, create a new `foo_feature_map.cc` (ex.\n[`content_feature_map`](/content/browser/android/content_feature_map.cc)) with:\n    * `kFeaturesExposedToJava` array with a pointer to your `base::Feature`.\n    * `GetFeatureMap` with a static `base::android::FeatureMap` initialized\n      with `kFeaturesExposedToJava`.\n    * `JNI_FooFeatureList_GetNativeMap` simply calling `GetFeatureMap`.\n2. In Java, create a `FooFeatureMap.java` class extending `FeatureMap.java`\n   (ex. [`ContentFeatureMap`](/content/public/android/java/src/org/chromium/content/browser/ContentFeatureMap.java)) with:\n    * A `getInstance()` that returns the singleton instance.\n    * A single `long getNativeMap()` as @NativeMethods.\n    * An `@Override` for the abstract `getNativeMap()` simply calling\n      `FooFeatureMapJni.get().getNativeMap()`.\n3. Still in Java, `FooFeatures.java` with the String constants with the feature\n   names needs to be generated or created.\n    * Auto-generate it by writing a `FooFeatures.java.tmpl`. [See instructions\n      below]((#generating-foo-feature-list-java)).\n    * If `FooFeatures` cannot be auto-generated, keep the list of String\n      constants with the feature names in a `FooFeatures` or `FooFeatureList`\n      separate from the pure boilerplate `FooFeatureMap`.\n\n# Auto-generating FooFeatureList.java {#generating-foo-feature-list-java}\n\nAccessing C++ `base::Features` in Java is implemented via a Python script which\nanalyzes the `*_features.cc` file and generates the corresponding Java class,\nbased on a template file. The template file must be specified in the GN target.\nThis outputs Java String constants which represent the name of the\n`base::Feature`.\n\n## Usage\n\n1. Create a template file (ex. `FooFeatures.java.tmpl`). Change \"Copyright\n   2020\" to be whatever the year is at the time of writing (as you would for any\n   other file).\n   ```java\n    // Copyright 2020 The Chromium Authors\n    // Use of this source code is governed by a BSD-style license that can be\n    // found in the LICENSE file.\n\n    package org.chromium.foo;\n\n    // Be sure to escape any curly braces in your template by doubling as\n    // follows.\n    /**\n     * Contains features that are specific to the foo project.\n     */\n    public final class FooFeatures {{\n\n    {NATIVE_FEATURES}\n\n        // Prevents instantiation.\n        private FooFeatures() {{}}\n    }}\n   ```\n\n2. Add a new build target and add it to the `srcjar_deps` of an\n   `android_library` target:\n\n    ```gn\n    if (is_android) {\n      import(\"//build/config/android/rules.gni\")\n    }\n\n    if (is_android) {\n      java_cpp_features(\"java_features_srcjar\") {\n        # External code should depend on \":foo_java\" instead.\n        visibility = [ \":*\" ]\n        sources = [\n          \"//base/android/foo_features.cc\",\n        ]\n        template = \"//base/android/java_templates/FooFeatures.java.tmpl\"\n      }\n\n      # If there's already an android_library target, you can add\n      # java_features_srcjar to that target's srcjar_deps. Otherwise, the best\n      # practice is to create a new android_library just for this target.\n      android_library(\"foo_java\") {\n        srcjar_deps = [ \":java_features_srcjar\" ]\n      }\n    }\n    ```\n\n3. If you need to expose your flag in WebView, and you created a new\n   `android_library` in the previous step, then add a `deps` entry to\n   `common_java` in `//android_webview/BUILD.gn`.\n\n   If you don't need to expose a flag in WebView, then skip this and go to the\n   next step.\n\n   ```gn\n   android_library(\"common_java\") {\n     ...\n\n     deps = [\n       ...\n       \"//path/to:foo_java\",\n       ...\n     ]\n   }\n   ```\n\n4. The generated file `out/Default/gen/.../org/chromium/foo/FooFeatures.java`\n   would contain:\n\n    ```java\n    // Copyright $YEAR The Chromium Authors\n    // Use of this source code is governed by a BSD-style license that can be\n    // found in the LICENSE file.\n\n    package org.chromium.foo;\n\n    // Be sure to escape any curly braces in your template by doubling as\n    // follows.\n    /**\n     * Contains features that are specific to the foo project.\n     */\n    public final class FooFeatures {\n\n        // This following string constants were inserted by\n        //     java_cpp_features.py\n        // From\n        //     ../../base/android/foo_features.cc\n        // Into\n        //     ../../base/android/java_templates/FooFeatures.java.tmpl\n\n        // Documentation for the C++ Feature is copied here.\n        public static final String SOME_FEATURE = \"SomeFeature\";\n\n        // ...snip...\n\n        // Prevents instantiation.\n        private FooFeatures() {}\n    }\n    ```\n\n### Troubleshooting\n\nThe script only supports limited syntaxes for declaring C++ base::Features. You\nmay see an error like the following during compilation:\n\n```\n...\norg/chromium/foo/FooFeatures.java:41: error: duplicate declaration of field: MY_FEATURE\n    public static final String MY_FEATURE = \"MyFeature\";\n```\n\nThis can happen if you've re-declared a feature for mutually-exclusive build\nconfigs (ex. the feature is enabled-by-default for one config, but\ndisabled-by-default for another). Example:\n\n```c++\n#if defined(...)\nBASE_FEATURE(kMyFeature, \"MyFeature\", base::FEATURE_ENABLED_BY_DEFAULT);\n#else\nBASE_FEATURE(kMyFeature, \"MyFeature\", base::FEATURE_DISABLED_BY_DEFAULT);\n#endif\n```\n\nThe `java_cpp_features` rule doesn't know how to evaluate C++ preprocessor\ndirectives, so it generates two identical Java fields (which is what the\ncompilation error is complaining about). Fortunately, the workaround is fairly\nsimple. Rewrite the definition to only use directives around the enabled state:\n\n```c++\nBASE_FEATURE(kMyFeature,\n             \"MyFeature\",\n#if defined(...)\n             base::FEATURE_ENABLED_BY_DEFAULT\n#else\n             base::FEATURE_DISABLED_BY_DEFAULT\n#endif\n};\n\n```\n\n\n## See also\n* [Accessing C++ Enums In Java](android_accessing_cpp_enums_in_java.md)\n* [Accessing C++ Switches In Java](android_accessing_cpp_switches_in_java.md)\n\n## Code\n* [Generator code](/build/android/gyp/java_cpp_features.py) and\n  [Tests](/build/android/gyp/java_cpp_features_tests.py)\n* [GN template](/build/config/android/rules.gni)\n"
  },
  {
    "path": "platforms/android/android_accessing_cpp_enums_in_java",
    "title": "Accessing C++ Enums In Java",
    "content": "# Accessing C++ Enums In Java\n\n[TOC]\n\n## Introduction\n\nAccessing C++ enums in Java is implemented via a Python script which analyzes\nthe C++ enum and spits out the corresponding Java class. The enum needs to be\nannotated in a particular way. By default, the generated class name will be the\nsame as the name of the enum. If all the names of the enum values are prefixed\nwith the MACRO\\_CASED\\_ name of the enum those prefixes will be stripped from\nthe Java version.\n\n## Features\n* Customize the package name of the generated class using the\n`GENERATED_JAVA_ENUM_PACKAGE` directive (required)\n* Customize the class name using the `GENERATED_JAVA_CLASS_NAME_OVERRIDE`\ndirective (optional)\n* Strip enum entry prefixes to make the generated classes less verbose using\nthe `GENERATED_JAVA_PREFIX_TO_STRIP` directive (optional)\n* Follows best practices by using\n[IntDef Instead of Enum](/styleguide/java/java.md#IntDef-Instead-of-Enum)\n* Generate the `flag` attribute using the `GENERATED_JAVA_IS_FLAG` directive (optional)\n* Copies comments that directly precede enum entries into the generated Java\nclass\n\n## Usage\n\n1. Add directives to your C++ enum. Only the `GENERATED_JAVA_ENUM_PACKAGE`\n   directive is required:\n\n    ```cpp\n    // GENERATED_JAVA_ENUM_PACKAGE: org.chromium.chrome\n    // GENERATED_JAVA_CLASS_NAME_OVERRIDE: FooBar\n    // GENERATED_JAVA_PREFIX_TO_STRIP: BAR_\n    // GENERATED_JAVA_IS_FLAG: true\n    enum SomeEnum {\n      BAR_A = 1 << 0,\n      BAR_B = 1 << 1,\n      BAR_C = BAR_B,\n    };\n    ```\n\n2. Add a new build target and add it to the `srcjar_deps` of an\n   `android_library` target:\n\n    ```gn\n    if (is_android) {\n      import(\"//build/config/android/rules.gni\")\n    }\n\n    if (is_android) {\n      java_cpp_enum(\"java_enum_srcjar\") {\n        # External code should depend on \":foo_java\" instead.\n        visibility = [ \":*\" ]\n        sources = [\n          # Include the .h or .cc file(s) which defines the enum(s).\n          \"base/android/native_foo_header.h\",\n        ]\n      }\n\n      # If there's already an android_library target, you can add\n      # java_enum_srcjar to that target's srcjar_deps. Otherwise, the best\n      # practice is to create a new android_library just for this target.\n      android_library(\"foo_java\") {\n        srcjar_deps = [ \":java_enum_srcjar\" ]\n\n        # Important: the generated enum uses the @IntDef annotation provided by\n        # this dependency.\n        deps = [ \"//third_party/androidx:androidx_annotation_annotation_java\" ]\n      }\n    }\n    ```\n\n3. The generated file `org/chromium/chrome/FooBar.java` would contain:\n\n    ```java\n    package org.chromium.chrome;\n\n    import androidx.annotation.IntDef;\n\n    import java.lang.annotation.Retention;\n    import java.lang.annotation.RetentionPolicy;\n\n    @IntDef(flag = true, value = {\n        FooBar.A, FooBar.B, FooBar.C\n    })\n    @Retention(RetentionPolicy.SOURCE)\n    public @interface FooBar {\n      int A = 1 << 0;\n      int B = 1 << 1;\n      int C = 1 << 1;\n    }\n    ```\n\n## Formatting Notes\n\n* Handling long package names:\n\n    ```cpp\n    // GENERATED_JAVA_ENUM_PACKAGE: (\n    //   org.chromium.chrome.this.package.is.too.long.to.fit.on.a.single.line)\n    ```\n\n* Enum entries\n    * Single line enums should look like this:\n\n        ```cpp\n        // GENERATED_JAVA_ENUM_PACKAGE: org.foo\n        enum NotificationActionType { BUTTON, TEXT };\n        ```\n\n    * Multi-line enums should have one enum entry per line, like this:\n\n        ```cpp\n        // GENERATED_JAVA_ENUM_PACKAGE: org.foo\n        enum NotificationActionType {\n          BUTTON,\n          TEXT\n        };\n        ```\n\n    * Multi-line enum entries are allowed but should be formatted like this:\n\n        ```cpp\n        // GENERATED_JAVA_ENUM_PACKAGE: org.foo\n        enum NotificationActionType {\n          LongKeyNumberOne,\n          LongKeyNumberTwo,\n          ...\n          LongKeyNumberThree =\n              LongKeyNumberOne | LongKeyNumberTwo | ...\n        };\n        ```\n\n* Preserving comments\n\n    ```cpp\n    // GENERATED_JAVA_ENUM_PACKAGE: org.chromium\n    enum CommentEnum {\n      // This comment will be preserved.\n      ONE,\n      TWO, // This comment will NOT be preserved.\n      THREE\n    }\n    ```\n\n    ```java\n    ...\n    public @interface CommentEnum {\n      ...\n      /**\n       * This comment will be preserved.\n       */\n      int ONE = 0;\n      int TWO = 1;\n      int THREE = 2;\n    }\n    ```\n\n## Troubleshooting\n\n### Symbol not found/could not resolve IntDef\n\nYou may see an error like this when compiling:\n\n```shell\n$ autoninja -C out/Default base/foo_java\nutil.build_utils.CalledProcessError: Command failed: ...\norg/chromium/chrome/FooBar.java:13: error: symbol not found androidx.annotation.IntDef\nHint: Add \"//third_party/androidx:androidx_annotation_annotation_java\" to deps of //base/foo_java\nimport androidx.annotation.IntDef;\n       ^\norg/chromium/chrome/FooBar.java:18: error: could not resolve IntDef\n@IntDef({\n^\n```\n\nThe fix is to add\n`\"//third_party/androidx:androidx_annotation_annotation_java\"` to the `deps` of\nthe `android_library`. Note: **do not** add this to the `java_cpp_enum` target\nby mistake, otherwise you'll see a new error:\n\n```shell\n$ autoninja -C out/Default base/foo_java\n[0/1] Regenerating ninja files\nERROR at //base/BUILD.gn:194:12: Assignment had no effect.\n    deps = [ \"//third_party/androidx:androidx_annotation_annotation_java\" ]\n           ^--------------------------------------------------------------\nYou set the variable \"deps\" here and it was unused before it went\nout of scope.\n...\n```\n\n## See also\n* [Accessing C++ Switches In Java](android_accessing_cpp_switches_in_java.md)\n* [Accessing C++ Features In Java](android_accessing_cpp_features_in_java.md)\n\n## Code\n* [Generator\ncode](https://cs.chromium.org/chromium/src/build/android/gyp/java_cpp_enum.py?dr=C&sq=package:chromium)\nand\n[Tests](https://cs.chromium.org/chromium/src/build/android/gyp/java_cpp_enum_tests.py?dr=C&q=java_cpp_enum_tests&sq=package:chromium&l=1)\n* [GN\ntemplate](https://cs.chromium.org/chromium/src/build/config/android/rules.gni?q=java_cpp_enum.py&sq=package:chromium&dr=C&l=458)\n"
  },
  {
    "path": "performance/README",
    "title": "Performance Documentation",
    "content": "# Performance Documentation\r\n\r\nThis section contains documentation for performance optimization, profiling, and performance analysis in Chromium.\r\n\r\n## Profiling & Analysis\r\n\r\n- [Profiling](profiling.md) - General profiling techniques and tools\r\n- [Profiling Content Shell on Android](profiling_content_shell_on_android.md) - Android-specific profiling\r\n\r\n## Optimization Techniques\r\n\r\n- [Profile-Guided Optimization (PGO)](pgo.md) - Using PGO for performance improvements\r\n- [Order Files](orderfile.md) - Code layout optimization for improved performance\r\n\r\n## Performance Monitoring\r\n\r\nPerformance monitoring in Chromium involves:\r\n\r\n### Key Metrics\r\n- **Startup Time**: Time from process launch to first meaningful paint\r\n- **Memory Usage**: Peak and sustained memory consumption\r\n- **CPU Usage**: Processing efficiency across different workloads\r\n- **Graphics Performance**: Frame rates and rendering efficiency\r\n\r\n### Profiling Tools\r\n- **Chrome DevTools**: Built-in performance profiling\r\n- **Systrace/Perfetto**: System-level tracing on Android\r\n- **ETW**: Event Tracing for Windows\r\n- **Instruments**: macOS profiling tool\r\n- **perf**: Linux performance analysis\r\n\r\n### Optimization Strategies\r\n- **Code Layout**: Using order files for better cache locality\r\n- **Profile-Guided Optimization**: Compiler optimizations based on runtime profiles\r\n- **Memory Management**: Efficient allocation and deallocation patterns\r\n- **Threading**: Proper task distribution across threads\r\n\r\n## Performance Best Practices\r\n\r\n1. **Measure First**: Always profile before optimizing\r\n2. **Target Bottlenecks**: Focus on the most impactful optimizations\r\n3. **Consider All Platforms**: Performance characteristics vary by platform\r\n4. **Validate Changes**: Ensure optimizations don't introduce regressions\r\n\r\n## Related Documentation\r\n\r\n- [Architecture Overview](../architecture/overview.md)\r\n- [Threading and Tasks](../architecture/threading_and_tasks.md)\r\n- [Development Tools](../development/)\r\n- [Platform-Specific Docs](../platforms/)\r\n"
  },
  {
    "path": "performance/profiling_content_shell_on_android",
    "title": "Profiling Content Shell on Android",
    "content": "# Profiling Content Shell on Android\n\nBelow are the instructions for setting up profiling for Content Shell on\nAndroid. This will let you generate profiles for ContentShell. This will require\nlinux, building an userdebug Android build, and wiping the device.\n\n[TOC]\n\n## Prepare your device.\n\nYou need an Android 4.2+ device (Galaxy Nexus, Nexus 4, 7, 10, etc.) which you\ndon’t mind erasing all data, rooting, and installing a userdebug build on.\n\n## Get and build `content_shell_apk` for Android\n\nMore detailed insturctions in [android_build_instructions.md](android_build_instructions.md).\n\n```shell\nninja -C out/Release content_shell_apk\n```\n\n## Setup the physical device\n\nPlug in your device. Make sure you can talk to your device, try:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb shell ls\n```\n\n## Root your device and install a userdebug build\n\n1.  This may require building your own version of Android:\n    http://source.android.com/source/building-devices.html\n1.  A build that works is: `manta / android-4.2.2_r1` or\n    `master / full_manta-userdebug`.\n\n## Root your device\n\n1.  Run `adb root`. Every time you connect your device you’ll want to run this.\n1.  If adb is not available, make sure to run `. build/android/envsetup.sh`\n\nIf you get the error `error: device offline`, you may need to become a developer\non your device before Linux will see it. On Jellybean 4.2.1 and above this\nrequires going to “about phone” or “about tablet” and clicking the build number\n7 times:\nhttp://androidmuscle.com/how-to-enable-usb-debugging-developer-options-on-nexus-4-and-android-4-2-devices/\n\n## Enable profiling\n\nRebuild `content_shell_apk` with profiling enabled.\n\nWith GN:\n\n    gn args out/Profiling\n    # add \"enable_profiling = true\"\n    ninja -C out/Profiling content_shell_apk\n    export CHROMIUM_OUTPUT_DIR=\"$PWD/out/Profiling\"\n\n## Run a Telemetry perf profiler\n\nYou can run any Telemetry benchmark with `--profiler=perf`, and it will:\n\n1.  Download `perf` and `perfhost`\n2.  Install on your device\n3.  Run the test\n4.  Setup symlinks to work with the `--symfs` parameter\n\nYou can also run \"manual\" tests with Telemetry, more information here:\nhttps://www.chromium.org/developers/telemetry/profiling#TOC-Manual-Profiling---Android\n\nThe following steps describe building `perf`, which is no longer necessary if\nyou use Telemetry.\n\n## Use `adb_profile_chrome`\n\nEven if you're not running a Telemetry test, you can use Catapult to\nautomatically push binaries and pull the profile data for you.\n\n    build/android/adb_profile_chrome --browser=content_shell --perf\n\nWhile you still have to build, install and launch the APK yourself, Catapult\nwill take care of creating the symfs etc. (i.e. you can skip the \"not needed for\nTelemetry\" steps below).\n\n## Install `/system/bin/perf` on your device (not needed for Telemetry)\n\n    # From inside the Android source tree (not inside Chromium)\n    mmm external/linux-tools-perf/\n    adb remount # (allows you to write to the system image)\n    adb sync\n    adb shell perf top # check that perf can get samples (don’t expect symbols)\n\n## Install and Run ContentShell\n\nInstall with the following:\n\n    out/Release/bin/content_shell_apk run\n\nIf `content_shell` “stopped unexpectedly” use `adb logcat` to debug.\n\n## Setup a `symbols` directory with symbols from your build (not needed for Telemetry)\n\n1.  Figure out exactly what path `content_shell_apk` (or chrome, etc) installs\n    to.\n    *   On the device, navigate ContentShell to about:crash\n\n\n    adb logcat | grep libcontent_shell_content_view.so\n\nYou should find a path that’s something like\n`/data/app-lib/org.chromium.content_shell-1/libcontent_shell_content_view.so`\n\n1.  Make a symbols directory\n    ```\n    mkdir symbols (this guide assumes you put this next to src/)\n    ```\n1.  Make a symlink from your symbols directory to your un-stripped\n    `content_shell`.\n\n    ```\n    # Use whatever path in app-lib you got above\n    mkdir -p symbols/data/app-lib/org.chromium.content_shell-1\n    ln -s `pwd`/src/out/Release/lib/libcontent_shell_content_view.so \\\n        `pwd`/symbols/data/app-lib/org.chromium.content_shell-1\n    ```\n\n## Install `perfhost_linux` locally (not needed for Telemetry)\n\nNote: modern versions of perf may also be able to process the perf.data files\nfrom the device.\n\n1.  `perfhost_linux` can be built from:\n    https://android.googlesource.com/platform/external/linux-tools-perf/.\n1.  Place `perfhost_linux` next to symbols, src, etc.\n\n    chmod a+x perfhost_linux\n\n## Actually record a profile on the device!\n\nRun the following:\n\n    out/Release/content_shell_apk ps (look for the pid of the sandboxed_process)\n    adb shell perf record -g -p 12345 sleep 5\n    adb pull /data/perf.data\n\n## Create the report\n\n1.  Run the following:\n\n    ```\n    ./perfhost_linux report -g -i perf.data --symfs symbols/\n    ```\n\n1.  If you don’t see chromium/webkit symbols, make sure that you built/pushed\n    Release, and that the symlink you created to the .so is valid!\n\n## Add symbols for the kernel\n\n1.  By default, /proc/kallsyms returns 0 for all symbols, to fix this, set\n    `/proc/sys/kernel/kptr_restrict` to `0`:\n\n    ```\n    adb shell echo “0” > /proc/sys/kernel/kptr_restrict\n    ```\n\n1.  See http://lwn.net/Articles/420403/ for explanation of what this does.\n\n    ```\n    adb pull /proc/kallsyms symbols/kallsyms\n    ```\n\n1.  Now add --kallsyms to your perfhost\\_linux command:\n    ```\n    ./perfhost_linux report -g -i perf.data --symfs symbols/ \\\n        --kallsyms=symbols/kallsyms\n    ```\n"
  },
  {
    "path": "performance/profiling",
    "title": "CPU Profiling Chrome",
    "content": "# CPU Profiling Chrome\n\n\n[TOC]\n\n## Introduction\n\nThese are instructions for collecting a CPU profile of chromium. All of the profiling methods described here produce output that can be view using the `pprof` tool. `pprof` is highly customizable; here's a screenshot of some example `pprof` output:\n\n![pprof output screenshot](./media/profile-screenshot.png)\n\nThis doc is intended to be an authoritative one-stop resource for profiling chromium. At the time of writing, there are a number of existing docs with profiling instructions, in varying states of obsolescence:\n\n* [./linux/profiling.md](./linux/profiling.md)\n* [./profiling_content_shell_on_android.md](./profiling_content_shell_on_android.md)\n* https://www.chromium.org/developers/profiling-chromium-and-webkit\n* https://www.chromium.org/developers/telemetry/profiling\n\n***promo\nCPU profiling is not to be confused with tracing or task profiling:\n\n* https://www.chromium.org/developers/how-tos/trace-event-profiling-tool\n* https://www.chromium.org/developers/threaded-task-tracking\n***\n\n# Profiling on Linux\n\n## General checkout setup\nProfiling should preferably be done on an official build. Make sure the following appears in your `args.gn` file:\n\n    is_official_build = true\n\n## Profiling a process or thread for a defined period of time using perf\n\nFirst, make sure you have the `linux-perf` package installed:\n\n    $ sudo apt-get install linux-perf\n\nAfter starting up the browser and loading the page you want to profile, press 'Shift-Escape' to bring up the task manager, and get the Process ID of the process you want to profile.\n\nRun the perf tool like this:\n\n    $ perf record -g -p <Process ID> -o <output file>\n\n*** promo\nTo adjust the sampling frequency, use the `-F` argument, e.g., `-F 1000`.\n***\n*** promo\nIf this fails to collect any samples on a Cloudtop/VM (presumably while profiling tests),\ntry adding `-e cpu-clock`.\n***\n\nTo stop profiling, press `Control-c` in the terminal window where `perf` is running. Run `pprof` to view the results, providing the path to the browser executable; e.g.:\n\n    $ pprof -web src/out/Release/chrome <perf output file>\n\n*** promo\n`pprof` is packed with useful features for visualizing profiling data. Try `pprof --help` for more info.\n***\n\n*** promo\nTip for Googlers: running `gcert` first will make `pprof` run faster, and eliminate some useless spew to the terminal.\n***\n\nIf you want to profile all renderer processes use the custom `--renderer-cmd-prefix` profiling script:\n\n  $ src/out/Release/chrome --renderer-cmd-prefix=\"tools/profiling/linux-perf-renderer-cmd.sh\"\n\nIf you want to limit the profile to a single thread, run:\n\n    $ ps -T -p <Process ID>\n\nFrom the output, find the Thread ID (column header \"SPID\") of the thread you want. Now run perf:\n\n    $ perf record -g -t <Thread ID> -o <output file>\n\nUse the same `pprof` command as above to view the single-thread results.\n\n## Profiling the renderer process for a period defined in javascript\n\nYou can generate a highly-focused profile for any period that can be defined in javascript using the `chrome.gpuBenchmarking` javascript interface. First, adding the following command-line flags when you start chrome:\n\n    $ chrome --enable-gpu-benchmarking --no-sandbox [...]\n\nOpen devtools, and in the console, use `chrome.gpuBenchmarking.startProfiling` and `chrome.gpuBenchmarking.stopProfiling` to define a profiling period. e.g.:\n\n    > chrome.gpuBenchmarking.startProfiling('perf.data'); doSomething(); chrome.gpuBenchmarking.stopProfiling()\n\n`chrome.gpuBenchmarking` has a number of useful methods for simulating user-gesture-initiated actions; for example, to profile scrolling:\n\n    > chrome.gpuBenchmarking.startProfiling('perf.data'); chrome.gpuBenchmarking.smoothScrollByXY(0, 1000, () => { chrome.gpuBenchmarking.stopProfiling() });\n\n## Profiling content_shell with callgrind\n\nThis section contains instructions on how to do profiling using the callgrind/cachegrind tools provided by valgrind. This is not a sampling profiler, but a profiler based on running on a simulated CPU. The instructions are Linux-centered, but might work on other platforms too.\n\n#### Install valgrind\n\n```\nsudo apt-get install valgrind\n```\n\n#### Profile\n\nRun `content_shell` with callgrind to create a profile. A `callgrind.<pid>` file will be dumped when exiting the browser or stopped with CTRL-C:\n\n```\nvalgrind --tool=callgrind content_shell --single-process --no-sandbox <url>\n```\n\nAlternatively use cachegrind which will give you CPU cycles per code line:\n\n```\nvalgrind --tool=cachegrind content_shell --single-process --no-sandbox <url>\n```\n\nUsing single-process is for simple profiling of the renderer. It should be possible to run in multi-process and attach to a renderer process.\n\n#### Install KCachegrind\n\nWarning: this will install a bunch of KDE dependencies.\n\n```\nsudo apt-get install kcachegrind\n```\n\n#### Explore with KCachegrind\n\n```\nkcachegrind callgrind.<pid>\n```\n\n# Profiling on Android\n\nAndroid (Nougat and later) supports profiling using the [simpleperf](https://developer.android.com/ndk/guides/simpleperf) tool.\n\nFollow the [instructions](./android_build_instructions.md) for building and installing chromium on android. With chromium running on the device, run the following command to start profiling on the browser process (assuming your build is in `src/out/Release`):\n\n    $ src/out/Release/bin/chrome_public_apk profile\n    Profiler is running; press Enter to stop...\n\nOnce you stop the profiler, the profiling data will be copied off the device to the host machine and post-processed so it can be viewed in `pprof`, as described above.\n\nTo profile the renderer process, you must have just one tab open in chromium, and use a command like this:\n\n    $ src/out/Release/bin/chrome_public_apk profile --profile-process=renderer\n\nTo limit the profile to a single thread, use a command like this:\n\n    $ src/out/Release/bin/chrome_public_apk profile --profile-process=renderer --profile-thread=main\n\nThe `--profile-process` and `--profile-thread` arguments support most of the common process names ('browser', 'gpu', 'renderer') and thread names ('main', 'io', 'compositor', etc.). However, if you need finer control of the process and/or thread to profile, you can specify an explicit Process ID or Thread ID. Check out the usage message for more info:\n\n    $ src/out/Release/bin/chrome_public_apk help profile\n\n\nBy default, simpleperf will collect CPU cycles. To collect other events, use a command like this:\n\n    $ src/out/Release/bin/chrome_public_apk profile --profile-process=renderer --profile-events cpu-cycles,branch-misses,branch-instructions,cache-references,cache-misses,stalled-cycles-frontend,stalled-cycles-backend\n\n# Profiling on ChromeOS\n\nFollow the [simple chrome instructions](https://chromium.googlesource.com/chromiumos/docs/+/HEAD/simple_chrome_workflow.md), to build\nand deploy chrome to your chromeos device.  These instructions will set up a\nbuild directory for you, so be sure to `gn args out_${SDK_BOARD}/Release` to\nedit them and add the gn args listed above.\n\nThe easiest way to get a profile is to ssh to your device, which here will\nbe referred to as `chromeos-box`, but replace that with whatever ip or hostname\nyour device is.  ssh to your device, create a folder in `/tmp` (which usually\nhas more space than `/`) and record performance for the entire device.  When\nyou're done, use scp to copy the perf.data back to your desk and use pprof\nas per normal on that perf.data file.\n\nHere's an example:\n\n    $ ssh root@chromeos-box\n    localhost ~ # export CPUPROFILE_FREQUENCY=3000\n    localhost ~ # mkdir -p /tmp/perf\n    localhost ~ # cd /tmp/perf\n    localhost /tmp/perf # perf record -g -a -e cycles\n    ^C\n    [ perf record: Woken up 402 times to write data ]\n    [ perf record: Captured and wrote 100.797 MB perf.data (489478 samples) ]\n    localhost /tmp/perf # exit\n    $ scp root@chromeos-box:/tmp/perf/perf.data .\n    $ pprof -web out_${SDK_BOARD}/Release/chrome perf.data\n\nNote: this will complain about missing chromeos symbols.  Even pointing\nPPROF\\_BINARY\\_PATH at the expanded `debug-board.tgz` file that came along with\nthe chromeos image does not seem to work.  If you can make this work, please\nupdate this doc!\n\n# Profiling during a perf benchmark run\n\nThe perf benchmark runner can generate a CPU profile over the course of running a perf test. Currently, this is supported only on Linux and Android. To get info about the relevant options, run:\n\n    $ src/tools/perf/run_benchmark help run\n\n... and look for the `--interval-profiling-*` options. For example, to generate a profile of the main thread of the renderer process during the \"page interactions\" phase of a perf benchmark, you might run:\n\n    $ src/tools/perf/run_benchmark run <benchmark name> --interval-profiling-target=renderer:main --interval-profiling-period=interactions --interval-profiling-frequency=2000\n\nThe profiling data will be written into the `artifacts/` sub-directory of your perf benchmark output directory (default is `src/tools/perf`), to files with the naming pattern `*.profile.pb`. You can use `pprof` to view the results, as described above.\n\n# Googlers Only\n\nIf you use `pprof -proto chrome-profile-renderer-12345` to turn your perf data\ninto a proto file, you can then use that resulting file with internal tools.\nSee [http://go/cprof/user#fs-profiles](http://go/cprof/user#fs-profiles])\nfor instructions on how to go about this.\n\n# macOS\n\n## General tricks\n\n### Using PIDs in commands\n\nMany of the profiling tools expect you to provide the PID of the process to profile. If the tool used does not support finding the application by name or you would like to run the command for many processes it can be useful to use `pgrep` to find the PIDs.\n\nFind the PID for Chromium (browser process):\n\n    $ pgrep -X Chromium\nFind the PID for all child processes of Chromium:\n\n    $ pgrep -P $CHROMIUM_PID\nCombine commands to run tool for Chromium and all its children:\n\n    $ cat <(pgrep -x Chromium) <(pgrep -P $(pgrep -x Chromium)) | xargs $MY_TOOL --pid\n\n## Checkout setup\nProfiling should always be done on a build that represents the performance of official builds as much as possible. `is_official_build` enables some additional optimizations like PGO.\n\n    is_debug = false\n    is_component_build = false\n    is_official_build = true\n\n    # Most profiling techniques on macOS will work with minimal symbols for local builds.\n    # You should try and use minimal symbols when starting out because most tools will take\n    # an incredibly long time to process the symbols and in some cases will freeze the application\n    # while doing so. symbol_level sets the level for all parts of Chromium. The\n    # blink and v8 settings allow overriding this to set higher or lower levels\n    # for those components.\n    blink_symbol_level = 0\n    v8_symbol_level = 0\n    symbol_level = 0\n\n## Viewing traces.\nOnce collected the traces produced by any tool in this section can be converted to pprof using [InstrumentsToPprof](https://github.com/google/instrumentsToPprof#instrumentstopprof).\n\n## Tools\n\n### Sample\n#### Pros\n* Ships with macOS.\n* Traces can be symbolized after capturing.\n#### Cons\n* Has substantial observer impact and can interfere with the application, especially while loading symbols.\n* Does not differentiate between idle and active stacks so filtering is needed. Also obscures CPU impact of functions that sleep.\n\n#### Usage\nSample stacks of $pid for 10 seconds grabbing a stack every 1ms. [-maydie] to still have stacks if process exits.\n\n    $ sample $pid 10 1 -mayDie -f ./output.txt\n\n### Instruments\n#### Pros\n* Ships with macOS.\n* Can produce much more than sampling profiles via different modes.\n* Is low overhead.\n* Only captures cpu-active stacks (In Time Profiler mode) so no idle stack filtering is needed.\n#### Cons\n* Cannot produce human-readable reports fully automatically. (Requires use of GUI)\n* Built-in trace viewer is quite underpowered.\n\n#### Usage\nTo get a trace use either the GUI in the \"Time Profiler\" mode or this command:\n\n    $ xcrun -r xctrace record --template 'Time Profiler' --all-processes --time-limit 30s --output 'profile.trace'\n\n### DTrace\n#### Pros\n* Ships with macOS.\n* Can produce much more than sampling profiles via different probes.\n* Supports scripting.\n* Is low overhead.\n* Only captures cpu-active stacks so no idle stack filtering is needed.\n* Can be used fully from the command-line / script.\n#### Cons\n* Requires partially disabling SIP\n\n#### SIP\nBy default `dtrace` does not work well with [SIP](https://support.apple.com/en-us/HT204899). Disabling SIP as a whole is not recommended and instead should be done only for DTrace using these steps:\n\n* Reboot in recovery mode\n* Start a shell\n* Execute `csrutil enable --without dtrace --without debug`\n* Reboot\n\n#### Usage\nTo get sampled cpu stacks\n\n    $ dtrace -p $PID -o $OUTPUT_FILE -n \"profile-1001/pid == $PID/ {{ @[ustack()] = count(); }}\"\n\nTo get stacks that caused wake-ups\n\n    $ dtrace -p $PID -o $OUTPUT_FILE -n \"mach_kernel::wakeup/pid == $PID/ {{ @[ustack()] = count(); }}\"\n"
  },
  {
    "path": "performance/pgo",
    "title": "Profile-Guided Optimization (PGO)",
    "content": "# Profile-Guided Optimization (PGO)\n\n## Generating PGO Profiles via Bots\n\nSee [go/chrome-pgo-internal] (Googlers only).\n\n[go/chrome-pgo-internal]: https://goto.google.com/chrome-pgo-internal\n\n## Generating PGO Profiles Manually\n\nNormally devs don't need to worry about this and can use the default profile\nfor official builds.  The default profile can be fetched by adding\n`\"checkout_pgo_profiles\": True` to `custom_vars` in the gclient config and\nrunning `gclient runhooks`.\n\nTo produce an executable built with a custom PGO profile:\n\n* Produce the instrumented executable using the following gn args:\n\n  ```\n  chrome_pgo_phase = 1\n  enable_resource_allowlist_generation = false\n  is_official_build = true\n  symbol_level = 0\n  use_remoteexec = true\n  ```\n\n  For android you need these in addition:\n  ```\n  target_os = \"android\"\n  target_cpu = \"arm64\"\n  ```\n\n* Run representative benchmarks to produce profiles\n\n  `python3 tools/pgo/generate_profile.py -C out/builddir`\n\n  If collecting profiles on an android device, add a browser name like one of\n  [these][browser_names]:\n\n  ```\n  python3 tools/pgo/generate_profile.py -C out/builddir \\\n      --android-browser android-trichrome-chrome-google-bundle\n  ```\n\n  You can find available browsers using:\n\n  ```\n  tools/perf/run_benchmark run --browser=list\n  ```\n\n  By default, some benchmark replay archives require special access permissions.\n  For more details and to request access, please refer to [Telemetry\n  documentation][telemetry_docs].  You can also choose to run\n  `generate_profile.py` without these benchmarks, using the\n  `--run-public-benchmarks-only` flag. However, note that doing so may produce a\n  profile that isn't fully representative.\n\n   ```\n  python3 tools/pgo/generate_profile.py -C out/builddir \\\n      --android-browser android-trichrome-chrome-google-bundle \\\n      --run-public-benchmarks-only\n  ```\n\n  If `generate_profile.py` fails with `ServiceException: 401 Anonymous caller\n  does not have storage.objects.get access to the Google Cloud Storage object.`,\n  then run `download_from_google_storage --config` (with your @google address;\n  enter 0 as project-id).\n\n  This will produce `out/builddir/profile.profdata`\n\n* Produce the final PGO'd executable with the following gn args (and additional\n  android args, if any):\n\n  ```\n  enable_resource_allowlist_generation = false\n  is_official_build = true\n  symbol_level = 0\n  use_remoteexec = true\n  pgo_data_path = \"//out/builddir/profile.profdata\"\n  ```\n\n[browser_names]: https://source.chromium.org/chromium/chromium/src/+/main:third_party/catapult/telemetry/telemetry/internal/backends/android_browser_backend_settings.py;l=400;drc=bf85e76dc3467385a623e9bf11ab950cf2889ca5\n[telemetry_docs]: https://www.chromium.org/developers/telemetry/upload_to_cloud_storage/#request-access-for-google-partners\n\n## How It Works\n\n`chrome_pgo_phase` is defined in [`build/config/compiler/pgo/pgo.gni`][pgo_gni].\nThis GN variable can be one of 0, 1, or 2, meaning \"don't use profile\",\n\"generating profile\", and \"use profile\" respectively. See [pgo.gni][pgo_gni] for\ndetails on platform-specific GN variables that determine which phase is used in\neach build.\n\nWhich file under `//chrome/build/pgo_profiles/` gets used? It depends on both\nthe platform and [`_pgo_target`][pgo_target]. For example, for 64-bit android,\nthe file `//chrome/build/android-arm64.pgo.txt` contains the name of the\n`*.profdata` file that is used as the PGO profile by default if no other profile\nis specified via the GN arg `pgo_data_path`.\n\n[pgo_gni]: https://source.chromium.org/chromium/chromium/src/+/main:build/config/compiler/pgo/pgo.gni\n[pgo_target]: https://source.chromium.org/chromium/chromium/src/+/main:build/config/compiler/pgo/BUILD.gn;l=88;drc=3d2e089ad74a30754376571531e00615de96061e\n\n## Background Reading\n\nhttps://clang.llvm.org/docs/UsersManual.html#profile-guided-optimization\n\nhttps://source.android.com/docs/core/perf/pgo\n"
  },
  {
    "path": "performance/overview",
    "title": "Performance & Optimization Overview",
    "content": "# Performance & Optimization Overview\r\n\r\nPerformance is critical to Chromium's success. This section covers the tools, techniques, and methodologies used to analyze, optimize, and maintain Chromium's performance across different platforms and use cases.\r\n\r\n## 🎯 What You'll Learn\r\n\r\n- **Performance Analysis**: How to identify bottlenecks and performance issues\r\n- **Optimization Techniques**: Advanced strategies for improving performance\r\n- **Profiling Tools**: Using profilers to understand runtime behavior\r\n- **Platform-Specific Optimization**: Optimizing for different operating systems\r\n\r\n## 📊 Performance Areas\r\n\r\n### Core Performance Topics\r\n- [Profiling Techniques](profiling) - Understanding runtime behavior and bottlenecks\r\n- [Profile Guided Optimization (PGO)](pgo) - Using runtime data to improve compilation\r\n- [Order File Optimization](orderfile) - Optimizing binary layout for better performance\r\n\r\n### Platform-Specific Performance\r\n- [Profiling Content Shell on Android](profiling_content_shell_on_android) - Mobile performance analysis\r\n\r\n## 🛠️ Performance Tools\r\n\r\n### **Profiling Tools**\r\n- **CPU Profilers**: Understand where time is spent\r\n- **Memory Profilers**: Analyze memory usage patterns  \r\n- **Network Profilers**: Optimize network performance\r\n- **Graphics Profilers**: Analyze rendering performance\r\n\r\n### **Optimization Techniques**\r\n- **Code Optimization**: Compiler optimizations and code improvements\r\n- **Memory Optimization**: Reducing memory footprint and allocation overhead\r\n- **Cache Optimization**: Improving data locality and cache performance\r\n- **Network Optimization**: Reducing latency and bandwidth usage\r\n\r\n### **Measurement & Analysis**\r\n- **Benchmarking**: Establishing performance baselines\r\n- **A/B Testing**: Measuring performance impact of changes\r\n- **Continuous Monitoring**: Tracking performance over time\r\n- **Regression Detection**: Identifying performance regressions\r\n\r\n## 📈 Performance Metrics\r\n\r\n### **Key Performance Indicators**\r\n- **Startup Time**: How quickly Chromium launches\r\n- **Page Load Time**: How fast web pages load\r\n- **Memory Usage**: RAM and storage consumption\r\n- **Battery Life**: Power efficiency on mobile devices\r\n- **Frame Rate**: Smoothness of animations and scrolling\r\n\r\n### **Measurement Strategies**\r\n- **Synthetic Benchmarks**: Controlled performance tests\r\n- **Real-World Metrics**: User experience measurements\r\n- **Lab Testing**: Controlled environment testing\r\n- **Field Testing**: Real-world usage data\r\n\r\n## 🚀 Getting Started\r\n\r\n1. **Begin Here**: [Profiling Techniques](profiling) - Learn the fundamentals\r\n2. **Deep Dive**: [Profile Guided Optimization](pgo) - Advanced optimization\r\n3. **Platform Focus**: Choose platform-specific guides based on your target\r\n4. **Advanced**: [Order File Optimization](orderfile) - Binary-level optimization\r\n\r\n## 🎯 Performance Best Practices\r\n\r\n### **Development Practices**\r\n- ✅ Profile before optimizing\r\n- ✅ Measure the impact of changes\r\n- ✅ Consider all platforms and use cases\r\n- ✅ Balance performance with maintainability\r\n- ✅ Use appropriate data structures and algorithms\r\n\r\n### **Common Pitfalls to Avoid**\r\n- ❌ Premature optimization\r\n- ❌ Optimizing without measuring\r\n- ❌ Ignoring memory implications\r\n- ❌ Platform-specific assumptions\r\n- ❌ Breaking functionality for performance\r\n\r\n## 🔬 Performance Testing\r\n\r\nPerformance testing in Chromium involves:\r\n- **Automated Benchmarks**: Continuous performance monitoring\r\n- **Manual Testing**: Human evaluation of performance\r\n- **Stress Testing**: Performance under extreme conditions\r\n- **Regression Testing**: Ensuring changes don't hurt performance\r\n\r\n## 🔗 Related Sections\r\n\r\n- [🧪 Testing & QA](../development/testing/testing_in_chromium) - Performance testing strategies\r\n- [🛠️ Development Workflow](../development/clang) - Tools for performance development\r\n- [🖥️ Platform-Specific](../platforms/android) - Platform optimization guides\r\n- [🏗️ Core Architecture](../architecture/overview) - Understanding performance implications of design\r\n"
  },
  {
    "path": "performance/orderfile",
    "title": "Orderfile",
    "content": "# Orderfile\n\nAn orderfile is a list of symbols that defines an ordering of functions. One can\nmake a static linker, such as LLD, respect this ordering when generating a\nbinary.\n\nReordering code this way can improve startup performance by fetching machine\ncode to memory more efficiently, since it requires fetching fewer pages from\ndisk, and a big part of the I/O work is done sequentially by the readahead.\n\nCode reordering can also improve memory usage by keeping the used code in a\nsmaller number of memory pages. It can also reduce TLB and L1i cache misses by\nplacing functions commonly called together closely in memory.\n\n## Generating Orderfiles Manually\n\nTo generate an orderfile you can run the `orderfile_generator_backend.py`\nscript. You will need an Android device connected with\n[adb](https://developer.android.com/tools/adb) to generate the orderfile as the\ngeneration pipeline will need to run benchmarks on a device.\n\nExample:\n```\ntools/cygprofile/orderfile_generator_backend.py --target-arch=arm64 --use-remoteexec\n```\n\nYou can specify the architecture (arm or arm64) with `--target-arch`. For quick\nlocal testing you can use `--streamline-for-debugging`. To build using Reclient,\nuse `--use-remoteexec` (Googlers only). There are several other options you can\nuse to configure/debug the orderfile generation. Use the `-h` option to view the\nvarious options.\n\nNB: If your checkout is non-internal you must use the `--public` option.\n\nTo build Chrome with a locally generated orderfile, use the\n`chrome_orderfile_path=<path_to_orderfile>` GN arg.\n\n## Orderfile Performance Testing\n\nOrderfiles can be tested using\n[Pinpoint](https://chromium.googlesource.com/chromium/src/+/main/docs/speed/perf_trybots.md).\nTo do this, please create and upload a Gerrit change overriding the value of\n[`chrome_orderfile_path`](https://source.chromium.org/chromium/chromium/src/+/main:build/config/compiler/BUILD.gn;l=217-223;drc=3a829695d83990141babd25dee7f2f94c005cae4)\nto, for instance, `//path/to/my_orderfile` (relative to `src`), where\n`my_orderfile` is the orderfile that needs to be evaluated. The orderfile should\nbe added to the local branch and uploaded to Gerrit along with\n`build/config/compiler/BUILD.gn`. This Gerrit change can then be used as an\n\"experiment patch\" for a Pinpoint try job.\n\n## Triaging Performance Regressions\n\nOccasionally, an orderfile roll will cause performance problems on perfbots.\nThis typically triggers an alert in the form of a bug report, which contains a\ngroup of related regressions like the one shown\n[here](https://crbug.com/344654892).\n\nIn such cases it is important to keep in mind that effectiveness of the\norderfile is coupled with using a recent PGO profile when building the native\ncode. As a result some orderfile improvements (or effective no-ops) register as\nregressions on perfbots using non-PGO builds, which is the most common perfbot\nconfiguration.\n\nIf a new regression does not include alerts from the\n[android-pixel6-perf-pgo](https://ci.chromium.org/ui/p/chrome/builders/luci.chrome.ci/android-pixel6-perf-pgo)\n(the only Android PGO perfbot as of 2024-06) then the first thing to check is to\nquery the same benchmark+metric combinations for the PGO bot. If the graphs\ndemonstrate no regression, feel free to close the issue as WontFix(Intended\nBehavior). However, not all benchmarks are exercised on the PGO bot\ncontinuously. If there is no PGO coverage for a particular benchmark+metric\ncombination, this combination can be checked on Pinpoint with the right perfbot\nchoice ([example](https://crbug.com/344665295)).\n\nFinally, the PGO+orderfile coupling exists only on arm64. Most speed\noptimization efforts on Android are focused on this configuration. On arm32 the\nmost important orderfile optimization is for reducing memory used by machine\ncode. Only one benchmark measures it: `system_health.memory_mobile`.\n\n## Orderfile Pipeline\n\nThe `orderfile_generator_backend.py` script runs several key steps:\n\n1. **Build and install Chrome with orderfile instrumentation.** This uses the\n[`-finstrument-function-entry-bare`](https://clang.llvm.org/docs/ClangCommandLineReference.html#cmdoption-clang-finstrument-function-entry-bare)\nClang command line option to insert instrumentation for function entry. The\nbuild will be generated in `out/arm_instrumented_out/` or\n`out/arm64_instrumented_out`, depending on the CPU architecture (instruction\nset).\n\n2. **Run the benchmarks and collect profiles.** These benchmarks can be found\nin [orderfile.py](../tools/perf/contrib/orderfile/orderfile.py). These profiles\nare a list of function offsets into the binary that were called during execution\nof the benchmarks.\n\n3. **Cluster the symbols from the profiles to generate the orderfile.**\nThe offsets are processed and merged using a\n[clustering](../tools/cygprofile/cluster.py) algorithm to produce an orderfile.\n\n4. **Run benchmarks on the final orderfile.** We run some benchmarks to compare\nthe performance with/without the orderfile. You can supply the `--no-benchmark`\nflag to skip this step.\n"
  },
  {
    "path": "modules/storage-cache",
    "title": "Storage & Cache",
    "content": "# Storage & Cache\r\n\r\nEfficient storage and caching are critical to Chromium’s performance and user experience. This article covers both the low-level HTTP cache and higher-level web storage APIs, how they interact, and where to look in the source for each.\r\n\r\n---\r\n\r\n## 1. Overview\r\n\r\n- **Scope**  \r\n  - **HTTP Cache**: response caching for network requests  \r\n  - **In-Memory Cache**: fast lookup for small resources  \r\n  - **Web Storage**: LocalStorage, SessionStorage, IndexedDB, Cache API  \r\n  - **Quota & Storage Partitions**: per-origin isolation and limits  \r\n- **Goals**  \r\n  - **Performance**: avoid unnecessary network trips, disk I/O  \r\n  - **Correctness**: respect cache validation (ETags, freshness)  \r\n  - **Security**: isolate origin data, enforce quotas  \r\n\r\n---\r\n\r\n## 2. HTTP Cache (`net/disk_cache/` & `net/http/http_cache_*`)\r\n\r\n### 2.1 Architecture\r\n\r\n```text\r\nRenderer        Browser        Network Service\r\n   │               │                  │\r\n   └─URLLoader───▶│                  │\r\n           │      └──▶HttpCache───▶HttpNetworkTransaction──▶Socket\r\nHttpCache sits in between URLRequest and HttpNetworkTransaction.\r\n\r\nDisk Cache (LRU) implemented in net/disk_cache/simple/simple_entry_impl.cc.\r\n\r\nMemory Cache keeps small objects in RAM (SimpleBackend, MemBackendImpl).\r\n\r\n2.2 Key Files\r\nnet/http/http_cache.cc / .h\r\n\r\nnet/disk_cache/simple/simple_backend_impl.cc\r\n\r\nnet/disk_cache/backend_factory.h\r\n\r\n2.3 Eviction & Validation\r\nLRU eviction when size limit (default ~300 MB) is reached.\r\n\r\nCache-Control headers drive freshness; revalidate with ETag/Last-Modified.\r\n\r\nConditional Requests: 304 responses update stored entry metadata.\r\n\r\n3. Quota & Storage Partitions\r\n3.1 Storage Partitioning\r\nEach BrowserContext (Profile) has one or more StoragePartitions.\r\n\r\nPartitions isolate data by origin and by mode (e.g. default, incognito).\r\n\r\nDefined in content/browser/storage_partition_impl.cc.\r\n\r\n3.2 Quota Management\r\nQuotaManager (content/browser/quota/quota_manager_impl.cc)\r\n\r\nTracks usage per origin.\r\n\r\nEnforces soft/hard limits (default ~6 GB per origin on desktop).\r\n\r\nQuotaClient interfaces for each storage type (IndexedDB, FileSystem, Cache API).\r\n\r\n4. Web Storage APIs\r\n4.1 LocalStorage & SessionStorage\r\nLocalStorage stored in SQLite under Local Storage/ directory.\r\n\r\nSessionStorage tied to single top-level browsing context.\r\n\r\nCode lives in content/browser/dom_storage/ and content/renderer/dom_storage/.\r\n\r\n4.2 IndexedDB\r\nHigh-level object store, transactional.\r\n\r\nBacked by LevelDB in third_party/blink/renderer/modules/indexeddb/.\r\n\r\nQuota interactions via IndexedDBQuotaClient.\r\n\r\n4.3 Cache API (Service Workers)\r\nProgrammatic cache of Request/Response pairs.\r\n\r\nImplemented in service_worker/ under cache_storage/.\r\n\r\nUses the same DiskCache backend under the hood.\r\n\r\n5. Interaction with Network Stack\r\nCache-Control Overrides\r\n\r\nHTTP fetch in Service Workers can bypass the HTTP cache.\r\n\r\nStale-while-revalidate\r\n\r\nCustomizable via headers and Cache API strategies.\r\n\r\n6. Debugging & Instrumentation\r\nchrome://cache (legacy) or chrome://net-export for HTTP cache traces.\r\n\r\nchrome://quota-internals shows per-origin usage and limits.\r\n\r\nchrome://indexeddb-internals inspects IndexedDB databases.\r\n\r\nLogging flags:\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/chrome --enable-logging=stderr --v=1 --log-net-log=netlog.json\r\n7. Testing & Tools\r\nUnit tests in net/disk_cache/ and content/browser/quota/.\r\n\r\nnet/tools/quic_client/ can test cache behavior over HTTP/3.\r\n\r\nUse storage_browsertest (in content/test/) to automate web storage scenarios.\r\n\r\n8. Best Practices & Extensions\r\nCache only what you can revalidate: avoid caching sensitive data.\r\n\r\nClean up on unload for SessionStorage in embedded contexts.\r\n\r\nCustom URLRequestJob to layer additional caching logic:\r\n\r\ncpp\r\nCopy\r\nEdit\r\nURLRequestFilter::GetInstance()->AddUrlInterceptor(\r\n    \"https://example.com/\", std::make_unique<MyCacheInterceptor>());\r\n9. Next Steps\r\nDive into Security → Security Model to see how origin isolation is enforced.\r\n\r\nExplore Modules → Networking (HTTP) to understand how HTTP caching hooks in.\r\n\r\nRead Debugging → Debugging Tools for end-to-end cache debugging patterns."
  },
  {
    "path": "modules/overview",
    "title": "Modules & Components",
    "content": "# Modules & Components\r\n\r\nWelcome to the Modules section! This area provides detailed documentation about the core modules and components that make up the Wanderlust custom Chromium browser.\r\n\r\n## What You'll Find Here\r\n\r\nThis section covers the major functional modules of our Chromium implementation:\r\n\r\n- **[JavaScript V8 Engine](javascript-v8.md)**: V8 JavaScript engine integration and customizations\r\n- **[Networking & HTTP](networking-http.md)**: Network stack, HTTP handling, and protocol implementations\r\n- **[Storage & Cache](storage-cache.md)**: Storage systems, caching mechanisms, and data persistence\r\n- **[Storage Cache Details](storage-cache/)**: In-depth storage cache implementation details\r\n\r\n## Module Architecture\r\n\r\nOur custom Chromium browser is built with a modular architecture where each module handles specific functionality:\r\n\r\n### Core Engine Modules\r\n- **V8 JavaScript Engine**: Handles JavaScript execution, optimization, and runtime management\r\n- **Networking Stack**: Manages all network communications, HTTP/HTTPS protocols, and connection handling\r\n- **Storage Systems**: Provides data persistence, caching, and storage management\r\n\r\n### Module Interactions\r\n\r\nEach module is designed to work independently while maintaining clean interfaces with other components:\r\n\r\n- **JavaScript ↔ Networking**: Script-initiated network requests and responses\r\n- **Networking ↔ Storage**: Caching network resources and managing cached content\r\n- **Storage ↔ JavaScript**: Persistent storage APIs accessible from web content\r\n\r\n## Understanding Module Documentation\r\n\r\nEach module documentation includes:\r\n- **Purpose and Responsibilities**: What the module does and why it exists\r\n- **API Interfaces**: Public interfaces and integration points\r\n- **Implementation Details**: Internal architecture and design decisions\r\n- **Configuration Options**: Customizable settings and parameters\r\n- **Performance Considerations**: Optimization strategies and performance tips\r\n\r\n## Development Guidelines\r\n\r\nWhen working with modules:\r\n\r\n1. **Understand Dependencies**: Review how modules interact with each other\r\n2. **Follow Interfaces**: Use established APIs and avoid direct internal access\r\n3. **Consider Performance**: Each module affects overall browser performance\r\n4. **Maintain Compatibility**: Ensure changes don't break other modules\r\n\r\n## Integration with Browser Architecture\r\n\r\nThese modules integrate with the broader browser architecture covered in:\r\n- [Architecture Overview](../architecture/overview.md): System-level design and process model\r\n- [Security Model](../security/overview.md): Security boundaries and sandboxing\r\n- [Debugging Tools](../debugging/overview.md): Module-specific debugging techniques\r\n\r\n## Module-Specific Resources\r\n\r\n- **JavaScript V8**: Engine internals, optimization, and script execution\r\n- **Networking**: Protocol handling, connection management, and network security\r\n- **Storage**: File systems, databases, caching strategies, and data management\r\n\r\n---\r\n\r\n*Explore individual modules to understand their specific implementations, or start with the [JavaScript V8 documentation](javascript-v8.md) for core engine functionality.*\r\n"
  },
  {
    "path": "modules/networking-http",
    "title": "Networking (HTTP)",
    "content": "# Networking (HTTP)\r\n\r\nChromium’s HTTP stack underpins all web communication. In this article we’ll trace an HTTP request from the browser down to the network, cover caching, QUIC, and show you where to hook in or inspect traffic.\r\n\r\n---\r\n\r\n## 1. Overview\r\n\r\n- **Components**  \r\n  - **Network Service** (`services/network/`)  \r\n  - **URLLoader / URLRequest** abstractions  \r\n  - **HTTP Cache** (`net/http/http_cache_*.cc`)  \r\n  - **QUIC & HTTP/2** via `net/quic/` and `net/spdy/`  \r\n- **Goals**  \r\n  - Performance: multiplexing, caching, prioritization  \r\n  - Security: TLS validation, safe headers  \r\n  - Flexibility: pluggable protocols, proxy support\r\n\r\n---\r\n\r\n## 2. The Network Service\r\n\r\nChromium runs its network stack in a separate process by default:\r\n\r\n```text\r\nBrowser Process  ←───Mojo───→  Network Service Process\r\nEntrypoint: services/network/network_service.cc\r\n\r\nIPC Interface: network_service.mojom\r\n\r\nManages global resources (DNS cache, socket pools, proxy config).\r\n\r\n3. URLLoader / URLRequest Lifecycle\r\nURLLoaderFactory\r\n\r\nCreated in the browser, sent over Mojo to Renderers.\r\n\r\nRenderer calls CreateLoaderAndStart().\r\n\r\nURLLoader\r\n\r\nImplements network::mojom::URLLoader.\r\n\r\nWraps a URLRequest (non-Mojo) or directly uses UrlLoader on non-NetworkService builds.\r\n\r\nURLRequest (net/url_request/url_request.cc)\r\n\r\nCore state machine: redirect handling, auth, retries.\r\n\r\nDelegates to HttpStreamFactory for transport.\r\n\r\ntext\r\nCopy\r\nEdit\r\nRenderer → URLLoader → URLRequest → HttpNetworkTransaction → Socket\r\n4. HTTP Transport\r\nHttpNetworkTransaction (net/http/http_network_transaction.cc)\r\n\r\nSerializes headers and body, parses responses.\r\n\r\nHonors HttpRequestHeaders, HttpResponseHeaders.\r\n\r\nConnection Pool (net/http/http_stream_factory.cc)\r\n\r\nReuses idle connections (keep-alive).\r\n\r\nCategorizes by host:port, proxy, SSL.\r\n\r\n5. HTTP/2 & QUIC\r\nChromium supports modern protocols for speed:\r\n\r\nHTTP/2 (SPDY)\r\n\r\nMultiplexed streams over a single TCP connection.\r\n\r\nImplementation under net/spdy/.\r\n\r\nQUIC (HTTP/3)\r\n\r\nRuns atop UDP.\r\n\r\nImplemented in net/quic/.\r\n\r\nHandshake, stream abstraction, congestion control.\r\n\r\nEnable via GN args:\r\n\r\ngn\r\nCopy\r\nEdit\r\nenable_http2=true\r\nenable_quic=true\r\n6. Caching Layer\r\nDisk Cache (net/disk_cache/)\r\n\r\nStores responses keyed by URL, vary headers.\r\n\r\nLRU eviction.\r\n\r\nMemory Cache\r\n\r\nFast cache of small responses.\r\n\r\nCache API Hooks\r\n\r\nHttpCache sits between URLRequest and HttpNetworkTransaction.\r\n\r\ntext\r\nCopy\r\nEdit\r\nURLRequest → HttpCache → (hit: serve; miss: HttpNetworkTransaction)\r\n7. Proxy & DNS Resolution\r\nProxy Configuration\r\n\r\nRead from system or PAC scripts (proxy_config_service.cc).\r\n\r\nDNS\r\n\r\nHostResolver API with built-in DNS cache.\r\n\r\nAsync lookups via dns_client.cc.\r\n\r\n8. Prioritization & Throttling\r\nResourceScheduler (net/http/resource_scheduler.cc)\r\n\r\nAssigns priorities (e.g. script > image).\r\n\r\nLimits max concurrent requests per host.\r\n\r\n9. Security & Certificate Validation\r\nCertVerifier (net/cert/cert_verifier.cc)\r\n\r\nValidates TLS certificates, OCSP stapling.\r\n\r\nTransportSecurityState\r\n\r\nHSTS, HPKP policies.\r\n\r\nSandbox\r\n\r\nSocket operations restricted by sandbox policy.\r\n\r\n10. Debugging & Instrumentation\r\nLogging\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/chrome --enable-logging=stderr --v=1\r\nchrome://net-internals (legacy) or chrome://net-export\r\n\r\nTracing\r\n\r\nNET_LOG category in chrome://tracing.\r\n\r\nUnit Tests\r\n\r\nUnder net/http/ and net/tools/quic_client/.\r\n\r\n11. Extensions & Hooks\r\nIf you want to inject custom behavior:\r\n\r\nURLRequestJob\r\n\r\nCreate a factory via URLRequestFilter::RegisterProtocolHandler.\r\n\r\nNetworkDelegate\r\n\r\nIntercept headers, auth events in NetworkService.\r\n\r\n12. Next Steps\r\nDeep dive: Modules → Storage & Cache to see how responses are stored.\r\n\r\nExplore Security → Security Model for TLS sandbox details.\r\n\r\nExperiment: build with enable_quic=true and capture QUIC frames with Wireshark."
  },
  {
    "path": "modules/javascript-v8",
    "title": "JavaScript (V8)",
    "content": "# JavaScript (V8)\r\n\r\nChromium embeds Google’s high-performance V8 JavaScript engine to power all script execution in web pages and browser internals. This article explores how V8 fits into Chromium’s architecture, its compilation & execution pipeline, memory management, embedding APIs, and debugging tools.\r\n\r\n---\r\n\r\n## 1. Role of V8 in Chromium\r\n\r\n- **Where it lives**  \r\n  - In the **renderer process**, under `src/third_party/v8/`  \r\n  - Blink calls into V8 via the **V8 embedder APIs** in `content/renderer/v8_*`  \r\n- **What it does**  \r\n  - Parses, compiles, and executes all JavaScript on web pages  \r\n  - Runs extension & internal scripts (e.g. DevTools, PDF viewer)  \r\n\r\n---\r\n\r\n## 2. V8 Engine Architecture\r\n\r\n```text\r\nJavaScript Source\r\n      ↓\r\n LEXING / PARSING\r\n      ↓           ↘\r\n  AST           ↘  BYTECODE (Ignition)\r\n      ↓              ↓\r\n  TURBOFan IR ← OPTIMIZING COMPILER\r\n      ↓\r\nNATIVE MACHINE CODE\r\nParser & AST\r\n\r\nparser.cc builds the Abstract Syntax Tree.\r\n\r\nUses “pre-parsing” to quickly skip heavy functions.\r\n\r\nIgnition Interpreter\r\n\r\nBytecode generator (bytecode-generator.cc) produces compact bytecodes.\r\n\r\nInterpreter (interpreter.cc) executes without initial machine-code compile.\r\n\r\nTurbofan Optimizing Compiler\r\n\r\nHot functions are profiled, then passed through Turbofan (turbofan/)\r\n\r\nGenerates highly optimized machine code.\r\n\r\nDeoptimization & On-Stack Replacement\r\n\r\nIf assumptions break (e.g. type change), deopt back to baseline.\r\n\r\n3. Memory Management & Garbage Collection\r\nHeaps & Spaces\r\n\r\nYoung Generation (Scavenge, Semi-Spaces)\r\n\r\nOld Generation (Mark-Sweep, Mark-Compact)\r\n\r\nLarge Object Space\r\n\r\nIncremental & Concurrent GC\r\n\r\nScavenges small heaps quickly\r\n\r\nMarks & compacts old generation on background threads\r\n\r\nHandles & LocalHandles\r\n\r\nC++ wrappers ensuring safe pointer movement across GCs\r\n\r\n4. V8 Embedding in Chromium\r\nIsolates (v8::Isolate)\r\n\r\nOne per renderer process by default; isolates encapsulate heaps & contexts.\r\n\r\nContexts (v8::Context)\r\n\r\nExecute code in separate global object environments (e.g. <iframe>).\r\n\r\nBindings & Templates\r\n\r\nNative classes/functions exposed to JS via FunctionTemplate and ObjectTemplate.\r\n\r\nBlink defines DOM APIs by wiring its C++ implementations to V8.\r\n\r\nMicrotasks & Promise Hooks\r\n\r\nChromium pumps the microtask queue between tasks to implement Promises.\r\n\r\n5. Compilation & Startup\r\nSnapshotting\r\n\r\nStartup snapshot captures pre-compiled builtins & standard library for faster cold starts.\r\n\r\nBuild Flags\r\n\r\nControlled via GN args:\r\n\r\ngn\r\nCopy\r\nEdit\r\nv8_enable_pointer_compression = true\r\nv8_enable_slow_dchecks = false\r\nv8_static_rooting = true\r\nDebug vs Release\r\n\r\nDebug builds include extra checks, slower GC; release builds optimize for speed.\r\n\r\n6. Debugging & Profiling\r\nDevTools Protocol\r\n\r\nV8 exposes the Debugger, Profiler, and Heap domains.\r\n\r\nConnect via chrome://inspect or embed custom tools.\r\n\r\nCPU & Memory Profiles\r\n\r\nCapture JS call stacks, optimize hotspots in Turbofan.\r\n\r\nHeap snapshots to find leaks.\r\n\r\nLogging & Flags\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/chrome --js-flags=\"--trace-gc --trace-opt --prof\"\r\nExternal Tools\r\n\r\nHeap Inspector, d8 shell for iterative testing.\r\n\r\n7. Web Workers & Service Workers\r\nWorker Contexts\r\n\r\nEach worker spawns its own V8 isolate & event loop.\r\n\r\nShared ArrayBuffer & Atomics\r\n\r\nEnables parallel JS with shared memory.\r\n\r\n8. Compatibility & Standards\r\nECMAScript Versions\r\n\r\nV8 keeps pace with ES6+ features: modules, async/await, proxies, BigInt.\r\n\r\nWebAssembly\r\n\r\nIntegrated via wasm.cc modules; both interpreter and tier-up compilation.\r\n\r\n9. Extending & Hooking In\r\nCustom Builtins\r\n\r\nAdd new functions by extending V8’s builtins in src/third_party/v8/src/builtins.\r\n\r\nInspector API\r\n\r\nEmbed the Inspector protocol in other tools or integrations.\r\n\r\nFlags & Experiments\r\n\r\nFeature-flag new JS proposals before standardization.\r\n\r\n10. Next Steps\r\nRead Modules → Storage & Cache to see how JS resources (scripts, modules) are cached.\r\n\r\nExplore Debugging → Debugging Tools for end-to-end JS debugging setups.\r\n\r\nClone and play with the d8 shell in third_party/v8/tools/d8 for hands-on experimentation.\r\n\r\n"
  },
  {
    "path": "modules/storage-cache/overview",
    "title": "Storage Cache Implementation",
    "content": "# Storage Cache Implementation\r\n\r\nWelcome to the Storage Cache detailed implementation section! This area provides in-depth technical documentation about the storage and caching systems used in the Wanderlust custom Chromium browser.\r\n\r\n## What You'll Find Here\r\n\r\nThis section contains detailed implementation documentation:\r\n\r\n- **[Disk Cache Design Principles](disk-cache-design-principles.md)**: Core principles and design decisions for disk-based caching\r\n- **Cache Architecture**: Detailed technical architecture of the storage cache system\r\n- **Performance Optimization**: Strategies for optimizing cache performance and efficiency\r\n- **Cache Management**: Policies for cache eviction, cleanup, and maintenance\r\n\r\n## Storage Cache Overview\r\n\r\nThe storage cache system is a critical component that provides:\r\n\r\n### Core Functionality\r\n- **Resource Caching**: Efficient storage and retrieval of web resources\r\n- **Disk Management**: Intelligent disk space usage and management\r\n- **Performance Optimization**: Fast access to frequently used resources\r\n- **Memory Efficiency**: Balanced memory and disk usage strategies\r\n\r\n### Cache Types\r\n- **HTTP Cache**: Standard HTTP caching for web resources\r\n- **Application Cache**: Offline application resource storage\r\n- **Service Worker Cache**: Programmatic cache management for PWAs\r\n- **Browser Cache**: Internal browser resource caching\r\n\r\n## Technical Architecture\r\n\r\n### Cache Hierarchy\r\n1. **Memory Cache**: Fast RAM-based caching for immediate access\r\n2. **Disk Cache**: Persistent storage for long-term resource caching\r\n3. **Network Fallback**: Fetch from network when cache misses occur\r\n\r\n### Storage Strategies\r\n- **LRU Eviction**: Least Recently Used cache eviction policies\r\n- **Size Management**: Intelligent cache size limits and management\r\n- **Compression**: Resource compression for efficient storage\r\n- **Indexing**: Fast lookup and retrieval mechanisms\r\n\r\n## Implementation Details\r\n\r\n### Design Principles\r\nThe storage cache follows specific design principles outlined in:\r\n- [Disk Cache Design Principles](disk-cache-design-principles.md): Fundamental design decisions and rationale\r\n\r\n### Performance Considerations\r\n- **Access Patterns**: Optimized for common web browsing patterns\r\n- **Concurrency**: Thread-safe operations for multi-process access\r\n- **I/O Optimization**: Efficient disk read/write operations\r\n- **Memory Management**: Careful memory usage to avoid system impact\r\n\r\n## Integration Points\r\n\r\n### Module Integration\r\nThe storage cache integrates with:\r\n- **Networking Module**: Caching network responses and resources\r\n- **JavaScript Engine**: Providing cached resources to script execution\r\n- **Rendering Engine**: Fast access to cached stylesheets, images, and assets\r\n\r\n### Browser Architecture\r\n- **Process Model**: Cache access across different browser processes\r\n- **Security Boundaries**: Secure cache isolation between origins\r\n- **Resource Loading**: Integration with the browser's resource loading pipeline\r\n\r\n## Development and Debugging\r\n\r\n### Cache Debugging\r\n- **Cache Inspection**: Tools for examining cache contents and state\r\n- **Performance Monitoring**: Tracking cache hit rates and performance metrics\r\n- **Debug Interfaces**: Internal pages for cache analysis and debugging\r\n\r\n### Configuration Options\r\n- **Cache Policies**: Configurable caching behaviors and policies\r\n- **Size Limits**: Adjustable cache size limits and thresholds\r\n- **Debugging Flags**: Development flags for cache debugging and analysis\r\n\r\n## Related Documentation\r\n\r\nFor broader context, see:\r\n- [Storage & Cache Overview](../storage-cache.md): High-level storage and cache documentation\r\n- [Modules Overview](../overview.md): How storage fits into the overall module architecture\r\n- [Architecture](../../architecture/overview.md): System-level cache architecture\r\n- [Debugging](../../debugging/overview.md): Tools for cache debugging and analysis\r\n\r\n---\r\n\r\n*Begin with the [disk cache design principles](disk-cache-design-principles.md) to understand the fundamental design decisions behind our storage cache implementation.*\r\n"
  },
  {
    "path": "modules/storage-cache/disk-cache-design-principles",
    "title": "Disk Cache Design Principles",
    "content": "# Disk Cache Design Principles\r\n\r\nThis deep-dive covers Chromium’s on-disk cache: its goals, on-disk format, key interfaces, and implementation notes.\r\n\r\n---\r\n\r\n## 1. Overview\r\n\r\nThe disk cache stores web-fetched resources for fast later access. Key characteristics:\r\n\r\n- **Size Bound**  \r\n  The cache must not grow without limit; an eviction algorithm decides when to remove old entries.\r\n- **Crash Resilience**  \r\n  Survives application crashes by discarding only entries in use at crash time. A full system crash may still wipe the entire cache.\r\n- **Efficient Access**  \r\n  Supports both synchronous and asynchronous operations with low latency.\r\n- **Conflict Avoidance**  \r\n  Layout prevents simultaneous-store conflicts (cache thrashing).\r\n- **Entry Removal**  \r\n  Individual entries can be removed; existing handles continue to function, and new opens fail as if the entry never existed.\r\n- **Single-threaded Assumption**  \r\n  Callers share one thread; callbacks are posted on the thread’s message loop to avoid reentrancy.\r\n\r\n---\r\n\r\n## 2. External Interfaces\r\n\r\nChromium’s cache API (in `src/net/disk_cache/disk_cache.h`) exposes:\r\n\r\n- **`disk_cache::Backend`**  \r\n  Enumerate entries, open existing or create new entries.\r\n- **`disk_cache::Entry`**  \r\n  Read/write data streams for a single resource.\r\n\r\nEach entry has a unique key (e.g., `http://example.com/favicon.ico`). Data is stored in separate streams (HTTP headers vs. payload), indexed by stream ID in `Entry::ReadData`/`WriteData`.\r\n\r\n---\r\n\r\n## 3. On-Disk Format\r\n\r\nAll cache files live under a `cache/` directory. Chromium uses at least:\r\n\r\n- **1 Index File**  \r\n  Contains a memory-mapped hash table mapping keys to addresses.\r\n- **≥4 Block-Files** (`data_n`)  \r\n  Store fixed-size blocks (e.g., 256 B). Each file grows by 1 KB-aligned increments and chains to same-size files via headers.\r\n- **Separate Files** (`f_xx`)  \r\n  Resources > 16 KB bypass block-files and live in standalone files.\r\n\r\n### 3.1 Cache Address\r\n\r\nA 32-bit value directing to:\r\n\r\n- A block-file (file number, start block, block count, block type)\r\n- A separate file (hex identifier)\r\n\r\nDefined in `disk_cache/addr.h`, it enables unified handling of diverse data.\r\n\r\n### 3.2 Index File Structure\r\n\r\nDefined in `disk_cache/disk_format.h`:\r\n\r\n```text\r\nIndexHeader\r\nHashTable (size ≥ kIndexTableSize, actual length in header.table_len)\r\n```\r\n\r\n- Memory-mapped for rapid key→address lookup (using low-order hash bits).\r\n- Header includes magic number + major/minor version; a major bump is incompatible.\r\n\r\n### 3.3 Block-File Structure\r\n\r\nAlso in `disk_cache/disk_format.h`:\r\n\r\n- **Header (8 KB)**: memory-mapped bitmap tracks up to ~64K blocks.\r\n- **Data blocks**: fixed size (e.g., 256 B). Files grow by 1 024 blocks until full, then link to a new file of same block size via `next_file`.\r\n\r\nBlocks align on 4-block boundaries to simplify allocation:\r\n\r\n1. A record uses 1–4 contiguous blocks.  \r\n2. Allocations jump to the next aligned region if needed.  \r\n\r\nHeader fields `empty_slots` and `hints` optimize allocation and detect crashes mid-update.\r\n\r\n### 3.4 Cache Entry Layout\r\n\r\nEach entry splits into two structures:\r\n\r\n- **`EntryStore`** (1–4 blocks of 256 B)  \r\n  Stores key hash, stream addresses, pointer to next collision, and optional out-of-line key pointer.\r\n- **`RankingsNode`** (36 B, its own block-files)  \r\n  Tracks eviction metadata; marked in-use while open.\r\n\r\n### 3.5 The Big Picture\r\n\r\nA typical cache has:\r\n\r\n1. One index file  \r\n2. Multiple block-files (256 B, 1 KB, 4 KB) chained by size  \r\n3. Separate files for large payloads\r\n\r\nEntries link across streams and files via cache addresses.  \r\n![](../../../img/modules/the-big-picture.png)\r\n\r\n---\r\n\r\n## 4. Implementation Notes\r\n\r\n### 4.1 Two Backends\r\n\r\n- **Disk-based** (`disk_cache/backend_impl.cc`, `entry_impl.cc`)  \r\n- **In-memory** (`disk_cache/mem_backend_impl.cc`) for Incognito mode\r\n\r\nCache types (media, AppCache, general) share APIs but may differ in eviction.\r\n\r\n### 4.2 Lower-Level I/O\r\n\r\nOS abstractions in `disk_cache/file.h` & `mapped_file.h`.  \r\n`disk_cache::BlockFiles` manages block-file access.  \r\n`StorageBlock<T>` template handles loading/storing of `EntryStore` or `RankingsNode`.\r\n\r\n### 4.3 Eviction\r\n\r\n- **Ranking lists**: `disk_cache/rankings` (and `mem_rankings`).  \r\n- **Eviction logic**: `disk_cache/eviction` implements LRU and reuse/age-aware variants.  \r\n- **Transactions**: ensure consistency across crashes.  \r\n- **Multiple lists** by reuse frequency plus a “recently evicted” list.\r\n\r\n### 4.4 Buffering\r\n\r\n- Buffers up to 16 KB per stream before first disk write.  \r\n- Grows to 1 MB or a global cap.  \r\n- Minimizes disk I/O for small entries and computes record size for address allocation.\r\n\r\n### 4.5 Deleting Entries\r\n\r\n- `Doom*()` methods mark entries for deferred deletion after all handles close.  \r\n- Doomed entries removed from index; new opens create fresh entries.\r\n\r\n### 4.6 Enumeration\r\n\r\n- Example: `URLRequestViewCacheJob` iterates entries (order not guaranteed, can be slow).\r\n\r\n### 4.7 Sparse Data\r\n\r\n- Supports two streams: regular + sparse.  \r\n- Sparse data split into child entries linked from a parent (`disk_cache/sparse_control`).\r\n\r\n### 4.8 Dedicated Cache Thread\r\n\r\n- A background cache thread handles file I/O, offloading the browser I/O thread (IPC/UI).  \r\n- Uses task posting (`in_flight_io` / `in_flight_backend_io`).  \r\n- No locking: some calls (e.g. `GetDataSize`) may race with writes.\r\n\r\n---\r\n\r\n## 5. Data Integrity\r\n\r\nBalancing performance vs. crash resilience:\r\n\r\n- Leverages OS file-cache; no journaling to avoid complexity.  \r\n- Memory-mapped headers flush latest state on crash.  \r\n- On system crash, eviction lists may corrupt; the fallback is to discard the cache rather than risk inconsistency.\r\n\r\n*End of Disk Cache Design Principles Deep Dive*\r\n\r\n"
  },
  {
    "path": "introduction/overview",
    "title": "Chromium Knowledge Base Overview (v134+)",
    "content": "# Chromium Knowledge Base Overview (v134+)\r\n\r\nWelcome to the **Wanderlust Knowledge Base**! This comprehensive resource is designed to help you navigate, understand, and contribute to modern Chromium's sophisticated codebase and the **custom-browser project** built upon it.\r\n\r\n---\r\n\r\n## 1. What Is Modern Chromium? (v134+)\r\n\r\n- **Definition**  \r\n  Chromium is the cutting-edge open-source browser engine powering Google Chrome, Microsoft Edge, Opera, Brave, and our **custom-browser project**. As of v134+, it represents one of the most sophisticated software architectures ever built.\r\n\r\n- **Modern Goals & Achievements**  \r\n  – **Performance Excellence**: 60+ FPS rendering, sub-100ms navigation, Core Web Vitals optimization  \r\n  – **Advanced Security**: Site isolation, Control Flow Integrity (CFI), Privacy Sandbox integration  \r\n  – **Cross-Platform Mastery**: Windows, macOS, Linux, Chrome OS, Android, iOS with platform-specific optimizations  \r\n  – **Web Standards Leadership**: WebGPU, WebAssembly, Progressive Web Apps, and emerging APIs  \r\n\r\n- **v134+ Evolution & Community**  \r\n  – **Origin**: Started by Google in 2008, now a massive collaborative effort  \r\n  – **Modern Scale**: 25+ million lines of code, 1000+ daily commits, global contributor network  \r\n  – **Contribution Ecosystem**: Chromium Bug Tracker, Gerrit code review, specialized mailing lists  \r\n  – **Innovation Hub**: Driving web platform evolution and browser technology advancement  \r\n\r\n---\r\n\r\n## 2. Why Explore Modern Chromium Source? (v134+)\r\n\r\n- **Advanced Learning Opportunities**  \r\n  – **Modern C++20/23 Practices**: Template metaprogramming, memory safety, performance optimization  \r\n  – **Service-Oriented Architecture**: Microservice design with Mojo IPC, process coordination  \r\n  – **Graphics & Rendering**: Viz compositor, GPU acceleration, advanced rendering pipelines  \r\n  – **Security Engineering**: Multi-layered security, sandboxing, exploit mitigation techniques  \r\n\r\n- **Professional Development**  \r\n  – **Performance Engineering**: Memory optimization, threading, real-time systems  \r\n  – **Cross-Platform Development**: Platform abstraction, native integration, responsive design  \r\n  – **Large-Scale Software Architecture**: Managing complexity, modularity, maintainability  \r\n  – **Web Technology Innovation**: Implementing cutting-edge web standards and APIs  \r\n\r\n- **Custom Browser Development**  \r\n  – **Feature Implementation**: Adding custom functionality to browser components  \r\n  – **Performance Tuning**: Optimizing for specific use cases and hardware configurations  \r\n  – **Security Enhancements**: Implementing additional security layers and privacy features  \r\n  – **Integration Possibilities**: Connecting with external services, APIs, and platforms  \r\n\r\n---\r\n\r\n## 3. Modern Multi-Process Architecture (v134+)\r\n\r\nChromium's sophisticated architecture has evolved significantly:\r\n\r\n### Core Processes\r\n- **Browser Process**: Central coordinator with enhanced UI management and service orchestration\r\n- **Renderer Processes**: Site-isolated content rendering with strict security boundaries\r\n- **GPU Process**: Unified Viz compositor with Out-of-Process Rasterization (OOP-R)\r\n- **Network Service**: Dedicated network process with HTTP/3 and QUIC support\r\n\r\n### Modern Service Ecosystem\r\n- **Audio Service**: Isolated audio processing and hardware acceleration\r\n- **Storage Service**: Centralized data management with enhanced privacy controls\r\n- **Device Service**: Secure hardware access with permission management\r\n- **ML Service**: On-device machine learning with TensorFlow Lite integration\r\n- **Utility Processes**: Sandboxed processing for various specialized tasks\r\n\r\n### Advanced Features (v134+)\r\n- **Site Isolation**: Per-origin process boundaries for enhanced security\r\n- **Mojo IPC**: Type-safe inter-process communication with capability-based security\r\n- **Service Manager**: Intelligent service coordination and dependency management\r\n- **Enhanced Sandboxing**: Platform-specific security with CFI and memory protection\r\n\r\n_(Explore detailed sections: [Process Model](../architecture/process-model.md), [Render Pipeline](../architecture/render-pipeline.md), [IPC Internals](../architecture/ipc-internals.md))_\r\n\r\n---\r\n\r\n## 4. Custom-Browser Project Structure\r\n\r\nOur enhanced directory layout integrates custom modifications with upstream Chromium:\r\n\r\n```text\r\ncustom-browser/\r\n├── .gclient                 # Chromium source synchronization\r\n├── package.json             # Project configuration and dependencies\r\n├── requirements.txt         # Python development tools\r\n├── lib/                     # Python utilities and development tools\r\n│   ├── logger.py           # Advanced console logging with colors\r\n│   └── utils.py            # Common utility functions\r\n├── scripts/                # Automation and build scripts\r\n│   └── init.py             # Project initialization and setup\r\n├── patches/                # Custom Chromium modifications\r\n├── docs/                   # Project-specific documentation\r\n└── src/                    # Chromium source tree with enhancements\r\n    ├── chrome/             # Browser UI and Chrome-specific features\r\n    ├── content/            # Core browser engine and renderer\r\n    ├── custom/             # 🎯 Our custom browser modifications\r\n    ├── components/         # Reusable feature modules\r\n    ├── services/           # Modern Mojo-based services\r\n    ├── third_party/        # External dependencies (Blink, V8, Skia)\r\n    ├── net/               # Advanced networking (HTTP/3, QUIC, DNS)\r\n    ├── gpu/               # Graphics and Viz compositor\r\n    ├── ui/                # Cross-platform UI framework\r\n    ├── base/              # Fundamental utilities and abstractions\r\n    └── build/             # Build system and configuration\r\n```\r\n\r\n### Key Integration Points\r\n- **`src/custom/`**: Our browser enhancements and modifications\r\n- **`lib/`**: Development tools specific to our workflow\r\n- **`scripts/`**: Project automation and initialization\r\n- **`patches/`**: Required patches to upstream Chromium\r\n\r\n---\r\n\r\n## 5. Modern Web Technologies & Features (v134+)\r\n\r\n### Cutting-Edge Web APIs\r\n- **WebGPU**: Next-generation graphics API with compute shader support\r\n- **WebAssembly (WASM)**: High-performance code execution with SIMD and threading\r\n- **Origin Private File System**: Secure file system access for web applications\r\n- **Web Locks**: Cross-tab coordination and resource management\r\n- **Web Streams**: Efficient data processing with backpressure handling\r\n\r\n### Privacy & Security Innovations\r\n- **Privacy Sandbox**: Cookieless advertising with Topics API and FLEDGE\r\n- **Trust Tokens**: Anti-fraud mechanisms without fingerprinting\r\n- **Attribution Reporting**: Privacy-preserving conversion measurement\r\n- **Enhanced Site Isolation**: Protection against Spectre-style attacks\r\n\r\n### Performance Optimizations\r\n- **Core Web Vitals**: LCP, FID, CLS optimization at the engine level\r\n- **Navigation API**: Smooth page transitions with shared element animations\r\n- **Container Queries**: Responsive design without layout thrashing\r\n- **CSS Cascade Layers**: Advanced styling control and organization\r\n\r\n---\r\n\r\n## 6. Development Workflow & Tools (v134+)\r\n\r\n### Getting Started\r\n1. **Environment Setup**: `npm run install:python` for development tools\r\n2. **Project Initialization**: `npm run init` to fetch Chromium and dependencies\r\n3. **Build Configuration**: `gn gen out/Default` with modern build options\r\n4. **Compilation**: `ninja -C out/Default chrome` for browser executable\r\n\r\n### Modern Development Tools\r\n- **Advanced Debugging**: Chrome DevTools integration with process inspection\r\n- **Performance Profiling**: Real-time Core Web Vitals measurement\r\n- **Security Analysis**: Comprehensive sandbox and IPC monitoring\r\n- **Code Navigation**: Intelligent cross-referencing and documentation\r\n\r\n### Essential Debugging Resources\r\n```bash\r\n# Modern debugging pages\r\nchrome://gpu/              # GPU capabilities and Viz status\r\nchrome://process-internals/ # Process and service monitoring\r\nchrome://tracing/          # Advanced performance timeline\r\nchrome://mojo-internals/   # IPC and service inspection\r\nchrome://components/       # Component status and versions\r\n```\r\n\r\n---\r\n\r\n## 7. Learning Paths & Next Steps\r\n\r\n### For New Developers\r\n1. **Start Here**: [Project Layout](../getting-started/project-layout.md) - Understanding the codebase structure\r\n2. **Architecture Deep Dive**: [Browser Components](../architecture/browser-components.md) - Modern component overview\r\n3. **Hands-On**: [Setup & Build](../getting-started/setup-build.md) - Get your development environment running\r\n\r\n### For Advanced Contributors\r\n1. **Process Architecture**: [Process Model](../architecture/process-model.md) - Multi-process design and security\r\n2. **Rendering Engine**: [Render Pipeline](../architecture/render-pipeline.md) - From HTML to pixels\r\n3. **Communication**: [IPC Internals](../architecture/ipc-internals.md) - Mojo and modern IPC patterns\r\n\r\n### Specialized Topics\r\n- **Security**: [Security Model](../security/security-model.md) - Sandboxing and exploit mitigation\r\n- **Networking**: [HTTP & Networking](../modules/networking-http.md) - Modern network stack\r\n- **Storage**: [Storage & Cache](../modules/storage-cache.md) - Data persistence and privacy\r\n- **JavaScript**: [V8 Integration](../modules/javascript-v8.md) - JavaScript engine internals\r\n\r\n---\r\n\r\n## 8. Community & Contribution\r\n\r\n### Stay Connected\r\n- **Chromium Blog**: Latest architectural decisions and feature announcements\r\n- **Chrome Platform Status**: Track implementation of new web standards\r\n- **Chromium Groups**: Specialized mailing lists for different areas of development\r\n\r\n### Contributing Guidelines\r\n- **Code Style**: Follow Chromium's comprehensive style guide\r\n- **Testing**: Implement thorough unit and integration tests\r\n- **Documentation**: Update relevant documentation with code changes\r\n- **Security**: Consider security implications for all modifications\r\n\r\n### Custom Browser Development\r\n- **Feature Planning**: Design features that integrate cleanly with Chromium's architecture\r\n- **Upstream Compatibility**: Maintain compatibility with Chromium updates\r\n- **Performance**: Profile and optimize custom features for production use\r\n- **Security**: Implement security reviews for all custom functionality\r\n\r\n---\r\n\r\n**Welcome to the future of browser development!** This knowledge base will guide you through the intricacies of modern Chromium architecture and help you build exceptional browsing experiences with the custom-browser project.\r\n\r\n**Quick Links**:\r\n- 🚀 [Get Started](../getting-started/setup-build.md)\r\n- 🏗️ [Architecture Overview](../architecture/browser-components.md)\r\n- 🔧 [Development Guide](../getting-started/project-layout.md)\r\n- 🛡️ [Security Model](../security/security-model.md)\r\n"
  },
  {
    "path": "getting-started/setup-build",
    "title": "Setup & Build",
    "content": "# Setup & Build\r\n\r\nThis guide shows you how to fetch, build, and run Chromium from source on **Linux**, **macOS**, or **Windows**.\r\n\r\n---\r\n\r\n## 1. Prerequisites\r\n\r\nBefore you begin, make sure you have:\r\n\r\n- A supported OS:\r\n  - **Linux**: Ubuntu 20.04+ or equivalent\r\n  - **macOS**: 10.15+ (Intel or Apple Silicon)\r\n  - **Windows**: 10 (x64)\r\n- **Disk space**: At least 30 GB free\r\n- **RAM**: ≥ 8 GB (16 GB+ recommended)\r\n- **Tools**:\r\n  - **Python 3.8+** (for build scripts)\r\n  - **Git** (2.25+)\r\n  - **Depot Tools** (Google’s repo of Chromium helper scripts)\r\n\r\n### 1.1 Installing Depot Tools\r\n\r\n```bash\r\n# Clone Depot Tools somewhere in your PATH:\r\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git ~/depot_tools\r\nexport PATH=\"$PATH:$HOME/depot_tools\"\r\n# (Add the export line to your shell rc: ~/.bashrc, ~/.zshrc, or PowerShell profile)\r\n```\r\n\r\n## 2. Fetching the Source\r\n\r\n### 2.1 Create a working directory:\r\n\r\n```bash\r\nmkdir -p ~/chromium && cd ~/chromium\r\n```\r\n\r\n### 2.2 Fetch the code:\r\n\r\n```bash\r\nfetch --nohooks chromium\r\ncd src\r\n```\r\n\r\n### 2.3 Install additional hooks:\r\n\r\n```bash\r\ngclient sync --with_branch_heads --with_tags\r\n```\r\n\r\n## 3. Configuring Your Build\r\n\r\nChromium uses GN for meta-build configuration and Ninja as the build engine.\r\n\r\n### 3.1 Generate build files:\r\n\r\n```bash\r\ngn gen out/Default --args='\r\n  is_debug=false            # or true for a debug build\r\n  symbol_level=1            # 0=no symbols, 1=debug symbols only\r\n  is_component_build=true   # modules are built as shared libs\r\n'\r\n```\r\n\r\n### 3.2 Common args:\r\n\r\n```text\r\nis_debug=true               # Debug build (with assertions & logging)\r\nis_official_build=false     # Disable Google-branded splash screens\r\nenable_nacl=false           # Disable Native Client (optional)\r\nremove_webcore_debug_symbols=true  # Strip extra symbols\r\n```\r\n\r\n### 4. Building\r\n\r\nFrom the src/ directory:\r\n\r\n```bash\r\nninja -C out/Default chrome\r\n```\r\n\r\n- -C out/Default tells Ninja where your build files live.\r\n- chrome is the target; you can also build content_shell, browser_tests, etc.\r\n\r\nTip: On multi-core machines you can speed up builds:\r\n\r\n```bash\r\nninja -C out/Default -j8\r\n```\r\n\r\n(where 8 ≈ number of CPU cores)\r\n\r\n## 5. Running Your Build\r\n\r\n- Linux & macOS:\r\n\r\n```bash\r\nout/Default/chrome        # Launches your custom build\r\n```\r\n\r\n- Windows (PowerShell):\r\n\r\n```powershell\r\n.\\out\\Default\\chrome.exe\r\n```\r\n\r\nYou can pass any Chromium CLI flags, for example:\r\n\r\n```bash\r\nout/Default/chrome --enable-logging --v=1\r\n```\r\n\r\n## 6. Iterating & Incremental Builds\r\n\r\nAfter code changes, simply rerun:\r\n\r\n```bash\r\nninja -C out/Default\r\n```\r\n\r\nNinja only rebuilds what’s necessary, so incremental iterations are fast.\r\n\r\n## 7. Common Issues & Troubleshooting\r\n\r\nSymptom\tPossible Fix\r\nfetch is not found\tEnsure depot_tools is in your PATH\r\ngclient sync errors out\tDelete src/.gclient_entries and retry\r\nGN complains about bad args\tRun gn args out/Default --list to verify flags\r\nOut-of-memory during build\tLower -j jobs or increase swap space\r\n\r\n## 8. Next Steps\r\n\r\n- Dive into Project Layout: see how src/ is organized\r\n- Explore Architecture → Process Model: understand multi-process design\r\n- Try a Debug Build and play with logging flags\r\n\r\n"
  },
  {
    "path": "getting-started/project-layout",
    "title": "Project Layout",
    "content": "# Project Layout\r\n\r\nThe custom-browser project is a Chromium-based browser implementation with a structured development environment. This guide provides a comprehensive tour of the project's directories and files to help you navigate and understand the codebase effectively.\r\n\r\n---\r\n\r\n## 1. Top-Level Structure\r\n\r\n```text\r\ncustom-browser/\r\n├── .gclient                 # gclient configuration for Chromium sync\r\n├── .gitignore               # files to ignore in Git\r\n├── package.json             # project configuration and npm scripts\r\n├── package-lock.json        # locked npm dependencies\r\n├── requirements.txt         # Python dependencies\r\n├── .npmrc                   # npm configuration\r\n├── README.md                # project overview and setup guide\r\n├── lib/                     # Python utilities and libraries\r\n├── scripts/                 # automation and build scripts\r\n├── patches/                 # Chromium patches and modifications\r\n├── docs/                    # project documentation (currently empty)\r\n└── src/                     # Chromium source code and custom modules\r\n```\r\n\r\n### Configuration Files\r\n\r\n- **`.gclient`** - Defines how to fetch Chromium source and dependencies\r\n- **`package.json`** - Contains project metadata, npm scripts, and custom project configurations\r\n- **`requirements.txt`** - Python packages required for development tools\r\n- **`.npmrc`** - npm registry and configuration settings\r\n\r\n### Development Directories\r\n\r\n- **`lib/`** - Python utilities for logging, development tools, and helper functions\r\n- **`scripts/`** - Automation scripts for project initialization and build processes\r\n- **`patches/`** - Custom patches and modifications to Chromium source\r\n- **`docs/`** - Project documentation (expandable for future documentation)\r\n- **`src/`** - Main Chromium source tree with custom modifications\r\n\r\n---\r\n\r\n## 2. The lib/ Directory\r\n\r\nThe `lib/` directory contains Python utilities that support the development workflow:\r\n\r\n```text\r\nlib/\r\n├── __init__.py              # Python package initialization\r\n├── logger.py                # Advanced colored console logging utility\r\n├── logger_demo.py           # Demonstration of logging capabilities\r\n├── utils.py                 # General utility functions\r\n└── __pycache__/             # Python bytecode cache\r\n```\r\n\r\n### Key Components\r\n\r\n- **`logger.py`** - Provides rich console output with progress bars, spinners, and colored logging\r\n- **`logger_demo.py`** - Shows examples of how to use the logging system\r\n- **`utils.py`** - Common utility functions used across the project\r\n\r\n---\r\n\r\n## 3. The scripts/ Directory\r\n\r\nContains automation scripts for project setup and development:\r\n\r\n```text\r\nscripts/\r\n├── init.py                  # Main project initialization script\r\n└── av/                      # Additional automation scripts\r\n```\r\n\r\n### Main Scripts\r\n\r\n- **`init.py`** - Handles project setup, dependency fetching, and environment configuration\r\n- **`av/`** - Directory for additional automation and validation scripts\r\n\r\n---\r\n\r\n## 4. The src/ Directory\r\n\r\nAfter running project initialization, the Chromium source code lives in `src/`. This follows the standard Chromium layout with custom additions:\r\n\r\n```text\r\nsrc/\r\n├── chrome/                  # Chrome browser shell & UI code\r\n├── content/                 # Blink/V8 embedder & shared browser logic  \r\n├── cc/                      # Compositor & layered rendering\r\n├── gpu/                     # GPU process, drivers, and command buffer\r\n├── net/                     # Networking stack (HTTP, QUIC, proxies)\r\n├── ui/                      # Cross-platform UI abstraction\r\n├── components/              # Reusable modules (autofill, payments, etc.)\r\n├── third_party/             # External dependencies and libraries\r\n├── tools/                   # Build-time code generation & helper scripts\r\n├── custom/                  # Custom-core project (our modifications)\r\n├── build/                   # Build system configuration\r\n├── base/                    # Fundamental utilities and abstractions\r\n├── sandbox/                 # Security sandbox implementation\r\n├── services/                # Mojo-based services\r\n└── out/                     # Build output directory\r\n```\r\n\r\n### Core Directories\r\n\r\n- **`chrome/`** - Entry points, Chrome UI (tabs, omnibox, menus), and platform-specific code\r\n- **`content/`** - Integrates Blink (rendering) and V8 (JavaScript), plus IPC and navigation\r\n- **`custom/`** - **Our custom modifications and extensions to Chromium**\r\n- **`net/`** - Implements HTTP(S), QUIC, caching, cookies, proxy resolution\r\n- **`ui/`** - Cross-platform windowing, input events, and vector graphics\r\n\r\n### Custom Integration\r\n\r\n- **`custom/`** - Contains the custom-core project with our browser modifications\r\n- **`out/`** - Generated during build process, contains compiled binaries and intermediates\r\n\r\n---\r\n\r\n## 5. Build Outputs & Configuration\r\n\r\n```text\r\nsrc/out/\r\n└── Default/               # Default build configuration\r\n    ├── obj/               # Intermediate object files\r\n    ├── chrome.exe         # Built browser executable (Windows)\r\n    ├── *.dll              # Shared libraries (if component build)\r\n    └── *.ninja_log        # Build logs and timing information\r\n```\r\n\r\nBuild outputs are generated using the GN (Generate Ninja) build system:\r\n- Use `gn gen out/Default` to generate build files\r\n- Use `ninja -C out/Default chrome` to build the browser\r\n\r\n---\r\n\r\n## 6. Project Configuration & Metadata\r\n\r\n### Build Configuration Files\r\n\r\n- **`BUILD.gn`** files - Scattered throughout the tree, define build targets\r\n- **`.gn`** files - Build system templates and configuration\r\n- **`DEPS`** - External dependency definitions and version pinning\r\n- **`PRESUBMIT.py`** - Pre-commit hooks for code quality checks\r\n\r\n### Project Management\r\n\r\n- **`package.json` config section** - Defines custom project dependencies and their locations\r\n- **`scripts/init.py`** - Automates the fetching and setup of configured projects\r\n- **`.gclient`** - Controls Chromium source synchronization\r\n\r\n---\r\n\r\n## 7. Development Workflow\r\n\r\n### Initial Setup\r\n\r\n1. **Install dependencies**: `npm run install:python`\r\n2. **Initialize project**: `npm run init`\r\n3. **Configure build**: Navigate to `src/` and run `gn gen out/Default`\r\n4. **Build browser**: `ninja -C out/Default chrome`\r\n\r\n### Directory Navigation Tips\r\n\r\n| Need to... | Look in... |\r\n|------------|------------|\r\n| Modify networking behavior | `src/net/` |\r\n| Customize UI components | `src/ui/` or `src/chrome/browser/ui/` |\r\n| Add browser features | `src/custom/` (our modifications) |\r\n| Debug build issues | `src/build/` or build logs in `src/out/` |\r\n| Work with Python tools | `lib/` directory |\r\n| Add automation scripts | `scripts/` directory |\r\n\r\n### Code Search and Navigation\r\n\r\n- Use **VS Code** or your preferred IDE for local code navigation\r\n- **Chromium Code Search**: https://source.chromium.org for upstream Chromium reference\r\n- **grep/ripgrep**: Fast text search across the large codebase\r\n- **BUILD.gn search**: `find . -name BUILD.gn | xargs grep <target>` to locate build definitions\r\n\r\n---\r\n\r\n## 8. Custom Integration Points\r\n\r\n### Our Modifications\r\n\r\n- **`src/custom/`** - Contains the custom-core project with our browser enhancements\r\n- **`lib/`** - Development tools and utilities specific to our workflow\r\n- **`scripts/`** - Automation for project management and initialization\r\n- **`patches/`** - Any required patches to upstream Chromium code\r\n\r\n### Development Best Practices\r\n\r\n- Keep custom code in the `src/custom/` directory when possible\r\n- Use the Python utilities in `lib/` for consistent logging and development experience\r\n- Follow Chromium's coding standards for modifications to core directories\r\n- Document custom features and modifications in the `docs/` directory\r\n\r\n---\r\n\r\n**Navigation Tips:**\r\n- Start with the `README.md` for project overview and setup\r\n- Use `lib/logger_demo.py` to understand the development tools\r\n- Explore `src/custom/` for our specific browser modifications\r\n- Reference upstream Chromium documentation for core functionality understanding\r\n"
  },
  {
    "path": "getting-started/overview",
    "title": "Getting Started with Wanderlust",
    "content": "# Getting Started with Wanderlust\r\n\r\nWelcome to the Getting Started section! This is your entry point for setting up and beginning development with the Wanderlust custom Chromium browser project.\r\n\r\n## What You'll Find Here\r\n\r\nThis section provides everything you need to get up and running:\r\n\r\n- **[Project Layout](project-layout.md)**: Understanding the overall project structure and organization\r\n- **[Code Directory Structure](code-directory-structure.md)**: Detailed breakdown of the codebase organization\r\n- **[Setup & Build](setup-build.md)**: Complete setup instructions and build process\r\n\r\n## Quick Start Guide\r\n\r\nFollow these steps to get started with Wanderlust development:\r\n\r\n1. **Understand the Project**: Review the project layout and directory structure\r\n2. **Set Up Environment**: Follow the setup and build instructions\r\n3. **Explore the Codebase**: Familiarize yourself with the code organization\r\n4. **Build the Project**: Complete your first successful build\r\n5. **Start Development**: Begin making changes and contributions\r\n\r\n## Prerequisites\r\n\r\nBefore diving into development, ensure you have:\r\n- Appropriate development environment (Windows/Linux/macOS)\r\n- Required build tools and dependencies\r\n- Understanding of Chromium's build system\r\n- Basic knowledge of C++, JavaScript, and web technologies\r\n\r\n## Next Steps\r\n\r\nAfter completing the getting started process:\r\n\r\n- Explore the [Architecture](../architecture/overview.md) to understand system design\r\n- Review [Modules](../modules/overview.md) for component-specific information\r\n- Check [Debugging](../debugging/overview.md) for troubleshooting techniques\r\n- Read [Contributing](../contributing/overview.md) guidelines for contribution workflow\r\n\r\n## Development Workflow\r\n\r\n1. **Setup**: Complete the initial setup and build process\r\n2. **Explore**: Understand the codebase structure and architecture\r\n3. **Develop**: Make changes following our coding standards\r\n4. **Test**: Verify your changes work correctly\r\n5. **Contribute**: Submit your improvements following our contribution guidelines\r\n\r\n## Support\r\n\r\nIf you encounter issues during setup:\r\n- Check the troubleshooting sections in each guide\r\n- Review the [Debugging](../debugging/overview.md) section for common solutions\r\n- Consult the [Contributing](../contributing/overview.md) section for development workflow questions\r\n\r\n---\r\n\r\n*Ready to start? Begin with our [project layout guide](project-layout.md) to understand the overall structure, then proceed to [setup and build instructions](setup-build.md).*\r\n"
  },
  {
    "path": "getting-started/code-directory-structure",
    "title": "Code Directory Structure",
    "content": "# Code Directory Structure\r\n\r\nThis page expands on *Project Layout* with a detailed directory-by-directory tour of the custom-browser project's code organization, including both the top-level project structure and the Chromium `src/` tree.\r\n\r\n---\r\n\r\n## Project Root Directories\r\n\r\n### Configuration & Setup Files\r\n\r\n- **`.gclient`** – Controls Chromium source synchronization and dependency management\r\n- **`.gitignore`** – Specifies files to ignore in version control\r\n- **`package.json`** – Project metadata, npm scripts, and custom project configurations\r\n- **`package-lock.json`** – Locked npm dependency versions for reproducible builds\r\n- **`requirements.txt`** – Python package dependencies for development tools\r\n- **`.npmrc`** – npm registry settings and configuration\r\n\r\n### Development Support Directories\r\n\r\n#### `lib/` – Python Development Utilities\r\n- **`__init__.py`** – Python package initialization file\r\n- **`logger.py`** – Advanced colored console logging with progress bars and spinners\r\n- **`logger_demo.py`** – Interactive demonstration of logging system capabilities\r\n- **`utils.py`** – Common utility functions used across development scripts\r\n- **`__pycache__/`** – Python bytecode cache (auto-generated)\r\n\r\n#### `scripts/` – Automation & Build Scripts\r\n- **`init.py`** – Main project initialization script that handles dependency fetching\r\n- **`av/`** – Additional automation and validation scripts for development workflow\r\n\r\n#### `patches/` – Chromium Modifications\r\n- Contains custom patches and modifications to upstream Chromium code\r\n- Organized patches for specific features or bug fixes\r\n- Used when direct source modifications aren't feasible\r\n\r\n#### `docs/` – Project Documentation\r\n- Currently empty but designated for project-specific documentation\r\n- Future home for custom browser feature documentation and development guides\r\n\r\n---\r\n\r\n## The `src/` Directory Structure\r\n\r\nAfter running `npm run init`, the Chromium source code is synchronized into the `src/` directory. Here's a comprehensive breakdown:\r\n\r\n### Core Browser Components\r\n\r\n#### `chrome/` – Browser Shell & UI\r\n- **`app/`** – Application entry points and main() functions\r\n- **`browser/`** – Browser process logic, UI controllers, and feature implementations\r\n- **`common/`** – Shared code between browser and renderer processes\r\n- **`renderer/`** – Renderer process specific code and extensions\r\n- **`test/`** – Chrome-specific test utilities and test data\r\n- **Platform directories**: `android/`, `ios/`, `mac/`, `win/`, `linux/` for platform-specific code\r\n\r\n#### `content/` – Core Browser Engine\r\n- **`browser/`** – Browser process implementation (navigation, resource loading, IPC)\r\n- **`renderer/`** – Renderer process implementation (DOM, JavaScript execution)\r\n- **`common/`** – Shared interfaces and utilities between processes\r\n- **`gpu/`** – GPU process integration code\r\n- **`utility/`** – Utility process implementations\r\n- **`public/`** – Public APIs for embedders\r\n\r\n#### `custom/` – **Our Custom Modifications** 🎯\r\n- **Custom-core project** with our browser enhancements and modifications\r\n- Contains project-specific features and customizations\r\n- Organized to minimize conflicts with upstream Chromium updates\r\n\r\n### Rendering & Graphics\r\n\r\n#### `cc/` – Compositor & Rendering Pipeline\r\n- **`layers/`** – Layer tree implementation for hardware acceleration\r\n- **`trees/`** – Layer tree host and commit/activation logic\r\n- **`animation/`** – Animation system integration\r\n- **`raster/`** – Tile rasterization and GPU texture management\r\n\r\n#### `gpu/` – Graphics Processing\r\n- **`command_buffer/`** – GPU command buffer implementation\r\n- **`config/`** – GPU driver and capability detection\r\n- **`ipc/`** – Inter-process communication for GPU operations\r\n- **`vulkan/`** – Vulkan graphics API integration\r\n\r\n#### `ui/` – Cross-Platform UI Toolkit\r\n- **`base/`** – Fundamental UI primitives and utilities\r\n- **`views/`** – Native widget toolkit abstraction\r\n- **`gfx/`** – 2D graphics, fonts, and image handling\r\n- **`events/`** – Input event handling and dispatch\r\n- **Platform directories**: `gtk/`, `win/`, `cocoa/`, `ozone/` for platform UI\r\n\r\n### Networking & Communication\r\n\r\n#### `net/` – Networking Stack\r\n- **`http/`** – HTTP/HTTPS protocol implementation\r\n- **`quic/`** – QUIC protocol support\r\n- **`dns/`** – DNS resolution and caching\r\n- **`cookies/`** – Cookie storage and management\r\n- **`proxy/`** – Proxy detection and configuration\r\n- **`ssl/`** – SSL/TLS certificate handling\r\n\r\n#### `ipc/` – Inter-Process Communication\r\n- **Core IPC** – Message passing between browser processes\r\n- **Mojo integration** – Modern IPC system built on Mojo\r\n- **Security boundaries** – Process isolation and sandboxing support\r\n\r\n### Core Infrastructure\r\n\r\n#### `base/` – Fundamental Utilities\r\n- **`containers/`** – Custom container classes and algorithms\r\n- **`memory/`** – Memory management utilities and smart pointers\r\n- **`task/`** – Task scheduling and thread pool management\r\n- **`files/`** – File system operations and path handling\r\n- **`strings/`** – String utilities and manipulation functions\r\n\r\n#### `build/` – Build System Configuration\r\n- **GN files** – Build target definitions and dependencies\r\n- **Config files** – Compiler flags, toolchain settings\r\n- **Scripts** – Build automation and CI/CD integration\r\n\r\n#### `tools/` – Development Tools\r\n- **`gn/`** – Generate Ninja build system\r\n- **`clang/`** – Clang-based tools (format, tidy, static analysis)\r\n- **`metrics/`** – Performance and usage metrics collection\r\n- **`variations/`** – A/B testing and feature flag infrastructure\r\n\r\n### Web Technologies\r\n\r\n#### `third_party/` – External Dependencies\r\n- **`blink/`** – Web rendering engine (WebKit fork)\r\n- **`v8/`** – JavaScript engine\r\n- **`skia/`** – 2D graphics library\r\n- **`webrtc/`** – Real-time communication\r\n- **`protobuf/`** – Protocol buffer serialization\r\n- **`zlib/`**, **`libpng/`**, **`libjpeg/`** – Media format libraries\r\n\r\n#### `components/` – Reusable Feature Modules\r\n- **`autofill/`** – Form auto-completion functionality\r\n- **`bookmarks/`** – Bookmark management\r\n- **`history/`** – Browsing history storage and search\r\n- **`password_manager/`** – Password storage and auto-fill\r\n- **`payments/`** – Web payments API implementation\r\n- **`sync/`** – Cross-device data synchronization\r\n\r\n### Security & Sandboxing\r\n\r\n#### `sandbox/` – Security Isolation\r\n- **Platform-specific** – Windows, macOS, Linux sandboxing implementations\r\n- **Policy files** – Security policy definitions and enforcement\r\n- **IPC restrictions** – Communication constraints between processes\r\n\r\n#### `crypto/` – Cryptographic Operations\r\n- **Encryption/decryption** – Symmetric and asymmetric cryptography\r\n- **Hashing** – Secure hash algorithms\r\n- **Certificate handling** – X.509 certificate validation\r\n\r\n### Services & Extensions\r\n\r\n#### `services/` – Mojo-Based Services\r\n- **`network/`** – Network service for process isolation\r\n- **`storage/`** – Storage APIs (IndexedDB, Cache API, etc.)\r\n- **`device/`** – Hardware device access APIs\r\n- **`media/`** – Media capture and playback services\r\n\r\n#### `extensions/` – Browser Extensions\r\n- **`api/`** – Extension API implementations\r\n- **`browser/`** – Extension host and management\r\n- **`renderer/`** – Extension content script injection\r\n\r\n### Build Outputs\r\n\r\n#### `out/` – Compilation Results\r\n- **`Default/`** – Default build configuration output\r\n  - **`obj/`** – Intermediate object files and libraries\r\n  - **`chrome.exe`** – Main browser executable (Windows)\r\n  - **`*.dll`** – Dynamic libraries (Windows component builds)\r\n  - **`resources/`** – Packaged resources and assets\r\n  - **`locales/`** – Internationalization files\r\n\r\n---\r\n\r\n## Directory Relationships & Dependencies\r\n\r\n### Layer Architecture\r\n```\r\n┌─────────────────────────────────────┐\r\n│ chrome/ (Browser Shell & Features)  │\r\n├─────────────────────────────────────┤\r\n│ components/ (Reusable Modules)      │\r\n├─────────────────────────────────────┤\r\n│ content/ (Browser Engine)           │\r\n├─────────────────────────────────────┤\r\n│ third_party/ (External Libraries)   │\r\n├─────────────────────────────────────┤\r\n│ base/ (Fundamental Utilities)       │\r\n└─────────────────────────────────────┘\r\n```\r\n\r\n### Custom Integration Points\r\n- **`custom/`** integrates at the `chrome/` and `components/` level\r\n- **`lib/`** and **`scripts/`** support development workflow\r\n- **`patches/`** modifies any level when direct changes aren't possible\r\n\r\n### Development Workflow Directories\r\n1. **Start here**: `lib/logger_demo.py` to understand tooling\r\n2. **Project setup**: `scripts/init.py` for dependency management\r\n3. **Custom features**: `src/custom/` for browser modifications\r\n4. **Build configuration**: `src/build/` for compilation settings\r\n5. **Output inspection**: `src/out/Default/` for built artifacts\r\n\r\n---\r\n\r\n**See also:**\r\n- [Project Layout](project-layout.md) for high-level project organization\r\n- [Architecture → Browser Components](../architecture/browser-components.md) for runtime component relationships\r\n- [Setup & Build](setup-build.md) for development environment configurationructure"
  },
  {
    "path": "development/rust",
    "title": "Rust in Chromium",
    "content": "# Rust in Chromium\n\n[TOC]\n\n# Why?\n\nHandling untrustworthy data in non-trivial ways is a major source of security\nbugs, and it's therefore against Chromium's security policies\n[to do it in the Browser or Gpu process](../docs/security/rule-of-2.md) unless\nyou are working in a memory-safe language.\n\nRust provides a cross-platform memory-safe language so that all platforms can\nhandle untrustworthy data directly from a privileged process, without the\nperformance overheads and complexity of a utility process.\n\n# Status\n\nThe Rust toolchain is enabled for and supports all platforms and development\nenvironments that are supported by the Chromium project. The first milestone\nto include full production-ready support was M119.\n\nRust can be used anywhere in the Chromium repository (not just `//third_party`)\nsubject to [current interop capabilities][interop-rust-doc], however it is\ncurrently subject to a internal approval and FYI process. Googlers can view\ngo/chrome-rust for details. New usages of Rust are documented at\n[`rust-fyi@chromium.org`](https://groups.google.com/a/chromium.org/g/rust-fyi).\n\nFor questions or help, reach out to `rust-dev@chromium.org` or `#rust` on the\n[Chromium Slack](https://www.chromium.org/developers/slack/).\n\nIf you use VSCode, we have [additional advice below](#using-vscode).\n\n# Adding a third-party Rust library\n\nThird-party libraries are pulled from [crates.io](https://crates.io), but\nChromium does not use Cargo as a build system.\n\n## Third-party review\n\nAll third-party crates need to go through third-party review. See\n[//docs/adding_to_third_party.md](adding_to_third_party.md) for instructions on\nhow to have a library reviewed.\n\n## Importing a crate from crates.io\n\nThe `//third_party/rust/chromium_crates_io/Cargo.toml` file defines the set of crates\ndepended on from first-party code. Any transitive dependencies will be found\nfrom those listed there. The file is a [standard `Cargo.toml` file](\nhttps://doc.rust-lang.org/cargo/reference/manifest.html), though the crate\nitself is never built, it is only used to collect dependencies through the\n`[dependencies]` section.\n\nTo use a third-party crate \"bar\" version 3 from first party code:\n1. Change directory to the root `src/` dir of Chromium.\n1. Add the crate to `//third_party/rust/chromium_crates_io/Cargo.toml`:\n   * `vpython3 ./tools/crates/run_gnrt.py add foo` to add the latest version of `foo`.\n   * `vpython3 ./tools/crates/run_gnrt.py add foo@1.2.3` to add a specific version of `foo`.\n   * Or, directly through (nightly) cargo:\n     `cargo run --release --manifest-path tools/crates/gnrt/Cargo.toml --target-dir out/gnrt add foo`\n   * Or, edit the Cargo.toml by hand, finding the version you want from [crates.io](https://crates.io).\n1. Download the crate's files:\n   * `./tools/crates/run_gnrt.py vendor` to download the new crate.\n   * Or, directly through (nightly) cargo:\n     `cargo run --release --manifest-path tools/crates/gnrt/Cargo.toml --target-dir out/gnrt vendor`\n   * This will also apply any patches in `//third_party/rust/chromium_crates_io/patches`\n     for the crates. If a patch can not apply, the crate's download will be cancelled and\n     an error will be printed. See [patching errors](#patching-errors) below for how to resolve\n     this.\n1. (optional) If the crate is only to be used by tests and tooling, then\n   specify the `\"test\"` group in `//third_party/rust/chromium_crates_io/gnrt_config.toml`:\n   ```\n   [crate.foo]\n   group = \"test\"\n   ```\n1. Generate the `BUILD.gn` file for the new crate:\n   * `vpython3 ./tools/crates/run_gnrt.py gen`\n   * Or, directly through (nightly) cargo:\n     `cargo run --release --manifest-path tools/crates/gnrt/Cargo.toml --target-dir out/gnrt gen`\n1. Verify if all new dependencies are already audited by running `cargo vet`\n   See [`rust-unsafe.md#cargo-vet-policy`](rust-unsafe.md#cargo-vet-policy) for\n   more details.  This boils down to:\n   * `./tools/crates/run_cargo_vet.py check`\n   * If `check` fails, then there are missing audits, which need to be added to\n     `//third_party/rust/chromium_crates_io/supply-chain/audits.toml`.\n1. Add the new files to git:\n   * `git add -f third_party/rust/chromium_crates_io/vendor`.\n     (The `-f` is important, as files may be skipped otherwise from a\n     `.gitignore` inside the crate.)\n   * `git add third_party/rust`\n1. Upload the CL. If there is any `unsafe` usage then Security experts will need to\n   audit the \"ub-risk\" level.  See\n   [`rust-unsafe.md#code-review-policy`](rust-unsafe.md#code-review-policy) for\n   more details.\n\n### Cargo features\n\nTo enable a feature \"spaceships\" in the crate, change the entry in\n`//third_party/rust/chromium_crates_io/Cargo.toml` to include the feature:\n```toml\n[dependencies]\nbar = { version = \"3\", features = [ \"spaceships\" ] }\n```\n\n### Patching third-party crates\n\nYou may patch a crate in tree, but save any changes made into a diff file in\na `//third_party/rust/chromium_crates_io/patches/` directory for the crate.\nThe diff file should be generated by `git-format-patch` each new patch numbered\nconsecutively so that they can be applied in order. For example, these files\nmight exist if the \"foo\" crate was patched with a couple of changes:\n\n```\n//third_party/rust/chromium_crates_io/patches/foo/patches/0001-Edit-the-Cargo-toml.diff\n//third_party/rust/chromium_crates_io/patches/foo/patches/0002-Other-changes.diff\n```\n\nThe recommended procedure to create such patches is:\n\n1. Commit the plain new version of the crate to your local git branch\n2. Modify the crate as necessary\n3. Commit that modified version\n4. Use `git format-patch <unpatched version>` to generate the patch files\n5. Add the patch files in a new, third, commit\n6. Squash them, or rely on `git cl upload` doing so\n\n#### Patching errors\n\nIf `gnrt vendor` fails to apply a patch for a crate, it will cancel the download of that\ncrate rather than leave it in a broken state. To recreate patches, first get a pristine\ncopy of the crate by using the `--no-patches` argument:\n\n1. Download the crate without applying patches:\n   * `vpython3 ./tools/crates/run_gnrt.py vendor --no-patches=<CRATE_NAME>`\n2. Then recreate the patches as described in [Patching third-party crates](\n   #patching-third_party-crates).\n\nTo verify the patches work, remove the vendored crate directory in\n`//third_party/rust/chromium_crates_io/vendor/`, named after the crate name\nand version. Then run the `vendor` action without `--no-patches` which will\ndownload the crate and apply the patches:\n   * `vpython3 ./tools/crates/run_gnrt.py vendor`\n\n## Security\n\nIf a shipping library needs security review (has any `unsafe`), and the review\nfinds it's not satisfying the [rule of 2](../docs/security/rule-of-2.md), then\nmove it to the `\"sandbox\"` group in `//third_party/rust/chromium_crates_io/gnrt_config.toml`\nto make it clear it can't be used in a privileged process:\n```\n[crate.foo]\ngroup = \"sandbox\"\n```\n\nIf a transitive dependency moves from `\"safe\"` to `\"sandbox\"` and causes\na dependency chain across the groups, it will break the `gnrt vendor` step.\nYou will need to fix the new crate so that it's deemed safe in unsafe review,\nor move the other dependent crates out of `\"safe\"` as well by setting their\ngroup in `gnrt_config.toml`.\n\n# Updating existing third-party crates\n\nThird-party crates will get updated semi-automatically through the process\ndescribed in\n[`../tools/crates/create_update_cl.md`](../tools/crates/create_update_cl.md).\nIf you nevertheless need to manually update a crate to its latest minor\nversion, then follow the steps below:\n\n1. Change directory to the root `src/` dir of Chromium.\n1. Update the versions in `//third_party/rust/chromium_crates_io/Cargo.toml`.\n   * `vpython3 ./tools/crates/run_gnrt.py update <crate name>`\n   * Or, directly through (nightly) cargo:\n     `cargo run --release --manifest-path tools/crates/gnrt/Cargo.toml --target-dir out/gnrt update <crate name>`\n1. Download any updated crate's files:\n   * `./tools/crates/run_gnrt.py vendor`\n   * If you want to restrict the update to certain crates, add the crate names\n     as arguments to `vendor`, like: `./tools/crates/run_gnrt.py vendor\n     <crate-name>`\n   * Or, directly through (nightly) cargo:\n     `cargo run --release --manifest-path tools/crates/gnrt/Cargo.toml --target-dir out/gnrt vendor`\n1. Add the downloaded files to git:\n   * `git add -f third_party/rust/chromium_crates_io/vendor`\n   * The `-f` is important, as files may be skipped otherwise from a\n     `.gitignore` inside the crate.\n1. Generate the `BUILD.gn` files\n   * `vpython3 ./tools/crates/run_gnrt.py gen`\n   * Or, directly through (nightly) cargo:\n     `cargo run --release --manifest-path tools/crates/gnrt/Cargo.toml --target-dir out/gnrt gen`\n1. Add the generated files to git:\n   * `git add -f third_party/rust`\n\n### Directory structure for third-party crates\n\nThe directory structure for a crate \"foo\" version 3.4.2 is:\n```\n//third_party/\n    rust/\n        foo/  (for the \"foo\" crate)\n            v3/  (version 3.4.2 maps to the v3 epoch)\n                BUILD.gn  (generated by gnrt gen)\n                README.chromium  (generated by gnrt vendor)\n        chromium_crates_io/\n            vendor/\n                foo-3.4.2  (crate sources downloaded from crates.io)\n            patches/\n                foo/  (patches for the \"foo\" crate)\n                    0001-Edit-the-Cargo-toml.diff\n                    0002-Other-changes.diff\n            Cargo.toml\n            Cargo.lock\n            gnrt_config.toml\n```\n\n## Writing a wrapper for binding generation\n\nMost Rust libraries will need a more C++-friendly API written on top of them in\norder to generate C++ bindings to them. The wrapper library can be placed\nin `//third_party/rust/<cratename>/<epoch>/wrapper` or at another single place\nthat all C++ goes through to access the library. The [CXX](https://cxx.rs) is\nused to generate bindings between C++ and Rust.\n\nSee\n[`//third_party/rust/serde_json_lenient/v0_1/wrapper/`](\nhttps://source.chromium.org/chromium/chromium/src/+/main:third_party/rust/serde_json_lenient/v0_1/wrapper/)\nand\n[`//components/qr_code_generator`](\nhttps://source.chromium.org/chromium/chromium/src/+/main:components/qr_code_generator/;l=1;drc=b185db5d502d4995627e09d62c6934590031a5f2)\nfor examples.\n\nRust libraries should use the\n[`rust_static_library`](\nhttps://source.chromium.org/chromium/chromium/src/+/main:build/rust/rust_static_library.gni)\nGN template (not the built-in `rust_library`) to integrate properly into the\nmixed-language Chromium build and get the correct compiler options applied to\nthem.\n\nThe [CXX](https://cxx.rs) tool is used for generating C++ bindings to Rust\ncode. Since it requires explicit declarations in Rust, an wrapper shim around a\npure Rust library is needed. Add these Rust shims that contain the CXX\n`bridge` macro to the `cxx_bindings` GN variable in the `rust_static_library`\nto have CXX generate a C++ header for that file. To include the C++ header\nfile, rooted in the `gen` output directory, use\n```\n#include \"the/path/to/the/rust/file.rs.h\"\n```\n\n# Logging\n\nUse the [log](https://docs.rs/log) crate's macros in place of base `LOG`\nmacros from C++. They do the same things. The `debug!` macro maps to\n`DLOG(INFO)`, the `info!` macro maps to `LOG(INFO)`, and `warn!` and `error!`\nmap to `LOG(WARNING)` and `LOG(ERROR)` respectively. The additional `trace!`\nmacro maps to `DLOG(INFO)` (but there is [WIP to map it to `DVLOG(INFO)`](\nhttps://chromium-review.googlesource.com/c/chromium/src/+/5996820)).\n\nNote that the standard library also includes a helpful\n[`dbg!`](https://doc.rust-lang.org/std/macro.dbg.html) macro which writes\neverything about a variable to `stderr`.\n\nLogging may not yet work in component builds:\n[crbug.com/374023535](https://crbug.com/374023535).\n\n# Tracing\n\nTODO: [crbug.com/377915495](https://crbug.com/377915495).\n\n# Strings\n\nPrefer to use [`BString`](https://docs.rs/bstr/latest/bstr/struct.BString.html)\nand [`BStr`](https://docs.rs/bstr/latest/bstr/struct.BStr.html) to work with\nstrings in first-party code instead of `std::String` and `str`. These types do\nnot require the strings to be valid UTF-8, and avoid error handling or panic\ncrashes when working with strings from C++ and/or from the web. Because the\nweb is not UTF-8 encoded, many strings in Chromium are also not.\n\nIn cross-language bindings, `&[u8]` can be used to represent a string until\nnative support for `BStr` is available in our interop tooling. A `u8` slice\ncan be converted to `BStr` or treated as a string with\n[`ByteSlice`](https://docs.rs/bstr/latest/bstr/trait.ByteSlice.html).\n\n# Using VSCode\n\n1. Ensure you're using the `rust-analyzer` extension for VSCode, rather than\n   earlier forms of Rust support.\n2. Run `gn` with the `--export-rust-project` flag, such as:\n   `gn gen out/Release --export-rust-project`.\n3. `ln -s out/Release/rust-project.json rust-project.json`\n4. When you run VSCode, or any other IDE that uses\n   [rust-analyzer](https://rust-analyzer.github.io/) it should detect the\n   `rust-project.json` and use this to give you rich browsing, autocompletion,\n   type annotations etc. for all the Rust within the Chromium codebase.\n5. Point rust-analyzer to the rust toolchain in Chromium. Otherwise you will\n   need to install Rustc in your system, and Chromium uses the nightly\n   compiler, so you would need that to match. Add the following to\n   `.vscode/settings.json` in the Chromium checkout:\n   ```\n   {\n      // The rest of the settings...\n\n      \"rust-analyzer.cargo.extraEnv\": {\n        \"PATH\": \"../../third_party/rust-toolchain/bin:$PATH\",\n      }\n   }\n   ```\n   This assumes you are working with an output directory like `out/Debug` which\n   has two levels; adjust the number of `..` in the path according to your own\n   setup.\n\n# Using cargo\n\nIf you are building a throwaway or experimental tool, you might like to use pure\n`cargo` tooling rather than `gn` and `ninja`. Even then, you may choose\nto restrict yourself to the toolchain and crates that are already approved for\nuse in Chromium.\n\nHere's how.\n\n```\nexport PATH_TO_CHROMIUM_SRC=~/chromium/src\nmkdir my-rust-tool\ncd my-rust-tool\nmkdir .cargo\ncat <<END > .cargo/config.toml\n[source.crates-io]\nreplace-with = \"vendored-sources\"\n\n[source.vendored-sources]\ndirectory = \"$PATH_TO_CHROMIUM_SRC/third_party/rust/chromium_crates_io/vendor\"\nEND\n$PATH_TO_CHROMIUM_SRC/third_party/rust-toolchain/bin/cargo init --offline\n$PATH_TO_CHROMIUM_SRC/third_party/rust-toolchain/bin/cargo run --offline\n```\n\nMost `cargo` tooling works well with this setup; one exception is `cargo add`,\nbut you can still add dependencies manually to your `Cargo.toml`:\n\n```\n[dependencies]\nlog = \"0.4\"\n```\n\n[interop-rust-doc]: https://docs.google.com/document/d/1kvgaVMB_isELyDQ4nbMJYWrqrmL3UZI4tDxnyxy9RTE/edit?tab=t.0#heading=h.fpqr6hf3c3j0\n"
  },
  {
    "path": "development/rust-unsafe",
    "title": "`unsafe` Rust Guidelines",
    "content": "# `unsafe` Rust Guidelines\n\n## Code Review Policy {#code-review-policy}\n\nAll `unsafe` Rust code in Chromium needs to be reviewed and LGTM-ed by a member\nof the `unsafe-rust-in-chrome@google.com` group and the review must be cc'd to\nthe group for visibility.  This policy applies to both third-party code\n(e.g. under `//third_party/rust`) and first-party code.\n\n### How to request a review\n\nTo facilitate a code review please:\n\n* For each new or modified `unsafe` block, function, `impl`, etc.,\n  add an unresolved \"TODO: `unsafe` review\" comment in Gerrit.\n  You can consider using `tools/crates/create_draft_comments.py` to streamline\n  creating such comments.\n\n* Add `chrome-unsafe-rust-reviews@google.com` as a reviewer.\n\n### Scope of review\n\nNote that changes _anywhere_ in a crate that uses `unsafe` blocks may violate\nthe internal invariants on which those `unsafe` blocks rely. It is unrealistic\nto require a `unsafe-rust-in-chrome@google.com` review to re-audit all the\n`unsafe` blocks each time a crate is updated, but the crate `OWNERS` and other\nreviewers should be on the lookout for code changes which feel as though they\ncould affect invariants on which `unsafe` blocks rely.\n\n### `OWNERS` files guidance\n\nTo require `unsafe` review for certain `.rs` files\n(e.g. ones that use `unsafe` Rust)\nyou can forward from the file's `OWNERS` to\n`//third_party/rust/UNSAFE_RUST_OWNERS`\n(see comments in the latter for more details).\n\n### Soft SLA\n\nFor incremental changes (including updating a minor version of a crate under\n`//third_party/rust/chromium_crates_io`) the usual [Chromium responsiveness\nexpectations](cl_respect.md#expect-responsiveness) apply. (i.e. You should expect\nreviewer input within 1 business day.)\n\nFor bulk changes (e.g. importing a new crate and its transitive dependencies)\nthe turnaround time may be longer.  This depends mostly on the amount of\n`unsafe` code.  To streamline reviews and future maintainability, we ask you\nkindly to prefer crates that do *not* use `unsafe` Rust code.\n\n### Other notes\n\nBugs that track streamlining application of this policy are tracked under\nthe umbrella of https://crbug.com/393394872/dependencies.\n\n## `cargo vet` Policy {#cargo-vet-policy}\n\nCrates in `//third_party/rust/chromium_crates_io` need to be covered by `cargo\nvet` audits.  In other words, `tools/crates/run_cargo_vet.py check` should\nalways succeed (this is enforced by `//third_party/rust/PRESUBMIT.py`).\n\n### Audit criteria required for most crates\n\nAudit criteria required for a given crate depend on how the crate is used.  The\ncriteria are written to\n`third_party/rust/chromium_crates_io/supply-chain/config.toml` by\n`tools/crates/run_gnrt.py vendor` based on whether\n`third_party/rust/chromium_crates_io/gnrt_config.toml` declares that the crate\nis meant to be used (maybe transitively) in a `safe`, `sandbox`, or `test`\nenvironment.  For example, to declare that a crate is `safe` to be used in the\nbrowser process, it needs to be audited and certified to be `safe-to-deploy`,\n`ub-risk-2` or lower, and either `does-not-implement-crypto` or `crypto-safe`.\n\nNote that some audits can be done by any engineer (\"ub-risk-0\" and\n\"safe-to-run\") while others will require specialists from the\n`unsafe-rust-in-chrome@google.com` group (see the [\"Code Review Policy\"\nabove](#code-review-policy).  More details about audit criteria and the required\nexpertise are explained in the\n[auditing_standards.md](https://github.com/google/rust-crate-audits/blob/main/auditing_standards.md),\nwhich also provides guidance for conducting delta audits.\n\n### Some crates don't require an audit\n\nChromium implicitly trusts certain crate publishers.  Currently\nthere are two scenarios where such trust relationship may be established:\n\n* Trusting crates authored and maintained under https://github.com/rust-lang/\n  (e.g. `libc`, `hashbrown`), because they are closely related to the Rust\n  toolchain (i.e. the same group managed and publishes `rustc`,\n  `rustfmt`, `cargo`, `rustup`, etc.).\n* Trusting crates that are part of an OS SDK (e.g. `windows-...` crates).\n\nChromium uses both our own audits\n(stored in `third_party/rust/chromium_crates_io/supply-chain/audits.toml`)\nas well as audits imported from other parts of Google\n(e.g. Android, Fuchsia, etc.).  This means that adding a new crate does not\nnecessarily require a new audit if the crate has already been audited by\nother projects (in this case, `cargo vet` will record the imported audit\nin the `third_party/rust/chromium_crates_io/supply-chain/imports.lock` file).\n\n### How to run `cargo vet` in Chromium\n\nSee\n[Cargo Vet documentation](https://mozilla.github.io/cargo-vet/recording-audits.html)\nfor how to record the audit in `audits.toml`.\nThe `tools/crates/run_cargo_vet.py` may be used to invoke Chromium's copy of\n`cargo-vet`.\n\n"
  },
  {
    "path": "development/README",
    "title": "Development Documentation",
    "content": "# Development Documentation\r\n\r\nThis section contains documentation for developing Chromium, including build instructions, development tools, workflows, and best practices.\r\n\r\n## Build System\r\n\r\n### [Build Instructions](build/)\r\nPlatform-specific build instructions and configuration.\r\n\r\n- [Windows Build Instructions](build/windows_build_instructions.md)\r\n- [Mac Build Instructions](build/mac_build_instructions.md)\r\n- [iOS Build Instructions](build/ios_build_instructions.md)\r\n- [Android Build Instructions](build/android_build_instructions.md)\r\n- [ChromeOS Build Instructions](build/chromeos_build_instructions.md)\r\n\r\n## Development Tools\r\n\r\n### Code Analysis & Formatting\r\n- [Clang](clang.md) - Compiler setup and usage\r\n- [Clang Format](clang_format.md) - Code formatting\r\n- [Clang Tidy](clang_tidy.md) - Static analysis\r\n- [ClangD](clangd.md) - Language server for IDEs\r\n\r\n### Version Control\r\n- [Git Cookbook](git_cookbook.md) - Common Git workflows\r\n- [Git Tips](git_tips.md) - Advanced Git techniques\r\n- [Git Submodules](git_submodules.md) - Working with submodules\r\n\r\n### Code Review\r\n- [Code Reviews](code_reviews.md) - Code review process and best practices\r\n\r\n## Modern Language Support\r\n\r\n### Rust Integration\r\n- [Rust in Chromium](rust.md) - Using Rust in Chromium\r\n- [Rust Unsafe Guidelines](rust-unsafe.md) - Safety guidelines for unsafe Rust\r\n\r\n## Testing\r\n\r\n### [Testing Framework](testing/)\r\nComprehensive testing documentation including unit tests, integration tests, and testing best practices.\r\n\r\n## Development Workflows\r\n\r\n### Code Quality\r\n- Follow the [Coding Standards](../contributing/contributing.md#coding-conventions--style)\r\n- Use [Static Analysis Tools](clang_tidy.md)\r\n- Implement [Comprehensive Testing](testing/)\r\n\r\n### Performance\r\n- [Profiling Tools](../performance/)\r\n- [Optimization Guidelines](../performance/)\r\n\r\n## Related Documentation\r\n\r\n- [Contributing Guide](../contributing/contributing.md)\r\n- [Architecture Overview](../architecture/)\r\n- [Security Guidelines](../security/)\r\n- [Platform-Specific Docs](../platforms/)\r\n"
  },
  {
    "path": "development/overview",
    "title": "Development Workflow Overview",
    "content": "# Development Workflow Overview\r\n\r\nEffective Chromium development requires mastering a comprehensive set of tools, processes, and best practices. This section provides everything you need to become a productive Chromium contributor.\r\n\r\n## 🎯 What You'll Learn\r\n\r\n- **Version Control**: Advanced Git workflows for large codebases\r\n- **Code Quality**: Tools and practices for maintaining high code quality\r\n- **Review Process**: How to participate in Chromium's code review culture\r\n- **Language Guidelines**: Best practices for C++, Rust, and other languages\r\n\r\n## 📋 Development Essentials\r\n\r\n### Version Control Mastery\r\n- [Git Cookbook](git_cookbook) - Common Git operations and workflows\r\n- [Git Tips](git_tips) - Advanced Git techniques for Chromium development\r\n- [Git Submodules](git_submodules) - Working with Chromium's submodule dependencies\r\n\r\n### Code Review & Collaboration\r\n- [Code Reviews](code_reviews) - Chromium's code review process and best practices\r\n\r\n### Code Quality Tools\r\n- [Clang Overview](clang) - Understanding Chromium's primary compiler\r\n- [ClangD Setup](clangd) - IDE integration for better development experience\r\n- [Clang Format](clang_format) - Automated code formatting\r\n- [Clang Tidy](clang_tidy) - Static analysis and code improvement suggestions\r\n- [Clang Static Analyzer](clang_static_analyzer) - Advanced static analysis tools\r\n\r\n### Language-Specific Guidelines\r\n- [Rust in Chromium](rust) - Using Rust in the Chromium codebase\r\n- [Rust Unsafe Guidelines](rust-unsafe) - Best practices for unsafe Rust code\r\n\r\n### Maintenance & Operations\r\n- [Clang Gardening](clang_gardening) - Maintaining the build infrastructure\r\n- [Clang Sheriffing](clang_sheriffing) - Monitoring and maintaining code quality\r\n\r\n## 🛠️ Tool Categories\r\n\r\n### **Essential Daily Tools**\r\n- Git (version control)\r\n- Clang (compiler and toolchain)\r\n- Code review system\r\n- Build system (GN/Ninja)\r\n\r\n### **Code Quality Tools**\r\n- ClangFormat (formatting)\r\n- ClangTidy (linting)\r\n- Static analyzers\r\n- Coverage tools\r\n\r\n### **Advanced Tools**\r\n- Refactoring tools\r\n- Performance profilers\r\n- Memory analyzers\r\n- Security scanners\r\n\r\n## 🚀 Getting Started\r\n\r\n1. **Start Here**: [Git Cookbook](git_cookbook) - Master the basics\r\n2. **Next**: [Code Reviews](code_reviews) - Understand the review process  \r\n3. **Then**: [Clang Overview](clang) - Set up your development environment\r\n4. **Finally**: Choose language-specific guides based on your work\r\n\r\n## 🎯 Success Metrics\r\n\r\nAfter completing this section, you should be able to:\r\n- ✅ Navigate the Chromium codebase efficiently\r\n- ✅ Submit well-formatted, high-quality patches\r\n- ✅ Participate effectively in code reviews\r\n- ✅ Use debugging and profiling tools confidently\r\n- ✅ Follow Chromium's coding standards and conventions\r\n\r\n## 🔗 Related Sections\r\n\r\n- [🧪 Testing & QA](testing/testing_in_chromium) - Quality assurance practices\r\n- [⚡ Performance](../performance/profiling) - Performance optimization tools\r\n- [🤝 Contributing](../contributing/contributing) - How to contribute to Chromium\r\n"
  },
  {
    "path": "development/git_tips",
    "title": "Git Tips",
    "content": "# Git Tips\n\nWhen using Git, there are a few tips that are particularly useful when working\non the Chromium codebase, especially due to its size.\n\n[TOC]\n\n## Remember the basic git convention:\n\n    git COMMAND [FLAGS] [ARGUMENTS]\n\nVarious git commands have underlying executable with a hyphenated name, such as\n`git-grep`, but these can also be called via the `git` wrapper script as\n`git grep` (and `man` should work either way too).\n\n## Git references\n\nThe following resources can provide background on how Git works:\n\n*   [Git-SVN Crash Course](http://git-scm.com/course/svn.html) -- this crash\n    course is useful for Subversion users switching to Git.\n*   [Think Like (a) Git](http://think-like-a-git.net/) -- does a great job of\n    explaining the main purpose of Git operations.\n*   [Git User's Manual](http://schacon.github.com/git/user-manual.html) -- a\n    great resource to learn more about ho to use Git properly.\n*   [A Visual Git Reference](https://marklodato.github.io/visual-git-guide/index-en.html)\n    -- a resource that explains various Git operations for visual learners.\n*   [Git Cheat Sheet](http://cheat.errtheblog.com/s/git) -- now that you\n    understand Git, here's a cheat sheet to quickly remind you of all the\n    commands you need.\n\n## Optimizing (Speeding up) Git for a Large Repository\n\nGit has numerous options, among which some are intended to optimize for large\nrepositories.\n[feature.manyFiles](https://git-scm.com/docs/git-config#Documentation/git-config.txt-featuremanyFiles)\nis a convenient option that turns on the group of options that optimize for\nlarge repositories. Run the following inside the Chromium git repository:\n\n    git config feature.manyFiles true\n\n## Configuring the output of \"git log\"\n\nBy default, the date that \"git log\" displays is the \"author date.\" In Chromium,\nthis generally corresponds to the date that the committed patch was last\nuploaded. In most cases, however, the date that is of interest is the date that\nthe patch was committed in the tree. To configure \"git log\" to instead display\nthe latter date for your Chromium checkout, execute the following command:\n\n```shell\ngit config format.pretty 'format:%C(auto,yellow)commit %H%C(auto)%d%nAuthor:    %an <%ae>%nCommitted: %cd%n%n%w(0,4,4)%B%-%n'\n```\n\nIf you want to change *all* your repos (e.g., because you have multiple Chromium\ncheckouts and don't care about having the default for other repos), add\n\"--global\" after \"config\" in the above command.\n\n## Committing changes\n\nFor a simple workflow (always commit all changed files, don't keep local\nrevisions), the following script handles check; you may wish to call it `gci`\n(git commit) or similar.\n\nAmending a single revision is generally easier for various reasons, notably for\nrebasing and for checking that CLs have been committed. However, if you don't\nuse local revisions (a local branch with multiple revisions), you should make\nsure to upload revisions periodically to code review if you ever need to go to\nan old version of a CL.\n\n```bash\n#!/bin/bash\n# Commit all, amending if not initial commit.\nif git status | grep -q \"Your branch is ahead of 'origin/main' by 1 commit.\"\nthen\n  git commit --all --amend\nelse\n  git commit --all  # initial, not amendment\nfi\n```\n\n## Listing and changing branches\n\n```shell\ngit branch  # list branches\ngit checkout -  # change to last branch\n```\n\nTo quickly list the 5 most recent branches, add the following to `.gitconfig`\nin the `[alias]` section:\n\n```shell\nlast5 = \"!git for-each-ref --sort=committerdate refs/heads/ \\\n    --format='%(committerdate:short) %(refname:short)' | tail -5 | cut -c 12-\"\n```\n\nA nicely color-coded list, sorted in descending order by date, can be made by\nthe following bash function:\n\n```bash\ngit-list-branches-by-date() {\n  local current_branch=$(git rev-parse --symbolic-full-name --abbrev-ref HEAD)\n  local normal_text=$(echo -ne '\\E[0m')\n  local yellow_text=$(echo -ne '\\E[0;33m')\n  local yellow_bg=$(echo -ne '\\E[7;33m')\n  git for-each-ref --sort=-committerdate \\\n      --format=$'  %(refname:short)  \\\n          \\t%(committerdate:short)\\t%(authorname)\\t%(objectname:short)' \\\n          refs/heads \\\n      | column -t -s $'\\t' -n \\\n      | sed -E \"s:^  (${current_branch}) :* ${yellow_bg}\\1${normal_text} :\" \\\n      | sed -E \"s:^  ([^ ]+):  ${yellow_text}\\1${normal_text}:\"\n}\n```\n\n## Searching\n\nUse `git-grep` instead of `grep` and `git-ls-files` instead of `find`, as these\nsearch only files in the index or _tracked_ files in the work tree, rather than\nall files in the work tree.\n\nNote that `git-ls-files` is rather simpler than `find`, so you'll often need to\nuse `xargs` instead of `-exec` if you want to process matching files.\n\n## Global changes\n\nTo make global changes across the source tree, it's often easiest to use `sed`\nwith `git-ls-files`, using `-i` for in-place changing (this is generally safe,\nas we don't use symlinks much, but there are few places that do). Remember that\nyou don't need to use `xargs`, since sed can take multiple input files. E.g., to\nstrip trailing whitespace from C++ and header files:\n\n    sed -i -E 's/\\s+$//' $(git ls-files '*.cpp' '*.h')\n\n\nYou may also find `git-grep` useful for limiting the scope of your changes,\nusing `-l` for listing files.\n\n    sed -i -E '...' $(git grep -lw Foo '*.cpp' '*.h')\n\nRemember that you can restrict sed actions to matching (or non-matching) lines.\nFor example, to skip lines with a line comment, use the following:\n\n    '\\,//, ! s/foo/bar/g'\n\n## Diffs\n\n    git diff --shortstat\n\nDisplays summary statistics, such as:\n\n    2104 files changed, 9309 insertions(+), 9309 deletions(-)\n"
  },
  {
    "path": "development/git_submodules",
    "title": "Git submodules",
    "content": "# Git submodules\n\nA Git submodule is a Git repository inside another Git repository. Chromium\nproject doesn't rely on Git submodules directly. Instead, gclient sync is used\nto manage Git dependencies.\n\nIn 2023Q3, we started to move source of Git dependencies from DEPS files to Git\nsubmodules. While we do our best to hide complexities of submodules, some will\nbe exposed.\n\n[TOC]\n\n## A quick introduction to Git submoduldes\n\n[Git submodules](https://git-scm.com/docs/gitsubmodules) are managed via the\ncombination of `.gitmodules` files and gitlinks. `.gitmodules` is a text file\nthat configures submodules, and each submodule entry contains the path to the\nsubmodule's worktree and the URL of the submodule. Gitlink is a special type of\nfile in the Git database that tracks a submodule commit.\n\nYou can find an example of Git dependency below. Note that gclient-condition is\na custom property used by gclient and not git. It's identical to `condition` in\n`DEPS` and the allowed variables are defined in `vars = {` section of `DEPS`.\n\n`.gitmodules`:\n\n```\n[submodule \"third_party/catapult\"]\n\tpath = third_party/catapult\n\turl = https://chromium.googlesource.com/catapult.git\n\tgclient-condition = checkout_linux\n```\n\ngitlink entry, retrieved using `git ls-files -s -- third_party/catapult`:\n\n```\n160000 0b39a694c0b61392d1180520ed1c13e390029c41 0       third_party/catapult\n```\n\nCorresponding DEPS entry would look like:\n\n```\n  'third_party/catapult': {\n    'url': 'https://chromium.googlesource.com/catapult.git@0b39a694c0b61392d1180520ed1c13e390029c41',\n    'condition': 'checkout_linux',\n}\n```\n\n## How to avoid accidental Git submodule updates?\n\nThe simplest approach is to always run gclient sync after updating chromium\ncheckout (e.g. after `git pull`, or `git checkout`). You can automate that by\nadding post-checkout hook (example below). To confirm there are no changes, run\n`git status`. If you use `git commit -a`, check the \"Changes to be committed\"\nsection that shows up in the edit commit message.\n\n### Automatically run gclient sync after git pull / git checkout\n\nWe need to have Git two hooks: post-checkout and post-merge. In chromium/src\ndirectory, edit `.git/hooks/post-checkout`:\n\n```\n#!/bin/sh\n\nset -u\ngclient sync\n```\n\nand set it to be executable: `chmod +x .git/hooks/post-checkout`. Repeat the\nsame for `.git/hooks/post-merge`.\n\nMore information about githooks can be found\n[here](https://git-scm.com/docs/githooks).\n\n### Git status shows modified dependencies. What does that mean?\n\nIf a submodule is checked out at a different commit than one tracked by its\nparent, `git status` in the parent repo will show unstaged changes with \"new\ncommits\" in parenthesis, such as:\n\n```\nmodified: <git deps name> (new commits)\n```\n\nCommands like `git commit -a` or `git add *|.|-A|u` WILL include this in your\ncommit and your CL (which is likely NOT what you want).\n\nInstead you can:\n\n```\n# Run gclient sync to sync dependencies\ngclient sync\n# check git status again\n\n# OR\ngit add <file> # for each file you want to stage\n# Then commit your staged files\ngit commit -v -m \"Fix foo/bar\"\n```\n\nIf a submodule has uncommitted changes (i.e. you made some manual changes to the\naffected submodule), running `git status` in its parent repo will show them as\nunstaged changes:\n\n```\n  modified: <git deps name> (modified content)\n\n# or\n\n  modified: <git deps name> (untracked content)\n```\n\nIt's not possible to add those changes to the parent repository. You can ignore\nsuch status, or you can cd into submodule and address it. E.g. you may delete\nuntracked files (content) or reset modified content to match HEAD.\n\n## I accidentally staged Git submodule (not yet committed)\n\nIf you accidentally stage a Git submodule, you can unstage it by running `git\nrestore --staged <path to submodule>`.\n\n## I accidentally committed Git submodule\n\nWe will need to create either a commit that sets it back to old value, or amend\nthe commit that added it. You can try to run `gclient sync` to bring the commit\nback to what is expected. If that doesn't work, you can use `gclient setdep -r\n<path>@<old hash>`, run `gclient gitmodules` to sync all submodules commits back\nto what is in DEPS, or check detailed instructions in [Managing\ndependencies](dependencies.md).\n\nNOTE: setdep for chromium/src is always prefixed with src/. For example, if you\nare updating v8, the command would be `gclient setdep -r src/v8@<hash>.\n\n## Workflows with submodules\n\n### Submodules during 'git status', 'git commit', and 'git add'\n\nFor `git status`, submodules that show up under `Changes not staged for commit`\ncan be hidden with `git -c diff.ignoreSubmodules=all status`\n\nFor `git commit -a` you can exclude all submodules with\n`git -c diff.ignoreSubmodules=all commit -a`.\n\n`git add` does NOT support `diff.ignoreSubmodules`. Submodules that were\nhidden from you with `git -c diff.ignoreSubmodules=all status` would still\nbe staged with `git add .|--all|-A` and therefore committed with\n`git -c diff.ignoreSubmodules=all commit`.\n\nInstead you can run `git add ':(exclude,attr:builtin_objectmode=160000)'` which\nwill stage all changes except for submodules.\n\n(git assigns `160000` as the objectmode submodules. You can read more about\n[`builtin_objectmode`](https://kernel.googlesource.com/pub/scm/git/git/+/refs/heads/next/Documentation/gitattributes.txt#110)\nand magic [pathspecs](https://git-scm.com/docs/gitglossary#Documentation/gitglossary.txt-aiddefpathspecapathspec))\n\nTo make these commands shorter, you can create git aliases for them by adding\nthe following to your $HOME/.gitconfig (globally) or src/.git/config file (just\nchromium/src):\n```\n[alias]\n        # 's', 'c', or whatever alias you want for each command\n        s = -c diff.ignoreSubmodules=all status\n        c = -c diff.ignoreSubmodules=all commit -a\n        d = -c diff.ignoreSubmodules=all difftool --dir-diff\n        a = add ':(exclude,attr:builtin_objectmode=160000)'\n```\nWith the above, you can execute these commands by running `git s`, `git c`, etc.\nOr you may also use the pre-commit git hook detailed below.\n\n### Understanding diff.ignoreSubmodules\n\n`git config diff.ignoreSubmodules` sets a default behavior for `diff`, `status`,\nand several other git subcommands, using one of the [supported values of\n`--ignore-submodules`](https://www.git-scm.com/docs/git-diff/#Documentation/git-diff.txt---ignore-submodulesltwhengt).\n\nBy default, `gclient sync` sets this to `dirty` as a local config in the\nchromium checkout. This elides submodule output for `git status` in a clean\ncheckout, but will show submodules as modified when developers locally touch\nthem.\n\nManually setting this to `all` elides such output in all cases. This also omits\nsubmodule changes from `git commit -a`, which can decrease the likelihood of\naccidental submodule commits. However, it does not omit such changes from\n`git add -A`, meaning developers who use this flow are actually _more_ likely to\ncommit accidental changes, since they'll be invisible beforehand unless\ndevelopers manually set `--ignore-submodules=dirty` or use a lower-level command\nsuch as `git diff-tree`.\n\nBecause `all` can result in misleading output and doesn't fully prevent\naccidental submodule commits, typical developers are likely better-served by\nleaving this configured to `dirty` and installing the\n[commit hook described below](#install-hook) to prevent such commits.\nAccordingly, `gclient sync` will warn if it detects a different setting locally;\ndevelopers who understand the consequences can silence the warning via the\n`GCLIENT_SUPPRESS_SUBMODULE_WARNING` environment variable.\n\n### Submodules during a 'git rebase-update'\nWhile resolving merge conflicts during a `git rebase-update` you may see\nsubmodules show up in unexpected places.\n\n#### Submodules under \"Changes not staged for commit\"\nSubmodules under this section can be safely ignored. This simply shows that the\nlocal commits of these submodules do not match the latest pinned commits fetched\nfrom remote. In other words, these submodules have been rolled since your last\n`git rebase-update`.\n\nIf you use a diff tool like meld you can run:\n`git -c diff.ignoreSubmodules=all difftool --dir-diff`\nto prevent these submodules from showing up in your diff tool.\n\n#### Submodules under \"Unmerged paths\"\nIf Submodules show up under this section it means that new revisions were\ncommitted for those submodules (either intentional or unintentionally) and these\nsubmodules were also rolled at remote. So now there is a conflict.\n\nIf you DID NOT intentionally make any submdoules changes, you should run:\n`gclient gitmodules`. This will update the submdoules for you, to match whatever\ncommits are listed in DEPS (which you have just pulled from remote).\n\nIf you DID intentionally roll submodules, you can resolve this conflict just by\nresetting it:\n`gclient setdep -r {path}@{hash}`\n\n## Install a hook to help detect unintentional submodule commits {#install-hook}\n\ndepot_tools provides an opt-in pre-commit hook to detect unintentional submodule\n changes during `git commit` and remove them from the commit.\n\nTo install the hook: `gclient installhooks`\n\nIf there is an existing pre-commit hook, gclient will instruct you how to update\nit. If you have already installed this hook, gclient will do nothing.\n\nTo uninstall the hook, in `chromium/src` `rm .git/hooks/pre-commit` if you have\nno other hooks. Otherwise update `.git/hooks/pre-commit` to remove the gclient\nprovided hook.\n\nTo bypass this hook run `git commit --no-verify` (which bypasses all hooks you\n may have) OR set the following environment variable: `SKIP_GITLINK_PRECOMMIT=1`\n(which bypasses this specific hook).\n\nNote that this is currently and best effort solution and does not guarantee\nthat unintentional commits will always be detected. The team will iterate\nquickly on this hook to fill in other gaps and behavior is subject to change.\nPlease file an [issue](https://bugs.chromium.org/p/chromium/issues/entry?components=Infra%3ESDK&labels=submodules-feedback&cc=sokcevic@chromium.org,jojwang@chromium.org&description=Please%20steps%20to%20reproduce%20the%20problem:%0A%0ADo%20you%20have%20any%20custom%20environment%20setups%20like%20git%20hooks%20or%20git%20configs%20that%20you%20have%20set%20yourself%0A%0APlease%20attach%20output%20of:%0Agit%20config%20-l%0Agit%20map-branches%20-vv%0A%0AIf%20this%20is%20an%20issue%20with%20git%20cl%20upload%20please%20include%20the%20git%20trace%20file%20for%20the%20problematic%20run%20found%20in:%0A%3Cdepot_tools_path%3E/traces/%3Clatest%20trace%3E) for any feedback.\n\n## FAQ\n\n### Why do we have Git dependencies in both DEPS and Git submodules?\n\nLots of Chromium infrastructure already parse DEPS file directly. Instead of a\nmassive switch, it's easier to transition to Git submodules this way. Moreover,\nunwanted Git submodule updates can be detected and developers can be warned.\n\n### How do I manually roll Git submodule?\n\nSee the [dependencies](dependencies.md) page.\n\n### I got a conflict on a submodule, how do I resolve it?\n\nFirst, you will need to determine what is the right commit hash. If you\naccidentally committed a gitlink, which got in the meantime updated, you most\nlikely want to restore the original updated gitlink. You can run `gclient\ngitmodules`, which will take care of all unmerged submodule paths, and set it to\nmatch DEPS file.\n\nIf you prefer to manually resolve it, under git status, you will see \"Unmerged\npaths\". If those are submodules, you want to restore them by running the\nfollowing command:\n\n```\ngit restore --staging <affected path>\n```\n\n### How do I see what revision is pinned?\n\n`gclient getdep` will return whatever commit is pinned for the deps in `DEPS`\n(unstaged, staged, or committed). If the repo is using git submodules only\n(and has no git deps in `DEPS`) it will return the whatever pinned commit is\nstaged or committed.\n\n```\ngclient getdep -r <path>\n```\n\n\nIf you want to keep your gitlink, then run `git add <affected path>`.\n\n### How can I provide feedback?\n\nPlease file [a bug under Infra>SDK\ncomponent](https://bugs.chromium.org/p/chromium/issues/entry?components=Infra%3ESDK).\n"
  },
  {
    "path": "development/git_cookbook",
    "title": "Git Cookbook",
    "content": "# Git Cookbook\n\nA collection of git recipes to do common git tasks.\n\nSee also [Git Tips](git_tips.md).\n\n[TOC]\n\n## Introduction\n\nThis is designed to be a cookbook for common command sequences/tasks relating to\ngit, git-cl, and how they work with Chromium development. It might be a little\nlight on explanations.\n\nIf you are new to git, or do not have much experience with a distributed version\ncontrol system, you should also check out\n[The Git Community Book](http://book.git-scm.com/) for an overview of basic git\nconcepts and general git usage. Knowing what git means by branches, commits,\nreverts, and resets (as opposed to what SVN means by them) will help make the\nfollowing much more understandable.\n\n## Chromium-specific Git Extensions\n\nChromium ships a large number of git extensions in depot_tools. Some (like\n`git cl`) are required for the Chromium development workflow, while others\n(like `git map-branches`) are simple utilities to make your life easier.\nPlease take a look at the full\n[depot_tools tutorial](https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/depot_tools_tutorial.html),\nand at the extensive\n[man pages](https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/depot_tools.html)\nfor all the extensions.\n\n## Excluding file(s) from git-cl, while preserving them for later use\n\nSince git-cl assumes that the diff between your current branch and its tracking\nbranch is what should be used for the CL, the goal is to remove the unwanted\nfiles from the current branch, and preserve them in another branch.\n\n### Method #1: Reset your current branch, and selectively commit files\n\n1.  `git log`  See the list of your commits. Find the hash of the last commit\n     before your changes.\n1.  `git reset --soft abcdef` where abcdef is the hash found in the step above.\n1.  `git commit <files_for_this_cl> -m \"files to upload\"` commit the files you\n     want included in the CL here.\n1.  `git new-branch new_branch_name` Create a new branch for the\n    files that you want to exclude.\n1.  `git commit -a -m \"preserved files\"` Commit the rest of the files.\n\n### Method #2: Create a new branch, reset, then commit files to preserve\n\nThis method creates a new branch from your current one to preserve your changes.\nThe commits on the new branch are undone, and then only the files you want to\npreserve are recommitted.\n\n1.  `git checkout -b new_branch_name` This preserves your old files.\n1.  `git log` See the list of your commits. Find the hash of the last commit\n    before your changes.\n1.  `git reset --soft abcdef` Where abcdef is the hash found in the step above.\n1.  `git commit <files_to_preserve> -m \"preserved files\"` Commit the found files\n    into the `new_branch_name`.\n\nThen revert your files however you'd like in your old branch. The files listed\nin step 4 will be saved in `new_branch_name`\n\n### Method #3: Cherry-pick changes into review branches\n\nIf you are systematic in creating separate local commits for independent\nchanges, you can make a number of different changes in the same client and then\ncherry-pick each one into a separate review branch.\n\n1.  Make and commit a set of independent changes.\n1.  `git log`  # see the hashes for each of your commits.\n1.  repeat checkout, cherry-pick, upload steps for each change 1..n\n    1.  `git new-branch review-changeN` Create a new review branch\n        tracking origin\n    1.  `git cherry-pick <hash of change N>`\n    1.  `git cl upload`\n\nIf a change needs updating due to review comments, you can go back to your main\nworking branch, update the commit, and re-cherry-pick it into the review branch.\n\n1.  `git checkout <working branch>`\n1.  Make changes.\n1.  If the commit you want to update is the most recent one:\n    1.  `git commit --amend <files>`\n1.  If not:\n    1.  `git commit <files>`\n    1.  `git rebase -i origin`  # use interactive rebase to squash the new\n        commit into the old one.\n1.  `git log`  # observe new hash for the change\n1.  `git checkout review-changeN`\n1.  `git reset --hard`  # remove the previous version of the change\n1.  `git cherry-pick <new hash of change N>`\n1.  `git cl upload`\n\n## Sharing code between multiple machines\n\nAssume Windows computer named vista, and a Linux one named penguin.\nPrerequisite: both machines have git clones of the main git tree.\n\n```shell\nvista$ git remote add linux ssh://penguin/path/to/git/repo\nvista$ git fetch linux\nvista$ git branch -a   # should show \"linux/branchname\"\nvista$ git checkout -b foobar linux/foobar\nvista$ hack hack hack; git commit -a\nvista$ git push linux  # push branch back to linux\npenguin$ git reset --hard  # update with new stuff in branch\n```\n\nNote that, by default, `gclient sync` will update all remotes. If your other\nmachine (i.e., `penguin` in the above example) is not always available,\n`gclient sync` will timeout and fail trying to reach it. To fix this, you may\nexclude your machine from being fetched by default:\n\n    vista$ git config --bool remote.linux.skipDefaultUpdate true\n\n## Reverting commits\n\nThe command `git revert X` patches in the inverse of a particular commit.\nUsing this command is one way of making a revert:\n\n```shell\ngit checkout origin   # start with trunk\ngit revert abcdef\ngit cl upload\n```\n\n## Retrieving, or diffing against an old file revision\n\nGit works in terms of commits, not files. Thus, working with the history of a\nsingle file requires modified version of the show and diff commands.\n\n```shell\n# Find the commit you want in the file's commit log.\ngit log path/to/file\n# This prints out the file contents at commit 123abc.\ngit show 123abc:path/to/file\n# Diff the current version against path/to/file against the version at\n# path/to/file\ngit diff 123abc -- path/to/file\n```\n\nWhen invoking `git show` or `git diff`, the `path/to/file` is **not relative the\nthe current directory**. It must be the full path from the directory where the\n.git directory lives. This is different from invoking `git log` which\nunderstands relative paths.\n\n## Reusing a Git mirror\n\nIf you have a nearby copy of a Git repo, you can quickly bootstrap your copy\nfrom that one then adjust it to point it at the real upstream one.\n\n1.  Clone a nearby copy of the code you want: `git clone coworker-machine:/path/to/repo`\n1.  Change the URL your copy fetches from to point at the real git repo:\n    `git remote set-url origin https://chromium.googlesource.com/chromium/src.git`\n1.  Update your copy: `git fetch`\n1.  Delete any extra branches that you picked up in the initial clone:\n    `git prune origin`\n"
  },
  {
    "path": "development/code_reviews",
    "title": "Code Reviews",
    "content": "# Code Reviews\n\nCode reviews are a central part of developing high-quality code for Chromium.\nAll change lists (CLs) must be reviewed.\n\nThis page documents policy rules regarding code changes.\n\nSee also:\n- The general patch, upload, and land process in [contributing code](contributing.md#code-review)\n- [Code of conduct](../CODE_OF_CONDUCT.md)\n- [Respectful Changes](cl_respect.md)\n- [Respectful Code Reviews](cr_respect.md)\n- The code review changes and OWNERS policy changes launched on March 24, 2021, see\n[Mandatory Code Review and Native OWNERS](code_review_owners.md).\n\n# Code review policies\n\nAny [committer](https://www.chromium.org/getting-involved/become-a-committer/#what-is-a-committer) can review code, but\nan owner must provide a review for each directory you are touching. Ideally you should choose\nreviewers who are familiar with the area of code you are touching. If you have doubts, look\nat the `git blame` for the file and the `OWNERS` files ([more info](#owners-files)).\n\nTo indicate a positive review, the reviewer provides a `Code-Review +1` in\nGerrit, also known as an LGTM (\"Looks Good To Me\"). A score of \"-1\" indicates\nthe change should not be submitted as-is.\n\nSubmissions to the chromium/src repository by a change contributor who is not a Chromium\ncommitter require two committers to Code-Review+1 the submission. If the owner of the CL\nis already a committer, then only one other committer is needed to review.\n\nIf you have multiple reviewers, provide a message indicating what you expect\nfrom each reviewer. Otherwise people might assume their input is not required\nor waste time with redundant reviews.\n\nPlease also read [Respectful Changes](cl_respect.md) and\n[Respectful Code Reviews](cr_respect.md).\n\nThere are also a [collection of tips](cl_tips.md) for productive reviews, though\nthese are advisory and not policy.\n\n#### Expectations for all reviewers\n\n*   As a reviewer, aim to provide actionable feedback 3 times per work day. The\n    expectation is that if you're in the same time zone as the CL author, there\n    are 3 review iterations. If there is a time zone divide, aim for 2 review\n    iterations.\n\n*   Use the status field in Gerrit settings to indicate if you're away and when\n    you'll be back.\n\n*   Don't generally discourage people from sending you code reviews. This\n    includes using a blanket \"slow\" in your status field.\n\n## OWNERS files\n\nIn various directories there are files named `OWNERS` that list the email\naddresses of people qualified to review changes in that directory. You must\nget a positive review from an owner of each directory your change touches.\n\nOwners files are recursive, so each file also applies to its subdirectories.\nIt's generally best to pick more specific owners. People listed in higher-level\ndirectories may have less experience with the code in question. For example,\nthe reviewers in the `//chrome/browser/component_name/OWNERS` file will likely\nbe more familiar with code in `//chrome/browser/component_name/sub_component`\nthan reviewers in the higher-level `//chrome/OWNERS` file.\n\nMore detail on the owners file format is provided [here](#owners-file-details).\n\n*Tip:* The `git cl owners` command can help find owners. Gerrit also provides\nthis functionality in the Reviewers field of CLs.\n\nWhile owners must approve all patches, any committer can contribute to the\nreview. In some directories the owners can be overloaded or there might be\npeople not listed as owners who are more familiar with the low-level code in\nquestion. In these cases it's common to request a low-level review from an\nappropriate person, and then request a high-level owner review once that's\ncomplete. As always, be clear what you expect of each reviewer to avoid\nduplicated work.\n\nOwners do not have to pick other owners for reviews. Since they should already\nbe familiar with the code in question, a thorough review from any appropriate\ncommitter is sufficient.\n\n#### Expectations of owners\n\nThe existing owners of a directory approve additions to the list. It is\npreferable to have many directories, each with a smaller number of specific\nowners rather than large directories with many owners. Owners must be\n[committers](https://www.chromium.org/getting-involved/become-a-committer/)\nwith at least 3 months' tenure, and in addition should:\n\n  * Demonstrate excellent judgment, teamwork and ability to uphold\n    [Chromium development principles](contributing.md).\n\n  * Be already acting as an owner, providing high-quality reviews and design\n    feedback.\n\n  * Have submitted a substantial number of non-trivial changes to the affected\n    directory.\n\n  * Have committed or reviewed substantial work to the affected directory\n    within the last ninety days.\n\n  * Have the bandwidth to contribute to reviews in a timely manner. If the load\n    is unsustainable, work to expand the number of owners. Don't try to\n    discourage people from sending reviews, including writing \"slow\" or\n    \"emeritus\" after your name.\n\nSeldom-updated directories may have exceptions to the \"substantiality\" and\n\"recency\" requirements.\n\nDirectories in `//third_party` should list those most familiar with the\nlibrary, regardless of how often the code is updated.\n\n#### Removal of owners\n\nIf a code owner is not meeting the [expectations of\nowners](#expectations-of-owners) listed above for more than one quarter (and\nthey are not on a leave during that time), then they may be removed by any\nco-owner or an owner from the parent directory after a 4-week notice, using\nthe following process:\n\n  * Upload a change removing the owner and copy all owners in that directory,\n    including the owner in question.\n  * If the affected owner approves the change, it may be landed immediately.\n  * Otherwise, the change author must wait five working days for feedback from\n    the other owners.\n    * After that time has elapsed, if the change has received 3 approvals\n      with no objections from anyone else, the change may be landed.\n    * If the directory does not have 4 owners, then the decision should\n      be escalated to the owners of the parent directory (or directories)\n      as necessary to provide enough votes.\n    * If there are objections, then the decision should be escalated to\n      the [../ATL_OWNERS](../ATL_OWNERS) for resolution.\n\nNote: For the purpose of not slowing down code review, Chromium removes\ninactive owners (e.g., those who made no contributions for multiple quarters)\non a regular basis. The script does not take into account personal situations\nlike a long leave. If you were inactive only for a certain period of time\nwhile you were on a long leave and have been meeting the above owner's\nexpectations in other times, you can create a CL to re-add yourself and land\nafter getting local owner's approval (you can refer to this policy in the CL).\nThe removal script will cc the removed owner and one other owner to avoid spam.\n\n### OWNERS file details\n\nRefer to the [owners plugin](https://github.com/GerritCodeReview/plugins_code-owners/blob/master/resources/Documentation/backend-find-owners.md)\nfor all details on the file format.\n\nThis example indicates that two people are owners, in addition to any owners\nfrom the parent directory. `git cl owners` will list the comment after an\nowner address, so this is a good place to include restrictions or special\ninstructions.\n```\n# You can include comments like this.\na@chromium.org\nb@chromium.org  # Only for the frobinator.\n```\n\nA `*` indicates that all committers are owners:\n```\n*\n```\n\nThe text `set noparent` will stop owner propagation from parent directories.\nThis should be rarely used. If you want to use `set noparent` except for IPC\nrelated files, please first reach out to chrome-atls@google.com.\n\nYou have to use `set noparent` together with a reference to a file that lists\nthe owners for the given use case. Approved use cases are listed in\n`//build/OWNERS.setnoparent`. Owners listed in those files are expected to\nexecute special governance functions such as ATL reviews or ipc security review.\nEvery set of owners should implement their own means of auditing membership. The\nminimum expectation is that membership in those files is reevaluated on\nproject, or affiliation changes.\n\nIn this example, only the ATLs are owners:\n```\nset noparent\nfile://ATL_OWNERS\n```\n\nThe `per-file` directive allows owners to be added that apply only to files\nmatching a pattern. In this example, owners from the parent directory\napply, plus one person for some classes of files, and all committers are\nowners for the readme:\n```\nper-file foo_bar.cc=a@chromium.org\nper-file foo.*=a@chromium.org\n\nper-file readme.txt=*\n```\n\nOther `OWNERS` files can be included by reference by listing the path to the\nfile with `file://...`. This example indicates that only the people listed in\n`//ipc/SECURITY_OWNERS` can review the messages files:\n```\nper-file *_messages*.h=set noparent\nper-file *_messages*.h=file://ipc/SECURITY_OWNERS\n```\n\nFile globbing is supported using the\n[simple path expression](https://github.com/GerritCodeReview/plugins_code-owners/blob/master/resources/Documentation/path-expressions.md#simple-path-expressions)\nformat.\n\nOwners annotated with `#{LAST_RESORT_SUGGESTION}` in their comment will be\nomitted when suggesting code owners, except if dropping these code owners would\nmake the suggestion result empty or if these code owners are already reviewers\nof the change.\n\n### Owners-Override\n\nSetting the `Owners-Override +1` label will bypass OWNERS enforcement. Active\n[gardeners](gardener.md), Release Program Managers,\n[Large Scale Changes](#large-scale-changes),\n[Global Approvers](#global-approvals) reviewers,\n[Chrome ATLs](../ATL_OWNERS)\nhave this capability. The power to use Owners-Override should be restricted\nas follows:\n\n  * Active gardeners and Release Program Managers can set Owners-Override only\n    on CLs needed for gardening and releasing (e.g., revert, reland, test fix,\n    cherry-pick).\n  * Large Scale Change reviewers can set Owners-Override only on gardening CLs\n    and CLs about the approved Large Scale Change.\n  * Global approvers can set Owners-Override only on gardening CLs and\n    mechanical CLs associated with their API changes. For example,\n    //base/OWNERS can set Owners-Override on mechanical CLs associated with\n    //base/ API changes.\n  * Chrome ATLs can set Owners-Override on any changes to help with cases that\n    cannot be handled by the above groups and expedite CLs when LSC is too\n    heavyweight. However, please use one of the above groups before asking\n    Chrome ATLs.\n\nWhen you need Owners-Override on gardening CLs, please reach out to the\nActive Sheriffs and Release Program Managers first. If none of them is\navailable, please send an email to lsc-owners-override@chromium.org for help.\n\nNote that Owners-Override by itself is not enough on your own CLs. Where this\nmatters is when you are gardening. For example, if you want to revert or\ndisable a test, your Owners-Override on the CL is not enough. You also need\neither another committer to LGTM the CL or, for clean reverts, a `Bot-Commit:\n+1` from the [rubber-stamper bot](#automated-code_review).\n\nWhen setting Owners-Override it is your responsibility to confirm that every\nfile (and line) in the patch has been appropriately reviewed.\n\n## Mechanical changes\n\n### Global Approvals\nFor one-off CLs, API owners of `base`, `build`, `content`,\n`third_party/blink/public` and `url` can `Owners-Override +1` a change to their\nAPIs to avoid waiting for rubberstamp +1s from affected directories' owners.\nThis should only be used for mechanical updates, but global approvers are free\nto use their judgement in determining which mechanical changes they understand\nwell enough to approve (rather than limit strictly to calls into code\nthey own).\n\nFor a change that impacts many directories but doesn't need area-specific\nexpertise to review, please ask any global approver or Chrome ATL to\napprove the change rather than incur unnecessary review cost on a larger number\nof reviewers.\n\n### Large Scale Changes\nYou can use the [Large Scale Changes](process/lsc/large_scale_changes.md)\nprocess to get approval to bypass OWNERS enforcement for large changes like\nrefactoring, architectural changes, or other repetitive code changes across the\nwhole codebase. This is used for work that span many dozen CLs.\n\n## Documentation updates\n\nDocumentation updates require code review. We may revisit this decision in the\nfuture.\n\n## Automated code-review\n\nFor verifiably safe changes like translation files, clean reverts, and clean\ncherry-picks, we have automation that will vote +1 on the `Bot-Commit` label\nallowing the CL to be submitted without human code-review. Add `Rubber Stamper`\n(rubber-stamper@appspot.gserviceaccount.com) to your CL as a reviewer to\nactivate this automation. It will scan the CL after about 1 minute and reply\nwith its verdict. `Bot-Commit` votes are not sticky between patchsets and so\nonly add the bot once the CL is finalized.\n\nWhen combined with the [`Owners-Override`](#owners_override) power, gardeners\ncan effectively revert and reland on their own.\n\nRubber Stamper never provides OWNERS approval, by design. It's intended to be\nused by those who have owners in the directory modified or who are gardeners. If\nit provided both code review and OWNERS approval, that would be an abuse vector:\nthat would allow anyone who can create a revert or cherry-pick to land it\nwithout any other person being involved (e.g. the silent revert of security\npatches).\n\nChanges not supported by `Rubber Stamper` always need a +1 from another\ncommitter.\n"
  },
  {
    "path": "development/clang_tool_refactoring",
    "title": "Clang Tool Refactoring",
    "content": "# Clang Tool Refactoring\n\n[TOC]\n\n## Introduction\n\nClang tools can help with global refactorings of Chromium code. Clang tools can\ntake advantage of clang's AST to perform refactorings that would be impossible\nwith a traditional find-and-replace regexp:\n\n*   Constructing `scoped_ptr<T>` from `NULL`: <https://crbug.com/173286>\n*   Implicit conversions of `scoped_refptr<T>` to `T*`: <https://crbug.com/110610>\n*   Rename everything in Blink to follow Chromium style: <https://crbug.com/563793>\n*   Clean up of deprecated `base::Value` APIs: <https://crbug.com/581865>\n\n## Caveats\n\n* Invocations of a clang tool runs on on only one build config at a time. For\nexample, running the tool across a `target_os=\"win\"` build won't update code\nthat is guarded by `OS_POSIX`. Performing a global refactoring will often\nrequire running the tool once for each build config.\n\n## Prerequisites\n\nA Chromium checkout created with `fetch` should have everything needed.\n\nFor convenience, add `third_party/llvm-build/Release+Asserts/bin` to `$PATH`.\n\n## Writing the tool\n\nLLVM uses C++11 and CMake. Source code for Chromium clang tools lives in\n[//tools/clang]. It is generally easiest to use one of the already-written tools\nas the base for writing a new tool.\n\nChromium clang tools generally follow this pattern:\n\n1.  Instantiate a\n    [`clang::ast_matchers::MatchFinder`][clang-docs-match-finder].\n2.  Call `addMatcher()` to register\n    [`clang::ast_matchers::MatchFinder::MatchCallback`][clang-docs-match-callback]\n    actions to execute when [matching][matcher-reference] the AST.\n3.  Create a new `clang::tooling::FrontendActionFactory` from the `MatchFinder`.\n4.  Run the action across the specified files with\n    [`clang::tooling::ClangTool::run`][clang-docs-clang-tool-run].\n5.  Serialize generated [`clang::tooling::Replacement`][clang-docs-replacement]s\n    to `stdout`.\n\nOther useful references when writing the tool:\n\n*   [Clang doxygen reference][clang-docs]\n*   [Tutorial for building tools using LibTooling and\n    LibASTMatchers][clang-tooling-tutorial]\n\n### Edit serialization format\n```\n==== BEGIN EDITS ====\nr:::path/to/file/to/edit:::offset1:::length1:::replacement text\nr:::path/to/file/to/edit:::offset2:::length2:::replacement text\nr:::path/to/file2/to/edit:::offset3:::length3:::replacement text\ninclude-user-header:::path/to/file2/to/edit:::-1:::-1:::header/file/to/include.h\n\n       ...\n\n==== END EDITS ====\n```\n\nThe header and footer are required. Each line between the header and footer\nrepresents one edit. Fields are separated by `:::`, and the first field must\nbe `r` (for replacement) or `include-user-header`.\nA deletion is an edit with no replacement text.\n\nThe edits are applied by [`apply_edits.py`](#Running), which understands certain\nconventions:\n\n*   The clang tool should munge newlines in replacement text to `\\0`. The script\n    knows to translate `\\0` back to newlines when applying edits.\n*   When removing an element from a 'list' (e.g. function parameters,\n    initializers), the clang tool should emit a deletion for just the element.\n    The script understands how to extend the deletion to remove commas, etc. as\n    needed.\n\nTODO: Document more about `SourceLocation` and how spelling loc differs from\nexpansion loc, etc.\n\n### Why not RefactoringTool?\nWhile clang has a [`clang::tooling::RefactoringTool`](http://clang.llvm.org/doxygen/classclang_1_1tooling_1_1RefactoringTool.html)\nto automatically apply the generated replacements and save the results, it\ndoesn't work well for Chromium:\n\n*   Clang tools run actions serially, so run time scales poorly to tens of\n    thousands of files.\n*   A parsing error in any file (quite common in NaCl source) prevents any of\n    the generated replacements from being applied.\n\n## Building\nSynopsis:\n\n```shell\ntools/clang/scripts/build.py --bootstrap --without-android --without-fuchsia \\\n  --extra-tools rewrite_to_chrome_style\n```\n\nRunning this command builds the [Oilpan plugin][//tools/clang/blink_gc_plugin],\nthe [Chrome style plugin][//tools/clang/plugins], and the [Blink to Chrome style\nrewriter][//tools/clang/rewrite_to_chrome_style]. Additional arguments to\n`--extra-tools` should be the name of subdirectories in [//tools/clang].\n\nIt is important to use --bootstrap as there appear to be [bugs](https://crbug.com/580745)\nin the clang library this script produces if you build it with gcc, which is the default.\n\nOnce clang is bootsrapped, incremental builds can be done by invoking `ninja` in\nthe `third_party/llvm-build/Release+Asserts` directory. In particular,\nrecompiling solely the tool you are writing can be accomplished by executing\n`ninja rewrite_to_chrome_style` (replace `rewrite_to_chrome_style` with your\ntool's name).\n\n## Running\nFirst, build all Chromium targets to avoid failures due to missing dependencies\nthat are generated as part of the build:\n\n```shell\nninja -C out/Debug  # For non-Windows\nninja -d keeprsp -C out/Debug  # For Windows\n\n# experimental alternative:\n$gen_targets = $(ninja -C out/Debug -t targets all \\\n                 | grep '^gen/[^: ]*\\.[ch][pc]*:' \\\n                 | cut -f 1 -d :)\nninja -C out/Debug $gen_targets\n```\n\nNote that running the clang tool with precompiled headers enabled currently\nproduces errors. This can be avoided by setting\n`enable_precompiled_headers = false` in the build's gn args.\n\nThen run the actual clang tool to generate a list of edits:\n\n```shell\ntools/clang/scripts/run_tool.py --tool <path to tool> \\\n  --generate-compdb\n  -p out/Debug <path 1> <path 2> ... >/tmp/list-of-edits.debug\n```\n\n`--generate-compdb` can be omitted if the compile DB was already generated and\nthe list of build flags and source files has not changed since generation.\n\nIf cross-compiling, specify `--target_os`. See `gn help target_os` for\npossible values. For example, when cross-compiling a Windows build on\nLinux/Mac, use `--target_os=win`.\n\n`<path 1>`, `<path 2>`, etc are optional arguments to filter the files to run\nthe tool against. This is helpful when sharding global refactorings into smaller\nchunks. For example, the following command will run the `empty_string` tool\nagainst just the `.c`, `.cc`, `.cpp`, `.m`, `.mm` files in `//net`.  Note that\nthe filtering is not applied to the *output* of the tool - the tool can emit\nedits that apply to files outside of `//net` (i.e. edits that apply to headers\nfrom `//base` that got included by source files in `//net`).\n\n```shell\ntools/clang/scripts/run_tool.py --tool empty_string  \\\n  --generate-compdb \\\n  -p out/Debug net >/tmp/list-of-edits.debug\n```\n\nNote that some header files might only be included from generated files (e.g.\nfrom only from some `.cpp` files under out/Debug/gen).  To make sure that\ncontents of such header files are processed by the clang tool, the clang tool\nneeds to be run against the generated files.  The only way to accomplish this\ntoday is to pass `--all` switch to `run_tool.py` - this will run the clang tool\nagainst all the sources from the compilation database.\n\nFinally, apply the edits as follows:\n\n```shell\ncat /tmp/list-of-edits.debug \\\n  | tools/clang/scripts/extract_edits.py \\\n  | tools/clang/scripts/apply_edits.py -p out/Debug <path 1> <path 2> ...\n```\n\nThe apply_edits.py tool will only apply edits to files actually under control of\n`git`.  `<path 1>`, `<path 2>`, etc are optional arguments to further filter the\nfiles that the edits are applied to.  Note that semantics of these filters is\ndistinctly different from the arguments of `run_tool.py` filters - one set of\nfilters controls which files are edited, the other set of filters controls which\nfiles the clang tool is run against.\n\n## Debugging\nDumping the AST for a file:\n\n```shell\nclang++ -Xclang -ast-dump -std=c++14 foo.cc | less -R\n```\n\nUsing `clang-query` to dynamically test matchers (requires checking out\nand building [clang-tools-extra][]):\n\n```shell\nclang-query -p path/to/compdb base/memory/ref_counted.cc\n```\n\n`printf` debugging:\n\n```c++\n  clang::Decl* decl = result.Nodes.getNodeAs<clang::Decl>(\"decl\");\n  decl->dumpColor();\n  clang::Stmt* stmt = result.Nodes.getNodeAs<clang::Stmt>(\"stmt\");\n  stmt->dumpColor();\n```\n\nBy default, the script hides the output of the tool. The easiest way to change\nthat is to `return 1` from the `main()` function of the clang tool.\n\n## Testing\nSynposis:\n\n```shell\ntools/clang/scripts/test_tool.py <tool name> [--apply-edits]\n```\n\nThe name of the tool binary and the subdirectory for the tool in\n`//tools/clang` must match. The test runner finds all files that match the\npattern `//tools/clang/<tool name>/tests/*-original.cc`, and runs the tool\nacross those files.\nIf `--apply-edits` switch is presented, tool outputs are applied to respective\nfiles and compared to the `*-expected.cc` version. If there is a mismatch, the\nresult is saved in `*-actual.cc`.\nWhen `--apply-edits` switch is not presented, tool outputs are compared to\n`*-expected.txt` and if different, the result is saved in `*-actual.txt`. Note\nthat in this case, only one test file is expected.\n\n[//tools/clang]: https://chromium.googlesource.com/chromium/src/+/main/tools/clang/\n[clang-docs-match-finder]: http://clang.llvm.org/doxygen/classclang_1_1ast__matchers_1_1MatchFinder.html\n[clang-docs-match-callback]: http://clang.llvm.org/doxygen/classclang_1_1ast__matchers_1_1MatchFinder_1_1MatchCallback.html\n[matcher-reference]: http://clang.llvm.org/docs/LibASTMatchersReference.html\n[clang-docs-clang-tool-run]: http://clang.llvm.org/doxygen/classclang_1_1tooling_1_1ClangTool.html#acec91f63b45ac7ee2d6c94cb9c10dab3\n[clang-docs-replacement]: http://clang.llvm.org/doxygen/classclang_1_1tooling_1_1Replacement.html\n[clang-docs]: http://clang.llvm.org/doxygen/index.html\n[clang-tooling-tutorial]: http://clang.llvm.org/docs/LibASTMatchersTutorial.html\n[//tools/clang/blink_gc_plugin]: https://chromium.googlesource.com/chromium/src/+/main/tools/clang/blink_gc_plugin/\n[//tools/clang/plugins]: https://chromium.googlesource.com/chromium/src/+/main/tools/clang/plugins/\n[//tools/clang/rewrite_to_chrome_style]: https://chromium.googlesource.com/chromium/src/+/main/tools/clang/rewrite_to_chrome_style/\n[clang-tools-extra]: (https://github.com/llvm-mirror/clang-tools-extra)\n"
  },
  {
    "path": "development/clang_tidy",
    "title": "Clang Tidy",
    "content": "# Clang Tidy\n\n[TOC]\n\n## Introduction\n\n[clang-tidy](http://clang.llvm.org/extra/clang-tidy/) is a clang-based C++\n“linter” tool. Its purpose is to provide an extensible framework for diagnosing\nand fixing typical programming errors, like style violations, interface misuse,\nor bugs that can be deduced via static analysis.\n\n## Where is it?\n\nclang-tidy is available in two places in Chromium:\n\n- In Chromium checkouts\n- In code review on Gerrit\n\nClang-tidy automatically runs on any CL that Chromium committers upload to\nGerrit, and will leave code review comments there. This is the recommended way\nof using clang-tidy.\n\n## Enabled checks\n\nChromium globally enables a subset of all of clang-tidy's checks (see\n`${chromium}/src/.clang-tidy`). We want these checks to cover as much as we\nreasonably can, but we also strive to strike a reasonable balance between signal\nand noise on code reviews. Hence, a large number of clang-tidy checks are\ndisabled.\n\n### Adding a new check\n\nNew checks require review from cxx@chromium.org. If you propose a check and it\ngets approved, you may turn it on, though please note that this is only\nprovisional approval: we get signal from users clicking \"Not Useful\" on\ncomments. If feedback is overwhelmingly \"users don't find this useful,\" the\ncheck may be removed.\n\nTraditionally, petitions to add checks include [an\nevaluation](https://docs.google.com/document/d/1i1KmXtDD4j_qjhmAdGlJ6UkYXByVX1Kp952Zusdcl5k/edit?usp=sharing)\nof the check under review. Crucially, this includes two things:\n\n- a count of how many times this check fires across Chromium\n- a random sample (>30) of places where the check fires across Chromium\n\nIt's expected that the person proposing the check has manually surveyed every\nclang-tidy diagnostic in the sample, noting any bugs, odd behaviors, or\ninteresting patterns they've noticed. If clang-tidy emits FixIts, these are\nexpected to be considered by the evaluation, too.\n\nAn example of a previous proposal email thread is\n[here](https://groups.google.com/a/chromium.org/g/cxx/c/iZ6-Y9ZhC3Q/m/g-8HzqmbAAAJ).\n\n#### Evaluating: running clang-tidy across Chromium\n\nRunning clang-tidy requires some setup. First, you'll need to sync clang-tidy,\nwhich requires adding `checkout_clang_tidy` to your `.gclient` file:\n\n```\nsolutions = [\n  {\n    'custom_vars': {\n      'checkout_clang_tidy': True,\n    },\n  }\n]\n```\n\nYour next run of `gclient runhooks` should cause clang-tidy to be synced.\n\nTo run clang-tidy across all of Chromium, you'll need a checkout of Chromium's\n[tools/build/](https://chromium.googlesource.com/chromium/tools/build) repository.\nOnce you have that and a Chromium `out/` dir with an `args.gn`, running\nclang-tidy across all of Chromium is a single command:\n\n```\n$ cd ${chromium}/src\n$ ${chromium_tools_build}/recipes/recipe_modules/tricium_clang_tidy/resources/tricium_clang_tidy_script.py \\\n    --base_path $PWD \\\n    --out_dir out/Linux \\\n    --findings_file all_findings.json \\\n    --clang_tidy_binary $PWD/third_party/llvm-build/Release+Asserts/bin/clang-tidy \\\n    --all\n```\n\nTo only run clang-tidy against certain files, replace the `--all` parameter with\nthe individual file paths.\n\nAll clang-tidy checks are run on Linux builds of Chromium, so please set up your\n`args.gn` to build Linux.\n\n`all_findings.json` is where all of clang-tidy's findings will be dumped. The\nformat of this file is detailed in `tricium_clang_tidy_script.py`.\n\n**Note** that the above command will use Chromium's top-level `.clang-tidy` file\n(or `.clang-tidy` files scattered throughout `third_party/`, depending on the\nfiles we lint. In order to test a *new* check, it's recommended that you use\n`tricium_clang_tidy_script.py`'s `--tidy_checks` flag. Usage of this looks like:\n\n```\n$ cd ${chromium}/src\n$ ${chromium_build}/recipes/recipe_modules/tricium_clang_tidy/resources/tricium_clang_tidy_script.py \\\n    --base_path $PWD \\\n    --out_dir out/Linux \\\n    --findings_file all_findings.json \\\n    --clang_tidy_binary $PWD/third_party/llvm-build/Release+Asserts/bin/clang-tidy \\\n    --tidy_checks='-*,YOUR-NEW-CHECK-NAME-HERE'\n    --all\n```\n\n### Ignoring a check\n\nIf a check is invalid on a particular piece of code, clang-tidy supports `//\nNOLINT` and `// NOLINTNEXTLINE` for ignoring all lint checks in the current and\nnext lines, respectively. To suppress a specific lint, you can put it in\nparenthesis, e.g., `// NOLINTNEXTLINE(modernize-use-nullptr)`. For more, please\nsee [the documentation](\nhttps://clang.llvm.org/extra/clang-tidy/#suppressing-undesired-diagnostics).\n\n**Please note** that adding comments that exist only to silence clang-tidy is\nactively discouraged. These comments clutter code, can easily get\nout-of-date, and don't provide much value to readers. Moreover, clang-tidy only\ncomplains on Gerrit when lines are touched, and making Chromium clang-tidy clean\nis an explicit non-goal; making code less readable in order to silence a\nrarely-surfaced complaint isn't a good trade-off.\n\nIf clang-tidy emits a diagnostic that's incorrect due to a subtlety in the code,\nadding an explanantion of what the code is doing with a trailing `NOLINT` may be\nfine. Put differently, the comment should be able to stand on its own even if we\nremoved the `NOLINT`. The fact that the comment also silences clang-tidy is a\nconvenient side-effect.\n\nFor example:\n\nNot OK; comment exists just to silence clang-tidy:\n\n```\n// NOLINTNEXTLINE\nfor (int i = 0; i < arr.size(); i++) {\n  // ...\n}\n```\n\nNot OK; comment exists just to verbosely silence clang-tidy:\n\n```\n// Clang-tidy doesn't get that we can't range-for-ize this loop. NOLINTNEXTLINE\nfor (int i = 0; i < arr.size(); i++) {\n  // ...\n}\n```\n\nNot OK; it's obvious that this loop modifies `arr`, so the comment doesn't\nactually clarify anything:\n\n```\n// It'd be invalid to make this into a range-for loop, since the body might add\n// elements to `arr`. NOLINTNEXTLINE\nfor (int i = 0; i < arr.size(); i++) {\n  if (i % 4) {\n    arr.push_back(4);\n    arr.push_back(2);\n  }\n}\n```\n\nOK; comment calls out a non-obvious property of this loop's body. As an\nafterthought, it silences clang-tidy:\n\n```\n// It'd be invalid to make this into a range-for loop, since the call to `foo`\n// here might add elements to `arr`. NOLINTNEXTLINE\nfor (int i = 0; i < arr.size(); i++) {\n  foo();\n  bar();\n}\n```\n\nIn the end, as always, what is and isn't obvious at some point is highly\ncontext-dependent. Please use your best judgement.\n\n## But I want to run it locally\n\nIf you want to sync the officially-supported `clang-tidy` to your workstation,\nadd the following to your .gclient file:\n\n```\nsolutions = [\n  {\n    'custom_vars': {\n      'checkout_clang_tidy': True,\n    },\n  },\n]\n```\n\nIf you already have `solutions` and `custom_vars`, just add\n`checkout_clang_tidy` to the existing `custom_vars` map.\n\nOnce the above update has been made, run `gclient runhooks`, and clang-tidy\nshould appear at `src/third_party/llvm-build/Release+Asserts/bin/clang-tidy` if\nyour Chromium tree is sufficiently up-to-date.\n\n### Running clang-tidy locally\n\n**Note** that the local flows with clang-tidy are experimental, and require an\nLLVM checkout. Tricium is happy to run on WIP CLs, and we strongly encourage its\nuse.\n\nThat said, assuming you have the LLVM sources available, you'll need to bring\nyour own `clang-apply-replacements` binary if you want to use the `-fix` option\nnoted below.\n\n**Note:** If you're on a system that offers a clang tools through its package\nmanager (e.g., on Debian/Ubuntu, `sudo apt-get install clang-tidy clang-tools`),\nyou might not need an LLVM checkout to make the required binaries and scripts\n(`clang-tidy`, `run-clang-tidy` and `clang-apply-replacements`) available in\nyour `$PATH`. However, the system packaged binaries might be several versions\nbehind Chromium's toolchain, so not all flags are guaranteed to work. If this is\na problem, consider building clang-tidy from the same revision the current\ntoolchain is using, rather than filing a bug against the toolchain component.\nThis can be done as follows:\n```\ntools/clang/scripts/build_clang_tools_extra.py \\\n    --fetch out/Release clang-tidy clang-apply-replacements\n```\nRunning clang-tidy is then (hopefully) simple.\n1.  Build chrome normally.\n```\nninja -C out/Release chrome\n```\n2.  Export Chrome's compile command database\n```\ngn gen out/Release --export-compile-commands\n```\n3.  Enter the build directory\n```\ncd out/Release\n```\n4.  Run clang-tidy.\n```\n<PATH_TO_LLVM_SRC>/clang-tools-extra/clang-tidy/tool/run-clang-tidy.py \\\n    -p . \\# Set the root project directory, where compile_commands.json is.\n    # Set the clang-tidy binary path, if it's not in your $PATH.\n    -clang-tidy-binary <PATH_TO_LLVM_BUILD>/bin/clang-tidy \\\n    # Set the clang-apply-replacements binary path, if it's not in your $PATH\n    # and you are using the `fix` behavior of clang-tidy.\n    -clang-apply-replacements-binary \\\n        <PATH_TO_LLVM_BUILD>/bin/clang-apply-replacements \\\n    # The checks to employ in the build. Use `-*,...` to omit default checks.\n    -checks=<CHECKS> \\\n    -header-filter=<FILTER> \\# Optional, limit results to only certain files.\n    -fix \\# Optional, used if you want to have clang-tidy auto-fix errors.\n    'chrome/browser/.*' # A regex of the files you want to check.\n\nCopy-Paste Friendly (though you'll still need to stub in the variables):\n<PATH_TO_LLVM_SRC>/clang-tools-extra/clang-tidy/tool/run-clang-tidy.py \\\n    -p . \\\n    -clang-tidy-binary <PATH_TO_LLVM_BUILD>/bin/clang-tidy \\\n    -clang-apply-replacements-binary \\\n        <PATH_TO_LLVM_BUILD>/bin/clang-apply-replacements \\\n    -checks=<CHECKS> \\\n    -header-filter=<FILTER> \\\n    -fix \\\n    'chrome/browser/.*'\n```\n\nNote that the source file regex must match how the build specified the file.\nThis means that on Windows, you must use (escaped) backslashes even from a bash\nshell.\n\n### Questions\n\nQuestions about the local flow? Reach out to rdevlin.cronin@chromium.org,\nthakis@chromium.org, or gbiv@chromium.org.\n\nQuestions about the Gerrit flow? Email tricium-dev@google.com or\ninfra-dev+tricium@chromium.org, or file a bug against `Infra>LUCI>BuildService>PreSubmit>Tricium`.\nPlease CC gbiv@chromium.org and dcheng@chromium.org on any of these.\n\nDiscoveries? Update the doc!\n"
  },
  {
    "path": "development/clang_static_analyzer",
    "title": "The Clang Static Analyzer",
    "content": "# The Clang Static Analyzer\n\nThe Clang C/C++ compiler comes with a static analyzer which can be used to find\nbugs using path sensitive analysis. Path sensitive analysis is\na technique that explores all the possible branches in code and\nrecords the codepaths that might lead to bad or undefined behavior,\nlike an uninitialized reads, use after frees, pointer leaks, and so on.\n\nSee the [official Clang static analyzer page](http://clang-analyzer.llvm.org/)\nfor more background information.\n\nWe used to have a bot that continuously ran with the static analyzer,\nbut people used to not look at it much.\n\nThe static analyzer can still be invoked with [clang-tidy](clang_tidy.md).\n\n## Recommended checks\nClang's static analyzer comes with a wide variety of checkers. Some of the\nchecks aren't useful because they are intended for different languages,\nplatforms, or coding conventions than the ones used for Chromium development.\n\nCheckers we found useful were:\n\n    -analyzer-checker=core\n    -analyzer-checker=cpp\n    -analyzer-checker=unix\n    -analyzer-checker=deadcode\n\nAs of this writing, the checker suites we support are\n[core](https://clang-analyzer.llvm.org/available_checks.html#core_checkers),\n[cplusplus](https://clang-analyzer.llvm.org/available_checks.html#cplusplus_checkers), and\n[deadcode](https://clang-analyzer.llvm.org/available_checks.html#deadcode_checkers).\n\nTo easily run these checks against Chromium code via clang-tidy, follow\n[these](clang_tidy.md#evaluating_running-clang_tidy-across-chromium)\ninstructions to pull down `tricium_clang_tidy.py` and then pass the following\nargument when invoking the script (`-*` disables all checks and then the\nremaining check name globs enable each category of checks):\n```\n--tidy_checks=\"-*,clang-analyzer-core*,clang-analyzer-cplusplus*,clang-analyzer-unix*,clang-analyzer-deadcode*\"\n```\nA full list of Clang analyzer checks can be found in the\n[Clang-Tidy Checks List](https://clang.llvm.org/extra/clang-tidy/checks/list.html).\n\n## Addressing false positives\n\nSome of the errors you encounter will be false positives, which occurs when the\nstatic analyzer naively follows codepaths which are practically impossible to\nhit at runtime. Fortunately, we have a tool at our disposal for guiding the\nanalyzer away from impossible codepaths: assertion handlers like\nDCHECK/CHECK/LOG(FATAL).  The analyzer won't check the codepaths which we\nassert are unreachable.\n\nAn example would be that if the analyzer detected the function argument\n`*my_ptr` might be null and dereferencing it would potentially segfault, you\nwould see the error `warning: Dereference of null pointer (loaded from variable\n'my_ptr')`.  If you know for a fact that my_ptr will not be null in practice,\nthen you can place an assert at the top of the function: `DCHECK(my_ptr)`. The\nanalyzer will no longer generate the warning.\n\nBe mindful about only specifying assertions which are factually correct! Don't\nDCHECK recklessly just to quiet down the analyzer. :)\n\nOther types of false positives and their suppressions:\n* Unreachable code paths. To suppress, add the `ANALYZER_SKIP_THIS_PATH();`\n  directive to the relevant code block.\n* Dead stores. To suppress, use `[[maybe_unused]]`. This also suppresses dead\n  store warnings on conventional builds without static analysis enabled!\n\nSee the definitions of the `ANALYZER_*` macros in base/logging.h for more\ndetailed information about how the annotations are implemented.\n\n## Logging bugs\n\nIf you find any issues with the static analyzer, or find Chromium code behaving\nbadly with the analyzer, please check the `Infra>CodeAnalysis` CrBug component\nto look for known issues, or file a bug if it is a new problem.\n"
  },
  {
    "path": "development/clang_sheriffing",
    "title": "Clang Sheriffing",
    "content": "# Clang Sheriffing\n\nSee [Clang Gardening](clang_gardening.md).\n"
  },
  {
    "path": "development/clang_gardening",
    "title": "Clang Gardening",
    "content": "# Clang Gardening\n\nChromium bundles its own pre-built version of [Clang](clang.md). This is done so\nthat Chromium developers have access to the latest and greatest developer tools\nprovided by Clang and LLVM (ASan, CFI, coverage, etc). In order to [update the\ncompiler](updating_clang.md) (roll clang), it has to be tested so that we can be\nconfident that it works in the configurations that Chromium cares about.\n\nWe maintain a [waterfall of\nbuilders](https://ci.chromium.org/p/chromium/g/chromium.clang/console) that\ncontinuously build fresh versions of Clang and use them to build and test\nChromium. \"Clang gardening\" is the process of monitoring that waterfall,\ndetermining if any compile or test failures are due to an upstream compiler\nchange, filing bugs upstream, and often reverting bad changes in LLVM. This\ndocument describes some of the processes and techniques for doing that.\n\nSome may find the\n[sheriff-o-matic](https://sheriff-o-matic.appspot.com/chromium.clang)\nview of the waterfall easier to work with.\n\nTo keep others informed, [file a\nbug](https://bugs.chromium.org/p/chromium/issues/entry).\nearlier rather than later for build breaks likely caused by changes in\nclang or the rest fo the toolchain. Make sure to set the component field to\n`Tools > LLVM`, which will include the entire Chrome toolchain (Lexan) team.\n\nAt the beginning of your gardener rotation, it may be\nuseful to [search for recent bot\nbreaks](https://bugs.chromium.org/p/chromium/issues/list?q=component%3ATools%3ELLVM&can=2&sort=-modified).\nWe prefer searching like this to having gardeners compose status email at the\nend of their week.\n\nIn addition to the waterfall, make sure\n[dry run attempts at updating clang](https://chromium-review.googlesource.com/q/path:tools/clang/scripts/update.py)\nare green. As part of the Clang release process we run upstream LLVM tests.\nIdeally these tests are covered by upstream LLVM bots and breakages are\nquickly noticed and fixed by the original author of a breaking commit,\nbut that is sadly not always the case.\n\nEach gardener should attempt to update the compiler by performing\n[a Clang roll](updating_clang.md) during their week, assuming the bots are\ngreen enough.\n\nThe gardener is also responsible for taking notes during the weekly Chrome toolchain\n(Lexan) status sync-up meeting.\n\n[TOC]\n\n## Disk out of space\n\nIf there are any issues with disk running out of space, file a go/bug-a-trooper\nbug, for example https://crbug.com/1105134.\n\n## Is it the compiler?\n\nChromium does not always build and pass tests in all configurations that\neveryone cares about. Some configurations simply take too long to build\n(ThinLTO) or be tested (dbg) on the CQ before committing. And, some tests are\nflaky. So, our console is often filled with red boxes, and the boxes don't\nalways need to be green to roll clang.\n\nOftentimes, if a bot is red with a test failure, it's not a bug in the compiler.\nTo check this, the easiest and best thing to do is to try to find a\ncorresponding builder that doesn't use ToT clang. For standard configurations,\nstart on the waterfall that corresponds to the OS of the red bot, and search\nfrom there. If the failing bot is Google Chrome branded, go to the (Google\ninternal) [official builder\nlist](https://uberchromegw.corp.google.com/i/official.desktop.continuous/builders/)\nand start searching from there.\n\nIf you are feeling charitable, you can try to see when the test failure was\nintroduced by looking at the history in the bot. One way to do this is to add\n`?numbuilds=200` to the builder URL to see more history. If that isn't enough\nhistory, you can manually binary search build numbers by editing the URL until\nyou find where the regression was introduced. If it's immediately clear what CL\nintroduced the regression (i.e.  caused tests to fail reliably in the official\nbuild configuration), you can simply load the change in gerrit and revert it,\nlinking to the first failing build that implicates the change being reverted.\n\nIf the failure looks like a compiler bug, these are the common failures we see\nand what to do about them:\n\n1. compiler crash\n1. compiler warning change\n1. compiler error\n1. miscompile\n1. linker errors\n\n## Compiler crash\n\nThis is probably the most common bug. The standard procedure is to do these\nthings:\n\n1. Open the `gclient runhooks` stdout log from the first red build.  Near the\n   top of that log you can find the range of upstream llvm revisions.  For\n   example:\n\n       From https://github.com/llvm/llvm-project\n           f917356f9ce..292e898c16d  master     -> origin/master\n\n1. File a crbug documenting the crash. Include the range, and any other bots\n   displaying the same symptoms.\n1. All clang crashes on the Chromium bots are automatically uploaded to\n   Cloud Storage. On the failing build, click the \"stdout\" link of the\n   \"process clang crashes\" step right after the red compile step. It will\n   print something like\n\n       processing heap_page-65b34d... compressing... uploading... done\n           gs://chrome-clang-crash-reports/v1/2019/08/27/chromium.clang-ToTMac-20955-heap_page-65b34d.tgz\n       removing heap_page-65b34d.sh\n       removing heap_page-65b34d.cpp\n\n   Use\n   `gsutil.py cp gs://chrome-clang-crash-reports/v1/2019/08/27/chromium.clang-ToTMac-20955-heap_page-65b34d.tgz .`\n   to copy it to your local machine. Untar with\n   `tar xzf chromium.clang-ToTMac-20955-heap_page-65b34d.tgz` and change the\n   included shell script to point to a locally-built clang. Remove the\n   `-Xclang -plugin` flags.  If you re-run the shell script, it should\n   reproduce the crash.\n1. Identify the revision that introduced the crash. First, look at the commit\n   messages in the LLVM revision range to see if one modifies the code near the\n   point of the crash. If so, try reverting it locally, rebuild, and run the\n   reproducer to see if the crash goes away.\n\n   If that doesn't work, use `git bisect`. Use this as a template for the bisect\n   run script:\n   ```shell\n   #!/bin/bash\n   cd $(dirname $0)  # get into llvm build dir\n   ninja -j900 clang || exit 125 # skip revisions that don't compile\n   ./t-8f292b.sh || exit 1  # exit 0 if good, 1 if bad\n   ```\n1. File an upstream bug like http://llvm.org/PR43016. Usually the unminimized repro\n   is too large for LLVM's bugzilla, so attach it to a (public) crbug and link\n   to that from the LLVM bug. Then revert with a commit message like\n   \"Revert r368987, it caused PR43016.\"\n1. If you want, make a reduced repro using CReduce.  Clang contains a handy wrapper around\n   CReduce that you can invoke like so:\n\n       clang/utils/creduce-clang-crash.py --llvm-bin bin \\\n           angle_deqp_gtest-d421b0.sh angle_deqp_gtest-d421b0.cpp\n\n   Attach the reproducer to the llvm bug you filed in the previous step. You can\n   disable Creduce's renaming passes with the options\n   `--remove-pass pass_clang rename-fun --remove-pass pass_clang rename-param\n   --remove-pass pass_clang rename-var --remove-pass pass_clang rename-class\n   --remove-pass pass_clang rename-cxx-method --remove-pass pass_clex\n   rename-toks` which makes it easier for the author to reason about and to\n   further reduce it manually.\n\n   If you need to do something the wrapper doesn't support,\n   follow the [official CReduce docs](https://embed.cs.utah.edu/creduce/using/)\n   for writing an interestingness test and use creduce directly.\n\n## Compiler warning change\n\nNew Clang versions often find new bad code patterns to warn on. Chromium builds\nwith `-Werror`, so improvements to warnings often turn into build failures in\nChromium. Once you understand the code pattern Clang is complaining about, file\na bug to do either fix or silence the new warning.\n\nIf this is a completely new warning, disable it by adding `-Wno-NEW-WARNING` to\n[this list of disabled\nwarnings](https://cs.chromium.org/chromium/src/build/config/compiler/BUILD.gn?l=1479)\nif `llvm_force_head_revision` is true. Here is [an\nexample](https://chromium-review.googlesource.com/1251622). This will keep the\nToT bots green while you decide what to do.\n\nSometimes, behavior changes and a pre-existing warning changes to warn on new\ncode. In this case, fixing Chromium may be the easiest and quickest fix. If\nthere are many sites, you may consider changing clang to put the new diagnostic\ninto a new warning group so you can handle it as a new warning as described\nabove.\n\nIf the warning is high value, then eventually our team or other contributors\nwill end up fixing the crbug and there is nothing more to do.  If the warning\nseems low value, pass that feedback along to the author of the new warning\nupstream. It's unlikely that it should be on by default or enabled by `-Wall` if\nusers don't find it valuable. If the warning is particularly noisy and can't be\neasily disabled without disabling other high value warnings, you should consider\nreverting the change upstream and asking for more discussion.\n\n## Compiler error\n\nThis rarely happens, but sometimes clang becomes more strict and no longer\naccepts code that it previously did. The standard procedure for a new warning\nmay apply, but it's more likely that the upstream Clang change should be\nreverted, if the C++ code in question in Chromium looks valid.\n\n## Miscompile\n\nMiscompiles tend to result in crashes, so if you see a test with the CRASHED\nstatus, this is probably what you want to do.\n\n1. Bisect object files to find the object with the code that changed. LLVM\n   contains `llvm/utils/rsp_bisect.py` which may be useful for bisecting object\n   files using an rsp file.\n1. Debug it with a traditional debugger\n\n## Linker error\n\n`ld.lld`'s `--reproduce` flag makes LLD write a tar archive of all its inputs\nand a file `response.txt` that contains the link command. This allows people to\nwork on linker bugs without having to have a Chromium build environment.\n\nTo use `ld.lld`'s `--reproduce` flag, follow these steps:\n\n1. Locally (build Chromium with a locally-built\n   clang)[https://chromium.googlesource.com/chromium/src.git/+/main/docs/clang.md#Using-a-custom-clang-binary]\n\n1. After reproducing the link error, build just the failing target with\n   ninja's `-v -d keeprsp` flags added:\n  `ninja -C out/gn base_unittests -v -d keeprsp`.\n\n1. Copy the link command that ninja prints, `cd out/gn`, paste it, and manually\n   append `-Wl,--reproduce,repro.tar`. With `lld-link`, instead append\n   `/reproduce:repro.tar`. (`ld.lld` is invoked through the `clang` driver, so\n   it needs `-Wl` to pass the flag through to the linker. `lld-link` is called\n   directly, so the flag needs no prefix.)\n\n1. Zip up the tar file: `gzip repro.tar`. This will take a few minutes and\n   produce a .tar.gz file that's 0.5-1 GB.\n\n1. Upload the .tar.gz to Google Drive. If you're signed in with your @google\n   address, you won't be able to make a world-shareable link to it, so upload\n   it in a Window where you're signed in with your @chromium account.\n\n1. File an LLVM bug linking to the file. Example: http://llvm.org/PR43241\n\nTODO: Describe object file bisection, identify obj with symbol that no longer\nhas the section.\n\n## ThinLTO Trouble\n\nSometimes, problems occur in ThinLTO builds that do not occur in non-LTO builds.\nThese steps can be used to debug such problems.\n\nNotes:\n\n - All steps assume they are run from the output directory (the same directory args.gn is in).\n\n - Commands have been shortened for clarity. In particular, Chromium build commands are\n   generally long, with many parts that you just copy-paste when debugging. These have\n   largely been omitted.\n\n - The commands below use \"clang++\", where in practice there would be some path prefix\n   in front of this. Make sure you are invoking the right clang++. In particular, there\n   may be one in the PATH which behaves very differently.\n\n### Get the full command that is used for linking\n\nTo get the command that is used to link base_unittests:\n\n```sh\n$ rm base_unittests\n$ ninja -n -d keeprsp -v base_unittests\n```\n\nThis will print a command line. It will also write a file called `base_unittests.rsp`, which\ncontains additional parameters to be passed.\n\n### Remove ThinLTO cache flags\n\nThinLTO uses a cache to avoid compilation in some cases. This can be confusing\nwhen debugging, so make sure to remove the various cache flags like\n`-Wl,--thinlto-cache-dir`.\n\n### Expand Thin Archives on Command Line\n\nExpand thin archives mentioned in the command line to their individual object files.\nThe script `tools/clang/scripts/expand_thin_archives.py` can be used for this purpose.\nFor example:\n\n```sh\n$ ../../tools/clang/scripts/expand_thin_archives.py -p=-Wl, -- @base_unittests.rsp > base_unittests.expanded.rsp\n```\n\nThe `-p` parameter here specifies the prefix for parameters to be passed to the linker.\nIf you are invoking the linker directly (as opposed to through clang++), the prefix should\nbe empty.\n\n```sh\n$ ../../tools/clang/scripts/expand_thin_archives.py -p='', -- @base_unittests.rsp > base_unittests.expanded.rsp\n```\n\n### Remove -Wl,--start-group and -Wl,--end-group\n\nEdit the link command to use the expanded command line, and remove any mention of `-Wl,--start-group`\nand `-Wl,--end-group` that surround the expanded command line. For example, if the original command was:\n\n    clang++ -fuse-ld=lld -o ./base_unittests -Wl,--start-group @base_unittests.rsp -Wl,--end-group\n\nthe new command should be:\n\n    clang++ -fuse-ld=lld -o ./base_unittests @base_unittests.expanded.rsp\n\nThe reason for this is that the `-start-lib` and `-end-lib` flags that expanding the command\nline produces cannot be nested inside `--start-group` and `--end-group`.\n\n### Producing ThinLTO Bitcode Files\n\nIn a ThinLTO build, what is normally the compile step that produces native object files\ninstead produces LLVM bitcode files. A simple example would be:\n\n```sh\n$ clang++ -c -flto=thin foo.cpp -o foo.o\n```\n\nIn a Chromium build, these files reside under `obj/`, and you can generate them using ninja.\nFor example:\n\n```sh\n$ ninja obj/base/base/lock.o\n```\n\nThese can be fed to `llvm-dis` to produce textual LLVM IR:\n   \n```\n$ llvm-dis -o - obj/base/base/lock.o | less\n```\n\nWhen using split LTO unit (`-fsplit-lto-unit`, which is required for\nsome features, CFI among them), this may produce a message like:\n\n    llvm-dis: error: Expected a single module\n\n   In that case, you can use `llvm-modextract`:\n   \n```sh\n$ llvm-modextract -n 0 -o - obj/base/base/lock.o | llvm-dis -o - | less\n```\n\n### Saving Intermediate Bitcode\n\nThe ThinLTO linking process proceeds in a number of stages. The bitcode that is\ngenerated during these stages can be saved by passing `-save-temps` to the linker:\n\n```\n$ clang++ -fuse-ld=lld -Wl,-save-temps -o ./base_unittests @base_unittests.expanded.rsp\n```\n\nThis generates files such as:\n - lock.o.0.preopt.bc\n - lock.o.3.import.bc\n - lock.o.5.precodegen.bc\n\nin the directory where lock.o is (obj/base/base).\n\nThese can be fed to `llvm-dis` to produce textual LLVM IR. They show\nhow the code is transformed as it progresses through ThinLTO stages.\nOf particular interest are:\n - .3.import.bc, which shows the IR after definitions have been imported from\n   other modules, but before optimizations. Running this through LLVM's `opt`\n   tool with the right optimization level can often reproduce issues.\n - .5.precodegen.bc, which shows the IR just before it is transformed to native\n   code. Running this through LLVM's `llc` tool with the right optimization level\n   can often reproduce issues.\n\nThe same `-save-temps` command also produces `base_unittests.resolution.txt`, which\nshows symbol resolutions. These look like:\n\n    -r=obj/base/test/run_all_base_unittests/run_all_base_unittests.o,main,plx\n\nIn this example, run_all_base_unittests.o contains a symbol named\nmain, with flags plx.\n   \nThe possible flags are:\n - p: prevailing: of symbols with this name, this one has been chosen.\n - l: final definition in this linkage unit.\n - r: redefined by the linker.\n - x: visible to regular (that is, non-LTO) objects.\n\n### Code Generation for a Single Module\n\nTo speed up debugging, it may be helpful to limit code generation to a single\nmodule if you know the name of the module (e.g. the module name is in a crash\ndump).\n\n`-Wl,--thinlto-single-module=foo` tells ThinLTO to only run\noptimizations/codegen on files matching the pattern and skip linking. This is\nhelpful especially in combination with `-Wl,-save-temps`.\n\n```sh\n$ clang++ -fuse-ld=lld -Wl,--thinlto-single-module=obj/base/base/lock.o -o ./base_unittests @base_unittests.expanded.rsp\n```\n\nYou should see\n\n```sh\n[ThinLTO] Selecting obj/base/base/lock.o to compile\n```\n\nbeing printed.\n\n## Tips and tricks\n\nFinding what object files differ between two directories:\n\n```\n$ diff -u <(cd out.good && find . -name \"*.o\" -exec sha1sum {} \\; | sort -k2) \\\n          <(cd out.bad  && find . -name \"*.o\" -exec sha1sum {} \\; | sort -k2)\n```\n\nOr with cmp:\n\n```\n$ find good -name \"*.o\" -exec bash -c 'cmp -s $0 ${0/good/bad} || echo $0' {} \\;\n```\n"
  },
  {
    "path": "development/clang_format",
    "title": "Using clang-format on Chromium C++ Code",
    "content": "# Using clang-format on Chromium C++ Code\n\n[TOC]\n\n*** note\nNOTE: This page does not apply to the Chromium OS project. See [Chromium Issue\n878506](https://bugs.chromium.org/p/chromium/issues/detail?id=878506#c10)\nfor updates.\n***\n\n## Easiest usage, from the command line\n\nTo automatically format a pending patch according to\n[Chromium style](/styleguide/c++/c++.md), run: `git cl format` from the command\nline. This should work on all platforms without any extra set up: the tool is\nintegrated with depot_tools and the Chromium checkout.\n\nLike other `git-cl` commands, this operates on a diff relative to the upstream\nbranch. Only the lines that changed in a CL will be reformatted. To see what\nclang-format would choose, commit any local changes and then run `git cl\nformat` followed by `git diff`. Alternatively, run `git cl format` and commit\nthe now-formatted code.\n\n## Editor integrations\n\nMany developers find it useful to integrate the clang-format tool with their\neditor of choice. As a convenience, the scripts for this are also available in\nyour checkout of Chrome under\n[src/third_party/clang-format/script/](https://source.chromium.org/chromium/chromium/src/+/HEAD:third_party/clang-format/script/).\n\nIf you use an editor integration, you should try to make sure that you're using\nthe version of clang-format that comes with your checkout. That way, you'll\nautomatically get updates and be running a tool that formats consistently with\nother developers. The binary lives under `src/buildtools`, but it's also in your\npath indirectly via a `depot_tools` launcher script:\n[clang-format](https://source.chromium.org/chromium/chromium/tools/depot_tools/+/HEAD:clang-format)\n([clang-format.bat](https://source.chromium.org/chromium/chromium/tools/depot_tools/+/HEAD:clang-format.bat) on Windows). Assuming that `depot_tools` is in your editor's `PATH`\nand the editor command runs from a working directory inside the Chromium\ncheckout, the editor scripts (which anticipate clang-format on the path) should\nwork.\n\nFor further guidance on editor integration, see these specific pages:\n\n*   [Sublime Text](https://www.chromium.org/developers/sublime-text#TOC-Format-selection-or-area-around-cursor-using-clang-format)\n*   [llvm's guidelines for vim, emacs, and bbedit](http://clang.llvm.org/docs/ClangFormat.html)\n*   [Visual Studio Code](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/vscode.md#useful-extensions)\n*   For vim, `:so tools/vim/clang-format.vim` and then hit cmd-shift-i (mac)\n    ctrl-shift-i (elsewhere) to indent the current line or current selection.\n\n## Reporting problems\n\nIf clang-format is broken, or produces badly formatted code, please file a\n[bug]. Assign it to thakis@chromium.org or dcheng@chromium.org, who will route\nit upstream.\n\n[bug]:\nhttps://code.google.com/p/chromium/issues/entry?comment=clang-format%20produced%20code%20that%20(choose%20all%20that%20apply):%20%0A-%20Doesn%27t%20match%20Chromium%20style%0A-%20Doesn%27t%20match%20blink%20style%0A-%20Riles%20my%20finely%20honed%20stylistic%20dander%0A-%20No%20sane%20human%20would%20ever%20choose%0A%0AHere%27s%20the%20code%20before%20formatting:%0A%0A%0AHere%27s%20the%20code%20after%20formatting:%0A%0A%0AHere%27s%20how%20it%20ought%20to%20look:%0A%0A%0ACode%20review%20link%20for%20full%20files/context:&summary=clang-format%20quality%20problem&cc=thakis@chromium.org&labels=Type-Bug,Build-Tools,OS-?,clang-format\n\n## Are robots taking over my freedom to choose where newlines go?\n\nMostly. At upload time, a presubmit check warns if a CL is not clang-formatted,\nbut this is a non-blocking warning, and the CL may still be submitted. Even so,\ntry to prefer clang-format's output when possible:\n\n- While clang-format does not necessarily format code the exact same way a human\n  might choose, it produces style-conformat code by design. This can allow\n  development and review time to be focused on discovering functional defects,\n  addressing readability/understandability concerns that can't be automatically\n  fixed by tooling, et cetera.\n- Continually fighting the tooling is a losing battle. Most Chromium developers\n  use clang-format. Large-scale changes will simply run `git cl format` once to\n  avoid having to deal with the particulars of formatting. Over time, this will\n  likely undo any carefully-curated manual formatting of the affected lines.\n\nThere is one notable exception where clang-format is often disabled: large\ntables of data are often surrounded by `// clang-format off` and `//\nclang-format on`. Try to use this option sparingly, as widespread usage makes\ntool-assisted refactoring more difficult.\n\nAgain, if clang-format produces something odd, please err on the side of\n[reporting an issue](#Reporting-problems): bugs that aren't reported can't be\nfixed.\n"
  },
  {
    "path": "development/clang_code_coverage_wrapper",
    "title": "The Clang Code Coverage Wrapper",
    "content": "# The Clang Code Coverage Wrapper\n\nThe Clang code coverage wrapper\n([//build/toolchain/clang_code_coverage_wrapper.py]) is a compiler wrapper that\nadds code coverage flags to the invocations of Clang C/C++ compiler, which can\nbe used to instrument and generate code coverage data for a subset of the source\nfiles. The main use case of this wrapper is to generate code coverage reports\nfor changed files at a per-CL level during try jobs.\n\nOne might ask: why this compiler wrapper instead of just instrumenting all the\nsource files?\n\nEfficiency is the main consideration because comparing to instrument everything,\nonly instrument what-is-needed takes less time to build the test targets,\nproduces binaries that are 10 times smaller, run tests twice faster and\ngenerates coverage reports significantly faster. An experiment was done to\nevaluate the effectiveness of instrument what-is-needed, and in the experiment,\n`unit_tests` is used as the sample test target and 8 randomly chosen C++ source\nfiles are used as the samples files to instrument. The results are presented in\nthe following table:\n\n| Type               | Build time | Binary size | Tests run time | Reports generation time |\n|--------------------|------------|-------------|----------------|-------------------------|\n| No instrumentation | 43m        | 240M        | 40s            | 0s                      |\n| What-is-needed     | 43m        | 241M        | 40s            | 0.14s                   |\n| Everything         | 53m        | 2.3G        | 80s            | 1m10s                   |\n\nIt can be seen from the results that the overhead introduced by instrument\neverything is huge, while it's negligible for instrument what-is-needed.\n\n## How to use the coverage wrapper?\nTo get the coverage wrapper hook into your build, add the following flags to\nyour `args.gn` file, assuming build directory is `out/Release`.\n\n```\nuse_clang_coverage = true\ncoverage_instrumentation_input_file = \"//out/Release/coverage_instrumentation_input.txt\"\n```\n\nThe path to the coverage instrumentation input file should be a source root\nabsolute path, and the file consists of multiple lines where each line\nrepresents a path to a source file, and the specified paths must be relative to\nthe root build directory. e.g. `../../base/task/post_task.cc` for build\ndirectory `out/Release`.\n\nThen, use the [//tools/code_coverage/coverage.py] script to build, run tests and\ngenerate code coverage reports for the exercised files.\n\n## Caveats\nOne caveat with this compiler wrapper is that it may introduce unexpected\nbehaviors in incremental builds when the file path to the coverage\ninstrumentation input file changes between consecutive runs, and the reason is\nthat the coverage instrumentation input file is explicitly passed to the\ncoverage wrapper as a command-line argument, which makes the path part of the\nNinja commands used to compile the source files, so change of the path between\nconsecutive runs causes Ninja to perform a full rebuild.\n\nDue to the reasons mentioned above, users of this script are strongly advised to\nalways use the same path such as\n`${root_build_dir}/coverage_instrumentation_input.txt`.\n\n## Want to learn more about code coverage in Chromium?\nFor more background information on how code coverage works in Chromium, please\nrefer to [code_coverage.md]\n\n## Contacts\n\n### Reporting problems\nWe're still evaluating the tools, for any breakage report and feature requests,\nplease [file a bug].\n\n### Mailing list\nFor questions and general discussions, please join [code-coverage group].\n\n[//build/toolchain/clang_code_coverage_wrapper.py]: ../build/toolchain/clang_code_coverage_wrapper.py\n[code_coverage.md]: testing/code_coverage.md\n[//tools/code_coverage/coverage.py]: ../tools/code_coverage/coverage.py\n[file a bug]: https://bugs.chromium.org/p/chromium/issues/entry?components=Infra%3ETest%3ECodeCoverage\n[code-coverage group]: https://groups.google.com/a/chromium.org/forum/#!forum/code-coverage\n"
  },
  {
    "path": "development/clangd",
    "title": "Clangd",
    "content": "# Clangd\n\n## Introduction\n\n[clangd](https://clangd.llvm.org/) is a clang-based [language server](https://langserver.org/).\nIt brings IDE features (e.g. diagnostics, code completion, code navigations) to\nyour editor.\n\n## Quick Start\n\n* [Get clangd](#getting-clangd)\n* Make sure generated ninja files are up-to-date\n* Optional: build chrome normally to get generated headers\n* Generate compilation database (note: it's not regenerated automatically):\n```\ntools/clang/scripts/generate_compdb.py -p out/Default > compile_commands.json\n```\n\n*** note\nNote: If you're using a different build directory, you'll need to replace `out/Default`\nin this and other commands with your build directory.\n***\n\n* Indexing is enabled by default (since clangd 9), note that this might consume\n  lots of CPU and RAM. There's also a\n  [remote-index service](https://github.com/clangd/chrome-remote-index/blob/main/docs/index.md)\n  to have an instant project-wide index without consuming local resources\n  (requires clangd 12+ built with remote index support).\n* Use clangd in your favourite editor\n\n## Getting clangd\n\nFor the best results, you should use a clangd that exactly matches the version\nof Clang used by Chromium. This avoids problems like mismatched versions of\ncompiler diagnostics.\n\nThe easiest way to do this is to set the `checkout_clangd` var in `.gclient`:\n\n```\nsolutions = [\n  {\n    \"url\": \"https://chromium.googlesource.com/chromium/src.git\",\n    \"managed\": False,\n    \"name\": \"src\",\n    \"custom_deps\": {},\n    \"custom_vars\": {\n      \"checkout_clangd\": True,\n    },\n  },\n]\n```\n\nAfter this, `gclient` will keep the binary at\n`third_party/llvm-build/Release+Asserts/bin/clangd` in sync with the version of\nClang used by Chromium.\n\nAlternatively, you may use the `build_clang_tools_extra.py` script to build\nclangd from source:\n\n```\ntools/clang/scripts/build_clang_tools_extra.py --fetch out/Default clangd\n```\n\nThe resulting binary will be at\n`out/Default/tools/clang/third_party/llvm/build/bin/clangd`.\n\nOnce you have an appropriate clangd binary, you must configure your editor to\nuse it, either by placing it first on your `PATH`, or through editor-specific\nconfiguration.\n\n*** note\nNote: The clangd provided by Chromium does not support optional features like\nremote indexing (see https://crbug.com/1358258), such that `clangd --version`\nwill not mention `grpc`, and you will see “Unknown Index key External” warnings\nin the clangd log.\n\nIf you want those features, you'll need to use a different build of clangd,\nsuch as the [clangd/clangd releases on\nGitHub](https://github.com/clangd/clangd/releases).\n***\n\n## Setting Up\n\n1. Make sure generated ninja files are up-to-date.\n\n```\ngn gen out/Default\n```\n\n2. Generate the compilation database, clangd needs it to know how to build a\nsource file.\n\n```\ntools/clang/scripts/generate_compdb.py -p out/Default > compile_commands.json\n```\n\nNote: the compilation database is not regenerated automatically. You need to\nregenerate it manually whenever build rules change, e.g., when you have new files\nchecked in or when you sync to head.\n\nIf using Windows PowerShell, use the following command instead to set the\noutput's encoding to UTF-8 (otherwise Clangd will hit \"YAML:1:4: error: Got\nempty plain scalar\" while parsing it).\n\n```\ntools/clang/scripts/generate_compdb.py -p out/Default | out-file -encoding utf8 compile_commands.json\n```\n\n3. Optional: build chrome normally. This ensures generated headers exist and are\nup-to-date. clangd will still work without this step, but it may give errors or\ninaccurate results for files which depend on generated headers.\n\n```\nninja -C out/Default chrome\n```\n\n4. Optional: configure clangd to use remote-index service for an instant\n   project-wide index and reduced local CPU and RAM usage. See\n   [instructions](https://github.com/clangd/chrome-remote-index/blob/main/docs/index.md).\n\n5. Use clangd in your favourite editor, see detailed [instructions](\nhttps://clangd.llvm.org/installation.html#editor-plugins).\n\n    * Optional: You may want to add `--header-insertion=never` to the clangd\n      flags, so that your editor doesn't automatically add incorrect #include\n      lines. The feature doesn't correctly handle some common Chromium headers\n      like `base/functional/callback_forward.h`.\n\n## Background Indexing\n\nclangd builds an incremental index of your project (all files listed in the\ncompilation database). The index improves code navigation features\n(go-to-definition, find-references) and code completion.\n\n* clangd only uses idle cores to build the index, you can limit the total amount\n  of cores by passing the *-j=\\<number\\>* flag;\n* the index is saved to the `.cache/clangd/index` in the project root; index\n  shards for common headers e.g. STL will be stored in\n  *$HOME/.cache/clangd/index*;\n* background indexing can be disabled by the `--background-index=false` flag;\n  Note that, disabling background-index will limit clangd’s knowledge about your\n  codebase to files you are currently editing.\n\nNote: the first index time may take hours (for reference, it took 2~3 hours on\na 48-core, 64GB machine). A full index of Chromium (including v8, blink) takes\n~550 MB disk space and ~2.7 GB memory in clangd.\n\nNote: [Remote-index service](https://github.com/clangd/chrome-remote-index/blob/main/docs/index.md)\nreplaces background-index with some downsides like being ~a day old (Clangd will\nstill know about your changes in the current editing session) and not covering\nall configurations (not available for mac&windows specific code or non-main\nbranches).\n\n## Questions\n\nIf you have any questions, reach out to\n[clangd/clangd](https://github.com/clangd/clangd) or clangd-dev@lists.llvm.org.\n"
  },
  {
    "path": "development/clang",
    "title": "Clang",
    "content": "# Clang\n\nChromium ships a prebuilt [clang](http://clang.llvm.org) binary.\nIt's just upstream clang built at a known-good revision that we\nbump every two weeks or so.\n\nThis is the only supported compiler for building Chromium.\n\n[TOC]\n\n## Using gcc on Linux\n\n`is_clang = false` will make the build use system gcc on Linux. There are no\nbots that test this and there is no guarantee it will work, but we accept\npatches for this configuration.\n\n## Mailing List\n\nhttps://groups.google.com/a/chromium.org/group/clang/topics\n\n## Using plugins\n\nThe\n[chromium style plugin](https://dev.chromium.org/developers/coding-style/chromium-style-checker-errors)\nis used by default when clang is used.\n\nIf you're working on the plugin, you can build it locally like so:\n\n1.  Run `./tools/clang/scripts/build.py --without-android --without-fuchsia`\n    to build the plugin.\n1.  Run `ninja -C third_party/llvm-build/Release+Asserts/` to build incrementally.\n1.  Build with clang like described above, but, if you use reclient, disable it.\n\nTo test the FindBadConstructs plugin, run:\n\n    (cd tools/clang/plugins/tests && \\\n     ./test.py ../../../../third_party/llvm-build/Release+Asserts/bin/clang)\n\nSince the plugin is rolled with clang changes, behavior changes to the plugin\nshould be guarded by flags to make it easy to roll clang. A general outline:\n1.  Implement new plugin behavior behind a flag.\n1.  Wait for a compiler roll to bring in the flag.\n1.  Start passing the new flag in `GN` and verify the new behavior.\n1.  Enable the new plugin behavior unconditionally and update the plugin to\n    ignore the flag.\n1.  Wait for another compiler roll.\n1.  Stop passing the flag from `GN`.\n1.  Remove the flag completely.\n\n## Using the clang static analyzer\n\nSee [clang_static_analyzer.md](clang_static_analyzer.md).\n\n## Windows\n\nclang is the default compiler on Windows. It uses MSVC's SDK, so you still need\nto have Visual Studio with C++ support installed.\n\n## Using a custom clang binary\n\nSet `clang_base_path` in your args.gn to the llvm build directory containing\n`bin/clang` (i.e. the directory you ran cmake). This must be an absolute\npath. You also need to disable chromium's clang plugin.\n\nHere's an example that also disables debug info and enables the component build\n(both not strictly necessary, but they will speed up your build):\n\n```\nclang_base_path = getenv(\"HOME\") + \"/src/llvm-build\"\nclang_use_chrome_plugins = false\nis_debug = false\nsymbol_level = 1\nis_component_build = true\n```\n\nOn Windows, for `clang_base_path` use something like this instead:\n\n```\nclang_base_path = \"c:/src/llvm-build\"\n```\n\nYou can then look in `out/gn/toolchain.ninja` and check that the `rule cc` and\n`rule cxx` commands run your clang binary.  If things look good, run `ninja\n-C out/gn` to build.\n\nChromium tries to be buildable with its currently pinned clang, and with clang\ntrunk. Set `llvm_force_head_revision = true` in your args.gn if the clang you're\ntrying to build with is closer to clang trunk than to Chromium's pinned clang\n(which `tools/clang/scripts/update.py --print-revision` prints).\n\n## Related documents\n\n* [Toolchain support](toolchain_support.md) gives an overview of clang\n  rolls, and documents when to revert clang rolls and how to file good\n  toolchain bugs.\n\n* [Updating clang](updating_clang.md) documents the mechanics of updating clang,\n  and which files are included in the default clang package.\n\n* [Clang Sheriffing](clang_sheriffing.md) contains instructions for how to debug\n  compiler bugs, for clang sheriffs.\n\n* [Clang Tool Refactoring](clang_tool_refactoring.md) has notes on how to build\n  and run refactoring tools based on clang's libraries.\n\n* [Updating Clang format binaries](updating_clang_format_binaries.md) has notes\n  on how to update clang-format.\n"
  },
  {
    "path": "development/testing/writing_web_tests",
    "title": "Writing Web Tests",
    "content": "# Writing Web Tests\n\n[TOC]\n\n## Overview\n\nWeb tests should be used to accomplish one of the following goals:\n\n1. The entire surface of Blink that is exposed to the Web should be covered by\n   tests that we contribute to [web-platform-tests](./web_platform_tests.md)\n   (WPT). This helps us avoid regressions, and helps us identify Web Platform\n   areas where the major browsers don't have interoperable implementations.\n   Furthermore, by contributing to projects such as WPT, we share the burden of\n   writing tests with the other browser vendors, and we help all the browsers\n   get better. This is very much in line with our goal to move the Web forward.\n2. When a Blink feature cannot be tested using the tools provided by WPT, and\n   cannot be easily covered by\n   [C++ unit tests](https://cs.chromium.org/chromium/src/third_party/blink/renderer/web/tests/?q=webframetest&sq=package:chromium&type=cs),\n   the feature must be covered by web tests, to avoid unexpected regressions.\n   These tests will use Blink-specific testing APIs that are only available in\n   [content_shell](./web_tests_in_content_shell.md).\n\nNote: if you are looking for a guide for the Web Platform Test, you should read\n[\"Web platform tests\"](./web_platform_tests.md) (WPT). This document does not\ncover WPT specific features/behaviors. **The WPT is recommended today rather than\ntest types mentioned below!**\n\n*** promo\nIf you know that Blink web tests are upstreamed to other projects, such as\n[test262](https://github.com/tc39/test262), please update this document. Most\nimportantly, our guidelines should to make it easy for our tests to be\nupstreamed. The\n[blink-dev mailing list](https://groups.google.com/a/chromium.org/forum/#!forum/blink-dev)\nwill be happy to help you harmonize our current guidelines with communal test\nrepositories.\n***\n\n### Test Types\n\nThere are four broad types of web tests, listed in the order of preference.\n\n* *JavaScript Tests* are the web test implementation of\n  [xUnit tests](https://en.wikipedia.org/wiki/XUnit). These tests contain\n  assertions written in JavaScript, and pass if the assertions evaluate to\n  true.\n* *Reference Tests* render a test page and a reference page, and pass if the two\n  renderings are identical, according to a pixel-by-pixel comparison. These\n  tests are less robust, harder to debug, and significantly slower than\n  JavaScript tests, and are only used when JavaScript tests are insufficient,\n  such as when testing paint code.\n* *Pixel Tests* render a test page and compare the result against a pre-rendered\n  baseline image in the repository. Pixel tests are less robust than the\n  first two types, because the rendering of a page is influenced by\n  many factors such as the host computer's graphics card and driver, the\n  platform's text rendering system, and various user-configurable operating\n  system settings. For this reason, it is common for a pixel test to have a\n  different reference image for each platform that Blink is tested on, and\n  the reference images are\n  [quite cumbersome to manage](./web_test_expectations.md). You\n  should only write a pixel test if you cannot use a reference test.\n* *Text Tests* output pure text which represents the DOM tree, the DOM inner\n  text, internal data structure of Blink like layout tree or graphics layer\n  tree, or any custom text that the text wants to output. The test passes if the\n  output matches a baseline text file in the repository. Text tests outputting\n  internal data structures are used as a last resort to test the internal quirks\n  of the implementation, and they should be avoided in favor of one of other\n  options.\n* *Audio tests* output audio results.\n\n*** aside\nA JavaScript test is actually a special kind of text test, but its text\nbaseline can be often omitted.\n***\n\n*** aside\nA test can be a reference/pixel test and a text test at the same time.\n***\n\n## General Principles\n\nTests should be written under the assumption that they will be upstreamed\nto the WPT project. To this end, tests should follow the\n[WPT guidelines](https://web-platform-tests.org/writing-tests/).\n\n\nThere is no style guide that applies to all web tests. However, some projects\nhave adopted style guides, such as the\n[ServiceWorker Tests Style guide](https://www.chromium.org/blink/serviceworker/testing).\n\nOur [document on web tests tips](./web_tests_tips.md) summarizes the most\nimportant WPT guidelines and highlights some JavaScript concepts that are worth\npaying attention to when trying to infer style rules from existing tests. If\nyou're unopinionated and looking for a style guide to follow, the document also\nsuggests some defaults.\n\n## JavaScript Tests\n\nWhenever possible, the testing criteria should be expressed in JavaScript. The\nalternatives, which will be described in future sections, result in slower and\nless reliable tests.\n\nAll new JavaScript tests should be written using the\n[testharness.js](https://github.com/web-platform-tests/wpt/tree/master/resources)\ntesting framework. This framework is used by the tests in the\n[web-platform-tests](https://github.com/web-platform-tests/wpt) repository,\nwhich is shared with all the other browser vendors, so `testharness.js` tests\nare more accessible to browser developers.\n\nSee the [API documentation](https://web-platform-tests.org/writing-tests/testharness-api.html)\nfor a thorough introduction to `testharness.js`.\n\nWeb tests should follow the recommendations of the above documentation.\nFurthermore, web tests should include relevant\n[metadata](https://web-platform-tests.org/writing-tests/css-metadata.html). The\nspecification URL (in `<link rel=\"help\">`) is almost always relevant, and is\nincredibly helpful to a developer who needs to understand the test quickly.\n\nBelow is a skeleton for a JavaScript test embedded in an HTML page. Note that,\nin order to follow the minimality guideline, the test omits the tags `<html>`,\n`<head>`, and `<body>`, as they can be inferred by the HTML parser.\n\n```html\n<!doctype html>\n<title>JavaScript: the true literal is immutable and equal to itself</title>\n<link rel=\"help\" href=\"https://tc39.github.io/ecma262/#sec-boolean-literals\">\n<script src=\"/resources/testharness.js\"></script>\n<script src=\"/resources/testharnessreport.js\"></script>\n<script>\n'use strict';\n\n// Synchronous test example.\ntest(() => {\n  const value = true;\n  assert_true(value, 'true literal');\n  assert_equals(value.toString(), 'true', 'the string representation of true');\n}, 'The literal true in a synchronous test case');\n\n// Asynchronous test example.\nasync_test(t => {\n  const originallyTrue = true;\n  setTimeout(t.step_func_done(() => {\n    assert_equals(originallyTrue, true);\n  }), 0);\n}, 'The literal true in a setTimeout callback');\n\n// Promise test example.\npromise_test(() => {\n  return new Promise((resolve, reject) => {\n    resolve(true);\n  }).then(value => {\n    assert_true(value);\n  });\n}, 'The literal true used to resolve a Promise');\n\n</script>\n```\n\nSome points that are not immediately obvious from the example:\n\n* When calling an `assert_` function that compares two values, the first\n  argument is the actual value (produced by the functionality being tested), and\n  the second argument is the expected value (known good, golden). The order\n  is important, because the testing harness relies on it to generate expressive\n  error messages that are relied upon when debugging test failures.\n* The assertion description (the string argument to `assert_` methods) conveys\n  the way the actual value was obtained.\n    * If the expected value doesn't make it clear, the assertion description\n      should explain the desired behavior.\n    * Test cases with a single assertion should omit the assertion's description\n      when it is sufficiently clear.\n* Each test case describes the circumstance that it tests, without being\n  redundant.\n    * Do not start test case descriptions with redundant terms like \"Testing\"\n      or \"Test for\".\n    * Test files with a single test case should omit the test case description.\n      The file's `<title>` should be sufficient to describe the scenario being\n      tested.\n* Asynchronous tests have a few subtleties.\n    * The `async_test` wrapper calls its function with a test case argument that\n      is used to signal when the test case is done, and to connect assertion\n      failures to the correct test.\n    * `t.done()` must be called after all the test case's assertions have\n      executed.\n    * Test case assertions (actually, any callback code that can throw\n      exceptions) must be wrapped in `t.step_func()` calls, so that\n      assertion failures and exceptions can be traced back to the correct test\n      case.\n    * `t.step_func_done()` is a shortcut that combines `t.step_func()` with a\n      `t.done()` call.\n\n*** promo\nWeb tests that load from `file://` origins must currently use relative paths\nto point to\n[/resources/testharness.js](../../third_party/blink/web_tests/resources/testharness.js)\nand\n[/resources/testharnessreport.js](../../third_party/blink/web_tests/resources/testharnessreport.js).\nThis is contrary to the WPT guidelines, which call for absolute paths.\nThis limitation does not apply to the tests in `web_tests/http`, which rely on\nan HTTP server, or to the tests in `web_tests/external/wpt`, which are\nimported from the [WPT repository](https://github.com/web-platform-tests/wpt).\n***\n\n### WPT Supplemental Testing APIs\n\nSome tests simply cannot be expressed using the Web Platform APIs. For example,\nsome tests that require a user to perform a gesture, such as a mouse click,\ncannot be implemented using Web APIs. The WPT project covers some of these cases\nvia supplemental testing APIs.\n\nWhen writing tests that rely on supplemental testing APIs, please consider the\ncost and benefits of having the tests\n[gracefully degrade to manual tests](./web_tests_with_manual_fallback.md) in\nthe absence of the testing APIs.\n\n*** promo\nIn many cases, the user gesture is not actually necessary. For example, many\nevent handling tests can use\n[synthetic events](https://developer.mozilla.org/docs/Web/Guide/Events/Creating_and_triggering_events).\n***\n\n### Relying on Blink-Specific Testing APIs\n\nTests that cannot be expressed using the Web Platform APIs or WPT's testing APIs\nuse Blink-specific testing APIs. These APIs are only available in\n[content_shell](./web_tests_in_content_shell.md), and should only be used as\na last resort.\n\nA downside of Blink-specific APIs is that they are not as well documented as the\nWeb Platform features. Learning to use a Blink-specific feature requires finding\nother tests that use it, or reading its source code.\n\nFor example, the most popular Blink-specific API is `testRunner`, which is\nimplemented in\n[content/web_test/renderer/test_runner.h](../../content/web_test/renderer/test_runner.h)\nand\n[content/web_test/renderer/test_runner.cc](../../content/web_test/renderer/test_runner.cc).\nBy skimming the `TestRunnerBindings::Install` method, we learn that the\ntestRunner API is presented by the `.testRunner` etc. objects. Reading the\n`TestRunnerBindings::GetObjectTemplateBuilder` method tells us what properties\nare available on the `testRunner` object.\n\nAnother popular Blink-specific API 'internals' defined in\n[third_party/blink/renderer/core/testing/internals.idl](../../third_party/blink/renderer/core/testing/internals.idl)\ncontains more direct access to blink internals.\n\n*** note\nIf possible, a test using blink-specific testing APIs should be written not to\ndepend on the APIs, so that it can also work directly in a browser. If the test\ndoes need the APIs to work, it should still check if the API is available before\nusing the API. Note that though we omit the `window.` prefix when using the\nAPIs, we should use the qualified name in the `if` statement:\n```javascript\n  if (window.testRunner)\n    testRunner.waitUntilDone();\n```\n***\n\n*** note\n`testRunner` is the most popular testing API because it is also used indirectly\nby tests that stick to Web Platform APIs. The `testharnessreport.js` file in\n`testharness.js` is specifically designated to hold glue code that connects\n`testharness.js` to the testing environment. Our implementation is in\n[third_party/blink/web_tests/resources/testharnessreport.js](../../third_party/blink/web_tests/resources/testharnessreport.js),\nand uses the `testRunner` API.\n***\n\nSee the [content/web_test/renderer/](../../content/web_test/renderer/) directory and\n[WebKit's LayoutTests guide](https://trac.webkit.org/wiki/Writing%20Layout%20Tests%20for%20DumpRenderTree)\nfor other useful APIs. For example, `eventSender`\n([content/shell/renderer/web_test/event_sender.h](../../content/web_test/renderer/event_sender.h)\nand\n[content/shell/renderer/web_test/event_sender.cc](../../content/web_test/renderer/event_sender.cc))\nhas methods that simulate events input such as keyboard / mouse input and\ndrag-and-drop.\n\nHere is a UML diagram of how the `testRunner` bindings fit into Chromium.\n\n[![UML of testRunner bindings configuring platform implementation](https://docs.google.com/drawings/u/1/d/1KNRNjlxK0Q3Tp8rKxuuM5mpWf4OJQZmvm9_kpwu_Wwg/export/svg?id=1KNRNjlxK0Q3Tp8rKxuuM5mpWf4OJQZmvm9_kpwu_Wwg&pageid=p)](https://docs.google.com/drawings/d/1KNRNjlxK0Q3Tp8rKxuuM5mpWf4OJQZmvm9_kpwu_Wwg/edit)\n\n### Text Test Baselines\n\nBy default, all the test cases in a file that uses `testharness.js` are expected\nto pass. However, in some cases, we prefer to add failing test cases to the\nrepository, so that we can be notified when the failure modes change (e.g., we\nwant to know if a test starts crashing rather than returning incorrect output).\nIn these situations, a test file will be accompanied by a baseline, which is an\n`-expected.txt` file that contains the test's expected output.\n\nThe baselines are generated automatically when appropriate by\n`run_web_tests.py`, which is described [here](./web_tests.md), and by the\n[rebaselining tools](./web_test_expectations.md).\n\nText baselines for `testharness.js` should be avoided, as having a text baseline\nassociated with a `testharness.js` test usually indicates the presence of a bug.\nFor this reason, CLs that add text baselines must include a\n[crbug.com](https://crbug.com) link for an issue tracking the removal of the\ntext expectations.\n\n* When creating tests that will be upstreamed to WPT, and Blink's current\n  behavior does not match the specification that is being tested, a text\n  baseline is necessary. Remember to create an issue tracking the expectation's\n  removal, and to link the issue in the CL description.\n* Web tests that cannot be upstreamed to WPT should use JavaScript to\n  document Blink's current behavior, rather than using JavaScript to document\n  desired behavior and a text file to document current behavior.\n\n*** promo\nBecause of [baseline fallback](./web_test_baseline_fallback.md), it may not be\npossible to [represent a platform-specific all-`PASS`\nstatus](https://crbug.com/1324638) by the platform baseline's absence. In such\nrare cases, `blink_tool.py rebaseline-cl` will generate a dummy baseline\nindicating to `run_web_tests.py` that all subtests are meant to pass:\n\n```\nThis is a testharness.js-based test.\nAll subtests passed and are omitted for brevity.\nSee https://chromium.googlesource.com/chromium/src/+/HEAD/docs/testing/writing_web_tests.md#Text-Test-Baselines for details.\nHarness: the test ran to completion.\n```\n\n`blink_tool.py optimize-baselines` will automatically remove these dummy\nbaselines once all platforms are all-`PASS`.\n***\n\n### The js-test.js Legacy Harness\n\n*** promo\nFor historical reasons, older tests are written using the `js-test` harness.\nThis harness is **deprecated**, and should not be used for new tests.\n***\n\nIf you need to understand old tests, the best `js-test` documentation is its\nimplementation at\n[third_party/blink/web_tests/resources/js-test.js](../../third_party/blink/web_tests/resources/js-test.js).\n\n`js-test` tests lean heavily on the Blink-specific `testRunner` testing API.\nIn a nutshell, the tests call `testRunner.dumpAsText()` to signal that the page\ncontent should be dumped and compared against a text baseline (an\n`-expected.txt` file). As a consequence, `js-test` tests are always accompanied\nby text baselines. Asynchronous tests also use `testRunner.waitUntilDone()` and\n`testRunner.notifyDone()` to tell the testing tools when they are complete.\n\n### Tests that use an HTTP Server\n\nBy default, tests are loaded as if via `file:` URLs. Some web platform features\nrequire tests served via HTTP or HTTPS, for example absolute paths (`src=/foo`)\nor features restricted to secure protocols.\n\nHTTP tests are those under `web_tests/http/tests` (or virtual variants). Use a\nlocally running HTTP server (Apache) to run them. Tests are served off of ports\n8000 and 8080 for HTTP, and 8443 for HTTPS. If you run the tests using\n`run_web_tests.py`, the server will be started automatically. To run the server\nmanually to reproduce or debug a failure:\n\n```bash\ncd src/third_party/blink/tools\n./run_blink_httpd.py\n```\n\nThe web tests will be served from `http://127.0.0.1:8000`. For example, to\nrun the test `http/tests/serviceworker/chromium/service-worker-allowed.html`,\nnavigate to\n`http://127.0.0.1:8000/serviceworker/chromium/service-worker-allowed.html`. Some\ntests will behave differently if you go to 127.0.0.1 instead of localhost, so\nuse 127.0.0.1.\n\nTo kill the server, hit any key on the terminal where `run_blink_httpd.py` is\nrunning, or just use `taskkill` or the Task Manager on Windows, and `killall` or\nActivity Monitor on MacOS.\n\nThe test server sets up an alias to the `web_tests/resources` directory. In\nHTTP tests, you can access the testing framework at e.g.\n`src=\"/resources/testharness.js\"`.\n\nTODO: Document [wptserve](http://wptserve.readthedocs.io/) when we are in a\nposition to use it to run web tests.\n\n## Reference Tests (Reftests)\n\nReference tests, also known as reftests, perform a pixel-by-pixel comparison\nbetween the rendered image of a test page and the rendered image of a reference\npage. Most reference tests pass if the two images match, but there are cases\nwhere it is useful to have a test pass when the two images do _not_ match.\n\nReference tests are more difficult to debug than JavaScript tests, and tend to\nbe slower as well. Therefore, they should only be used for functionality that\ncannot be covered by JavaScript tests.\n\nNew reference tests should follow the\n[WPT reftests guidelines](https://web-platform-tests.org/writing-tests/reftests.html).\nThe most important points are summarized below.\n\n* &#x1F6A7; The test page declares the reference page using a\n  `<link rel=\"match\">` or `<link rel=\"mismatch\">`, depending on whether the test\n  passes when the test image matches or does not match the reference image.\n* The reference page must not use the feature being tested. Otherwise, the test\n  is meaningless.\n* The reference page should be as simple as possible, and should not depend on\n  advanced features. Ideally, the reference page should render as intended even\n  on browsers with poor CSS support.\n* Reference tests should be self-describing.\n* Reference tests do _not_ include `testharness.js`.\n\n&#x1F6A7; Our testing infrastructure was designed for the\n[WebKit reftests](https://trac.webkit.org/wiki/Writing%20Reftests) that Blink\nhas inherited. The consequences are summarized below.\n\n* Each reference page must be in the same directory as its associated test.\n  Given a test page named `foo` (e.g. `foo.html` or `foo.svg`),\n    * The reference page must be named `foo-expected` (e.g.,\n      `foo-expected.html`) if the test passes when the two images match.\n    * The reference page must be named `foo-expected-mismatch` (e.g.,\n      `foo-expected-mismatch.svg`) if the test passes when the two images do\n      _not_ match.\n* Multiple references and chained references are not supported.\n\nThe following example demonstrates a reference test for\n[`<ol>`'s reversed attribute](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/ol).\nThe example assumes that the test page is named `ol-reversed.html`.\n\n```html\n<!doctype html>\n<link rel=\"match\" href=\"ol-reversed-expected.html\">\n\n<ol reversed>\n  <li>A</li>\n  <li>B</li>\n  <li>C</li>\n</ol>\n```\n\nThe reference page, which must be named `ol-reversed-expected.html`, is below.\n\n```html\n<!doctype html>\n\n<ol>\n  <li value=\"3\">A</li>\n  <li value=\"2\">B</li>\n  <li value=\"1\">C</li>\n</ol>\n```\n\n*** promo\nThe method for pointing out a test's reference page is still in flux, and is\nbeing discussed on\n[blink-dev](https://groups.google.com/a/chromium.org/d/topic/blink-dev/XsR6PKRrS1E/discussion).\n***\n\n## Pixel Tests\n\nA test creates an image result by default unless some `testRunner` API is\ncalled (e.g. `testRunner.dumpAsText()`, `testRunner.dumpAsLayout()`, see\n[text tests](#text-tests)) to suppress the image result. A test is a\n**pixel test** if it creates an image result but is not a reference test.\nThe image result is compared against an image baseline, which is an\n`-expected.png` file associated with the test, and the test passes if the\nimage result is identical to the baseline, according to a pixel-by-pixel\ncomparison.\n\nPixel tests should still follow the principles laid out above. Pixel tests pose\nunique challenges to the desire to have *self-describing* and *cross-platform*\ntests. The\n[WPT rendering test guidelines](https://web-platform-tests.org/writing-tests/rendering.html)\ncontain useful guidance. The most relevant pieces of advice are below.\n\n* Whenever possible, use a green paragraph / page / square to indicate success.\n  If that is not possible, make the test self-describing by including a textual\n  description of the desired (passing) outcome.\n* Only use the red color or the word `FAIL` to highlight errors. This does not\n  apply when testing the color red.\n* &#x1F6A7; Use the\n  [Ahem font](https://www.w3.org/Style/CSS/Test/Fonts/Ahem/README) to reduce the\n  variance introduced by the platform's text rendering system. This does not\n  apply when testing text, text flow, font selection, font fallback, font\n  features, or other typographic information.\n\n*** promo\nThe default size of the image result of a pixel test is 800x600px, because test\npages are rendered in an 800x600px viewport by default. Normally pixel tests\nthat do not specifically cover scrolling should fit in an 800x600px viewport\nwithout creating scrollbars.\n***\n\n*** promo\nThe recommendation of using Ahem in pixel tests is being discussed on\n[blink-dev](https://groups.google.com/a/chromium.org/d/topic/blink-dev/XsR6PKRrS1E/discussion).\n***\n\nThe following snippet includes the Ahem font in a web test.\n\n```html\n<style>\nbody {\n  font: 10px Ahem;\n}\n</style>\n<script src=\"/resources/ahem.js\"></script>\n```\n\n*** promo\nTests outside `web_tests/http` and `web_tests/external/wpt` currently need\nto use a relative path to\n[/third_party/blink/web_tests/resources/ahem.js](../../third_party/blink/web_tests/resources/ahem.js)\n***\n\n### Tests that need to paint, raster, or draw a frame of intermediate output\n\nA web test does not actually draw frames of output until the test exits.\nTests that need to generate a painted frame can use `runAfterLayoutAndPaint()`\ndefined in [third_party/blink/web_tests/resources/run-after-layout-and-paint.js](../../third_party/blink/web_tests/resources/run-after-layout-and-paint.js)\nwhich will run the machinery to put up a frame, then call the passed callback.\nThere is also a library at\n[third_party/blink/web_tests/paint/invalidation/resources/text-based-repaint.js](../../third_party/blink/web_tests/paint/invalidation/resources/text-based-repaint.js)\nto help with writing paint invalidation and repaint tests.\n\n### Tests for scrolling animations\n\nSome web tests need to ensure animations such as middle-click auto-scroll,\nfling, etc. get performed properly. When testing in display compositor pixel\ndump mode (now the standard), the standard behavior for tests is to\nsynchronously composite without rastering (to save time). However, animations\nrun upon surface activation, which only happens once rasterization is performed.\nTherefore, for these tests, an additional setting needs to be set. Near the\nbeginning of these tests, call `setAnimationRequiresRaster()` defined in\n[third_party/blink/web_tests/resources/compositor-controls.js](../../third_party/blink/web_tests/resources/compositor-controls.js)\nwhich will enable full rasterization during the test.\n\n## Text tests\n\nA **text test** outputs text result. The result is compared against a text\nbaseline which is an `-expected.txt` file associated with the test, and the\ntest passes if the text result is identical to the baseline. A test isn't a\ntext test by default until it calls some `testRunner` API to instruct the\ntest runner to output text. A text test can be categorized based on what kind of\ninformation that the text result represents.\n\n### Layout tree test\n\nIf a test calls `testRunner.dumpAsLayout()` or\n`testRunner.dumpAsLayoutWithPixelResults()`, The text result will be a\ntextual representation of Blink's\n[layout tree](https://developers.google.com/web/fundamentals/performance/critical-rendering-path/render-tree-construction)\n(called the render tree on that page) of the main frame of the test page.\nWith `testRunner.dumpChildFrames()` the text result will also include layout\ntree of child frames.\n\nLike pixel tests, the output of layout tree tests may depend on\nplatform-specific details, so layout tree tests often require per-platform\nbaselines. Furthermore, since the tests obviously depend on the layout tree\nstructure, that means that if we change the layout tree you have to rebaseline\neach layout tree test to see if the results are still correct and whether the\ntest is still meaningful. There are actually many cases where the layout tree\noutput is misstated (i.e., wrong), because people didn't want to have to update\nexisting baselines and tests. This is really unfortunate and confusing.\n\nFor these reasons, layout tree tests should **only** be used to cover aspects\nof the layout code that can only be tested by looking at the layout tree. Any\ncombination of the other test types is preferable to a layout tree test.\nLayout tree tests are\n[inherited from WebKit](https://webkit.org/blog/1456/layout-tests-practice/), so\nthe repository may have some unfortunate examples of layout tree tests.\n\nThe following page is an example of a layout tree test.\n\n```html\n<!doctype html>\n<style>\nbody { font: 10px Ahem; }\nspan::after {\n  content: \"pass\";\n  color: green;\n}\n</style>\n<script src=\"/resources/ahem.js\"></script>\n<script>\n  if (window.testRunner)\n    testRunner.dumpAsLayout();\n</script>\n<p><span>Pass if a green PASS appears to the right: </span></p>\n```\n\nThe test page produces the text result below.\n\n```\nlayer at (0,0) size 800x600\n  LayoutView at (0,0) size 800x600\nlayer at (0,0) size 800x30\n  LayoutBlockFlow {HTML} at (0,0) size 800x30\n    LayoutBlockFlow {BODY} at (8,10) size 784x10\n      LayoutBlockFlow {P} at (0,0) size 784x10\n        LayoutInline {SPAN} at (0,0) size 470x10\n          LayoutText {#text} at (0,0) size 430x10\n            text run at (0,0) width 430: \"Pass if a green PASS appears to the right: \"\n          LayoutInline {<pseudo:after>} at (0,0) size 40x10 [color=#008000]\n            LayoutTextFragment (anonymous) at (430,0) size 40x10\n              text run at (430,0) width 40: \"pass\"\n```\n\nNotice that the test result above depends on the size of the `<p>` text. The\ntest page uses the Ahem font (introduced above), whose main design goal is\nconsistent cross-platform rendering. Had the test used another font, its text\nbaseline would have depended on the fonts installed on the testing computer, and\non the platform's font rendering system. Please follow the pixel tests\nguidelines and write reliable layout tree tests!\n\nWebKit's layout tree is described in\n[a series of posts](https://webkit.org/blog/114/webcore-rendering-i-the-basics/)\non WebKit's blog. Some of the concepts there still apply to Blink's layout tree.\n\n### Text dump test\n\nIf `testRunner.dumpAsText()` or `testRunner.dumpAsTextWithPixelResults()`\nis called from a test, the test will dump the text contents of the main frame\nof the tested page. With `testRunner.dumpChildFrames()` the text\nresult will also include text contents of child frames. Actually a JavaScript\ntest is a special kind of text dump test which can often omit the text baseline.\n\nA test can override the default text dump by calling\n`testRunner.setCustomTextOutput(string)`. The string parameter can be any\ntext that the test wants to output. The [`internals` API](../../third_party/blink/renderer/core/testing/internals.idl]\nprovides methods to get textual representations of internal data structures that\ncan be used as the parameter of `testRunner.setCustomTextOutput()`.\n\n### Markup dump test\n\nIf a test calls `testRunner.dumpAsMarkup()`, the text result will be the DOM\nof the main frame of the test. With `testRunner.dumpChildFrames()` the text\nresult will also include DOM of child frames.\n\n## Audio tests\n\nIf a test calls `testRunner.setAudioData(array_buffer)`, the test will\ncreate an audio result. The result will be compared against an audio baseline\nwhich is an `-expected.wav` file associated with the test, and the test passes\nif the audio result is identical to the baseline.\n\n## Tests that are both pixel/reference tests and text tests\n\nIf a test calls `testRunner.dumpAsTextWithPixelResults()` or\n`testRunner.dumpAsLayoutWithPixelResults()`, the test is both a\npixel/reference test and a text test. It will output both pixel result and text\nresult.\n\nFor a test that is both a pixel/reference test and a text test, both pixel and\ntext results will be compared to baselines, and the test passes if each result\nmatches the corresponding baseline.\n\nMany of the [paint invalidation tests](../../third_party/blink/web_tests/paint/invalidation)\nare of this type. The pixel results (compared against `-expected.png` or\n`-expected.html`) ensure correct rendering, and the text results (compared\nagainst `-expected.txt`) ensure correct compositing and raster invalidation\n(without unexpected over and under invalidations).\n\nFor a layout tree test, whether you want a pixel test and/or a text test depends\non whether you care about the visual image, the details of how that image was\nconstructed, or both. It is possible for multiple layout trees to produce\nthe same pixel output, so it is important to make it clear in the test\nwhich outputs you really care about.\n\n## Directory Structure\n\nThe [web_tests directory](../../third_party/blink/web_tests) currently\nlacks a strict, formal structure. The following directories have special\nmeaning:\n\n* The `http/` directory hosts tests that require an HTTP server (see above).\n* The `resources/` subdirectory in every directory contains binary files, such\n  as media files, and code that is shared by multiple test files.\n\n*** note\nSome web tests consist of a minimal HTML page that references a JavaScript\nfile in `resources/`. Please do not use this pattern for new tests, as it goes\nagainst the minimality principle. JavaScript and CSS files should only live in\n`resources/` if they are shared by at least two test files.\n***\n"
  },
  {
    "path": "development/testing/web_test_expectations",
    "title": "Web Test Expectations and Baselines",
    "content": "# Web Test Expectations and Baselines\n\nThe primary function of the web tests is as a regression test suite; this\nmeans that, while we care about whether a page is being rendered correctly, we\ncare more about whether the page is being rendered the way we expect it to. In\nother words, we look more for changes in behavior than we do for correctness.\n\n[TOC]\n\nAll web tests have \"expected results\", or \"baselines\", which may be one of\nseveral forms. The test may produce one or more of:\n\n* A text file containing JavaScript log messages.\n* A text rendering of the Render Tree.\n* A screen capture of the rendered page as a PNG file.\n* WAV files of the audio output, for WebAudio tests.\n\nFor any of these types of tests, baselines are checked into the web_tests\ndirectory. The filename of a baseline is the same as that of the corresponding\ntest, but the extension is replaced with `-expected.{txt,png,wav}` (depending on\nthe type of test output). Baselines usually live alongside tests, with the\nexception when baselines vary by platforms; read\n[Web Test Baseline Fallback](web_test_baseline_fallback.md) for more\ndetails.\n\nLastly, we also support the concept of \"reference tests\", which check that two\npages are rendered identically (pixel-by-pixel). As long as the two tests'\noutput match, the tests pass. For more on reference tests, see\n[Writing ref tests](https://trac.webkit.org/wiki/Writing%20Reftests).\n\n## Failing tests\n\nWhen the output doesn't match, there are two potential reasons for it:\n\n* The port is performing \"correctly\", but the output simply won't match the\n  generic version. The usual reason for this is for things like form controls,\n  which are rendered differently on each platform.\n* The port is performing \"incorrectly\" (i.e., the test is failing).\n\nIn both cases, the convention is to check in a new baseline (aka rebaseline),\neven though that file may be codifying errors. This helps us maintain test\ncoverage for all the other things the test is testing while we resolve the bug.\n\n*** promo\nIf a test can be rebaselined, it should always be rebaselined instead of adding\nlines to TestExpectations.\n***\n\nBugs at [crbug.com](https://crbug.com) should track fixing incorrect behavior,\nnot lines in\n[TestExpectations](../../third_party/blink/web_tests/TestExpectations). If a\ntest is never supposed to pass (e.g. it's testing Windows-specific behavior, so\ncan't ever pass on Linux/Mac), move it to the\n[NeverFixTests](../../third_party/blink/web_tests/NeverFixTests) file. That\ngets it out of the way of the rest of the project.\n\nThere are some cases where you can't rebaseline and, unfortunately, we don't\nhave a better solution than either:\n\n1. Reverting the patch that caused the failure, or\n2. Adding a line to TestExpectations and fixing the bug later.\n\nIn this case, **reverting the patch is strongly preferred**.\n\nThese are the cases where you can't rebaseline:\n\n* The test is a reference test.\n* The test gives different output in release and debug; in this case, generate a\n  baseline with the release build, and mark the debug build as expected to fail.\n* The test is flaky, crashes or times out.\n* The test is for a feature that hasn't yet shipped on some platforms yet, but\n  will shortly.\n\n## Handling flaky tests\n\n<!-- TODO(crbug.com/40262793): Describe the current flakiness dashboard and\n    LUCI test history. -->\n\nOnce you decide that a test is truly flaky, you can suppress it using the\nTestExpectations file, as [described below](#updating-the-expectations-files).\nWe do not generally expect Chromium sheriffs to spend time trying to address\nflakiness, though.\n\n## How to rebaseline\n\nSince baselines themselves are often platform-specific, updating baselines in\ngeneral requires fetching new test results after running the test on multiple\nplatforms.\n\n### Rebaselining using try jobs\n\nThe recommended way to rebaseline for a currently-in-progress CL is to use\nresults from try jobs, by using the command-tool\n`third_party/blink/tools/blink_tool.py rebaseline-cl`:\n\n1. First, upload a CL.\n2. Trigger try jobs by running `blink_tool.py rebaseline-cl`. This should\n   trigger jobs on\n   [tryserver.blink](https://ci.chromium.org/p/chromium/g/tryserver.blink/builders).\n3. Wait for all try jobs to finish.\n4. Run `blink_tool.py rebaseline-cl` again to fetch new baselines.\n5. Commit the new baselines and upload a new patch.\n\nThis way, the new baselines can be reviewed along with the changes, which helps\nthe reviewer verify that the new baselines are correct. It also means that there\nis no period of time when the web test results are ignored.\n\n#### Handle bot timeouts\n\nWhen a change will cause many tests to fail, the try jobs may exit early because\nthe number of failures exceeds the limit, or the try jobs may timeout because\nmore time is needed for the retries. Rebaseline based on such results are not\nsuggested. The solution is to temporarily increase the number of shards in\n[`test_suite_exceptions.pyl`](/testing/buildbot/test_suite_exceptions.pyl) in your CL.\nChange the values back to its original value before sending the CL to CQ.\n\n#### Options\n\nThe tests which `blink_tool.py rebaseline-cl` tries to download new baselines for\ndepends on its arguments.\n\n* By default, it tries to download all baselines for tests that failed in the\n  try jobs.\n* If you pass `--only-changed-tests`, then only tests modified in the CL will be\n  considered.\n* You can also explicitly pass a list of test names, and then just those tests\n  will be rebaselined.\n* By default, it finds the try jobs by looking at the latest patchset. If you\n  have finished try jobs that are associated with an earlier patchset and you\n  want to use them instead of scheduling new try jobs, you can add the flag\n  `--patchset=n` to specify the patchset. This is very useful when the CL has\n  'trivial' patchsets that are created e.g. by editing the CL descrpition.\n\n### Rebaseline script in results.html\n\nWeb test results.html linked from bot job result page provides an alternative\nway to rebaseline tests for a particular platform.\n\n* In the bot job result page, find the web test results.html link and click it.\n* Choose \"Rebaseline script\" from the dropdown list after \"Test shown ... in format\".\n* Click \"Copy report\" (or manually copy part of the script for the tests you want\n  to rebaseline).\n* In local console, change directory into `third_party/blink/web_tests/platform/<platform>`.\n* Paste.\n* Add files into git and commit.\n\nThe generated command includes `blink_tool.py optimize-baselines <tests>` which\nremoves redundant baselines.\n\n### Local manual rebaselining\n\n```bash\nthird_party/blink/tools/run_web_tests.py --reset-results foo/bar/test.html\n```\n\nIf there are current expectation files for `web_tests/foo/bar/test.html`,\nthe above command will overwrite the current baselines at their original\nlocations with the actual results. The current baseline means the `-expected.*`\nfile used to compare the actual result when the test is run locally, i.e. the\nfirst file found in the [baseline search path](https://cs.chromium.org/search/?q=port/base.py+baseline_search_path).\n\nIf there are no current baselines, the above command will create new baselines\nin the platform-independent directory, e.g.\n`web_tests/foo/bar/test-expected.{txt,png}`.\n\nWhen you rebaseline a test, make sure your commit description explains why the\ntest is being re-baselined.\n\n### Rebaselining flag-specific expectations\n\nSee [Testing Runtime Flags](./web_tests.md#Testing-Runtime-Flags) for details\nabout flag-specific expectations.\n\nThe [Rebaseline Tool](#How-to-rebaseline) supports all flag-specific suites that\n[run in CQ/CI](/third_party/blink/tools/blinkpy/common/config/builders.json).\nYou may also rebaseline flag-specific results locally with:\n\n```bash\nthird_party/blink/tools/run_web_tests.py --flag-specific=config --reset-results foo/bar/test.html\n```\n\nNew baselines will be created in the flag-specific baselines directory, e.g.\n`web_tests/flag-specific/config/foo/bar/test-expected.{txt,png}`\n\nThen you can commit the new baselines and upload the patch for review.\n\nSometimes it's difficult for reviewers to review the patch containing only new\nfiles. You can follow the steps below for easier review.\n\n1. Copy existing baselines to the flag-specific baselines directory for the\n   tests to be rebaselined:\n   ```bash\n   third_party/blink/tools/run_web_tests.py --flag-specific=config --copy-baselines foo/bar/test.html\n   ```\n   Then add the newly created baseline files, commit and upload the patch.\n   Note that the above command won't copy baselines for passing tests.\n\n2. Rebaseline the test locally:\n   ```bash\n   third_party/blink/tools/run_web_tests.py --flag-specific=config --reset-results foo/bar/test.html\n   ```\n   Commit the changes and upload the patch.\n\n3. Request review of the CL and tell the reviewer to compare the patch sets that\n   were uploaded in step 1 and step 2 to see the differences of the rebaselines.\n\n## Kinds of expectations files\n\n* [TestExpectations](../../third_party/blink/web_tests/TestExpectations): The\n  main test failure suppression file. In theory, this should be used for\n  temporarily marking tests as flaky.\n  See [the `run_wpt_tests.py` doc](run_web_platform_tests.md) for information\n  about WPT coverage for Chrome.\n* [ASANExpectations](../../third_party/blink/web_tests/ASANExpectations):\n  Tests that fail under ASAN.\n* [CfTTestExpecations](../../third_party/blink/web_tests/CfTTestExpecations):\n  Tests that fail under Chrome for Testing\n* [LeakExpectations](../../third_party/blink/web_tests/LeakExpectations):\n  Tests that have memory leaks under the leak checker.\n* [MobileTestExpectations](../../third_party/blink/web_tests/MobileTestExpectations)\n  Tests that fails under Chrome Android and Chrome WebView platform.\n* [MSANExpectations](../../third_party/blink/web_tests/MSANExpectations):\n  Tests that fail under MSAN.\n* [NeverFixTests](../../third_party/blink/web_tests/NeverFixTests): Tests\n  that we never intend to fix (e.g. a test for Windows-specific behavior will\n  never be fixed on Linux/Mac). Tests that will never pass on any platform\n  should just be deleted, though.\n* [SlowTests](../../third_party/blink/web_tests/SlowTests): Tests that take\n  longer than the usual timeout to run. Slow tests are given 5x the usual\n  timeout.\n* [StaleTestExpectations](../../third_party/blink/web_tests/StaleTestExpectations):\n  Platform-specific lines that have been in TestExpectations for many months.\n  They're moved here to get them out of the way of people doing rebaselines\n  since they're clearly not getting fixed anytime soon.\n* [W3CImportExpectations](../../third_party/blink/web_tests/W3CImportExpectations):\n  A record of which W3C tests should be imported or skipped.\n\n### Flag-specific expectations files\n\nIt is possible to handle tests that only fail when run with a particular flag\nbeing passed to `content_shell`. See\n[web_tests/FlagExpectations/README.txt](../../third_party/blink/web_tests/FlagExpectations/README.txt)\nfor more.\n\n## Updating the expectations files\n\n### Ordering\n\nThe file is not ordered. If you put new changes somewhere in the middle of the\nfile, this will reduce the chance of merge conflicts when landing your patch.\n\n### Syntax\n\n*** promo\nPlease see [The Chromium Test List Format](http://bit.ly/chromium-test-list-format)\nfor a more complete and up-to-date description of the syntax.\n***\n\nThe syntax of the file is roughly one expectation per line. An expectation can\napply to either a directory of tests, or a specific tests. Lines prefixed with\n`# ` are treated as comments, and blank lines are allowed as well.\n\nThe syntax of a line is roughly:\n\n```\n[ bugs ] [ \"[\" modifiers \"]\" ] test_name_or_directory [ \"[\" expectations \"]\" ]\n```\n\n* Tokens are separated by whitespace.\n* **The brackets delimiting the modifiers and expectations from the bugs and the\n  test_name_or_directory are not optional**; however the modifiers component is optional. In\n  other words, if you want to specify modifiers or expectations, you must\n  enclose them in brackets.\n* If test_name_or_directory is a directory, it should be ended with `/*`, and all\n  tests under the directory will have the expectations, unless overridden by\n  more specific expectation lines. **The wildcard is intentionally only allowed at the\n  end of test_name_or_directory, so that it will be easy to reason about\n  which test(s) a test expectation will apply to.**\n* Lines are expected to have one or more bug identifiers, and the linter will\n  complain about lines missing them. Bug identifiers are of the form\n  `crbug.com/12345`, `code.google.com/p/v8/issues/detail?id=12345` or\n  `Bug(username)`.\n* If no modifiers are specified, the test applies to all of the configurations\n  applicable to that file.\n* If specified, modifiers can be one of `Fuchsia`, `Mac`, `Mac11`,\n  `Mac11-arm64`, `Mac12`, `Mac12-arm64`, `Mac13`, `Mac13-arm64`, `Mac14`,\n  `Mac14-arm64`, `Mac15`, `Mac15-arm64`, `Linux`, `Chrome`, `Win`, `Win10.20h2`,\n  `Win11`, `iOS17-Simulator`, and, optionally, `Release`, or `Debug`.\n  Check the `# tags: ...` comments [at the top of each\n  file](/third_party/blink/web_tests/TestExpectations#1) to see which modifiers\n  that file supports.\n* Some modifiers are meta keywords, e.g. `Win` represents `Win10.20h2` and `Win11`.\n  See the `CONFIGURATION_SPECIFIER_MACROS` dictionary in\n  [third_party/blink/tools/blinkpy/web_tests/port/base.py](../../third_party/blink/tools/blinkpy/web_tests/port/base.py)\n  for the meta keywords and which modifiers they represent.\n* Expectations can be one or more of `Crash`, `Failure`, `Pass`, `Slow`, or\n  `Skip`, `Timeout`.\n  Some results don't make sense for some files; check the `# results: ...`\n  comment at the top of each file to see what results that file supports.\n  If multiple expectations are listed, the test is considered \"flaky\" and any\n  of those results will be considered as expected.\n\nFor example:\n\n```\ncrbug.com/12345 [ Win Debug ] fast/html/keygen.html [ Crash ]\n```\n\nwhich indicates that the \"fast/html/keygen.html\" test file is expected to crash\nwhen run in the Debug configuration on Windows, and the tracking bug for this\ncrash is bug \\#12345 in the [Chromium issue tracker](https://crbug.com). Note\nthat the test will still be run, so that we can notice if it doesn't actually\ncrash.\n\nAssuming you're running a debug build on Mac 10.9, the following lines are\nequivalent (in terms of whether the test is performed and its expected outcome):\n\n```\nfast/html/keygen.html [ Skip ]\nBug(darin) [ Mac10.9 Debug ] fast/html/keygen.html [ Skip ]\n```\n\n### Semantics\n\n`Slow` causes the test runner to give the test 5x the usual time limit to run.\n`Slow` lines go in the\n[`SlowTests` file](../../third_party/blink/web_tests/SlowTests).\nA given line cannot have both Slow and Timeout.\n\nAlso, when parsing the file, we use two rules to figure out if an expectation\nline applies to the current run:\n\n1. If the configuration parameters don't match the configuration of the current\n   run, the expectation is ignored.\n2. Expectations that match more of a test name are used before expectations that\n   match less of a test name.\n\nIf a [virtual test] has no explicit expectations (following the rules above),\nit inherits its expectations from the base (nonvirtual) test.\n\n[virtual test]: /docs/testing/web_tests.md#Virtual-test-suites\n\nFor example, if you had the following lines in your file, and you were running a\ndebug build on `Mac10.10`:\n\n```\ncrbug.com/12345 [ Mac10.10 ] fast/html [ Failure ]\ncrbug.com/12345 [ Mac10.10 ] fast/html/keygen.html [ Pass ]\ncrbug.com/12345 [ Win11 ] fast/forms/submit.html [ Failure ]\ncrbug.com/12345 fast/html/section-element.html [ Failure Crash ]\n```\n\nYou would expect:\n\n* `fast/html/article-element.html` to fail with a text diff (since it is in the\n  fast/html directory).\n* `fast/html/keygen.html` to pass (since the exact match on the test name).\n* `fast/forms/submit.html` to pass (since the configuration parameters don't\n  match).\n* `fast/html/section-element.html` to either crash or produce a text (or image\n  and text) failure, but not time out or pass.\n* `virtual/foo/fast/html/article-element.html` to fail with a text diff. The\n  virtual test inherits its expectation from the first line.\n\nTest expectation can also apply to all tests under a directory (specified with a\nname ending with `/*`). A more specific expectation can override a less\nspecific expectation. For example:\n```\ncrbug.com/12345 virtual/composite-after-paint/* [ Skip ]\ncrbug.com/12345 virtual/composite-after-paint/compositing/backface-visibility/* [ Pass ]\ncrbug.com/12345 virtual/composite-after-paint/compositing/backface-visibility/test.html [ Failure ]\n```\n\n*** promo\nDuplicate expectations are not allowed within the file and will generate\nwarnings.\n***\n\nYou can verify that any changes you've made to an expectations file are correct\nby running:\n\n```bash\nthird_party/blink/tools/lint_test_expectations.py\n```\n\nwhich will cycle through all of the possible combinations of configurations\nlooking for problems.\n"
  },
  {
    "path": "development/testing/web_test_baseline_fallback",
    "title": "Web Test Baseline Fallback",
    "content": "# Web Test Baseline Fallback\n\n\n*** promo\nRead [Web Test Expectations and Baselines](web_test_expectations.md) first\nif you have not.\n***\n\nBaselines can vary by platforms, in which case we need to check in multiple\nversions of a baseline. Meanwhile, we would like to avoid storing identical\nbaselines by allowing a platform to fall back to another. This document first\nintroduces how platform-specific baselines are structured and how we search for\na baseline (the fallback mechanism), and then goes into the details of baseline\noptimization and rebaselining.\n\n[TOC]\n\n## Terminology\n\n* **Root directory**:\n    [`//src/third_party/blink/web_tests`](../../third_party/blink/web_tests)\n    is the root directory (of all the web tests and baselines). All relative\n    paths in this document start from this directory.\n* **Test name**: the name of a test is its relative path from the root\n    directory (e.g. `html/dom/foo/bar.html`).\n* **Baseline name**: replacing the extension of a test name with\n    `-expected.{txt,png,wav}` gives the corresponding baseline name.\n* **Virtual tests**: tests can have virtual variants. For example,\n    `virtual/gpu/html/dom/foo/bar.html` is the virtual variant of\n    `html/dom/foo/bar.html` in the `gpu` suite. Only the latter file exists on\n    disk, and is called the base of the virtual test. See\n    [Web Tests#Testing Runtime Flags](web_tests.md#testing-runtime-flags)\n    for more details.\n* **Platform directory**: each directory under\n    [`platform/`](../../third_party/blink/web_tests/platform) is a platform\n    directory that contains baselines (no tests) for that platform. Directory\n    names are in the form of `PLATFORM-VERSION` (e.g. `mac-mac10.12`), except\n    for the latest version of a platform which is just `PLATFORM` (e.g. `mac`).\n\n## Baseline fallback\n\nEach platform has a pre-configured fallback when a baseline cannot be found in\nthis platform directory. A general rule is to have older versions of an OS\nfalling back to newer versions. Besides, Android falls back to Linux, which then\nfalls back to Windows. Eventually, all platforms fall back to the root directory\n(i.e. the generic baselines that live alongside tests). The rules are configured\nby `FALLBACK_PATHS` in each Port class in\n[`//src/third_party/blink/tools/blinkpy/web_tests/port`](../../third_party/blink/tools/blinkpy/web_tests/port).\n\nAll platforms can be organized into a tree based on their fallback relations (we\nare not considering virtual test suites yet). See the lower half (the\nnon-virtual subtree) of this\n[graph](https://docs.google.com/drawings/d/13l3IUlSE99RoKjDwEWuY1O77simAhhF6Wi0fZdkSaMA/).\nWalking from a platform to the root gives the **search path** of that platform.\nWe check each directory on the search path in order and see if \"directory +\nbaseline name\" points to a file on disk (note that baseline names are relative\npaths), and stop at the first one found.\n\n### Virtual test suites\n\nNow we add virtual test suites to the picture, using a test named\n`virtual/gpu/html/dom/foo/bar.html` as an example to demonstrate the process.\nThe baseline search process for a virtual test consists of two passes:\n\n1. Treat the virtual test name as a regular test name and search for the\n   corresponding baseline name using the same search path, which means we are in\n   fact searching in directories like `platform/*/virtual/gpu/...`, and\n   eventually `virtual/gpu/...` (a.k.a. the virtual root).\n2. If no baseline can be found so far, we retry with the non-virtual (base) test\n   name `html/dom/foo/bar.html` and walk the search path again.\n\nThe [graph](https://docs.google.com/drawings/d/13l3IUlSE99RoKjDwEWuY1O77simAhhF6Wi0fZdkSaMA/)\nvisualizes the full picture. Note that the two passes are in fact the same with\ndifferent test names, so the virtual subtree is a mirror of the non-virtual\nsubtree. The two trees are connected by the virtual root that has different\nancestors (fallbacks) depending on which platform we start from; this is the\nresult of the two-pass baseline search.\n\n*** promo\n__Note:__ there are in fact two more places to be searched before everything\nelse: additional directories given via command line arguments and flag-specific\nbaseline directories. They are maintained manually and are not discussed in this\ndocument.\n***\n\n## Tooling implementation\n\nThis section describes the implications the fallback mechanism has on the\nimplementation details of tooling, namely `blink_tool.py`. If you are not\nhacking `blinkpy`, you can stop here.\n\n### Optimization\n\nWe can remove a baseline if it is the same as its fallback. An extreme example\nis that if all platforms have the same result, we can just have a single generic\nbaseline. Here is the algorithm used by\n[`blink_tool.py optimize-baselines`](../../third_party/blink/tools/blinkpy/common/checkout/baseline_optimizer.py)\nto optimize the duplication away.\n\nNotice from the previous section that the virtual and non-virtual parts are two\nidentically structured subtrees. Trees are easy to work with: we can simply\ntraverse the tree from leaves up to the root, and if there are two identical\nbaselines on two nodes on the path with no other nodes in between or all nodes\nin between have no baselines, keep the one closer to the root (delete the\nbaseline on the node further from the root).\n\nThe virtual root is special because it has multiple parents. Yet if we can cut\nthe edges between the two subtrees (i.e. to make the virtual subtree\nself-contained), we can apply the same algorithm to both of them. A subtree is\nself-contained when it does not need to fallback to ancestors, which can be\nguaranteed by placing a baseline on its root. If the virtual root already has a\nbaseline, we can simply ignore these edges without doing anything; otherwise, we\nneed to make sure all children of the virtual root have baselines by copying\nthe non-virtual fallbacks to the ones that do not (we cannot copy the generic\nbaseline to the virtual root because virtual platforms may have different\nresults).\n\nIn addition, the optimizer also removes redundant all-PASS testharness.js\nresults. Such baselines are redundant when there are no other fallbacks later\non the search path (including if the all-PASS baselines are at root), because\n`run_web_tests.py` assumes all-PASS testharness.js results when baselines can\nnot be found for a platform.\n\n### Rebaseline\n\nThe fallback mechanism also affects the rebaseline tool (`blink_tool.py\nrebaseline{-cl}`). When asked to rebaseline a test on some platforms, the tool\ndownloads results from corresponding try bots and put them into the respective\nplatform directories. This is potentially problematic. Because of the fallback\nmechanism, the new baselines may affect some other platforms that are not being\nrebaselining but fall back to the rebaselined platforms.\n\nThe solution is to copy the current baselines from the to-be-rebaselined\nplatforms to all the platforms that immediately fall back to them (i.e. down one\nlevel in the fallback tree) before downloading new baselines. This is done in a\nhidden internal command\n[`blink_tool.py copy-existing-baselines`](../../third_party/blink/tools/blinkpy/tool/commands/copy_existing_baselines.py),\nwhich is always executed by `blink_tool.py rebaseline`.\n\nFinally, `blink_tool.py rebaseline{-cl}` also does optimization in the end by\ndefault.\n"
  },
  {
    "path": "development/testing/web_tests_with_manual_fallback",
    "title": "Web Tests with Manual Fallback",
    "content": "# Web Tests with Manual Fallback\n\nSome Blink features cannot be automatically tested using the Web Platform. Prime\nexamples are the APIs that require\n[user activation](https://html.spec.whatwg.org/multipage/interaction.html#tracking-user-activation)\n(also known as _a user gesture_), such as [Fullscreen](https://developer.mozilla.org/en-US/docs/Web/API/Fullscreen_API).\nAutomated tests for these Blink features must rely on special APIs, which are\nonly exposed in testing environments, and are therefore not available in a\nnormal browser session.\n\nA popular pattern used in these tests is to rely on the user to perform some\nmanual steps in order to run the test case in a normal browser session. These\ntests are effectively\n[manual tests](https://web-platform-tests.org/writing-tests/manual.html), with\nadditional JavaScript code that automatically performs the desired manual steps,\nwhen loaded in an environment that exposes the needed testing APIs.\n\n## Motivation\n\nWeb tests that degrade to manual tests in the absence of testing APIs have\nthe following benefits.\n\n* The manual test component can be debugged in a normal browser session, using\n  the rich [developer tools](https://developer.chrome.com/devtools). Tests\n  without a manual fallback can only be debugged in the test runner.\n* The manual tests can run in other browsers, making it easy to check whether\n  our behavior matches other browsers.\n* The web tests can form the basis for manual tests that are contributed to\n  [web-platform-tests](./web_platform_tests.md).\n\nTherefore, the desirability of adding a manual fallback to a test heavily\ndepends on whether the feature under test is a Web Platform feature or a\nBlink-only feature, and on the developer's working style. The benefits above\nshould be weighed against the added design effort needed to build a manual test,\nand the size and complexity introduced by the manual fallback.\n\n## Development Tips\n\nA natural workflow for writing a web test that gracefully degrades to a\nmanual test is to first develop the manual test in a browser, and then add code\nthat feature-checks for testing APIs, and uses them to automate the test's\nmanual steps.\n\nManual tests should minimize the chance of user error. This implies keeping the\nmanual steps to a minimum, and having simple and clear instructions that\ndescribe all the configuration changes and user gestures that match the effect\nof the Blink-specific APIs used by the test.\n\n## Example\n\nBelow is an example of a fairly minimal test that uses a Blink-Specific API\n(`window.eventSender`), and gracefully degrades to a manual test.\n\n```html\n<!doctype html>\n<meta charset=\"utf-8\">\n<title>DOM: Event.isTrusted for UI events</title>\n<link rel=\"help\" href=\"https://dom.spec.whatwg.org/#dom-event-istrusted\">\n<link rel=\"help\" href=\"https://dom.spec.whatwg.org/#constructing-events\">\n<meta name=\"assert\"\n    content=\"Event.isTrusted is true for events generated by user interaction\">\n<script src=\"../../resources/testharness.js\"></script>\n<script src=\"../../resources/testharnessreport.js\"></script>\n\n<p>Please click on the button below.</p>\n<button>Click Me!</button>\n\n<script>\n'use strict';\n\nsetup({ explicit_timeout: true });\n\npromise_test(() => {\n  const button = document.querySelector('button');\n  return new Promise((resolve, reject) => {\n    const button = document.querySelector('button');\n    button.addEventListener('click', (event) => {\n      resolve(event);\n    });\n\n    if (window.eventSender) {\n      eventSender.mouseMoveTo(button.offsetLeft, button.offsetTop);\n      eventSender.mouseDown();\n      eventSender.mouseUp();\n    }\n  }).then((clickEvent) => {\n    assert_true(clickEvent.isTrusted);\n  });\n\n}, 'Click generated by user interaction');\n\n</script>\n```\n\nThe test exhibits the following desirable features:\n\n* It has a second specification URL (`<link rel=\"help\">`), because the paragraph\n  that documents the tested feature (referenced by the primary URL) is not very\n  informative on its own.\n* It links to the\n  [WHATWG Living Standard](https://wiki.whatwg.org/wiki/FAQ#What_does_.22Living_Standard.22_mean.3F),\n  rather than to a frozen version of the specification.\n* It contains clear instructions for manually triggering the test conditions.\n  The test starts with a paragraph (`<p>`) that tells the tester exactly what to\n  do, and the `<button>` that needs to be clicked is clearly labeled.\n* It disables the timeout mechanism built into `testharness.js` by calling\n  `setup({ explicit_timeout: true });`\n* It checks for the presence of the Blink-specific testing APIs\n  (`window.eventSender`) before invoking them. The test does not automatically\n  fail when the APIs are not present.\n* It uses [Promises](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise)\n  to separate the test setup from the assertions. This is particularly helpful\n  for manual tests that depend on a sequence of events to occur, as Promises\n  offer a composable way to express waiting for asynchronous events that avoids\n  [callback hell](http://stackabuse.com/avoiding-callback-hell-in-node-js/).\n\nNotice that the test is pretty heavy compared to a minimal JavaScript test that\ndoes not rely on testing APIs. Only use testing APIs when the desired testing\nconditions cannot be set up using Web Platform APIs.\n"
  },
  {
    "path": "development/testing/web_tests_tips",
    "title": "Web Tests Tips",
    "content": "# Web Tests Tips\n\nThe recommendations here are intended to help you write new tests that go\nthrough code review with a minimal number of round trips, remain useful as Blink\nevolves, and serve as an asset (rather than a liability) for the team.\n\nWhile reading existing web tests, please keep in mind that they represent\nsnapshots taken over many years of an ever-evolving collective opinion of what\ngood Web pages and solid tests should look like. Thus, it should not come as a\nsurprise that most existing web tests are not consistent with these\nrecommendations, and are not even consistent with each other.\n\n*** note\nThis document intentionally uses _should_ a lot more than _must_, as defined in\n[RFC 2119](https://www.ietf.org/rfc/rfc2119.txt). Writing web tests is a\ncareful act of balancing many concerns, and this humble document cannot possibly\ncapture the context that rests in the head of an experienced Blink engineer.\n***\n\n## General Principles\n\nThis section contains guidelines adopted from\n[web-platform-tests documentation](https://web-platform-tests.org/writing-tests/general-guidelines.html)\nand\n[WebKit's Wiki page on Writing good test cases](https://trac.webkit.org/wiki/Writing%20Layout%20Tests%20for%20DumpRenderTree),\nwith Blink-specific flavoring.\n\n### Concise\n\nTests should be **concise**, without compromising on the principles below. Every\nelement and piece of code on the page should be necessary and relevant to what\nis being tested. For example, don't build a fully functional signup form if you\nonly need a text field or a button.\n\nContent needed to satisfy the principles below is considered necessary. For\nexample, it is acceptable and desirable to add elements that make the test\nself-describing (see below), and to add code that makes the test more reliable\n(see below).\n\nContent that makes test failures easier to debug is considered necessary (to\nmaintaining a good development speed), and is both acceptable and desirable.\n\n*** promo\nConciseness is particularly important for reference tests and pixel tests, as\nthe test pages are rendered in an 800x600px viewport. Having content outside the\nviewport is undesirable because the outside content does not get compared, and\nbecause the resulting scrollbars are platform-specific UI widgets, making the\ntest results less reliable.\n***\n\n### Fast\n\nTests should be as **fast** as possible, without compromising on the principles\nbelow. Blink has several thousand web tests that are run in parallel, and\navoiding unnecessary delays is crucial to keeping our Commit Queue in good\nshape.\n\nAvoid\n[window.setTimeout](https://developer.mozilla.org/en-US/docs/Web/API/WindowTimers/setTimeout),\nas it wastes time on the testing infrastructure. Instead, use specific event\nhandlers, such as\n[window.onload](https://developer.mozilla.org/en-US/docs/Web/API/GlobalEventHandlers/onload),\nto decide when to advance to the next step in a test.\n\n### Reliable\n\nTests should be **reliable** and yield consistent results for a given\nimplementation. Flaky tests slow down your fellow developers' debugging efforts\nand the Commit Queue.\n\n`window.setTimeout` is again a primary offender here. Asides from wasting time\non a fast system, tests that rely on fixed timeouts can fail when on systems\nthat are slower than expected.\n\nWhen adding or significantly modifying a web test, use the command below to\nassess its flakiness. While not foolproof, this approach gives you some\nconfidence, and giving up CPU cycles for mental energy is a pretty good trade.\n\n```bash\nthird_party/blink/tools/run_web_tests.py path/to/test.html --repeat-each=100\n```\n\nThe\n[PSA on writing reliable web tests](https://docs.google.com/document/d/1Yl4SnTLBWmY1O99_BTtQvuoffP8YM9HZx2YPkEsaduQ/edit).\nalso has good guidelines for writing reliable tests.\n\n### Self-Describing\n\nTests should be **self-describing**, so that a project member can recognize\nwhether a test passes or fails without having to read the specification of the\nfeature being tested.\n\n`testharness.js` makes a test self-describing when used correctly. Other types\nof tests, such as reference tests and\n[tests with manual fallback](./web_tests_with_manual_fallback.md),\n[must be carefully designed](https://web-platform-tests.org/writing-tests/manual.html#requirements-for-a-manual-test)\nto be self-describing.\n\n### Minimal\n\nTests should require a **minimal** amount of cognitive effort to read and\nmaintain.\n\nAvoid depending on edge case behavior of features that aren't explicitly covered\nby the test. For example, except where testing parsing, tests should contain\nvalid markup (no parsing errors).\n\nTests should provide as much relevant information as possible when failing.\n`testharness.js` tests should prefer\n[rich assert_ functions](https://web-platform-tests.org/writing-tests/testharness-api.html#list-of-assertions)\nto combining `assert_true()` with a boolean operator. Using appropriate\n`assert_` functions results in better diagnostic output when the assertion\nfails.\n\n### Cross-Platform\n\nTests should be as **cross-platform** as reasonably possible. Avoid assumptions\nabout device type, screen resolution, etc. Unavoidable assumptions should be\ndocumented.\n\nWhen possible, tests should only use Web platform features, as specified\nin the relevant standards. When the Web platform's APIs are insufficient,\ntests should prefer to use WPT extended testing APIs, such as\n`wpt_automation`, over Blink-specific testing APIs.\n\nTest pages should use the HTML5 doctype (`<!doctype html>`) unless they\nspecifically cover\n[quirks mode](https://developer.mozilla.org/docs/Quirks_Mode_and_Standards_Mode)\nbehavior.\n\nTests should avoid using features that haven't been shipped by the\nactively-developed major rendering engines (Blink, WebKit, Gecko, Edge). When\nunsure, check [caniuse.com](http://caniuse.com/). By necessity, this\nrecommendation does not apply to the feature targeted by the test.\n\n*** note\nIt may be tempting have a test for a bleeding-edge feature X depend on feature\nY, which has only shipped in beta / development versions of various browsers.\nThe reasoning would be that all browsers that implement X will have implemented\nY. Please keep in mind that Chrome has un-shipped features that made it to the\nBeta channel in the past.\n***\n\n*** aside\n[ES2015](http://benmccormick.org/2015/09/14/es5-es6-es2016-es-next-whats-going-on-with-javascript-versioning/)\nis shipped by all major browsers under active development (except for modules),\nso using ES2015 features is acceptable.\n\nAt the time of this writing, ES2016 is not fully shipped in all major browsers.\n***\n\n### Self-Contained\n\nTests must be **self-contained** and not depend on external network resources.\n\nUnless used by multiple test files, CSS and JavaScript should be inlined using\n`<style>` and `<script>` tags. Content shared by multiple tests should be\nplaced in a `resources/` directory near the tests that share it. See below for\nusing multiple origins in a test.\n\n### File Names\n\nTest **file names** should describe what is being tested.\n\nFile names should use `snake-case`, but preserve the case of any embedded API\nnames. For example, prefer `document-createElement.html` to\n`document-create-element.html`.\n\n### Character Encoding\n\nTests should use the UTF-8 **character encoding**, which should be declared by\n`<meta charset=utf-8>`. A `<meta>` tag is not required (but is acceptable) for\ntests that only contain ASCII characters. This guideline does not apply when\nspecifically testing encodings.\n\nThe `<meta>` tag must be the first child of the document's `<head>` element. In\ndocuments that do not have an explicit `<head>`, the `<meta>` tag must follow\nthe doctype.\n\n## Coding Style\n\nNo coding style is enforced for web tests. This section highlights coding\nstyle aspects that are not consistent across our web tests, and suggests some\ndefaults for unopinionated developers. When writing web tests for a new part\nof the codebase, you can minimize review latency by taking a look at existing\ntests, and pay particular attention to these issues. Also beware of per-project\nstyle guides, such as the\n[ServiceWorker Tests Style guide](https://www.chromium.org/blink/serviceworker/testing).\n\n### Baseline\n\n[Google's JavaScript Style Guide](https://google.github.io/styleguide/jsguide.html)\nand\n[Google's HTML/CSS Style Guide](https://google.github.io/styleguide/htmlcssguide.xml)\nare a reasonable baseline for coding style defaults, with the caveat that web\ntests do not use Google Closure or JSDoc.\n\n### == vs ===\n\nJavaScript's\n[== operator](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Operators/Comparison_Operators#Equality_())\nperforms some\n[type conversion](http://www.ecma-international.org/ecma-262/6.0/#sec-abstract-equality-comparison).\non its arguments, which might be surprising to readers whose experience centers\naround C++ or Java. The\n[=== operator](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Operators/Comparison_Operators#Identity_strict_equality_())\nis much more similar to `==` in C++.\n\nUsing `===` everywhere is an easy default that saves you, your reviewer, and any\ncolleague that might have to debug test failures, from having to reason about\n[special cases for ==](http://dorey.github.io/JavaScript-Equality-Table/). At\nthe same time, some developers consider `===` to add unnecessary noise when `==`\nwould suffice. While `===` should be universally accepted, be flexible if your\nreviewer expresses a strong preference for `==`.\n\n### Let and Const vs Var\n\nJavaScript variable declarations introduced by\n[var](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Statements/var)\nare hoisted to the beginning of their containing function, which may be\nsurprising to C++ and Java developers. By contrast,\n[const](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Statements/const)\nand\n[let](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Statements/let)\ndeclarations are block-scoped, just like in C++ and Java, and have the added\nbenefit of expressing mutability intent.\n\nFor the reasons above, a reasonable default is to prefer `const` and `let` over\n`var`, with the same caveat as above.\n\n### Strict Mode\n\nJavaScript's\n[strict mode](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Strict_mode),\nactivated by adding `'use strict';` to the very top of a script, helps catch\nsome errors, such as mistyping a variable name, forgetting to declare a\nvariable, or attempting to change a read-only property.\n\nGiven that strict mode gives some of the benefits of using a compiler, adding it\nto every test is a good default. This does not apply when specifically testing\nsloppy mode behavior.\n\nSome developers argue that adding the `'use strict';` boilerplate can be\ndifficult to remember, weighs down smaller tests, and in many cases running a\ntest case is sufficient to discover any mistyped variable names.\n\n### Promises\n\n[Promises](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise)\nare a mechanism for structuring asynchronous code. When used correctly, Promises\navoid some of the\n[issues of callbacks](http://colintoh.com/blog/staying-sane-with-asynchronous-programming-promises-and-generators).\nFor these reasons, a good default is to prefer promises over other asynchronous\ncode structures.\n\nWhen using promises, be aware of the\n[execution order subtleties](https://jakearchibald.com/2015/tasks-microtasks-queues-and-schedules/)\nassociated with them. Here is a quick summary.\n\n* The function passed to `Promise.new` is executed synchronously, so it finishes\n  before the Promise is created and returned.\n* The functions passed to `then` and `catch` are executed in\n  _separate microtasks_, so they will be executed after the code that resolved\n  or rejected the promise finishes, but before any other event handler.\n\n### Classes\n\n[Classes](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Classes)\nare syntactic sugar for JavaScript's\n[prototypal inheritance](https://developer.mozilla.org/docs/Web/JavaScript/Inheritance_and_the_prototype_chain).\nCompared to manipulating prototypes directly, classes offer a syntax that is\nmore familiar to developers coming from other programming languages.\n\nA good default is to prefer classes over other OOP constructs, as they will make\nthe code easier to read for many of your fellow Chrome developers. At the same\ntime, most web tests are simple enough that OOP is not justified.\n\n### Character Encoding\n\nWhen HTML pages do not explicitly declare a character encoding, browsers\ndetermine the encoding using an\n[encoding sniffing algorithm](https://html.spec.whatwg.org/multipage/syntax.html#determining-the-character-encoding)\nthat will surprise most modern Web developers. Highlights include a default\nencoding that depends on the user's locale, and non-standardized\nbrowser-specific heuristics.\n\nThe easiest way to not have to think about any of this is to add\n`<meta charset=\"utf-8\">` to all your tests. This is easier to remember if you\nuse a template for your web tests, rather than writing them from scratch.\n\n## Tests with Manual Feedback\n\nTests that rely on the testing APIs exposed by WPT or Blink will not work when\nloaded in a standard browser environment. When writing such tests, default to\nhaving the tests gracefully degrade to manual tests in the absence of the\ntesting APIs.\n\nThe\n[document on web tests with manual feedback](./web_tests_with_manual_fallback.md)\ndescribes the approach in detail and highlights the trade-off between added test\nweight and ease of debugging.\n"
  },
  {
    "path": "development/testing/web_tests_linux",
    "title": "Running web tests on Linux",
    "content": "# Running web tests on Linux\n\n1.  Build `blink_tests` (see [Linux-specific build instructions](https://chromium.googlesource.com/chromium/src/+/main/docs/linux/build_instructions.md))\n1.  Checkout the web tests\n    *   If you have an entry in your `.gclient` file that includes\n        \"web_tests\", you may need to comment it out and sync.\n    *   You can run a subset of the tests by passing in a path relative to\n        `src/third_party/blink/web_tests/`.  For example,\n        `third_party/blink/tools/run_web_tests.py fast` will only run the tests under\n        `src/third_party/blink/web_tests/fast/`.\n1.  When the tests finish, any unexpected results should be displayed.\n\nSee [Web Tests](testing/web_tests.md)\nfor full documentation about set up and available options.\n\n## Pixel Tests\n\nThe pixel test results were generated on Ubuntu 10.4 (Lucid). If you're running\na newer version of Ubuntu, you will get some pixel test failures due to changes\nin freetype or fonts.  In this case, you can create a Lucid 64 chroot using\n`build/install-chroot.sh` to compile and run tests.\n\n## Fonts\n\n1.  Make sure you have all the dependencies installed by running\n\n```shell\nbuild/install-build-deps.sh\n```\n\n2.  Double check that\n\n```shell\nls third_party/test_fonts/test_fonts/\n```\n\nis not empty and lists the fonts downloaded through the `test_fonts`\nhook in the top level `DEPS` file.\n\n## Plugins\n\nIf `fast/dom/object-plugin-hides-properties.html` and\n`plugins/embed-attributes-style.html` are failing, try uninstalling\n`totem-mozilla` from your system:\n\n    sudo apt-get remove totem-mozilla\n\n## Configuration tips\n\n*   Use an optimized `content_shell` when rebaselining or running a lot of\n    tests. ([bug 8475](https://crbug.com/8475) is about how the debug output\n    differs from the optimized output.)\n\n    `ninja -C out/Release content_shell`\n\n*   Make sure you have wdiff installed: `sudo apt-get install wdiff` to get\n    prettier diff output.\n*   Some pixel tests may fail due to processor-specific rounding errors. Build\n    using a chroot jail with Lucid 64-bit user space to be sure that your system\n    matches the checked in baselines.  You can use `build/install-chroot.sh` to\n    set up a Lucid 64 chroot. Learn more about\n    [using a linux chroot](linux/using_a_chroot.md).\n\n## Getting a web test into a debugger\n\nThere are two ways:\n\n1.  Run `content_shell` directly rather than using `run_web_tests.py`. You\n    will need to pass some options:\n    *   `--no-timeout` to give you plenty of time to debug\n    *   the fully qualified path of the web test (rather than relative to\n        `blink/web_tests`).\n1.  Or, run as normal but with the\n    `--additional-drt-flag=--renderer-startup-dialog\n    --additional-drt-flag=--no-timeout --timeout-ms=86400000` flags. The first\n    one makes content\\_shell bring up a dialog before running, which then would\n    let you attach to the process via `gdb -p PID_OF_DUMPRENDERTREE`. The others\n    help avoid the test shell and DumpRenderTree timeouts during the debug\n    session.\n\n## Using an embedded X server\n\nIf you try to use your computer while the tests are running, you may get annoyed\nas windows are opened and closed automatically.  To get around this, you can\ncreate a separate X server for running the tests.\n\n1.  Install Xephyr (`sudo apt-get install xserver-xephyr`)\n1.  Start Xephyr as display 4: `Xephyr :4 -screen 1024x768x24`\n1.  Run the web tests in the Xephyr: `DISPLAY=:4 run_web_tests.py`\n\nXephyr supports debugging repainting. See the\n[Xephyr README](http://cgit.freedesktop.org/xorg/xserver/tree/hw/kdrive/ephyr/README)\nfor details. In brief:\n\n1.  `XEPHYR_PAUSE=$((500*1000)) Xephyr ...etc...  # 500 ms repaint flash`\n1.  `kill -USR1 $(pidof Xephyr)`\n\nIf you don't want to see anything at all, you can use Xvfb (should already be\ninstalled).\n\n1.  Start Xvfb as display 4: `Xvfb :4 -screen 0 1024x768x24`\n1.  Run the web tests in the Xvfb: `DISPLAY=:4 run_web_tests.py`\n\n## Tiling Window managers\n\nThe web tests want to run with the window at a particular size down to the\npixel level.  This means if your window manager resizes the window it'll cause\ntest failures.  This is another good reason to use an embedded X server.\n\n### xmonad\n\nIn your `.xmonad/xmonad.hs`, change your config to include a manageHook along\nthese lines:\n\n```\ntest_shell_manage = className =? \"Test_shell\" --> doFloat\nmain = xmonad $\n  defaultConfig\n    { manageHook = test_shell_manage <+> manageHook defaultConfig\n    ...\n```\n"
  },
  {
    "path": "development/testing/web_tests_in_content_shell",
    "title": "Running web tests using the content shell",
    "content": "# Running web tests using the content shell\n\n[TOC]\n\n## Compiling\n\nIf you want to run web tests,\n[build the target `blink_tests`](web_tests.md); this includes all the other\nbinaries required to run the tests.\n\n## Running\n\n### Using `run_web_tests.py`\n\nYou can run web tests using `run_web_tests.py` (in\n`src/third_party/blink/tools`).\n\n```bash\nthird_party/blink/tools/run_web_tests.py -t <build directory> storage/indexeddb\n```\nTo see a complete list of arguments supported, run with `--help`.\n\n*** promo\nWindows users need to use `third_party/blink/tools/run_web_tests.bat` instead.\n***\n\n*** promo\nYou can add `<path>/third_party/blink/tools` into `PATH` so that you can\nrun it from anywhere without the full path.\n***\n\n### Run Web Tests Directly with Content Shell\n\nIn some cases (e.g. for debugging), you can run web tests directly with\nContent Shell executable, with the `--run-web-tests` flag:\n\n```bash\nout/Default/content_shell --run-web-tests <url>|<full_test_source_path>|<relative_test_path>\n```\n\n`<relative_test_path>` is relative to the [web_tests](../../third_party/blink/web_tests)\ndirectory, regardless of the current directory.\n\nFor example:\n\n```bash\nout/Default/content_shell --run-web-tests fast/forms/001.html\n```\nor\n\n```bash\nout/Default/content_shell --run-web-tests \\\n    /home/user/chrome/src/third_party/blink/web_tests/fast/forms/001.html\n```\nor\n\n```bash\nout/Default/content_shell --run-web-tests ~/test/temp-test.html\n```\n\nBy default, it dumps the text result only (as the dump of pixels and audio\nbinary data is not human readable) and quits. This can meet the requirement of\nmost debugging requirements. If you need to interactively debug the test page\n(e.g. using devtools), you'll need to run Content Shell [as a simple\nbrowser](#As-a-simple-browser).\n\nIn rare cases, to run Content Shell in the exact same way as\n`run_web_tests.py` runs it, you need to run it in the\n[protocol mode](../../content/web_test/browser/test_info_extractor.h).\n\n*** note\nOn the Mac, use `Content Shell.app`, not `content_shell`.\n\n```bash\nout/Default/Content\\ Shell.app/Contents/MacOS/Content\\ Shell ...\n```\nOn Windows, use `content_shell.exe`.\n***\n\n#### Running HTTP Tests in Content Shell\n\nHTTP tests reside under [web_tests/http/tests](../../third_party/blink/web_tests/http/tests).\nYou need to start a web server first. By default it serves generated files from\nout/Release:\n\n```bash\nvpython3 third_party/blink/tools/run_blink_httpd.py -t <build directory>\n```\nThen run the test with a localhost URL:\n\n```bash\nout/Default/content_shell --run-web-tests http://localhost:8000/<test>\n```\n\nIt may be necessary specify [--enable-blink-features](https://source.chromium.org/search?q=%22--enable-blink-features%3D%22) explicitly for some tests.\n\n#### Running WPT Tests in Content Shell\n\nSimilar to HTTP tests, many WPT (a.k.a. web-platform-tests under\n[web_tests/external/wpt](../../third_party/blink/web_tests/external/wpt) directory)\ntests require some setup before running in Content Shell:\n\n```bash\nvpython3 third_party/blink/tools/run_blink_wptserve.py -t <build directory>\n```\n\nThen run the test:\n\n```bash\nout/Default/content_shell --run-web-tests http://localhost:8001/<test>\n```\n\nIf the test requires HTTPS (e.g. the file name contains \".https.\"), use the\nfollowing command instead:\n\n```bash\nout/Default/content_shell --run-web-tests https://localhost:8444/<test>\n```\n\n### As a simple browser\n\nYou can run the shell directly as a simple browser:\n\n```bash\nout/Default/content_shell\n```\n\nThis allows you see how your changes look in Chromium. You can inspect the page\nby right clicking and selecting 'Inspect Element'.\n\nYou can also use `--remote-debugging-port`\n\n```bash\nout/Default/content_shell --remote-debugging-port=9222\n```\nand open `http://localhost:9222` from another browser to inspect the page.\nThis is useful when you don't want DevTools to run in the same Content Shell,\ne.g. when you are logging a lot and don't want the log from DevTools\nor when DevTools is unstable in the current revision due to some bugs.\n\n#### Debug WPT\n\nIf you want to debug WPT with devtools in Content Shell, you will first need to\nstart the server:\n\n```bash\nvpython3 third_party/blink/tools/run_blink_wptserve.py\n```\n\nThen start Content Shell with some additional flags:\n\n```bash\nout/Default/content_shell --enable-experimental-web-platform-features --ignore-certificate-errors --host-resolver-rules=\"MAP nonexistent.*.test ^NOTFOUND, MAP *.test. 127.0.0.1, MAP *.test 127.0.0.1\"\n```\n\nYou are also able to debug the inside of Chromium with a debugger for\nparticular WPT tests. After starting the WPT server, run particular tests via\nContent Shell from the debugger with the following command.\n(Refer to your debugger's manual for how to start a program from your debugger.)\n\n```bash\nout/Default/content_shell --run-web-tests http://localhost:8001/<test>\n```\n\nChromium adopts multi-process architecture. If you want to debug the child\nrenderer processes, use `--single-process` Content Shell option, or\n`--renderer-startup-dialog` option and attach the debugger to the renderer\nprocesses after starting the tests. Refer to the Debugging section below for details.\n\n## Debugging\n\n### `--single-process`\n\nThe command line switch `--single-process` is useful for starting\ncontent_shell in gdb. In most cases, `--single-process` is good for debugging\nexcept when you want to debug the multiple process behavior or when we have\nsome bug breaking `--single-process` in particular cases.\n\n### Web tests\n\nSee [Run Web Tests Directly with Content Shell](#Run-Web-Tests-Directly-with-Content-Shell).\nIn most cases you don't need `--single-process` because `content_shell` is\nin single process mode when running most web tests.\n\nSee [DevTools frontend](../../third_party/devtools-frontend/src/README.md#basics)\nfor the commands that are useful for debugging devtools web tests.\n\n### In The Default Multiple Process Mode\n\nIn rare cases, you need to debug Content Shell in multiple process mode.\nYou can ask Content Shell to wait for you to attach a debugger once it spawns a\nrenderer process by adding the `--renderer-startup-dialog` flag:\n\n```bash\nout/Default/content_shell --renderer-startup-dialog --no-sandbox\n```\n\nDebugging workers and other subprocesses is simpler with\n`--wait-for-debugger-children`, which can have one of two values: `plugin` or\n`renderer`.\n\n## Future Work\n\n### Reusing existing testing objects\n\nTo avoid writing (and maintaining!) yet another test controller, it is desirable\nto reuse an existing test controller. A possible solution would be to change\nDRT's test controller to not depend on DRT's implementation of the Blink\nobjects, but rather on the Blink interfaces. In addition, we would need to\nextract an interface from the test shell object that can be implemented by\ncontent shell. This would allow for directly using DRT's test controller in\ncontent shell.\n"
  },
  {
    "path": "development/testing/web_tests_addressing_flake",
    "title": "Addressing Flaky Web Tests",
    "content": "# Addressing Flaky Web Tests\n\nThis document provides tips and tricks for reproducing and debugging flakes in\n[Web Tests](web_tests.md). If you are debugging a flaky Web Platform Test (WPT),\nyou may wish to check the specific [Addressing Flaky\nWPTs](web_platform_tests_addressing_flake.md) documentation.\n\nThis document assumes you are familiar with running Web Tests via\n`run_web_tests.py`; if you are not then [see\nhere](web_tests.md#Running-Web-Tests).\n\n[TOC]\n\n## Understanding builder results\n\nOften (e.g. by Flake Portal), you will be pointed to a particular build in which\nyour test has flaked. You will need the name of the specific build step that has\nflaked; usually for Web Tests this is `blink_web_tests` but there are variations\n(e.g. `not_site_per_process_blink_web_tests`).\n\nOn the builder page, find the appropriate step:\n\n![web_tests_blink_web_tests_step]\n\nWhile you can examine the individual shard logs to find your test output, it is\neasier to view the consolidated information, so scroll down to the **archive\nresults for blink\\_web\\_tests** step and click the `web_test_results` link:\n\n![web_tests_archive_blink_web_tests_step]\n\nThis will open a new tab with the results viewer. By default your test should be\nshown, but if it isn't then you can click the 'All' button in the 'Query' row,\nthen enter the test filename in the textbox beside 'Filters':\n\n![web_tests_results_viewer_query_filter]\n\nThere are a few ways that a Web Test can flake, and what the result means may\ndepend on the [test type](writing_web_tests.md#Test-Types):\n\n1. `FAIL` - the test failed. For reference or pixel tests, this means it did not\n   match the reference image. For JavaScript tests, the test either failed an\n   assertion *or* did not match the [baseline](web_test_expectations.md)\n   `-expected.txt` file checked in for it.\n   * For image tests, this status is reported as `IMAGE` (as in an image diff).\n   * For Javascript tests, this status is reported as `TEXT` (as in a text\n     diff).\n1. `TIMEOUT` - the test timed out before producing a result. This may happen if\n   the test is slow and normally runs close to the timeout limit, but is usually\n   caused by waiting on an event that never happens. These unfortunately [do not\n   produce any logs](https://crbug.com/487051).\n1. `CRASH` - the browser crashed while executing the test. There should be logs\n   associated with the crash available.\n1. `PASS` - this can happen! Web Tests can be marked as [expected to\n   fail](web_test_expectations.md), and if they then pass then that is an\n   unexpected result, aka a potential flake.\n\nClicking on the test row anywhere *except* the test name (which is a link to the\ntest itself) will expand the entry to show information about the failure result,\nincluding actual/expected results and browser logs if they exist.\n\nIn the following example, our flaky test has a `FAIL` result which is a flake\ncompared to its (default) expected `PASS` result. The test results (`TEXT` - as\nexplained above this is equivalent to `FAIL`), output, and browser log links are\nhighlighted.\n\n![web_tests_results_viewer_flaky_test]\n\n## Reproducing Web Test flakes\n\n>TODO: document how to get the args.gn that the bot used\n\n>TODO: document how to get the flags that the bot passed to `run_web_tests.py`\n\n### Repeatedly running tests\n\nFlakes are by definition non-deterministic, so it may be necessary to run the\ntest or set of tests repeatedly to reproduce the failure. Two flags to\n`run_web_tests.py` can help with this:\n\n* `--repeat-each=N` - repeats each test in the test set N times. Given a set of\n  tests A, B, and C, `--repeat-each=3` will run AAABBBCCC.\n* `--iterations=N` - repeats the entire test set N times. Given a set of tests\n  A, B, and C, `--iterations=3` will run ABCABCABC.\n\n## Debugging flaky Web Tests\n\n>TODO: document how to attach gdb\n\n### Seeing logs from content\\_shell\n\nWhen debugging flaky tests, it can be useful to add `LOG` statements to your\ncode to quickly understand test state. In order to see these logs when using\n`run_web_tests.py`, pass the `--driver-logging` flag:\n\n```\n./third_party/blink/tools/run_web_tests.py --driver-logging path/to/test.html\n```\n\n### Loading the test directly in content\\_shell\n\nWhen debugging a specific test, it can be useful to skip `run_web_tests.py` and\ndirectly run the test under `content_shell` in an interactive session. For many\ntests, one can just pass the test path to `content_shell`:\n\n```\nout/Default/content_shell third_party/blink/web_tests/path/to/test.html\n```\n\n**Caveat**: running tests like this is not equivalent to `run_web_tests.py`,\nwhich passes the `--run-web-tests` flag to `content_shell`. The\n`--run-web-tests` flag enables a lot of testing-only code in `content_shell`,\nbut also runs in a non-interactive mode.\n\nUseful flags to pass to get `content_shell` closer to the `--run-web-tests` mode\ninclude:\n\n* `--enable-blink-test-features` - enables status=test and status=experimental\n  features from `runtime_enabled_features.json5`.\n\n>TODO: document how to deal with tests that require a server to be running\n\n[web_tests_blink_web_tests_step]: images/web_tests_blink_web_tests_step.png\n[web_tests_archive_blink_web_tests_step]: images/web_tests_archive_blink_web_tests_step.png\n[web_tests_results_viewer_query_filter]: images/web_tests_results_viewer_query_filter.png\n[web_tests_results_viewer_flaky_test]: images/web_tests_results_viewer_flaky_test.png\n"
  },
  {
    "path": "development/testing/web_tests",
    "title": "Web Tests (formerly known as \"Layout Tests\" or \"LayoutTests\")",
    "content": "# Web Tests (formerly known as \"Layout Tests\" or \"LayoutTests\")\n\nWeb tests are used by Blink to test many components, including but not\nlimited to layout and rendering. In general, web tests involve loading pages\nin a test renderer (`content_shell`) and comparing the rendered output or\nJavaScript output against an expected output file.\n\nThis document covers running and debugging existing web tests. See the\n[Writing Web Tests documentation](./writing_web_tests.md) if you find\nyourself writing web tests.\n\nNote that we changed the term \"layout tests\" to \"web tests\".\nPlease assume these terms mean the identical stuff. We also call it as\n\"WebKit tests\" and \"WebKit layout tests\".\n\n[\"Web platform tests\"](./web_platform_tests.md) (WPT) are the preferred form of\nweb tests and are located at\n[web_tests/external/wpt](/third_party/blink/web_tests/external/wpt).\nTests that should work across browsers go there. Other directories are for\nChrome-specific tests only.\n\nNote: if you are looking for a guide for the Web Platform Test, you should read\n[\"Web platform tests\"](./web_platform_tests.md) (WPT). This document does not\ncover WPT specific features/behaviors.\n\nNote: if you are looking for a guide for running the Web Platform Tests with\nChrome, Chrome Android or WebView, you should read [\"Running Web Platform Tests with run_wpt_tests.py\"](./run_web_platform_tests.md).\n\n[TOC]\n\n## Running Web Tests\n\n### Supported Platforms\n\n* Linux\n* MacOS\n* Windows\n* Fuchsia\n\nAndroid is [not supported](https://crbug.com/567947).\n\n### Initial Setup\n\nBefore you can run the web tests, you need to build the `blink_tests` target\nto get `content_shell` and all of the other needed binaries.\n\n```bash\nautoninja -C out/Default blink_tests\n```\n\nOn **Mac**, you probably want to strip the content_shell binary before starting\nthe tests. If you don't, you'll have 5-10 running concurrently, all stuck being\nexamined by the OS crash reporter. This may cause other failures like timeouts\nwhere they normally don't occur.\n\n```bash\nstrip ./out/Default/Content\\ Shell.app/Contents/MacOS/Content\\ Shell\n```\n\n### Running the Tests\n\nThe test runner script is in `third_party/blink/tools/run_web_tests.py`.\n\nTo specify which build directory to use (e.g. out/Default, etc.)\nyou should pass the `-t` or `--target` parameter. If no directory is specified,\n`out/Release` will be used. To use the built-in `out/Default`, use:\n\n```bash\nthird_party/blink/tools/run_web_tests.py -t Default\n```\n\n*** promo\n* Windows users need to use `third_party\\blink\\tools\\run_web_tests.bat` instead.\n* Linux users should not use `testing/xvfb.py`; `run_web_tests.py` manages Xvfb\n  itself.\n***\n\nTests marked as `[ Skip ]` in\n[TestExpectations](../../third_party/blink/web_tests/TestExpectations)\nwon't be run by default, generally because they cause some intractable tool error.\nTo force one of them to be run, either rename that file or specify the skipped\ntest on the command line (see below) or in a file specified with --test-list\n(however, --skip=always can make the tests marked as `[ Skip ]` always skipped).\nRead the [Web Test Expectations documentation](./web_test_expectations.md) to\nlearn more about TestExpectations and related files.\n\n*** promo\nCurrently only the tests listed in\n[Default.txt](../../third_party/blink/web_tests/TestLists/Default.txt) are run\non the Fuchsia bots, since running all web tests takes too long on Fuchshia.\nMost developers focus their Blink testing on Linux. We rely on the fact that the\nLinux and Fuchsia behavior is nearly identical for scenarios outside those\ncovered by the smoke tests.\n***\n\n*** promo\nSimilar to Fuchsia's case, the tests listed in [Mac.txt]\n(../../third_party/blink/web_tests/TestLists/Mac.txt)\nare run on older mac version bots. By doing this we reduced the resources needed to run\nthe tests. This relies on the fact that the majority of web tests will behavior similarly on\ndifferent mac versions.\n***\n\nTo run only some of the tests, specify their directories or filenames as\narguments to `run_web_tests.py` relative to the web test directory\n(`src/third_party/blink/web_tests`). For example, to run the fast form tests,\nuse:\n\n```bash\nthird_party/blink/tools/run_web_tests.py fast/forms\n```\n\nOr you could use the following shorthand:\n\n```bash\nthird_party/blink/tools/run_web_tests.py fast/fo\\*\n```\n\n*** promo\nExample: To run the web tests with a debug build of `content_shell`, but only\ntest the SVG tests and run pixel tests, you would run:\n\n```bash\nthird_party/blink/tools/run_web_tests.py -t Default svg\n```\n***\n\nAs a final quick-but-less-robust alternative, you can also just use the\ncontent_shell executable to run specific tests by using (example on Windows):\n\n```bash\nout\\Default\\content_shell.exe --run-web-tests <url>|<full_test_source_path>|<relative_test_path>\n```\n\nas in:\n\n```bash\nout\\Default\\content_shell.exe --run-web-tests \\\n    c:\\chrome\\src\\third_party\\blink\\web_tests\\fast\\forms\\001.html\n```\nor\n\n```bash\nout\\Default\\content_shell.exe --run-web-tests fast\\forms\\001.html\n```\n\nbut this requires a manual diff against expected results, because the shell\ndoesn't do it for you. It also just dumps the text result only (as the dump of\npixels and audio binary data is not human readable).\nSee [Running Web Tests Using the Content Shell](./web_tests_in_content_shell.md)\nfor more details of running `content_shell`.\n\nTo see a complete list of arguments supported, run:\n\n```bash\nthird_party/blink/tools/run_web_tests.py --help\n```\n\n*** note\n**Linux Note:** We try to match the Windows render tree output exactly by\nmatching font metrics and widget metrics. If there's a difference in the render\ntree output, we should see if we can avoid rebaselining by improving our font\nmetrics. For additional information on Linux web tests, please see\n[docs/web_tests_linux.md](./web_tests_linux.md).\n***\n\n*** note\n**Mac Note:** While the tests are running, a bunch of Appearance settings are\noverridden for you so the right type of scroll bars, colors, etc. are used.\nYour main display's \"Color Profile\" is also changed to make sure color\ncorrection by ColorSync matches what is expected in the pixel tests. The change\nis noticeable, how much depends on the normal level of correction for your\ndisplay. The tests do their best to restore your setting when done, but if\nyou're left in the wrong state, you can manually reset it by going to\nSystem Preferences → Displays → Color and selecting the \"right\" value.\n***\n\n### Test Harness Options\n\nThis script has a lot of command line flags. You can pass `--help` to the script\nto see a full list of options. A few of the most useful options are below:\n\n| Option                      | Meaning |\n|:----------------------------|:--------------------------------------------------|\n| `--debug`                   | Run the debug build of the test shell (default is release). Equivalent to `-t Debug` |\n| `--nocheck-sys-deps`        | Don't check system dependencies; this allows faster iteration. |\n| `--verbose`                 |\tProduce more verbose output, including a list of tests that pass. |\n| `--reset-results`           |\tOverwrite the current baselines (`-expected.{png`&#124;`txt`&#124;`wav}` files) with actual results, or create new baselines if there are no existing baselines. |\n| `--fully-parallel`          | Run tests in parallel using as many child processes as the system has cores. |\n| `--driver-logging`          | Print C++ logs (LOG(WARNING), etc).  |\n\n## Success and Failure\n\nA test succeeds when its output matches the pre-defined expected results. If any\ntests fail, the test script will place the actual generated results, along with\na diff of the actual and expected results, into\n`src/out/Default/layout-test-results/`, and by default launch a browser with a\nsummary and link to the results/diffs.\n\nThe expected results for tests are in the\n`src/third_party/blink/web_tests/platform` or alongside their respective\ntests.\n\n*** note\nTests which use [testharness.js](https://github.com/w3c/testharness.js/)\ndo not have expected result files if all test cases pass.\n***\n\nA test that runs but produces the wrong output is marked as \"failed\", one that\ncauses the test shell to crash is marked as \"crashed\", and one that takes longer\nthan a certain amount of time to complete is aborted and marked as \"timed out\".\nA row of dots in the script's output indicates one or more tests that passed.\n\n## Test expectations\n\nThe\n[TestExpectations](../../third_party/blink/web_tests/TestExpectations) file (and related\nfiles) contains the list of all known web test failures. See the\n[Web Test Expectations documentation](./web_test_expectations.md) for more\non this.\n\n## Testing Runtime Flags\n\nThere are two ways to run web tests with additional command-line arguments:\n\n### --flag-specific\n\n```bash\nthird_party/blink/tools/run_web_tests.py --flag-specific=blocking-repaint\n```\nIt requires that `web_tests/FlagSpecificConfig` contains an entry like:\n\n```json\n{\n  \"name\": \"blocking-repaint\",\n  \"args\": [\"--blocking-repaint\", \"--another-flag\"]\n}\n```\n\nThis tells the test harness to pass `--blocking-repaint --another-flag` to the\ncontent_shell binary.\n\nIt will also look for flag-specific expectations in\n`web_tests/FlagExpectations/blocking-repaint`, if this file exists. The\nsuppressions in this file override the main TestExpectations files.\nHowever, `[ Slow ]` in either flag-specific expectations or base expectations\nis always merged into the used expectations.\n\nIt will also look for baselines in `web_tests/flag-specific/blocking-repaint`.\nThe baselines in this directory override the fallback baselines.\n\n*** note\n[BUILD.gn](../../BUILD.gn) assumes flag-specific builders always runs on linux bots, so\nflag-specific test expectations and baselines are only downloaded to linux bots.\nIf you need run flag-specific builders on other platforms, please update\nBUILD.gn to download flag-specific related data to that platform.\n***\n\nYou can also use `--additional-driver-flag` to specify additional command-line\narguments to content_shell, but the test harness won't use any flag-specific\ntest expectations or baselines.\n\n### Virtual test suites\n\nA *virtual test suite* can be defined in\n[web_tests/VirtualTestSuites](../../third_party/blink/web_tests/VirtualTestSuites),\nto run a subset of web tests with additional flags, with\n`virtual/<prefix>/...` in their paths. The tests can be virtual tests that\nmap to real base tests (directories or files) whose paths match any of the\nspecified bases, or any real tests under `web_tests/virtual/<prefix>/`\ndirectory. For example, you could test a (hypothetical) new mode for\nrepainting using the following virtual test suite:\n\n```json\n{\n  \"prefix\": \"blocking_repaint\",\n  \"platforms\": [\"Linux\", \"Mac\", \"Win\"],\n  \"bases\": [\"compositing\", \"fast/repaint\"],\n  \"args\": [\"--blocking-repaint\"]\n}\n```\n\nThis will create new \"virtual\" tests of the form\n`virtual/blocking_repaint/compositing/...` and\n`virtual/blocking_repaint/fast/repaint/...` which correspond to the files\nunder `web_tests/compositing` and `web_tests/fast/repaint`, respectively,\nand pass `--blocking-repaint` to `content_shell` when they are run.\n\nNote that you can run the tests with the following command line:\n\n```bash\nthird_party/blink/tools/run_web_tests.py virtual/blocking_repaint/compositing \\\n  virtual/blocking_repaint/fast/repaint\n```\n\nThese virtual tests exist in addition to the original `compositing/...` and\n`fast/repaint/...` tests. They can have their own expectations in\n`web_tests/TestExpectations`, and their own baselines. The test harness will\nuse the non-virtual expectations and baselines as a fallback. If a virtual\ntest has its own expectations, they will override all non-virtual\nexpectations. Otherwise the non-virtual expectations will be used. However,\n`[ Slow ]` in either virtual or non-virtual expectations is always merged\ninto the used expectations. If a virtual test is expected to pass while the\nnon-virtual test is expected to fail, you need to add an explicit `[ Pass ]`\nentry for the virtual test.\n\nThis will also let any real tests under `web_tests/virtual/blocking_repaint`\ndirectory run with the `--blocking-repaint` flag.\n\nThe \"platforms\" configuration can be used to skip tests on some platforms. If\na virtual test suites uses more than 5% of total test time, we should consider\nto skip the test suites on some platforms.\n\nThe \"prefix\" value should be unique. Multiple directories with the same flags\nshould be listed in the same \"bases\" list. The \"bases\" list can be empty,\nin case that we just want to run the real tests under `virtual/<prefix>`\nwith the flags without creating any virtual tests.\n\nA virtual test suite can have an optional `exclusive_tests` field to specify\nall (with `\"ALL\"`) or a subset of `bases` tests that will be exclusively run\nunder this virtual suite. The specified base tests will be skipped. Corresponding\nvirtual tests under other virtual suites that don't specify the tests in their\n`exclusive_tests` list will be skipped, too. For example (unrelated fields\nare omitted):\n\n```json\n{\n  \"prefix\": \"v1\",\n  \"bases\": [\"a\"],\n}\n{\n  \"prefix\": \"v2\",\n  \"bases\": [\"a/a1\", \"a/a2\"],\n  \"exclusive_tests\": \"ALL\",\n}\n{\n  \"prefix\": \"v3\",\n  \"bases\": [\"a\"],\n  \"exclusive_tests\": [\"a/a1\"],\n}\n```\n\nSuppose there are directories `a/a1`, `a/a2` and `a/a3`, we will run the\nfollowing tests:\n\n|      Suite |   a/a1  |   a/a2  | a/a3 |\n| ---------: | :-----: | :-----: | :--: |\n|       base | skipped | skipped | run  |\n| virtual/v1 | skipped | skipped | run  |\n| virtual/v2 |   run   |   run   | n/a  |\n| virtual/v3 |   run   | skipped | run  |\n\nIn a similar manner, a virtual test suite can also have an optional\n`skip_base_tests` field to specify all (with `\"ALL\"`) or a subset of `bases`\ntests that will be run under this virtual while the base tests will be skipped.\nThis will not affect other virtual suites.\n\n```json\n{\n  \"prefix\": \"v1\",\n  \"bases\": [\"a/a1\"],\n}\n{\n  \"prefix\": \"v2\",\n  \"bases\": [\"a/a1\"],\n  \"skip_base_tests\": \"ALL\",\n}\n```\nSuppose there are directories `a/a1` and `a/a2` we will run the following tests:\n\n|      Suite |   a/a1  |   a/a2  |\n| ---------: | :-----: | :-----: |\n|       base | skipped |   run   |\n| virtual/v1 |   run   |   n/a   |\n| virtual/v2 |   run   |   n/a   |\n\n\n### Choosing between flag-specific and virtual test suite\n\nFor flags whose implementation is still in progress, flag-specific expectations\nand virtual test suites represent two alternative strategies for testing both\nthe enabled code path and non-enabled code path. They are preferred to only\nsetting a [runtime enabled feature](../../third_party/blink/renderer/platform/RuntimeEnabledFeatures.md)\nto `status: \"test\"` if the feature has substantially different code path from\nproduction because the latter would cause loss of test coverage of the production\ncode path.\n\nConsider the following when choosing between virtual test suites and\nflag-specific suites:\n\n* The\n  [waterfall builders](https://dev.chromium.org/developers/testing/chromium-build-infrastructure/tour-of-the-chromium-buildbot)\n  and [try bots](https://dev.chromium.org/developers/testing/try-server-usage)\n  will run all virtual test suites in addition to the non-virtual tests.\n  Conversely, a flag-specific configuration won't automatically cause the bots\n  to test your flag - if you want bot coverage without virtual test suites, you\n  will need to follow [these instructions](#running-a-new-flag_specific-suite-in-cq_ci).\n\n* Due to the above, virtual test suites incur a performance penalty for the\n  commit queue and the continuous build infrastructure. This is exacerbated by\n  the need to restart `content_shell` whenever flags change, which limits\n  parallelism. Therefore, you should avoid adding large numbers of virtual test\n  suites. They are well suited to running a subset of tests that are directly\n  related to the feature, but they don't scale to flags that make deep\n  architectural changes that potentially impact all of the tests.\n\n* Note that using wildcards in virtual test path names (e.g.\n  `virtual/blocking_repaint/fast/repaint/*`) is not supported in\n  `run_web_tests.py` command line , but you can still use\n  `virtual/blocking_repaint` to run all real and virtual tests\n  in the suite or `virtual/blocking_repaint/fast/repaint/dir` to run real\n  or virtual tests in the suite under a specific directory.\n\n*** note\nWe can run a virtual test with additional flags. Both the virtual args and the\nadditional flags will be applied. The fallback order of baselines and\nexpectations will be: 1) flag-specific virtual, 2) non-flag-specific virtual,\n3) flag-specific base, 4) non-flag-specific base\n***\n\n### Running a New Flag-Specific Suite in CQ/CI\n\nAssuming you have already created a `FlagSpecificConfig` entry:\n\n1. File a resource request ([internal\n   docs](https://g3doc.corp.google.com/company/teams/chrome/ops/business/resources/resource-request-program.md?cl=head&polyglot=chrome-browser#i-need-new-resources))\n   for increased capacity in the `chromium.tests` swarming pool and wait for\n   approval.\n1. Define a new dedicated\n   [Buildbot test suite](https://source.chromium.org/chromium/chromium/src/+/main:testing/buildbot/test_suites.pyl;l=1516-1583;drc=0694b605fb77c975a065a3734bdcf3bd81fd8ca4;bpv=0;bpt=0)\n   with `--flag-specific` and possibly other special configurations (e.g., fewer shards).\n1. Add the Buildbot suite to the relevant `*-blink-rel` builder's\n   composition suite first\n   ([example](https://source.chromium.org/chromium/chromium/src/+/main:testing/buildbot/test_suites.pyl;l=5779-5780;drc=0694b605fb77c975a065a3734bdcf3bd81fd8ca4;bpv=0;bpt=0)).\n1. Add the flag-specific step name to the relevant builder in\n   [`builders.json`](https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/tools/blinkpy/common/config/builders.json;l=127-129;drc=ff938aaff9566b2cc442476a51835e0b90b1c6f6;bpv=0;bpt=0).\n   `rebaseline-cl` and the WPT importer will now create baselines for that suite.\n1. Rebaseline the new suite and add any necessary suppressions under\n   `FlagExpectations/`.\n1. Enable the flag-specific suite for CQ/CI by adding the Buildbot suite to the\n   desired builder.\n   This could be an existing CQ builder like\n   [`linux-rel`](https://source.chromium.org/chromium/chromium/src/+/main:testing/buildbot/test_suites.pyl;l=5828-5829;drc=0694b605fb77c975a065a3734bdcf3bd81fd8ca4;bpv=0;bpt=0)\n   or a dedicated builder like\n   [`linux-blink-web-tests-force-accessibility-rel`](https://source.chromium.org/chromium/chromium/src/+/main:infra/config/subprojects/chromium/try/tryserver.chromium.accessibility.star;drc=adad4c6d55e69783ba1f16d30f4bc7367e2e626a;bpv=0;bpt=0), which has customized location filters.\n\n## Tracking Test Failures\n\nAll bugs, associated with web test failures must have the\n[Test-Layout](https://crbug.com/?q=label:Test-Layout) label. Depending on how\nmuch you know about the bug, assign the status accordingly:\n\n* **Unconfirmed** -- You aren't sure if this is a simple rebaseline, possible\n  duplicate of an existing bug, or a real failure\n* **Untriaged** -- Confirmed but unsure of priority or root cause.\n* **Available** -- You know the root cause of the issue.\n* **Assigned** or **Started** -- You will fix this issue.\n\nWhen creating a new web test bug, please set the following properties:\n\n* Components: a sub-component of Blink\n* OS: **All** (or whichever OS the failure is on)\n* Priority: 2 (1 if it's a crash)\n* Type: **Bug**\n* Labels: **Test-Layout**\n\nYou can also use the _Layout Test Failure_ template, which pre-sets these\nlabels for you.\n\n## Debugging Web Tests\n\nAfter the web tests run, you should get a summary of tests that pass or\nfail. If something fails unexpectedly (a new regression), you will get a\n`content_shell` window with a summary of the unexpected failures. Or you might\nhave a failing test in mind to investigate. In any case, here are some steps and\ntips for finding the problem.\n\n* Take a look at the result. Sometimes tests just need to be rebaselined (see\n  below) to account for changes introduced in your patch.\n    * Load the test into a trunk Chrome or content_shell build and look at its\n      result. (For tests in the http/ directory, start the http server first.\n      See above. Navigate to `http://localhost:8000/` and proceed from there.)\n      The best tests describe what they're looking for, but not all do, and\n      sometimes things they're not explicitly testing are still broken. Compare\n      it to Safari, Firefox, and IE if necessary to see if it's correct. If\n      you're still not sure, find the person who knows the most about it and\n      ask.\n    * Some tests only work properly in content_shell, not Chrome, because they\n      rely on extra APIs exposed there.\n    * Some tests only work properly when they're run in the web-test\n      framework, not when they're loaded into content_shell directly. The test\n      should mention that in its visible text, but not all do. So try that too.\n      See \"Running the tests\", above.\n* If you think the test is correct, confirm your suspicion by looking at the\n  diffs between the expected result and the actual one.\n    * Make sure that the diffs reported aren't important. Small differences in\n      spacing or box sizes are often unimportant, especially around fonts and\n      form controls. Differences in wording of JS error messages are also\n      usually acceptable.\n    * `third_party/blink/tools/run_web_tests.py path/to/your/test.html` produces\n      a page listing all test results. Those which fail their expectations will\n      include links to the expected result, actual result, and diff. These\n      results are saved to `$root_build_dir/layout-test-results`.\n        * Alternatively the `--results-directory=path/for/output/` option allows\n          you to specify an alternative directory for the output to be saved to.\n    * If you're still sure it's correct, rebaseline the test (see below).\n      Otherwise...\n* If you're lucky, your test is one that runs properly when you navigate to it\n  in content_shell normally. In that case, build the Debug content_shell\n  project, fire it up in your favorite debugger, and load the test file either\n  from a `file:` URL.\n    * You'll probably be starting and stopping the content_shell a lot. In VS,\n      to save navigating to the test every time, you can set the URL to your\n      test (`file:` or `http:`) as the command argument in the Debugging section of\n      the content_shell project Properties.\n    * If your test contains a JS call, DOM manipulation, or other distinctive\n      piece of code that you think is failing, search for that in the Chrome\n      solution. That's a good place to put a starting breakpoint to start\n      tracking down the issue.\n    * Otherwise, you're running in a standard message loop just like in Chrome.\n      If you have no other information, set a breakpoint on page load.\n* If your test only works in full web-test mode, or if you find it simpler to\n  debug without all the overhead of an interactive session, start the\n  content_shell with the command-line flag `--run-web-tests`, followed by the\n  URL (`file:` or `http:`) to your test. More information about running web tests\n  in content_shell can be found [here](./web_tests_in_content_shell.md).\n    * In VS, you can do this in the Debugging section of the content_shell\n      project Properties.\n    * Now you're running with exactly the same API, theme, and other setup that\n      the web tests use.\n    * Again, if your test contains a JS call, DOM manipulation, or other\n      distinctive piece of code that you think is failing, search for that in\n      the Chrome solution. That's a good place to put a starting breakpoint to\n      start tracking down the issue.\n    * If you can't find any better place to set a breakpoint, start at the\n      `TestShell::RunFileTest()` call in `content_shell_main.cc`, or at\n      `shell->LoadURL() within RunFileTest()` in `content_shell_win.cc`.\n* Debug as usual. Once you've gotten this far, the failing web test is just a\n  (hopefully) reduced test case that exposes a problem.\n\n### Debugging HTTP Tests\n\nNote: HTTP Tests mean tests under `web_tests/http/tests/`,\nwhich is a subset of WebKit Layout Tests originated suite.\nIf you want to debug WPT's HTTP behavior, you should read\n[\"Web platform tests\"](./web_platform_tests.md) instead.\n\n\nTo run the server manually to reproduce/debug a failure:\n\n```bash\nthird_party/blink/tools/run_blink_httpd.py\n```\n\nThe web tests are served from `http://127.0.0.1:8000/`. For example, to\nrun the test\n`web_tests/http/tests/serviceworker/chromium/service-worker-allowed.html`,\nnavigate to\n`http://127.0.0.1:8000/serviceworker/chromium/service-worker-allowed.html`. Some\ntests behave differently if you go to `127.0.0.1` vs. `localhost`, so use\n`127.0.0.1`.\n\nTo kill the server, hit any key on the terminal where `run_blink_httpd.py` is\nrunning, use `taskkill` or the Task Manager on Windows, or `killall` or\nActivity Monitor on macOS.\n\nThe test server sets up an alias to the `web_tests/resources` directory. For\nexample, in HTTP tests, you can access the testing framework using\n`src=\"/js-test-resources/js-test.js\"`.\n\n### Tips\n\nCheck https://test-results.appspot.com/ to see how a test did in the most recent\n~100 builds on each builder (as long as the page is being updated regularly).\n\nA timeout will often also be a text mismatch, since the wrapper script kills the\ncontent_shell before it has a chance to finish. The exception is if the test\nfinishes loading properly, but somehow hangs before it outputs the bit of text\nthat tells the wrapper it's done.\n\nWhy might a test fail (or crash, or timeout) on buildbot, but pass on your local\nmachine?\n* If the test finishes locally but is slow, more than 10 seconds or so, that\n  would be why it's called a timeout on the bot.\n* Otherwise, try running it as part of a set of tests; it's possible that a test\n  one or two (or ten) before this one is corrupting something that makes this\n  one fail.\n* If it consistently works locally, make sure your environment looks like the\n  one on the bot (look at the top of the stdio for the webkit_tests step to see\n  all the environment variables and so on).\n* If none of that helps, and you have access to the bot itself, you may have to\n  log in there and see if you can reproduce the problem manually.\n\n### Debugging DevTools Tests\n\n* Do one of the following:\n    * Option A) Run from the `chromium/src` folder:\n      `third_party/blink/tools/run_web_tests.py --additional-driver-flag='--remote-debugging-port=9222' --additional-driver-flag='--remote-allow-origins=*' --additional-driver-flag='--debug-devtools' --timeout-ms=6000000`\n    * Option B) If you need to debug an http/tests/inspector test, start httpd\n      as described above. Then, run content_shell:\n      `out/Default/content_shell --remote-debugging-port=9222 --additional-driver-flag='--remote-allow-origins=*' --additional-driver-flag='--debug-devtools' --run-web-tests http://127.0.0.1:8000/path/to/test.html`\n* Open `http://localhost:9222` in a stable/beta/canary Chrome, click the single\n  link to open the devtools with the test loaded.\n* In the loaded devtools, set any required breakpoints and execute `test()` in\n  the console to actually start the test.\n\nNOTE: If the test is an html file, this means it's a legacy test so you need to add:\n* Add `window.debugTest = true;` to your test code as follows:\n\n  ```javascript\n  window.debugTest = true;\n  function test() {\n    /* TEST CODE */\n  }\n  ```\n\n### Reproducing flaky inspector protocol tests\n\nhttps://crrev.com/c/5318502 implemented logging for inspector-protocol tests.\nWith this CL for each test in stderr you should see Chrome DevTools Protocol\nmessages that the test and the browser exchanged.\n\nYou can use this log to reproduce the failure or timeout locally.\n\n* Prepare a log file and ensure each line contains one protocol message\nin the JSON format. Strip any prefixes or non-protocol messages from the\noriginal log.\n* Make sure your local test file version matches the version that produced\nthe log file.\n* Run the test using the log file:\n\n  ```sh\n  third_party/blink/tools/run_web_tests.py -t Release \\\n   --additional-driver-flag=\"--inspector-protocol-log=/path/to/log.txt\" \\\n   http/tests/inspector-protocol/network/url-fragment.js\n  ```\n\n## Bisecting Regressions\n\nYou can use [`git bisect`](https://git-scm.com/docs/git-bisect) to find which\ncommit broke (or fixed!) a web test in a fully automated way.  Unlike\n[bisect-builds.py](http://dev.chromium.org/developers/bisect-builds-py), which\ndownloads pre-built Chromium binaries, `git bisect` operates on your local\ncheckout, so it can run tests with `content_shell`.\n\nBisecting can take several hours, but since it is fully automated you can leave\nit running overnight and view the results the next day.\n\nTo set up an automated bisect of a web test regression, create a script like\nthis:\n\n```bash\n#!/bin/bash\n\n# Exit code 125 tells git bisect to skip the revision.\ngclient sync || exit 125\nautoninja -C out/Debug -j100 blink_tests || exit 125\n\nthird_party/blink/tools/run_web_tests.py -t Debug \\\n  --no-show-results --no-retry-failures \\\n  path/to/web/test.html\n```\n\nModify the `out` directory, ninja args, and test name as appropriate, and save\nthe script in `~/checkrev.sh`.  Then run:\n\n```bash\nchmod u+x ~/checkrev.sh  # mark script as executable\ngit bisect start <badrev> <goodrev>\ngit bisect run ~/checkrev.sh\ngit bisect reset  # quit the bisect session\n```\n\n## Rebaselining Web Tests\n\nSee [How to rebaseline](./web_test_expectations.md#How-to-rebaseline).\n\n## Known Issues\n\nSee\n[bugs with the component Blink>Infra](https://bugs.chromium.org/p/chromium/issues/list?can=2&q=component%3ABlink%3EInfra)\nfor issues related to Blink tools, include the web test runner.\n\n* If QuickTime is not installed, the plugin tests\n  `fast/dom/object-embed-plugin-scripting.html` and\n  `plugins/embed-attributes-setting.html` are expected to fail.\n* Fluent scrollbar rendering has some tweaks to geometry and behavior that are\n  just for web tests. These are described in the\n  [Fluent Scrollbars Visual Spec](https://bit.ly/fluent-scrollbars-visual-spec)\n  under \"Special rendering - Web tests\". We'd like to remove them eventually\n  ([crbug.com/382298324](https://crbug.com/382298324)).\n"
  },
  {
    "path": "development/testing/web_platform_tests_addressing_flake",
    "title": "Addressing Flaky WPTs",
    "content": "# Addressing Flaky WPTs\n\nThis document provides tips and tricks for reproducing and debugging flakes in\n[Web Platform Tests](web_platform_tests.md) (WPTs). As WPTs are a form of Web\nTest, you may also wish to refer to the documentation on [Addressing Flaky Web\nTests](web_tests_addressing_flake.md).\n\n## Debugging flaky WPTs\n\nSee also the documentation in [Addressing Flaky Web\nTests](web_tests_addressing_flake.md#Debugging-flaky-Web-Tests).\n\n### Loading WPT tests directly in content\\_shell\n\nWPT tests have to be loaded from a server, `wptserve`, to run properly. In a\nterminal, run:\n\n```\n./third_party/blink/tools/run_blink_wptserve.py\n```\n\nThis will start the necessary server(s), and print the ports they are listening\non. Most tests can just be loaded over the main HTTP server (usually\n`http://localhost:8001`), although some may require using the HTTPS server\ninstead.\n\nTo load a WPT test in content\\_shell, run:\n\n```\nout/Default/content_shell http://localhost:8001/path/to/test.html\n```\n\nHere, the `path/to/test.html` is relative to the root of\n`third_party/blink/web_tests/external/wpt`, e.g. `dom/historical.html`.\n\n**Caveat**: As with all Web Tests, running `content_shell` like this is not\nequivalent to what `run_web_tests.py` runs. See the same section in [Addressing\nFlaky Web\nTests](web_tests_addressing_flake.md#Loading-the-test-directly-in-content_shell)\nfor more details and some suggestions. In addition to that list, some WPTs\n(usually security-related) also expect a real domain and may behave differently\nwhen loaded from localhost.\n"
  },
  {
    "path": "development/testing/web_platform_tests",
    "title": "web-platform-tests",
    "content": "# web-platform-tests\n\nInteroperability between browsers is\n[critical](https://www.chromium.org/blink/platform-predictability) to Chromium's\nmission of improving the web. We believe that leveraging and contributing to a\nshared test suite is one of the most important tools in achieving\ninteroperability between browsers. The [web-platform-tests\nrepository](https://github.com/web-platform-tests/wpt) is the primary shared\ntest suite where all browser engines are collaborating.\n\nChromium has a 2-way import/export process with the upstream web-platform-tests\nrepository, where tests are imported into\n[web_tests/external/wpt](../../third_party/blink/web_tests/external/wpt)\nand any changes to the imported tests are also exported to web-platform-tests.\n\nSee https://web-platform-tests.org/ for general documentation on\nweb-platform-tests, including tips for writing and reviewing tests.\n\n[TOC]\n\n## Writing tests\n\nTo contribute changes to web-platform-tests, just commit your changes directly\nto [web_tests/external/wpt](../../third_party/blink/web_tests/external/wpt)\nand the changes will be automatically upstreamed within 24 hours.\n\nChanges involving adding, removing or modifying tests can all be upstreamed.\nAny changes outside of\n[external/wpt](../../third_party/blink/web_tests/external/wpt) will not be\nupstreamed, and any changes `*-expected.txt`, `OWNERS`, and `MANIFEST.json`,\nwill also not be upstreamed.\n\nRunning the web tests will automatically regenerate MANIFEST.json to pick up\nany local modifications.\n\nMost tests are written using testharness.js, see\n[Writing Web Tests](./writing_web_tests.md) and\n[Web Tests Tips](./web_tests_tips.md) for general guidelines.\n\n### Write tests against specifications\n\nTests in web-platform-tests are expected to match behavior defined by the\nrelevant specification. In other words, all assertions that a test makes\nshould be derived from a specification's normative requirements, and not go\nbeyond them. It is often necessary to change the specification to clarify what\nis and isn't required.\n\nWhen implementation experience is needed to inform the specification work,\n[tentative tests](https://web-platform-tests.org/writing-tests/file-names.html)\ncan be appropriate. It should be apparent in context why the test is tentative\nand what needs to be resolved to make it non-tentative.\n\n### Tests that require testing APIs\n\n#### `testdriver.js`\n\n[testdriver.js](https://web-platform-tests.org/writing-tests/testdriver.html)\nprovides a means to automate tests that cannot be written purely using web\nplatform APIs, similar to `internals.*` and `eventSender.*` in regular Blink\nweb tests.\n\nIf no testdriver.js API exists, check if it's a\n[known issue](https://github.com/web-platform-tests/wpt/labels/testdriver.js)\nand otherwise consider filing a new issue. For instructions on how to add a new\ntesting API, see [WPT Test Automation for\nChromium](https://docs.google.com/document/d/18BpD41vyX1cFZ77CE0a_DJYlGpdvyLlx3pwXVRxUzvI/preview#)\n\n#### MojoJS\n\nSome specs may define testing APIs (e.g.\n[WebUSB](https://wicg.github.io/webusb/test/)), which may be polyfilled with\ninternal API like [MojoJS](../../mojo/public/js/README.md).  MojoJS is only\nallowed in WPT for this purpose. Please reach out to\nblink-dev@chromium.org before following the process below for adding a new\ntest-only API:\n\n 1. Create a full list of `*.mojom.m.js` files that you need, including all\n    dependencies. Generated modules load dependencies recursively by default,\n    so you can check the network panel of DevTools to see the full list of\n    dependencies it loads.\n 2. Check [linux-archive-rel.json](../../infra/archive_config/linux-archive-rel.json) and add any\n    missing `*.mojom.m.js` files to the `mojojs.zip` archive. Globs are\n    supported in `filename`. Do not copy Mojom bindings into WPT.\n 3. Meanwhile in Chromium, you can create a helper for your WPT tests to do\n    browser-specific setup using\n    [test-only-api.js](../../third_party/blink/web_tests/external/wpt/resources/test-only-api.js).\n    See\n    [webxr_util.js](../../third_party/blink/web_tests/external/wpt/webxr/resources/webxr_util.js)\n    as an example. You can write tests using this helper right away, but they\n    will not work upstream (i.e. on https://wpt.fyi ) until your change in step\n    2 is included in official channels, as `mojojs.zip` is built alongside with\n    Chrome.\n\n#### `wpt_automation`\n\nAn alternative to the above options is to write manual tests that are automated\nwith scripts from\n[wpt_automation](../../third_party/blink/web_tests/external/wpt_automation).\nInjection of JS in manual tests is determined by `loadAutomationScript` in\n[testharnessreport.js](../../third_party/blink/web_tests/resources/testharnessreport.js).\n\nSuch tests still require case-by-case automation to run for other browser\nengines, but are more valuable than purely manual tests.\n\nManual tests that have no automation are still imported, but skipped in\n[NeverFixTests](../../third_party/blink/web_tests/NeverFixTests); see\n[issue 738489](https://crbug.com/738489).\n\n### Contribution process\n\nChanges made in\n[web_tests/external/wpt](../../third_party/blink/web_tests/external/wpt) are\n[automatically exported to GitHub](#exporting-tests).\n\nIt's still possible to make direct pull requests to web-platform-tests, see\nhttps://web-platform-tests.org/writing-tests/github-intro.html.\n\n### Adding new top-level directories\n\nEntirely new top-level directories should generally be added upstream, since\nthat's the only way to add an OWNERS file upstream. After adding a new top-level\ndirectory upstream, you should add a line for it in `W3CImportExpectations`.\n\nAdding the new directory (and `W3CImportExpectations` entry) in Chromium and\nlater adding an OWNERS file upstream also works.\n\n### `wpt_internal`\n\nIt is sometimes desirable to write WPT tests that either test Chromium-specific\nbehaviors, or that cannot yet be upstreamed to WPT (e.g. because the spec is\nvery nascent). For these cases, we maintain a separate directory,\n[wpt_internal](../../third_party/blink/web_tests/wpt_internal) that runs under the\nWPT testing infrastructure (e.g. uses wptserve, etc), but which is not\nupstreamed to WPT.\n\nPlease see the `wpt_internal`\n[README](../../third_party/blink/web_tests/wpt_internal/README.md) for more details.\n\n**Note**: A significant downside of `wpt_internal` is that your tests may be\nbroken by upstream changes to the resources scripts (e.g. `testharness.js`), as\n`wpt_internal` does not use the forked version of `testharness.js` used by all\nother non-`external/wpt` tests. Use of [new failure\nnotifications](#new-failure-notifications) is recommended to ensure you are\nnotified of breakages.\n\n## Running tests\n\nSame as Blink web tests, you can use\n[`run_web_tests.py`](web_tests.md#running-the-tests) to run any WPT test. This\nwill run WPT tests in Content Shell. You can also run [`run_wpt_tests.py`](run_web_platform_tests.md) to\nrun WPT tests with Chrome.\n\nOne thing to note is that glob patterns for WPT tests are not yet supported.\n\nSee [Running WPT tests in Content Shell](web_tests_in_content_shell.md#Running-WPT-Tests-in-Content-Shell)\nfor debugging etc.\n\n## Reviewing tests\n\nAnyone who can review code and tests in Chromium can also review changes in\n[external/wpt](../../third_party/blink/web_tests/external/wpt)\nthat will be automatically upstreamed. There will be no additional review in\nweb-platform-tests as part of the export process.\n\nIf upstream reviewers have feedback on the changes, discuss on the pull request\ncreated during export, and if necessary work on a new pull request to iterate\nuntil everyone is satisfied.\n\nWhen reviewing tests, check that they match the relevant specification, which\nmay not fully match the implementation. See also\n[Write tests against specifications](#Write-tests-against-specifications).\n\n## Importing tests\n\nChromium has a [mirror](https://chromium.googlesource.com/external/w3c/web-platform-tests/)\nof the GitHub repo and periodically imports a subset of the tests to\nrun as part of the regular Blink web test testing process.\n\nThe goals of this process are to be able to run web-platform-tests unmodified\nlocally just as easily as we can run the Blink tests, and ensure that we are\ntracking tip-of-tree in the web-platform-tests repository as closely as\npossible, and running as many of the tests as possible.\n\n### Automatic import process\n\nThere is an automatic process for updating the Chromium copy of\nweb-platform-tests. The import is done by the builder [wpt-importer\nbuilder][wpt-importer].\n\nThe easiest way to check the status of recent imports is to look at:\n\n-   Recent logs on LUCI for [wpt-importer builder][wpt-importer]\n-   Recent CLs created by [WPT\n    Autoroller](https://chromium-review.googlesource.com/q/owner:wpt-autoroller%2540chops-service-accounts.iam.gserviceaccount.com).\n\nThe import jobs will generally be green if either there was nothing to do,\nor a CL was successfully submitted.\n\nFor maintainers:\n\n-   The source lives in\n    [third_party/blink/tools/wpt_import.py](../../third_party/blink/tools/wpt_import.py).\n-   If the importer starts misbehaving, it can be disabled by landing a\n    [CL to skip the update step](https://crrev.com/c/1961906/).\n\n### New failure notifications\n\nThe importer automatically file bugs against a component when imported changes\nintroduce failures as long as test owners did not choose to opt-out the failure\nnotification mechanism. This includes new tests that fail in Chromium, as well\nas new failures introduced to an existing test. Test owners are encouraged to\ncreate an `DIR_METADATA` file in the appropriate `external/wpt/` subdirectory\nthat contains at least the `buganizer_public.component_id` field, which the\nimporter will use to file bugs.\nFor example, `external/wpt/css/css-grid/DIR_METADATA` looks like:\n\n```\nbuganizer_public {\n  component_id: 1415957\n}\nteam_email: \"layout-dev@chromium.org\"\n```\n\nWhen tests under `external/wpt/css/css-grid/` newly fail in a WPT import, the\nimporter will automatically file a bug against the `Chromium>Blink>Layout>Grid`\ncomponent in [issues.chromium.org](https://issues.chromium.org/issues), with\ndetails of which tests failed and the outputs.\nThe importer will also copy `layout-dev@chromium.org` (the `team_email`) and any\n`external/wpt/css/css-grid/OWNERS` on the bug.\n\nFailing tests are grouped according to the most specific `DIR_METADATA` that\nthey roll up to.\n\nTo opt-out of this notification, add `wpt.notify` field set to `NO` to the\ncorresponding `DIR_METADATA`.\nFor example, the following `DIR_METADATA` will suppress notification from tests\nunder the located directory:\n\n```\nbuganizer_public {\n  component_id: 1415957\n}\nteam_email: \"layout-dev@chromium.org\"\nwpt {\n  notify: NO\n}\n```\n\n### Skipped tests (and how to re-enable them)\n\nWe control which tests are imported via a file called\n[W3CImportExpectations](../../third_party/blink/web_tests/W3CImportExpectations),\nwhich has a list of directories to skip while importing.\n\nIn addition to the directories and tests explicitly skipped there, tests may\nalso be skipped for a couple other reasons, e.g. if the file path is too long\nfor Windows. To check what files are skipped in import, check the recent logs\nfor [wpt-importer builder][wpt-importer].\n\nIf you wish to un-skip some of the directories currently skipped in\n`W3CImportExpectations`, you can modify that file locally and commit it, and on\nthe next auto-import, the new tests should be imported.\n\nIf you want to import immediately (in order to try the tests out locally, etc)\nyou can also run `wpt-import`, but this is not required.\n\nRemember your import might fail due to GitHub's limit for unauthenticated\nrequests, so consider [passing your GitHub credentials](#GitHub-credentials) to\nthe script.\n\n### Waterfall failures caused by automatic imports.\n\nIf there are new test failures that start after an auto-import,\nthere are several possible causes, including:\n\n 1. New baselines for flaky tests were added (https://crbug.com/701234).\n 2. Modified tests should have new results for non-Release builds but they weren't added (https://crbug.com/725160).\n 3. New baselines were added for tests with non-deterministic test results (https://crbug.com/705125).\n\nBecause these tests are imported from the Web Platform tests, it is better\nto have them in the repository (and marked failing) than not, so prefer to\n[add test expectations](web_test_expectations.md) rather than reverting.\nHowever, if a huge number of tests are failing, please revert the CL so we\ncan fix it manually.\n\n[wpt-importer]: https://ci.chromium.org/p/infra/builders/luci.infra.cron/wpt-importer\n\n## Exporting tests\n\nIf you upload a CL with any changes in\n[third_party/blink/web_tests/external/wpt](../../third_party/blink/web_tests/external/wpt),\nonce your CL is ready to submit the exporter will create a provisional pull request with\nthose changes in the [upstream WPT GitHub repository](https://github.com/web-platform-tests/wpt/).\nThe exporter runs on [wpt-exporter builder][wpt-exporter].\n\nOnce you're ready to land your CL, please follow the link posted by the bot and\ncheck the status of the required checks of the GitHub PR. If it's green, go\nahead landing your CL and the exporter will automatically merge the PR.\n\nIf GitHub status is red on the PR, please try to resolve the failures before\nmerging. If you run into any issues, or if you have a CL with WPT changes that\nthe exporter did not pick up, please reach out to blink-dev@chromium.org.\n\nAdditional things to note:\n\n-   CLs that change over 1000 files will not be exported.\n-   All PRs use the\n    [`chromium-export`](https://github.com/web-platform-tests/wpt/pulls?utf8=%E2%9C%93&q=is%3Apr%20label%3Achromium-export) label.\n-   All PRs for CLs that haven't yet been landed in Chromium also use the\n    [`do not merge yet`](https://github.com/web-platform-tests/wpt/pulls?q=is%3Apr+is%3Aopen+label%3A%22do+not+merge+yet%22) label.\n-   The exporter cannot create upstream PRs for in-flight CLs with binary files\n    (e.g. webm files). An export PR will still be made after the CL lands.\n\n### Will the exported commits be linked to my GitHub profile?\n\nThe email you commit with in Chromium will be the author of the commit on\nGitHub. You can [add it as a secondary address on your GitHub\naccount](https://help.github.com/articles/adding-an-email-address-to-your-github-account/)\nto link your exported commits to your GitHub profile.\n\nIf you are a Googler, you can also register your GitHub account at go/github,\nmaking it easier for other Googlers to find you.\n\n### What if there are conflicts?\n\nThis cannot be avoided entirely as the two repositories are independent, but\nshould be rare with frequent imports and exports. When it does happen, manual\nintervention will be needed and in non-trivial cases you may be asked to help\nresolve the conflict.\n\n[wpt-exporter]: https://ci.chromium.org/p/infra/builders/luci.infra.cron/wpt-exporter\n\n## Notes for WPT infra maintainers\n\n### Importer\n\n#### Rubber-Stamper bot\n\nTo allow the importer to land CLs without human intervention, it utilizes the\n[Rubber-Stamper\nbot](https://chromium.googlesource.com/infra/infra/+/refs/heads/main/go/src/infra/appengine/rubber-stamper/README.md)\nto approve import CLs.\n\nAdding the Rubber-Stamper as a reviewer is one of the last steps the importer\ntakes, once tests have been rebaselined and the CQ passes. If the Rubber-Stamper\ncannot approve a CL, it will leave a comment on the CL explaining why - this\nwill also cause the importer to go red.\n\n![Rubber-Stamber bot rejecting a CL](images/wpt_import_rubber_stamper_reject.png)\n\nThere are two possibilities when the Rubber-Stamper rejects an import: either it\nis a valid rejection, because the import changes code files (`.py`, `.bat`,\n`.sh`), or it is invalid and we're missing an allowlist rule for a file the\nimporter is allowed to modify.\n\nFor valid rejections, it is the job of the rotation sheriff to land the CL\nmanually. You need to un-abandon the import, `CR+1` it yourself, and `CQ+2` it.\nIf you don't have permission to do that (e.g. are not a committer), contact\nblink-dev@chromium.org.\n\nFor invalid rejections, message blink-dev@chromium.org or add an exception\nrule yourself. [This is an example\nCL](https://chrome-internal-review.googlesource.com/c/infradata/config/+/3608170)\nthat adds an exception rule. (Note that you need internal access to access this\nrepository).\n\n#### Manual import\n\nTo pull the latest versions of the tests that are currently being imported, you\ncan also directly invoke the\n[wpt-import](../../third_party/blink/tools/wpt_import.py) script.\n\nThat script will pull the latest version of the tests from our mirrors of the\nupstream repositories. If any new versions of tests are found, they will be\ncommitted locally to your local repository. You may then upload the changes.\n\nRemember your import might fail due to GitHub's limit for unauthenticated\nrequests, so consider [passing your GitHub credentials](#GitHub-credentials) to\nthe script.\n\n### Exporter\n\n-   The source lives in\n    [third_party/blink/tools/wpt_export.py](../../third_party/blink/tools/wpt_export.py).\n-   If the exporter starts misbehaving (for example, creating the same PR over\n    and over again), put it in \"dry run\" mode by landing [this\n    CL](https://crrev.com/c/462381/).\n\n### GitHub credentials\n\nWhen manually running the `wpt-import` and `wpt-export` scripts, several\nrequests are made to GitHub to query the status of pull requests, look for\nexisting exported commits etc. GitHub has a [fairly\nlow](https://developer.github.com/v3/#rate-limiting) request limit for\nunauthenticated requests, so it is recommended that you let `wpt-export` and\n`wpt-import` use your GitHub credentials when sending requests:\n\n 1. Generate a new [personal access token](https://github.com/settings/tokens)\n 1. Set up your credentials by either:\n     * Setting the `GH_USER` environment variable to your GitHub user name\n       and the `GH_TOKEN` environment variable to the access token you have\n       just created **or**\n     * Creating a JSON file with two keys: `GH_USER`, your GitHub user name,\n       and `GH_TOKEN`, the access token you have just generated. After that,\n       pass `--credentials-json <path-to-json>` to `wpt-export` and\n       `wpt-import`.\n\n### Debugging failed web platform tests\n\nThis section explains the way to debug web platform tests.\nPlease build `blink_tests` before running commands below.\nIt is explained in [Running Web Tests](./web_tests.md#running-web-tests).\n\n#### Running test(s)\n\nThe way to run web tests is explained in [Running the\nTests](./web_tests.md#running-the-tests).\n\nAssume that you are writing the test named `wpt_internal/fake/foobar.html`.\nYou may want to run only the tests and you do not want to run all tests under\n`wpt_internal/fake`.  The following command narrows down the test to only\n`wpt_internal/fake/foobar.html`.\n\n```bash\nthird_party/blink/tools/run_web_tests.py -t Default \\\nthird_party/blink/web_tests/wpt_internal/fake/foobar.html\n```\n\n#### Logging\n\nDuring the debug, you may want to log what happens during the test.\nYou can use `console.log` in JavaScript to log arbitrary strings.\n\n```\ne.g.\nconsole.log('fake has been executed.');\nconsole.log('foo=' + foo);\n```\n\nLogs are written under `$root_build_dir/layout-test-results`.\nIf you have tested `wpt_internal/fake/foobar.html`, the log will be stored in\n`$root_build_dir/layout-test-results/wpt_internal/fake/foobar-stderr.txt`.\nYou can change output directory with `--results-directory=<output directory>`.\n\n#### Checking HTTP servers\n\nFor some test cases, you may use .headers file to set arbitrary HTTP headers.\nTo verify what is set to headers, you can run an HTTP server used for WPT\nby yourself. The following command starts the HTTP server for you:\n\n```bash\nthird_party/blink/tools/run_blink_wptserve.py\n```\n\nTo see headers returned by the server, you can use `curl -v`.\n`curl` will show headers in stderr. You may want to use `|& less` to\nsee output if it is too long.\n\n```bash\ncurl -v http://localhost:8081/wpt_internal/fake/foobar.html |& less\n```\n\n#### Debugging with a debugger\n\nYou are able to debug the inside of Chromium with a debugger for particular\nWPT tests. Refer to [Running web tests using the content shell](https://chromium.googlesource.com/chromium/src/+/refs/heads/main/docs/testing/web_tests_in_content_shell.md)\nfor details.\n"
  },
  {
    "path": "development/testing/using_crashpad_with_content_shell",
    "title": "Using crashpad with content shell",
    "content": "# Using crashpad with content shell\n\nWhen running web tests, it is possible to use\n[crashpad](../third_party/crashpad/)/[breakpad](../../third_party/breakpad/) to\ncapture stack traces on crashes while running without a debugger attached and\nwith the sandbox enabled.\n\n## Setup\n\nOn all platforms, build the target `blink_tests`.\n\n*** note\n**Mac:** Add `enable_dsyms = 1` to your [gn build\narguments](https://gn.googlesource.com/gn/+/main/docs/quick_start.md) before\nbuilding. This slows down linking several minutes, so don't just always set it\nby default.\n***\n\nThen, create a directory where the crash dumps will be stored:\n\n* Linux/Mac:\n  ```bash\n  mkdir /tmp/crashes\n  ```\n* Android:\n  ```bash\n  adb root\n  adb shell mkdir /data/data/org.chromium.content_shell_apk/cache\n  ```\n* Windows:\n  ```bash\n  mkdir %TEMP%\\crashes\n  ```\n\n## Running content shell with crashpad\n\nCrashpad can be enabled by passing `--enable-crash-reporter` and\n`--crash-dumps-dir` to content shell:\n\n* Linux:\n  ```bash\n  out/Debug/content_shell --enable-crash-reporter \\\n      --crash-dumps-dir=/tmp/crashes chrome://crash\n  ```\n* Mac:\n  ```bash\n  out/Debug/Content\\ Shell.app/Contents/MacOS/Content\\ Shell \\\n      --enable-crash-reporter --crash-dumps-dir=/tmp/crashes chrome://crash\n  ```\n* Windows:\n  ```bash\n  out\\Default\\content_shell.exe --enable-crash-reporter ^\n      --crash-dumps-dir=%TEMP%\\crashes chrome://crash\n  ```\n* Android:\n  ```bash\n  out/Default/bin/content_shell_apk install\n  out/Default/bin/content_shell_apk launch chrome://crash\n  --args=\"--enable-crash-reporter --crash-dumps-dir=/data/data/org.chromium.content_shell_apk/cache\"\n  ```\n\n## Retrieving the crash dump\n\nOn Android, we first have to retrieve the crash dump. On other platforms, this\nstep can be skipped.\n\n* Android:\n  ```bash\n  adb pull $(adb shell ls /data/data/org.chromium.content_shell_apk/cache/pending/*.dmp) /tmp/chromium-renderer-minidump.dmp\n  ```\n\n## Symbolizing the crash dump\n\nOn all platforms except for Windows, we need to convert the debug symbols to a\nformat that breakpad can understand.\n\n* Linux:\n  ```bash\n  components/crash/content/tools/generate_breakpad_symbols.py \\\n      --build-dir=out/Default --binary=out/Default/content_shell \\\n      --symbols-dir=out/Default/content_shell.breakpad.syms --clear --jobs=16\n  ```\n* Mac:\n  ```bash\n  components/crash/content/tools/generate_breakpad_symbols.py \\\n      --build-dir=out/Default \\\n      --binary=out/Default/Content\\ Shell.app/Contents/MacOS/Content\\ Shell \\\n      --symbols-dir=out/Default/content_shell.breakpad.syms --clear --jobs=16\n  ```\n* Android:\n  ```bash\n  components/crash/content/tools/generate_breakpad_symbols.py \\\n      --build-dir=out/Default \\\n      --binary=out/Default/lib/libcontent_shell_content_view.so \\\n      --symbols-dir=out/Default/content_shell.breakpad.syms --clear \\\n      --platform=android\n  ```\n\nNow we can generate a stack trace from the crash dump. Assuming the crash dump\nis in minidump.dmp:\n\n* Linux/Android/Mac:\n  ```bash\n  out/Default/minidump_stackwalk minidump.dmp out/Debug/content_shell.breakpad.syms\n  ```\n* Windows:\n  ```bash\n  \"c:\\Program Files (x86)\\Windows Kits\\8.0\\Debuggers\\x64\\cdb.exe\" ^\n      -y out\\Default -c \".ecxr;k30;q\" -z minidump.dmp\n  ```\n"
  },
  {
    "path": "development/testing/test_wrapper_api",
    "title": "Test Wrapper API",
    "content": "# Test Wrapper API\n\nIn order to simplify the calling conventions for tests that we run on our\ncontinuous integration system, we require them to follow a simple API\nconvention. For a given GN label //X:Y and build directory //Z in a checkout\n(e.g., //url:url_unittests and //out/Release), we expect there to be:\n\n    * A file `$Z/$Y.runtime_deps` containing a list of files needed to run\n      the test (in the format produced by `gn desc //Z //X:Y runtime_deps`,\n      which is a newline-separated list of paths relative to Z)\n    * An executable file `$Z/bin/run_$Y` which does everything needed to set\n      up and run the test with all of the appropriate flags. This will usually\n      be a vpython script.\n    * (on Windows) A file `$Z/bin/run_$Y.bat` file that will turn around\n      and invoke the corresponding run_$ vpython script.\n\nIf you create a directory snapshot with the files listed in the .runtime_deps\nfile, cd to $Z, and run bin/run_$Y, then the test should run to completion\nsuccessfully.\n\nThe script file MUST honor the `GTEST_SHARD_INDEX` and `GTEST_TOTAL_SHARDS`\nenvironment variables as documented in\n[the Test Executable API](test_executable_api.md) and SHOULD conform to\nthe Test Executable API in other respects (i.e., honor the\n`--isolated-script-test-filter` arg and other command line flags specified\nin that API).\n\nTODO(crbug.com/40564748): Convert everything to the Test Executable API, and\nchange the above SHOULD to a MUST.\n"
  },
  {
    "path": "development/testing/test_executable_api",
    "title": "The Chromium Test Executable API",
    "content": "# The Chromium Test Executable API\n\n[bit.ly/chromium-test-runner-api][1] (*)\n\n\n[TOC]\n\n## Introduction\n\nThis document defines the API that test executables must implement in order to\nbe run on the Chromium continuous integration infrastructure (the\n[LUCI][2]\nsystem using the `chromium` and `chromium_trybot` recipes).\n\n*** note\n**NOTE:** This document specifies the existing `isolated_scripts` API in the\nChromium recipe. Currently we also support other APIs (e.g., for\nGTests), but we should migrate them to use the `isolated_scripts` API.\nThat work is not currently scheduled.\n***\n\nThis spec applies only to functional tests and does not attempt to\nspecify how performance tests should work, though in principle they\ncould probably work the same way and possibly just produce different\noutput.\n\nThis document is specifically targeted at Chromium and assumes you are\nusing GN and Ninja for your build system. It should be possible to adapt\nthese APIs to other projects and build recipes, but this is not an\nimmediate goal. Similarly, if a project adapts this API and the related\nspecifications it should be able to reuse the functionality and tooling\nwe've built out for Chromium's CI system more easily in other LUCI\ndeployments.\n\n***\n**NOTE:** It bears repeating that this describes the current state of\naffairs, and not the desired end state. A companion doc,\n[Cleaning up the Chromium Testing Environment][3],\ndiscusses a possible path forward and end state.\n***\n\n## Building and Invoking a Test Executable\n\nThere are lots of different kinds of tests, but we want to be able to\nbuild and invoke them uniformly, regardless of how they are implemented.\n\nWe will call the thing being executed to run the tests a _test\nexecutable_ (or executable for short). This is not an ideal name, as\nthis doesn't necessarily refer to a GN executable target type; it may be\na wrapper script that invokes other binaries or scripts to run the\ntests.\n\nWe expect the test executable to run one or more tests. A _test_ must be\nan atomically addressable thing with a name that is unique to that\ninvocation of the executable, i.e., we expect that we can pass a list of\ntest names to the test executable and only run just those tests. Test\nnames must not contain a \"::\" (which is used as a separator between test\nnames) and must not contain a \"*\" (which could be confused with a glob\ncharacter) or start with a \"-\" (which would be confused with an\nindicator that you should skip the test). Test names should generally\nonly contain ASCII code points, as the infrastructure does not currently\nguarantee that non-ASCII code points will work correctly everywhere. We\ndo not specify test naming conventions beyond these requirements, and it\nis fully permissible for a test to contain multiple assertions which may\npass or fail; this design does not specify a way to interpret or handle\nthose \"sub-atomic\" assertions; their existence is opaque to this design.\nIn particular, this spec does not provide a particular way to identify\nand handle parameterized tests, or to do anything with test suites\nbeyond a supporting a limited form of globbing for specifying sets of\ntest names.\n\nTo configure a new test, you need to modify one to three files:\n\n*   The test must be listed in one or more test suites in\n    [//testing/buildbot/test_suites.pyl][4].  Most commonly the test will be\n    defined as a single string (e.g., \"base_unittests\"), which keys into an\n    entry in [//testing/buildbot/gn_isolate_map.pyl][5].  In some cases, tests\n    will reference a target and add additional command line arguments. These\n    entries (along with [//testing/buildbot/test_suite_exceptions.pyl][6] and\n    [//testing/buildbot/waterfalls.pyl][7]) determine where the tests will be\n    run. For more information on how these files work, see\n    [//testing/buildbot/README.md][8]\n*   Tests entries must ultimately reference an entry in\n    //testing/buildbot/gn_isolate_map.pyl. This file contains the mapping of\n    ninja compile targets to GN targets (specifying the GN label for the\n    latter); we need this mapping in order to be able to run `gn analyze`\n    against a patch to see which targets are affected by a patch. This file\n    also tells MB what kind of test an entry is (so we can form the correct\n    command line) and may specify additional command line flags. If you are\n    creating a test that is only a variant of an existing test, this may be the\n    only file you need to modify. (Technically, you could define a new test\n    solely in test_suites.pyl and reference existing gn_isolate_map.pyl\n    entries, but this is considered bad practice).\n*   Add the GN target itself to the appropriate build files. Make sure this GN\n    target contains all of the data and data_deps entries needed to ensure the\n    test isolate has all the files the test needs to run.  If your test doesn't\n    depend on new build targets or add additional data file dependencies, you\n    likely don't need this. However, this is increasingly uncommon.\n\n### Command Line Arguments\n\nThe executable must support the following command line arguments (aka flags):\n\n```\n--isolated-outdir=[PATH]\n```\n\nThis argument is required, and should be set to the directory created\nby the swarming task for the task to write outputs into.\n\n```\n--out-dir=[PATH]\n```\n\nThis argument mirrors `--isolated-outdir`, but may appear in addition to\nit depending on the bot configuration (e.g. IOS bots that specify the\n`out_dir_arg` mixin in //testing/buildbot/waterfalls.pyl). It only needs\nto be handled in these cases.\n\n```\n--isolated-script-test-output=[FILENAME]\n```\n\nThis argument is optional. If this argument is provided, the executable\nmust write the results of the test run in the [JSON Test\nResults Format](json_test_results_format.md) into\nthat file. If this argument is not given to the executable, the\nexecutable must not write the output anywhere. The executable should\nonly write a valid version of the file, and generally should only do\nthis at the end of the test run. This means that if the run is\ninterrupted, you may not get the results of what did run, but that is\nacceptable.\n\n```\n--isolated-script-test-filter=[STRING]\n```\n\nThis argument is optional. If this argument is provided, it must be a\ndouble-colon-separated list of strings, where each string either\nuniquely identifies a full test name or is a prefix plus a \"*\" on the\nend (to form a glob). The executable must run only the test matching\nthose names or globs. \"*\" is _only_ supported at the end, i.e., 'Foo.*'\nis legal, but '*.bar' is not. If the string has a \"-\" at the front, the\ntest (or glob of tests) must be skipped, not run. This matches how test\nnames are specified in the simple form of the [Chromium Test List\nFormat][9]. We use the double\ncolon as a separator because most other common punctuation characters\ncan occur in test names (some test suites use URLs as test names, for\nexample). This argument may be provided multiple times; how to treat\nmultiple occurrences (and how this arg interacts with\n--isolated-script-test-filter-file) is described below.\n\n```\n--isolated-script-test-filter-file=[FILENAME]\n```\n\nIf provided, the executable must read the given filename to determine\nwhich tests to run and what to expect their results to be. The file must\nbe in the [Chromium Test List Format][9] (either the simple or\ntagged formats are fine). This argument may be provided multiple times;\nhow to treat multiple occurrences (and how this arg interacts with\n`--isolated-script-test-filter`) is described below.\n\n```\n--isolated-script-test-launcher-retry-limit=N\n```\n\nBy default, tests are run only once if they succeed. If they fail, we\nwill retry the test up to N times (so, for N+1 total invocations of the\ntest) looking for a success (and stop retrying once the test has\nsucceed). By default, the value of N is 3. To turn off retries, pass\n`--isolated-script-test-launcher-retry-limit=0`. If this flag is provided,\nit is an error to also pass `--isolated-script-test-repeat` (since -repeat\nspecifies an explicit number of times to run the test, it makes no sense\nto also pass --retry-limit).\n\n```\n--isolated-script-test-repeat=N\n```\n\nIf provided, the executable must run a given test N times (total),\nregardless of whether the test passes or fails. By default, tests are\nonly run once (N=1) if the test matches an expected result or passes,\notherwise it may be retried until it succeeds, as governed by\n`--isolated-script-test-launcher-retry-limit`, above. If this flag is\nprovided, it is an error to also pass\n`--isolated-script-test-launcher-retry-limit` (since -repeat specifies an\nexplicit number of times to run the test, it makes no sense to also pass\n-retry-limit).\n\n```\n--xcode-build-version [VERSION]\n```\n\nThis flag is passed to scripts on IOS bots only, due to the `xcode_14_main`\nmixin in //testing/builtbot/waterfalls.pyl.\n\n```\n--xctest\n```\n\nThis flag is passed to scripts on IOS bots only, due to the `xctest`\nmixin in //testing/builtbot/waterfalls.pyl.\n\nIf \"`--`\" is passed as an argument:\n\n*   If the executable is a wrapper that invokes another underlying\n    executable, then the wrapper must handle arguments passed before the\n    \"--\" on the command line (and must error out if it doesn't know how\n    to do that), and must pass through any arguments following the \"--\"\n    unmodified to the underlying executable (and otherwise ignore them\n    rather than erroring out if it doesn't know how to interpret them).\n*   If the executable is not a wrapper, but rather invokes the tests\n    directly, it should handle all of the arguments and otherwise ignore\n    the \"--\". The executable should error out if it gets arguments it\n    can't handle, but it is not required to do so.\n\nIf \"--\" is not passed, the executable should error out if it gets\narguments it doesn't know how to handle, but it is not required to do\nso.\n\nIf the test executable produces artifacts, they should be written to the\nlocation specified by the dirname of the `--isolated-script-test-output`\nargument). If the `--isolated-script-test-output-argument` is not\nspecified, the executable should store the tests somewhere under the\nroot_build_dir, but there is no standard for how to do this currently\n(most tests do not produce artifacts).\n\nThe flag names are purposely chosen to be long in order to not conflict\nwith other flags the executable might support.\n\n### Environment variables\n\nThe executable must check for and honor the following environment variables:\n\n```\nGTEST_SHARD_INDEX=[N]\n```\n\nThis environment variable is optional, but if it is provided, it\npartially determines (along with `GTEST_TOTAL_SHARDS`) which fixed\nsubset of tests (or \"shard\") to run. `GTEST_TOTAL_SHARDS` must also be\nset, and `GTEST_SHARD_INDEX` must be set to an integer between 0 and\n`GTEST_TOTAL_SHARDS`. Determining which tests to run is described\nbelow.\n\n```\nGTEST_TOTAL_SHARDS=[N]\n```\n\nThis environment variable is optional, but if it is provided, it\npartially determines (along with `GTEST_TOTAL_SHARDS`) which fixed subset\nof tests (or \"shard\") to run. It must be set to a non-zero integer.\nDetermining which tests to run is described below.\n\n### Exit codes (aka return codes or return values)\n\nThe executable must return 0 for a completely successful run, and a\nnon-zero result if something failed. The following codes are recommended\n(2 and 130 coming from UNIX conventions):\n\n| Value    | Meaning |\n|--------- | ------- |\n| 0 (zero) | The executable ran to completion and all tests either ran as expected or passed unexpectedly.          |\n| 1        | The executable ran to completion but some tests produced unexpectedly failing results.                 |\n| 2        | The executable failed to start, most likely due to unrecognized or unsupported command line arguments. |\n| 130      | The executable run was aborted the user (or caller) in a semi-orderly manner (aka SIGKILL or Ctrl-C).  |\n\n## Filtering which tests to run\n\nBy default, the executable must run every test it knows about. However,\nas noted above, the `--isolated-script-test-filter` and\n`--isolated-script-test-filter-file` flags can be used to customize which\ntests to run. Either or both flags may be used, and either may be\nspecified multiple times.\n\nThe interaction is as follows:\n\n*   A test should be run only if it would be run when **every** flag is\n    evaluated individually.\n*   A test should be skipped if it would be skipped if **any** flag was\n    evaluated individually.\n\nIf multiple filters in a flag match a given test name, the longest match\ntakes priority (longest match wins). I.e.,. if you had\n`--isolated-script-test-filter='a*::-ab*'`, then `ace.html` would run but\n`abd.html` would not. The order of the filters should not matter. It is\nan error to have multiple expressions of the same length that conflict\n(e.g., `a*::-a*`).\n\nExamples are given below.\n\nIt may not be obvious why we need to support these flags being used multiple\ntimes, or together. There are two main sets of reasons:\n*   First, you may want to use multiple -filter-file arguments to specify\n    multiple sets of test expectations (e.g., the base test expectations and\n    then MSAN-specific expectations), or to specify expectations in one file\n    and list which tests to run in a separate file.\n*   Second, the way the Chromium recipes work, in order to retry a test step to\n    confirm test failures, the recipe doesn't want to have to parse the\n    existing command line, it just wants to append\n    --isolated-script-test-filter and list the\n    tests that fail, and this can cause the --isolated-script-test-filter\n    argument to be listed multiple times (or in conjunction with\n    --isolated-script-test-filter-file).\n\nYou cannot practically use these mechanisms to run equally sized subsets of the\ntests, so if you want to do the latter, use `GTEST_SHARD_INDEX` and\n`GTEST_TOTAL_SHARDS` instead, as described in the next section.\n\n## Running equally-sized subsets of tests (shards)\n\nIf the `GTEST_SHARD_INDEX` and `GTEST_TOTAL_SHARDS` environment variables are\nset, `GTEST_TOTAL_SHARDS` must be set to a non-zero integer N, and\n`GTEST_SHARD_INDEX` must be set to an integer M between 0 and N-1. Given those\ntwo values, the executable must run only every N<sup>th</sup> test starting at\ntest number M (i.e., every i<sup>th</sup> test where (i mod N) == M).  dd\n\nThis mechanism produces roughly equally-sized sets of tests that will hopefully\ntake roughly equal times to execute, but cannot guarantee the latter property\nto any degree of precision. If you need them to be as close to the same\nduration as possible, you will need a more complicated process. For example,\nyou could run all of the tests once to determine their individual running\ntimes, and then build up lists of tests based on that, or do something even\nmore complicated based on multiple test runs to smooth over variance in test\nexecution times. Chromium does not currently attempt to do this for functional\ntests, but we do something similar for performance tests in order to better\nachieve equal running times and device affinity for consistent results.\n\nYou cannot practically use the sharding mechanism to run a stable named set of\ntests, so if you want to do the latter, use the `--isolated-script-test-filter`\nflags instead, as described in the previous section.\n\nWhich tests are in which shard must be determined **after** tests have been\nfiltered out using the `--isolated-script-test-filter(-file)` flags.\n\nThe order that tests are run in is not otherwise specified, but tests are\ncommonly run either in lexicographic order or in a semi-fixed random order; the\nlatter is useful to help identify inter-test dependencies, i.e., tests that\nrely on the results of previous tests having run in order to pass (such tests\nare generally considered to be undesirable).\n\n## Examples\n\nAssume that out/Default is a debug build (i.e., that the \"Debug\" tag will\napply), and that you have tests named Foo.Bar.bar{1,2,3}, Foo.Bar.baz,\nand Foo.Quux.quux, and the following two filter files:\n\n```sh\n$ cat filter1\nFoo.Bar.*\n-Foo.Bar.bar3\n$ cat filter2\n# tags: [ Debug Release ]\n[ Debug ] Foo.Bar.bar2 [ Skip ]\n$\n```\n\n#### Filtering tests on the command line\n\n```sh\n$ out/Default/bin/run_foo_tests \\\n    --isolated_script-test-filter='Foo.Bar.*::-Foo.Bar.bar3'\n[1/2] Foo.Bar.bar1 passed in 0.1s\n[2/2] Foo.Bar.bar2 passed in 0.13s\n\n2 tests passed in 0.23s, 0 skipped, 0 failures.\n$\n```\n\n#### Using a filter file\n\n```sh\n$ out/Default/bin/run_foo_tests --isolated-script-test-filter-file=filter1\n[1/2] Foo.Bar.bar1 passed in 0.1s\n[2/2] Foo.Bar.bar2 passed in 0.13s\n\n2 tests passed in 0.23s, 0 skipped, 0 failures.\n```\n\n#### Combining multiple filters\n\n```sh\n$ out/Default/bin/run_foo_tests --isolated-script-test-filter='Foo.Bar.*' \\\n    --isolated-script-test-filter='Foo.Bar.bar2'\n[1/1] Foo.Bar.bar2 passed in 0.13s\n\nAll 2 tests completed successfully in 0.13s\n$ out/Default/bin/run_foo_tests --isolated-script-test-filter='Foo.Bar.* \\\n    --isolated-script-test-filter='Foo.Baz.baz'\nNo tests to run.\n$ out/Default/bin/run_foo_tests --isolated-script-test-filter-file=filter2 \\\n    --isolated-script-test-filter=-FooBaz.baz\n[1/4] Foo.Bar.bar1 passed in 0.1s\n[2/4] Foo.Bar.bar3 passed in 0.13s\n[3/4] Foo.Baz.baz passed in 0.05s\n\n3 tests passed in 0.28s, 2 skipped, 0 failures.\n$\n```\n\n#### Running one shard of tests\n\n```sh\n$ GTEST_TOTAL_SHARDS=3 GTEST_SHARD_INDEX=1 out/Default/bin/run_foo_tests\nFoo.Bar.bar2 passed in 0.13s\nFoo.Quux.quux1 passed in 0.02s\n\n2 tests passed in 0.15s, 0 skipped, 0 failures.\n$\n```\n\n## Related Work\n\nThis document only partially makes sense in isolation.\n\nThe [JSON Test Results Format](json_test_results_format.md) document\nspecifies how the results of the test run should be reported.\n\nThe [Chromium Test List Format][14] specifies in more detail how we can specify\nwhich tests to run and which to skip, and whether the tests are expected to\npass or fail.\n\nImplementing everything in this document plus the preceding three documents\nshould fully specify how tests are run in Chromium. And, if we do this,\nimplementing tools to manage tests should be significantly easier.\n\n[On Naming Chromium Builders and Build Steps][15] is a related proposal that\nhas been partially implemented; it is complementary to this work, but not\nrequired.\n\n[Cleaning up the Chromium Testing Conventions][3] describes a series of\nchanges we might want to make to this API and the related infrastructure to\nsimplify things.\n\nAdditional documents that may be of interest:\n*   [Testing Configuration Files][8]\n*   [The MB (Meta-Build wrapper) User Guide][10]\n*   [The MB (Meta-Build wrapper) Design Spec][11]\n*   [Test Activation / Deactivation (TADA)][12] (internal Google document only,\n    sorry)\n*   [Standardize Artifacts for Chromium Testing][13] is somewhat dated but goes\n    into slightly greater detail on how to store artifacts produced by tests\n    than the JSON Test Results Format does.\n\n## Document history\n\n\\[ Significant changes only. \\]\n\n| Date       | Comment  |\n| ---------- | -------- |\n| 2017-12-13 | Initial version. This tried to be a full-featured spec that defined common flags that devs might want with friendly names, as well the flags needed to run tests on the bots. |\n| 2019-05-24 | Second version. The spec was significantly revised to just specify the minimal subset needed to run tests consistently on bots given the current infrastructure. |\n| 2019-05-29 | All TODOs and discussion of future work was stripped out; now the spec only specifies how the `isolated_scripts` currently behave. Future work was moved to a new doc, [Cleaning up the Chromium Testing Environment][3]. |\n| 2019-09-16 | Add comment about ordering of filters and longest match winning for `--isolated-script-test-filter`. |\n| 2020-07-01 | Moved into the src repo and converted to Markdown. No content changes otherwise. |\n\n## Notes\n\n(*) The initial version of this document talked about test runners instead of\ntest executables, so the bit.ly shortcut URL refers to the test-runner-api instead of\nthe test-executable-api. The author attempted to create a test-executable-api link,\nbut pointed it at the wrong document by accident. bit.ly URLs can't easily be\nupdated :(.\n\n[1]: https://bit.ly/chromium-test-runner-api\n[2]: https://chromium.googlesource.com/infra/infra/+/main/doc/users/services/about_luci.md\n[3]: https://docs.google.com/document/d/1MwnIx8kavuLSpZo3JmL9T7nkjTz1rpaJA4Vdj_9cRYw/edit?usp=sharing\n[4]: ../../testing/buildbot/test_suites.pyl\n[5]: ../../testing/buildbot/gn_isolate_map.pyl\n[6]: ../../testing/buildbot/test_suite_exceptions.pyl\n[7]: ../../testing/buildbot/waterfalls.pyl\n[8]: ../../testing/buildbot/README.md\n[9]: https://bit.ly/chromium-test-list-format\n[10]: ../../tools/mb/docs/user_guide.md\n[11]: ../../tools/mb/docs/design_spec.md\n[12]: https://goto.google.com/chops-tada\n[13]: https://bit.ly/chromium-test-artifacts\n[14]: https://bit.ly/chromium-test-list-format\n[15]: https://bit.ly/chromium-build-naming\n"
  },
  {
    "path": "development/testing/test_descriptions",
    "title": "",
    "content": "See [Testing and infrastructure](https://sites.google.com/a/chromium.org/dev/developers/testing) for more information.\n\n| Type of test           | Description |\n|:-----------------------|:------------|\n|accessibility\\_unittests| |\n|angle\\_unittests        | |\n|app\\_list\\_unittests    | |\n|ash\\_unittests          | |\n|aura\\_unittests         | |\n|base\\_i18n\\_perftests   | |\n|base\\_perftests         |Performance tests for base module.|\n|base\\_unittests         |Tests the base module.|\n|blink\\_heap\\_unittests  | |\n|blink\\_platform\\_unittests| |\n|breakpad\\_unittests     | |\n|[browser\\_tests](https://sites.google.com/a/chromium.org/dev/developers/testing/browser-tests)|Tests the browser UI. Can not inject user input or depend on focus/activation behavior because it can be run in parallel processes and/or with a locked screen, headless etc. For tests sensitive to that, use interactive\\_ui\\_tests. For example, when tests need to navigate to chrome://hang (see chrome/browser/ui/webui/ntp/new\\_tab\\_ui\\_uitest.cc)|\n|chromedriver\\_unittests | |\n|content\\_browsertests   |Similar to browser\\_tests, but with a minimal shell contained entirely within content/. This test, as well as the entire content module, has no dependencies on chrome/.|\n|content\\_gl\\_tests      | |\n|content\\_perftests      | |\n|content\\_unittests      | |\n|courgette\\_unittests    | |\n|crypto\\_unittests       | |\n|curvecp\\_unittests      | |\n|device\\_unittests       |Tests for the device (Bluetooth, HID, USB, etc.) APIs.|\n|ffmpeg\\_tests           | |\n|ffmpeg\\_unittests       | |\n|gfx\\_unittests          | |\n|gpu\\_tests              | |\n|interactive\\_ui\\_tests  |Like browser\\_tests, but these tests do things like changing window focus, so that the machine running the test can't be used while the test is running. May include browsertests (derived from InProcessBrowserTest) to run in-process in case when the test is sensitive to focus transitions or injects user input/mouse events.|\n|ipc\\_tests              |Tests the IPC subsystem for communication between browser, renderer, and plugin processes.|\n|jingle\\_unittests       | |\n|media\\_unittests        | |\n|memory\\_test            | |\n|net\\_perftests          |Performance tests for the disk cache and cookie storage.|\n|net\\_unittests          |Unit tests network stack.|\n|[page\\_cycler\\_tests](https://sites.google.com/a/chromium.org/dev/developers/testing/page-cyclers)| |\n|performance\\_ui\\_tests  | |\n|plugin\\_tests           |Tests the plugin subsystem.|\n|ppapi\\_unittests        |Tests to verify Chromium recovery after hanging or crashing of renderers.|\n|printing\\_unittests     | |\n|reliability\\_tests      | |\n|safe\\_browsing\\_tests   | |\n|sql\\_unittests          | |\n|startup\\_tests          |Test startup performance of Chromium.|\n|sync\\_integration\\_tests| |\n|sync\\_unit\\_tests       | |\n|tab\\_switching\\_test    |Test tab switching functionality.|\n|telemetry\\_unittests    |Tests for the core functionality of the Telemetry performance testing framework. Not performance-sensitive.|\n|telemetry\\_perf\\_unittests|Smoke tests to catch errors running performance tests before they run on the chromium.perf waterfall. Not performance-sensitive.|\n|test\\_shell\\_tests      |A collection of tests within the Test Shell.|\n|[test\\_installer](https://sites.google.com/a/chromium.org/dev/developers/testing/windows-installer-tests)|Tests Chrome's installer for Windows|\n|ui\\_base\\_unittests     |Unit tests for //ui/base.|\n|unit\\_tests             |The kitchen sink for unit tests. These tests cover several modules within Chromium.|\n|url\\_unittests          | |\n|views\\_unittests        | |\n|wav\\_ola\\_test          | |\n|webkit\\_unit\\_tests     | |\n|webui tests             | Special type of browser\\_tests used for [WebUI features](https://chromium.googlesource.com/chromium/src/+/main/docs/webui/webui_explainer.md), see [here](https://docs.google.com/document/d/1Z18WTNv28z5FW3smNEm_GtsfVD2IL-CmmAikwjw3ryo/edit#) for more information on known issues with WebUI test infrastructure. |\n"
  },
  {
    "path": "development/testing/test_browser_dialog",
    "title": "Testing Chrome browser UI with TestBrowserUi",
    "content": "# Testing Chrome browser UI with TestBrowserUi\n\n\\#include \"[chrome/browser/ui/test/test_browser_ui.h]\"\n\n`TestBrowserUi` (and convenience class `TestBrowserDialog`) provide ways to\nregister an `InProcessBrowserTest` testing harness with a framework that invokes\nChrome browser UI in a consistent way. They optionally provide a way to invoke\nUI \"interactively\". This allows screenshots to be generated easily, with the\nsame test data, to assist with UI review. `TestBrowserUi` also provides a UI\nregistry so pieces of UI can be systematically checked for subtle changes and\nregressions.\n\n[TOC]\n\n## How to register UI\n\nIf registering existing UI, there's a chance it already has a test harness\ninheriting, using, or with `typedef InProcessBrowserTest` (or a descendant of\nit). If so, using `TestBrowserDialog` (for a dialog) is straightforward, and\n`TestBrowserUi` (for other types of UI) relatively so. Assume the existing\n`InProcessBrowserTest` is in `foo_browsertest.cc`:\n\n    class FooUiTest : public InProcessBrowserTest { ...\n\nChange this to inherit from `DialogBrowserTest` (for dialogs) or `UiBrowserTest`\n(for non-dialogs), and override `ShowUi(std::string)`. For non-dialogs, also\noverride `VerifyUi()` and `WaitForUserDismissal()`. See\n[Advanced Usage](#Advanced-Usage) for details.\n\n```cpp\nclass FooUiTest : public UiBrowserTest {\n public:\n  ..\n  // UiBrowserTest:\n  void ShowUi(const std::string& name) override {\n    /* Show Ui attached to browser() and leave it open. */\n  }\n  // These next two are not necessary if subclassing DialogBrowserTest.\n  bool VerifyUi() override {\n    /* Return true if the UI was successfully shown. */\n  }\n  void WaitForUserDismissal() override {\n    /* Block until the UI has been dismissed. */\n  }\n  ..\n};\n```\n\nFinally, add test invocations using the usual GTest macros, in\n`foo_browsertest.cc`:\n\n```cpp\nIN_PROC_BROWSER_TEST_F(FooUiTest, InvokeUi_default) {\n  ShowAndVerifyUi();\n}\n```\n\nNotes:\n\n*   The body of the test is always just \"`ShowAndVerifyUi();`\".\n*   \"`default`\" is the `std::string` passed to `ShowUi()` and can be\n    customized. See\n    [Testing additional UI \"styles\"](#Testing-additional-ui-styles).\n*   The text before `default` (in this case) must always be \"`InvokeUi_`\".\n\n### Concrete examples\n\n*   [chrome/browser/ui/ask_google_for_suggestions_dialog_browsertest.cc]\n*   [chrome/browser/infobars/infobars_browsertest.cc]\n\n##  Running the tests\n\nList the available pieces of UI with\n\n    $ ./browser_tests --gtest_filter=BrowserUiTest.Invoke\n    $ ./interactive_ui_tests --gtest_filter=BrowserInteractiveUiTest.Invoke\n\nE.g. `FooUiTest.InvokeUi_default` should be listed. To show the UI\ninteractively, run\n\n    # If FooUiTest is a browser test.\n    $ ./browser_tests --gtest_filter=BrowserUiTest.Invoke \\\n      --test-launcher-interactive --ui=FooUiTest.InvokeUi_default\n\n    # If FooUiTest is an interactive UI test.\n    $ ./interactive_ui_tests --gtest_filter=BrowserInteractiveUiTest.Invoke \\\n      --test-launcher-interactive --ui=FooUiTest.InvokeUi_default\n\n### Implementation\n\n`BrowserUiTest.Invoke` searches for gtests that have \"`InvokeUi_`\"  in their\nnames, so they can be collected in a list. Providing a `--ui` argument will\ninvoke that test case in a subprocess. Including `--test-launcher-interactive`\nwill set up an environment for that subprocess that allows interactivity, e.g.,\nto take screenshots. The test ends once the UI is dismissed.\n\nThe `FooUiTest.InvokeUi_default` test case **will still be run in the usual\nbrowser_tests test suite**. Ensure it passes, and isn’t flaky. This will\ngive your UI some regression test coverage. `ShowAndVerifyUi()` checks to ensure\nUI is actually created when it invokes `ShowUi(\"default\")`.\n\n`BrowserInteractiveUiTest` is the equivalent of `BrowserUiTest` for\ninteractive_ui_tests.\n\n### BrowserUiTest.Invoke\n\nThis is also run in browser_tests but, when run that way, the test case just\nlists the registered test harnesses (it does *not* iterate over them). A\nsubprocess is never created unless --ui is passed on the command line.\n\n## Advanced Usage\n\nIf your test harness inherits from a descendant of `InProcessBrowserTest` (one\nexample: [ExtensionBrowserTest]) then the `SupportsTestUi<>` and\n`SupportsTestDialog` templates are provided. E.g.\n\n```cpp\nclass ExtensionInstallDialogViewTestBase : public ExtensionBrowserTest { ...\n```\n\nbecomes\n\n```cpp\nclass ExtensionInstallDialogViewTestBase :\n    public SupportsTestDialog<ExtensionBrowserTest> { ...\n```\n\nIf you need to do any setup before `ShowUi()` is called, or any teardown in the\nnon-interactive case, you can override the `PreShow()` and `DismissUi()\nmethods.\n\n### Testing additional UI \"styles\"\n\nAdd additional test cases, with a different string after \"`InvokeUi_`\".\nExample:\n\n```cpp\nIN_PROC_BROWSER_TEST_F(CardUnmaskViewBrowserTest, InvokeUi_expired) {\n  ShowAndVerifyUi();\n}\n\nIN_PROC_BROWSER_TEST_F(CardUnmaskViewBrowserTest, InvokeUi_valid) {\n  ShowAndVerifyUi();\n}\n```\n\nThe strings \"`expired`\" or “`valid`” will be given as arguments to\n`ShowUi(std::string)`.\n\n## Rationale\n\nBug reference: [Issue 654151](http://crbug.com/654151).\n\nChrome has a lot of browser UI; often for obscure use-cases and often hard to\ninvoke. It has traditionally been difficult to be systematic while checking UI\nfor possible regressions. For example, to investigate changes to shared layout\nparameters which are testable only with visual inspection.\n\nFor Chrome UI review, screenshots need to be taken. Iterating over all the\n\"styles\" that UI may appear with is fiddly. E.g. a login or particular web\nserver setup may be required. It’s important to provide a consistent “look” for\nUI review (e.g. same test data, same browser size, anchoring position, etc.).\n\nSome UI lacks tests. Some UI has zero coverage on the bots. UI elements can have\ntricky lifetimes and common mistakes are repeated. TestBrowserUi runs simple\n\"Show UI\" regression tests and can be extended to do more.\n\nEven discovering the full set of UI present for each platform in Chrome is\n[difficult](http://crbug.com/686239).\n\n### Why browser_tests?\n\n*   `browser_tests` already provides a `browser()->window()` of a consistent\n    size that can be used as a dialog anchor and to take screenshots for UI\n    review.\n    *   UI review have requested that screenshots be provided with the entire\n        browser window so that the relative size of the UI element/change under\n        review can be assessed.\n\n*   Some UI already has a test harness with appropriate setup (e.g. test data)\n    running in browser_tests.\n    *   Supporting `BrowserUiTest` should require minimal setup and minimal\n        ongoing maintenance.\n\n*   An alternative is to maintain a working end-to-end build target executable\n    to do this, but this has additional costs (and is hard).\n    *    E.g. setup/teardown of low-level functions\n         (`InitializeGLOneOffPlatform()`, etc.).\n\n*   Why not chrome.exe?\n    *   E.g. a scrappy chrome:// page with links to invoke UI would be great!\n    *   But...\n        *   UI may have test data (e.g. credit card info) which shouldn’t be in\n        the release build.\n        *   UI may use EmbeddedTestServer.\n        *   Higher maintenance cost - can’t leverage existing test harnesses.\n\n## Future Work\n\n*   Opt in more UI!\n    *    Eventually, all of it.\n    *    A `DialogBrowserTest` for every descendant of `views::DialogDelegate`.\n\n*   Automatically generate screenshots (for each platform, in various languages)\n    *    Build upon [CL 2008283002](https://codereview.chromium.org/2008283002/)\n\n*   (maybe) Try removing the subprocess\n    *    Probably requires altering the browser_test suite code directly rather\n         than just adding a test case as in the current approach\n\n*   An automated test suite for UI\n    *    Test various ways to dismiss or hide UI, especially dialogs\n         *    e.g. native close (via taskbar?)\n         *    close parent window (possibly via task bar)\n         *    close parent tab\n         *    switch tabs\n         *    close via `DialogClientView::AcceptWindow` (and `CancelWindow`)\n         *    close via `Widget::Close`\n         *    close via `Widget::CloseNow`\n    *    Drag tab off browser into a new window\n    *    Fullscreen that may create a new window/parent\n\n*   Find obscure workflows for invoking UI that has no test coverage and causes\n    crashes (e.g. [http://crrev.com/426302](http://crrev.com/426302))\n    *   Supporting window-modal dialogs with a null parent window.\n\n*   Find memory leaks, e.g. [http://crrev.com/432320](http://crrev.com/432320)\n    *   \"Fix memory leak for extension uninstall dialog\".\n\n## Appendix: Sample output\n\n**$ ./out/gn_Debug/browser_tests --gtest_filter=BrowserUiTest.Invoke**\n```\nNote: Google Test filter = BrowserUiTest.Invoke\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from BrowserUiTest\n[ RUN      ] BrowserUiTest.Invoke\n[26879:775:0207/134949.118352:30434675...:INFO:browser_ui_browsertest.cc(46)\nPass one of the following after --ui=\n        AppInfoDialogBrowserTest.InvokeUi_default\n        AskGoogleForSuggestionsDialogTest.DISABLED_InvokeUi_default\n        BluetoothChooserBrowserTest.InvokeUi_ConnectedBubble\n        BluetoothChooserBrowserTest.InvokeUi_ConnectedModal\n/* and many more */\n[       OK ] BrowserUiTest.Invoke (0 ms)\n[----------] 1 test from BrowserUiTest (0 ms total)\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (1 ms total)\n[  PASSED  ] 1 test.\n[1/1] BrowserUiTest.Invoke (334 ms)\nSUCCESS: all tests passed.\n```\n\n**$ ./out/gn_Debug/browser_tests --gtest_filter=BrowserUiTest.Invoke\n--ui=CardUnmaskPromptViewBrowserTest.InvokeUi_expired**\n\n```\nNote: Google Test filter = BrowserUiTest.Invoke\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from BrowserUiTest\n[ RUN      ] BrowserUiTest.Invoke\nNote: Google Test filter = CardUnmaskPromptViewBrowserTest.InvokeDefault\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from CardUnmaskPromptViewBrowserTest, where TypeParam =\n[ RUN      ] CardUnmaskPromptViewBrowserTest.InvokeUi_expired\n/* 7 lines of uninteresting log spam */\n[       OK ] CardUnmaskPromptViewBrowserTest.InvokeUi_expired (1324 ms)\n[----------] 1 test from CardUnmaskPromptViewBrowserTest (1324 ms total)\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (1325 ms total)\n[  PASSED  ] 1 test.\n[       OK ] BrowserUiTest.Invoke (1642 ms)\n[----------] 1 test from BrowserUiTest (1642 ms total)\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (1642 ms total)\n[  PASSED  ] 1 test.\n[1/1] BrowserUiTest.Invoke (2111 ms)\nSUCCESS: all tests passed.\n```\n\n**$ ./out/gn_Debug/browser_tests --gtest_filter=BrowserUiTest.Invoke\n--ui=CardUnmaskPromptViewBrowserTest.InvokeUi_expired\n--test-launcher-interactive**\n```\n/*\n * Output as above, except the test are not interleaved, and the browser window\n * should remain open until the UI is dismissed\n */\n```\n\n[chrome/browser/ui/test/test_browser_ui.h]: https://cs.chromium.org/chromium/src/chrome/browser/ui/test/test_browser_ui.h\n[chrome/browser/ui/test/test_browser_dialog.h]: https://cs.chromium.org/chromium/src/chrome/browser/ui/test/test_browser_dialog.h\n[chrome/browser/ui/ask_google_for_suggestions_dialog_browsertest.cc]: https://cs.chromium.org/chromium/src/chrome/browser/ui/ask_google_for_suggestions_dialog_browsertest.cc?l=18&q=ShowUi\n[chrome/browser/infobars/infobars_browsertest.cc]: https://cs.chromium.org/chromium/src/chrome/browser/infobars/infobars_browsertest.cc?l=134&q=UiBrowserTest\n[ExtensionBrowserTest]: https://cs.chromium.org/chromium/src/chrome/browser/extensions/extension_browsertest.h?q=extensionbrowsertest&l=40\n"
  },
  {
    "path": "development/testing/testing_in_chromium",
    "title": "",
    "content": "Testing is an essential component of software development in Chromium,\nit ensures Chrome is behaving as we expect, and is critical to find bugs and\nregressions at early stage.\n\nThis document covers the high level overview of testing in Chromium,\nincluding what type of tests we have, what's the purpose for each test type,\nwhat tests are needed for new features etc.\n\n## Test Types\n\nThere are several different types of tests in Chromium to serve different purposes,\nsome types of test are running on multiple platforms, others are specific\nfor one platform.\n\n*   **[gtest]** is Google's C++ test framework,\n    which helps you write better C++ tests in Chromium.\n    gtest is test framework for unit tests in Chromium and browser tests are built on top of it.\n*   **Browser Tests** is built on top of gtest, and it is used to write integration tests\n    and e2e tests in Chromium.\n    <!-- TODO(leilei) Add link to browser tests --->\n*   **[Web Tests] (formerly known as \"Layout Tests\" or \"LayoutTests\")**\n    is used by Blink to test many components, including but not\n    limited to layout and rendering. In general, web tests involve loading pages\n    in a test renderer (`content_shell`) and comparing the rendered output or\n    JavaScript output against an expected output file.\n    Web Tests are required to launch new W3C API support in Chromium.\n*   **[Robolectric]** is build on top of JUnit 4. It emulates Android APIs so\n    that tests can be run on the host machine instead of on devices / emulators.\n*   **[Instrumentation Tests]** are JUnit tests that run on devices / emulators.\n*   **[EarlGrey]** is the integration testing framework used by Chromium for iOS.\n*   **[Telemetry]** is the performance testing framework used by Chromium.\n    It allows you to perform arbitrary actions on a set of web pages and\n    report metrics about it.\n*   **[Fuzzer Tests]** is used to uncover potential security & stability problems in Chromium.\n*   **[Tast]** is a test framework for system integration tests on Chrome OS.\n\n\nThe following table shows which types of test works on which platforms.\n\n|                             |  Linux  | Windows |   Mac   | Android |  iOS    |  CrOS   |\n|:----------------------------|:--------|:--------|:--------|:--------|:--------|:--------|\n| gtest(C++)                  | &#8730; | &#8730; | &#8730; | &#8730; | &#8730; | &#8730; |\n| Browser Tests(C++)          | &#8730; | &#8730; | &#8730; | &#8730; |         |         |\n| Web Tests(HTML, JS)         | &#8730; | &#8730; | &#8730; |         |         |         |\n| Telemetry(Python)           | &#8730; | &#8730; | &#8730; | &#8730; |         | &#8730; |\n| Robolectric(Java)           |         |         |         | &#8730; |         |         |\n| Instrumentation Tests(Java) |         |         |         | &#8730; |         |         |\n| EarlGrey                    |         |         |         |         | &#8730; |         |\n| Fuzzer Tests(C++)           | &#8730; | &#8730; | &#8730; | &#8730; |         | &#8730; |\n| Tast(Golang)                |         |         |         |         |         | &#8730; |\n\n*** note\n**Browser Tests Note**\n\nOnly subset of browser tests are enabled on Android:\n*   components_browsertests\n*   content_browsertests\n\nOther browser tests are not supported on Android yet. [crbug/611756]\ntracks the effort to enable them on Android.\n***\n\n*** note\n**Web Tests Note**\n\nWeb Tests were enabled on Android K before, but it is disabled on Android platform now,\nsee [this thread](https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/338WKwWPbPI/discussion) for more context.\n***\n\n*** note\n**Tast Tests Note**\n\nTast tests are written, maintained and gardened by ChromeOS engineers.\n\nChromeOS tests that Chrome engineers support should be (re)written in the following priority order:\n*   unit tests\n*   linux_chromeos browser_tests\n*   linux_chromeos interactive_ui_tests\n*   [Crosier tests](http://go/crosier)\n\nWhen a Tast test fails:\n*   If the change is written by a ChromeOS engineer, the ChromeOS [gardener](http://go/cros-gardening) can revert it.\n*   Otherwise the ChromeOS gardener can revert the Chrome-authored change accompanied by a test from the supported frameworks above or manual repro steps that a Chrome engineer can run under [linux_chromeos](../chromeos_build_instructions.md#Chromium-OS-on-Linux-linux_chromeos) (preferable) or using Simple Chrome VM [instructions](https://chromium.googlesource.com/chromiumos/docs/+/HEAD/simple_chrome_workflow.md).\n*   If the above is not possible, the ChromeOS gardener or ChromeOS feature owner should inform the author about the failure, and the author uses their best judgment on whether to revert, fix on trunk, or let ChromeOS engineers update the test (e.g. if test needs an update or if the test is just testing internal implementation details of Chrome but doesn't break user functionality).\n\n\n***\n\n## General Principles\n\n*   All the tests in Chromium running on CQ and main waterfall should be hermetic and stable.\n*   Add unit tests along with the code in same changelist instead of adding tests in future,\n    it is most likely no one will add tests later.\n*   Write enough unit tests to have good [code coverage](./code_coverage.md),\n    since they are fast and stable.\n*   Don't enable tests with external dependencies on CQ and main waterfall,\n    e.g. tests against live sites.\n    It is fine to check in those tests, but only run them on your own bots.\n*   Eventually, all tests should implement the\n    [Test Executable API](./test_executable_api.md) command line interface.\n\n## What tests are needed for new features\n\n* **Unit Tests** are needed no matter where the code is for your feature.\n  It is the best practice to add the unit tests\n  when you add new code or update existing code in the same changelist,\n  check out [Code Coverage in Gerrit](./code_coverage_in_gerrit.md)\n  for the instruction about how to see the code coverage in Gerrit.\n* **Browser Tests** are recommended for integration tests and e2e tests.\n  It will be great if you add browser tests to cover the major user\n  cases for your feature, even with some mocking.\n* **[Web Tests]** are required if you plan to launch new W3C APIs in Chrome.\n* **[Instrumentation Tests]** are recommended for features on Android, you only\n  need to write instrumentation features\n  if your feature is supported on Android for integration tests or e2e tests.\n* **EarlGrey Tests** are recommended for iOS only.\n* **[Telemetry] benchmarking or stories** are needed if existing telemetry\n  benchmarks or stories can't cover the performance for your feature,\n  you need to either add new story, but reuse existing metrics or\n  add new benchmarks for your feature. Talk to benchmarking team first\n  before start to add Telemetry benchmarks or stories.\n* **[Fuzzer Tests]** are recommended if your feature adds user facing APIs\n  in Chromium, it is recommended to write fuzzer tests to detect the security issue.\n\nRight now, code coverage is the only way we have to measure test coverage.\nThe following is the recommended thresholds for different code coverage levels:\n* >level 1(improving): >0%\n* >level 2(acceptable): 60%\n* >level 3(commendable): 75%\n* >level 4(exemplary): 90%\n\nGo to [code coverage dashboard](https://analysis.chromium.org/coverage/p/chromium) to check the code coverage for your project.\n\n\n## How to write new tests\n*  [Simple gtests]\n*  [Writing JUnit tests]\n*  [Writing Browser Tests]\n*  [Writing Instrumentation Tests]\n*  [Writing EarlGrey Tests]\n*  [Writing Telemetry Benchmarks/Stories]\n*  [Writing Web Tests](./writing_web_tests.md)\n*  [Write Fuzz Target]\n\n>TODO: add the link to the instruction about how to enable new tests in CQ and main waterfall\n\n## How to run tests\n\n### Run tests locally\n*  [Run gtest locally](#Run-gtest-locally)\n*  [Run browser tests locally]\n*  [Run tests on Android](./android_test_instructions.md#Running-Tests)\n   It includes the instructions to run gTests, JUnit tests and Instrumentation tests on Android.\n*  [Run EarlGrey tests locally](../ios/testing.md#running-tests-from-xcode)\n*  [Run Web Tests locally](./web_tests.md#running-web-tests)\n*  [Telemetry: Run benchmarks locally]\n*  [Run fuzz target locally]\n\n#### Run gtest locally\n\nBefore you can run a gtest, you need to build the appropriate launcher target\nthat contains your test, such as `blink_unittests`:\n\n```bash\nautoninja -C out/Default blink_unittests\n```\n\nTo run specific tests, rather than all tests in a launcher, pass\n`--gtest_filter=` with a pattern. The simplest pattern is the full name of a\ntest (SuiteOrFixtureName.TestName), but you can use wildcards:\n\n```bash\nout/Default/blink_unittests --gtest_filter='Foo*'\n```\n\nUse `--help` for more ways to select and run tests.\n\n### Run tests remotely(on Swarming)\n>TODO: add the link to the instruction about how to run tests on Swarming.\n\n## How to debug tests\n*  [Android Debugging Instructions]\n*  [Chrome OS Debugging Tips]\n*  [Debugging Web Tests]\n\n## How to deal with flaky tests\n\nGo to [LUCI Analysis] to find reports about flaky tests in your projects.\n\n* [Addressing Flaky GTests](./gtest_flake_tips.md)\n* [Addressing Flaky Web Tests](./web_tests_addressing_flake.md)\n* [Addressing Flaky WPTs](./web_platform_tests_addressing_flake.md)\n\nIf you cannot fix a flaky test in a short timeframe, disable it first to reduce\ndevelopment pain for other and then fix it later. \"[How do I disable a flaky\ntest]\" has instructions on how to disable a flaky test.\n\n## Other\n\nTests are not configured to upload metrics, such as UMA, UKM or crash reports.\n\n[gtest]: https://github.com/google/googletest\n[Simple gtests]: https://github.com/google/googletest/blob/main/docs/primer.md#simple-tests\n[Robolectric]: android_robolectric_tests.md\n[Instrumentation Tests]: https://chromium.googlesource.com/chromium/src/+/main/docs/testing/android_instrumentation_tests.md\n[EarlGrey]: https://github.com/google/EarlGrey\n[Telemetry]: https://chromium.googlesource.com/catapult/+/HEAD/telemetry/README.md\n[Fuzzer Tests]: https://chromium.googlesource.com/chromium/src/+/main/testing/libfuzzer/README.md\n[Tast]: https://chromium.googlesource.com/chromiumos/platform/tast/+/HEAD/README.md\n[Web Tests]: ./web_tests.md\n[crbug/611756]: https://bugs.chromium.org/p/chromium/issues/detail?id=611756\n[LUCI Analysis]: https://luci-analysis.appspot.com/\n[Write Fuzz Target]: https://chromium.googlesource.com/chromium/src/+/main/testing/libfuzzer/getting_started.md#write-fuzz-target\n[Telemetry: Run benchmarks locally]: https://chromium.googlesource.com/catapult/+/HEAD/telemetry/docs/run_benchmarks_locally.md\n[Run fuzz target locally]: https://chromium.googlesource.com/chromium/src/+/main/testing/libfuzzer/getting_started.md#build-and-run-fuzz-target-locally\n[Android Debugging Instructions]: https://chromium.googlesource.com/chromium/src/+/HEAD/docs/android_debugging_instructions.md\n[Chrome OS Debugging Tips]: ./chromeos_debugging_tips.md\n[Debugging Web Tests]: https://chromium.googlesource.com/chromium/src/+/HEAD/docs/testing/web_tests.md#Debugging-Web-Tests\n[code coverage dashboard]: https://analysis.chromium.org/p/chromium/coverage\n[How do I disable a flaky test]: https://www.chromium.org/developers/tree-sheriffs/sheriff-details-chromium#TOC-How-do-I-disable-a-flaky-test-\n"
  },
  {
    "path": "development/testing/run_web_platform_tests_on_android",
    "title": "Running Web Platform Tests on Android",
    "content": "# Running Web Platform Tests on Android\n\n## Overview\n\nThis document provides a guide to running Web Platform Tests on Android.\n\nFor general instruction for running the Web Platform Tests, you should read\n[Running Web Platform Tests with run_wpt_tests.py](./run_web_platform_tests.md).\n\n[TOC]\n\n## Initial Setup\n\nPlease follow the steps at [Checking out and building Chromium for Android\n](/docs/android_build_instructions.md) to\nsetup your local environment. Once that is done, you need to build one of the\nfollowing targets:\n\n```bash\nautoninja -C out/Default chrome_public_wpt        # For testing with Chrome Android\nautoninja -C out/Default trichrome_webview_wpt_64 # For testing with WebView\n```\n\n## Running the Tests\n\nOnce you have Chrome Android/WebView and `chromedriver` built, you can launch\n`run_wpt_tests.py` to run WPTs. You can either run the tests on an Android\nemulator or a real Android device.\n\nNote: You can enable [Incremental Install](/build/android/incremental_install/README.md)\nfor Chrome Android by setting `incremental_install = True` in your `args.gn`\nfile.\n\nThis will cause the test harness to use the build directory's incremental\ninstallation script instead of the default browser APK.\n\n### Running WPTs on an Android emulator\n\nYou will need to follow the steps in\n[Using an Android Emulator](/docs/android_emulator.md) to be ready to run the\nAndroid Emulator. Passing the `--avd-config` option to `run_wpt_tests.py` will\nlaunch an emulator that will be shut down after running the tests.\n\nThe example below runs `external/wpt/badging/badge-success.https.html` in Chrome\nAndroid on Android 13:\n\n```bash\nthird_party/blink/tools/run_wpt_tests.py \\\n  -t Default \\\n  -p clank \\\n  --avd-config=tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n  external/wpt/badging/badge-success.https.html\n```\n\n* `-t Default`: Use the build in `//out/Default/`\n* `-p clank`: Runs the tests using Chrome for Android (clank).\n* `--avd-config=tools/.../android_33_google_apis_x64.textpb`: Runs the tests on\nAndroid 13 emulator (Google API 33).\n\nTo run the example in WebView:\n\n```bash\n$ third_party/blink/tools/run_wpt_tests.py \\\n  -t Default \\\n  -p android_webview \\\n  --webview-provider out/Default/apks/TrichromeWebView64.apk \\\n  --additional-apk out/Default/apks/TrichromeLibrary64.apk \\\n  --avd-config=tools/android/avd/proto/android_33_google_apis_x64.textpb \\\n  external/wpt/badging/badge-success.https.html\n```\n\n* `-p webview`: Runs the tests using WebView.\n* `--webview-provider out/.../TrichromeWebView64.apk`: Specify\nTrichromeWebView64 as WebView APK.\n* `--additional-apk out/.../TrichromeLibrary64.apk`: Install TrichromeLibrary64\nneeded for the WebView APK after Android 10 (see [WebView Channels](/android_webview/docs/channels.md)).\n\nAlternatively, you can launch the emulator yourself and `run_wpt_tests.py` will\ndetect and connect to the emulator and run WPTs with it. This can save you the\ntime to repeatedly launch the emulator. The commands below show how this works.\n\n```bash\n$ tools/android/avd/avd.py start \\\n  --avd-config=tools/android/avd/proto/android_33_google_apis_x64.textpb\n$ third_party/blink/tools/run_wpt_tests.py \\\n  -t Default \\\n  -p clank \\\n  external/wpt/badging/badge-success.https.html\n```\n\n### Running WPTs on a real Android device\n\n`run_wpt_tests.py` should be able to work with a real device as long as the\ndevice can be found by `adb devices`. You will need to make sure the ABI matches\nand these [steps\n](/docs/android_build_instructions.md#installing-and-running-chromium-on-a-device)\nare followed.\n\n### Running Tests in CQ/CI\n\nThe builder `android-chrome-13-x64-wpt-android-specific` and\n`android-webview-13-x64-wpt-android-specific` builders run tests specified by\nthe [`android.filter`](/third_party/blink/web_tests/TestLists/android.filter)\nfile, which tests Android-specific behaviors. Developers can add additional\ntests to the list when necessary.\n\nTo satisfy different testing requirements, WPT coverage in CQ/CI is partitioned\nbetween suites that target different `//content` embedders:\n\nSuite Name | Browser Under Test | Harness | Tests Run\n--- | --- | --- | ---\n`android_blink_wpt_tests` | `chrome_android` | `run_wpt_tests.py` | Tests listed in [`android.filter`](#running-tests-in-cqci).\n`webview_blink_wpt_tests` | `android_webview` | `run_wpt_tests.py` | Tests listed in [`android.filter`](#running-tests-in-cqci).\n\n## Test expectations and Baselines\n\nThe\n[MobileTestExpectations](../../third_party/blink/web_tests/MobileTestExpectations)\nfile contains the list of all known Chrome Android and Chrome WebView specific\ntest failures, and it inherits or overrides test expectations from the default\n[TestExpectations](../../third_party/blink/web_tests/TestExpectations) file.\n\nFor baselines:\n* Chrome Android specific baselines reside at\n  `third_party/blink/web_tests/platform/android`, and fall back to\n  `third_party/blink/web_tests/platform/linux`.\n* WebView specific baselines reside at\n  `third_party/blink/web_tests/platform/webview`, and fall back to\n  `third_party/blink/web_tests/platform/linux`.\n\nTo update baselines:\n1. Trigger tryjob(s)\n  * For Chrome Androids: trigger `android-chrome-13-x64-wpt-android-specific`\n  * For WebView: trigger `android-webview-13-x64-wpt-android-specific`\n2. Run [the rebaseline tool](./web_test_expectations.md#How-to-rebaseline) after\n  the results are ready.\n\n## Known Issues\n\nFor runner bugs and feature requests, please file [an issue against\n`Blink>Infra`\n](https://issues.chromium.org/issues/new?component=1456928&template=1923166).\n"
  },
  {
    "path": "development/testing/run_web_platform_tests",
    "title": "Running Web Platform Tests with run_wpt_tests.py",
    "content": "# Running Web Platform Tests with run_wpt_tests.py\n\n`run_web_tests.py` runs web tests with content shell through [protocol mode].\nSee [web_tests.md](web_tests.md) for details.\n`run_wpt_tests.py` is a WebDriver-based alternative that can run [web platform\ntests] with [Chrome], [headless shell], Chrome Android, and WebView.\nThis document explains how to use `run_wpt_tests.py` in these scenarios.\n\n[web platform tests]: web_platform_tests.md\n[Chrome]: /chrome\n[headless shell]: /headless\n\n[TOC]\n\n## Running Web Platform Tests for Desktop Platforms\n\nOn Linux, macOS, and Windows, `run_wpt_tests.py` supports testing with [Chrome]\nor [headless shell].\nChrome is closer to the binary Google ships to end users, but is generally\nslower.\nHeadless shell is a lightweight alternative that suffices for testing features\nimplemented entirely in Blink.\n\n### Running Tests Locally\n\nFirst, you will need to build the `blink_tests` target as you were running web tests\nbefore. This will build `headless_shell`, `chrome`, `chromedriver`, `content_shell`\nand all other needed binaries to run web tests and WPTs.\n\n```bash\nautoninja -C out/Default blink_tests\n```\n\nOnce the build is done, running tests is very similar to how you would run\ntests with `run_web_tests.py`.\nFor example, to run all tests under `external/wpt/html/dom`, run:\n\n```bash\nthird_party/blink/tools/run_wpt_tests.py --target=Default --product=headless_shell external/wpt/html/dom\n```\n\n`--product` (or `-p`) selects which browser to test with.\nSupported values are:\n\n* `headless_shell` (default if `--product` is not specified)\n* `chrome`\n* `chrome_android` (aliased as `clank`; see\n  [additional instructions](#Running-Web-Platform-Tests-on-Android))\n* `android_webview` (aliased as `webview`; see\n  [additional instructions](#Running-Web-Platform-Tests-on-Android))\n\nAlso, consider using `-v` to get browser logs.\nIt can be provided multiple times to increase verbosity.\n\n`run_wpt_tests.py --help` shows a full description of `run_wpt_tests.py`'s CLI,\nwhich resembles that of `run_web_tests.py`.\n\n### Running Tests in CQ/CI\n\nTo satisfy different testing requirements, WPT coverage in CQ/CI is partitioned\nbetween suites that target different `//content` embedders:\n\nSuite Name | Browser Under Test | Harness | Tests Run\n--- | --- | --- | ---\n`headless_shell_wpt_tests` | `headless_shell` | `run_wpt_tests.py` | The default test suite for WPTs if not specified otherwise.\n`chrome_wpt_tests` | `chrome --headless=new` | `run_wpt_tests.py` | Tests that depend on the `//chrome` layer. Can be slow, so prefer `headless_shell` testing if possible.\n`blink_wpt_tests` | `content_shell --run-web-tests` | `run_web_tests.py` | Tests under [internal WPTs] plus any public WPTs not migrated yet.\n\nTo avoid redundant coverage, each WPT should run in exactly one suite listed\nabove.\nThe [`chrome.filter`][1] file lists tests that `chrome_wpt_tests` should run,\nand that `headless_shell_wpt_tests` and `blink_wpt_tests` should skip.\n[`content_shell.filter`][2] file lists tests that currently run in `blink_wpt_tests`,\nand will be removed once it only contains tests under `wpt_internal`. Tests\nnot listed in either file run in `headless_shell_wpt_tests` by default.\n\n*** note\nRunning tests in `blink_wpt_tests` is discouraged because `run_web_tests.py`\ndoesn't drive tests through standard WebDriver endpoints.\nThis can cause `blink_wpt_tests` results to diverge from the Chrome results\npublished to [wpt.fyi]. We should generally not add new tests to\ncontent_shell.filter.\n***\n\n[internal WPTs]: /third_party/blink/web_tests/wpt_internal\n\n### Test Expectations and Baselines\n\nTo suppress failures, `run_wpt_tests.py` uses the [same `*-expected.txt` and\nTestExpectations files](web_test_expectations.md) that `run_web_tests.py` uses.\n\n### Running webdriver tests with Chrome\n\n[wdspec tests] are a subset of WPT that verifies conformance to the WebDriver\nspecification.\n`run_wpt_tests.py` can run wdspec tests like any other WPT:\n\n```bash\nthird_party/blink/tools/run_wpt_tests.py -t Default -p chrome \\\n  external/wpt/webdriver/tests/classic/find_element/find.py\n```\n\nOn the bots, the `webdriver_wpt_tests` suite runs wdspec tests separately from\nthe other WPT types.\nThe `linux-blink-rel` builder can provide results for rebaselining.\n\n[wdspec tests]: https://web-platform-tests.org/writing-tests/wdspec.html\n\n## Running Web Platform Tests on Android\n\nSee [here](./run_web_platform_tests_on_android.md) for Android specific instructions.\n\n## Debugging Support\n\n### Text-Based Debuggers\n\nTo interactively debug WPTs, prefix the `run_wpt_tests.py` command with\n[`debug_renderer`][debug renderer] to attach a debugger to a desired renderer.\n\nFor other use cases, see [these debugging tips].\n\n[these debugging tips]: /docs/linux/debugging.md\n\n## FAQ\n\n* Do headless shell and Chrome support MojoJS bindings?\n    * Yes.\n      `run_wpt_tests.py` enables the `MojoJS` and `MojoJSTest` features and\n      serves `//out/<target>/gen/` as `/gen/` in wptserve.\n      However, in the public WPT suite, testdriver.js APIs must be backed by\n      fully-specified testing APIs (preferably implemented with WebDriver or\n      alternatively with MojoJS). Tests that rely on unspecified testing APIs\n      cannot be put in WPT, but may live in chromium's own wpt_internal.\n      See https://github.com/web-platform-tests/rfcs/issues/172 for additional\n      discussion.\n\n## Known Issues\n\nThe [`wptrunner-migration`\nhostlist](https://issues.chromium.org/hotlists/6224346) tracks test results\nwhere headless shell and content shell differ.\nFor runner bugs and feature requests, please file [an issue against\n`Blink>Infra`](https://issues.chromium.org/issues/new?component=1456928&template=1923166).\n\n[protocol mode]: /content/web_test/browser/test_info_extractor.h\n[debug renderer]: /third_party/blink/tools/debug_renderer\n[wpt.fyi]: https://wpt.fyi/results/?label=experimental&label=master&aligned\n\n[1]: /third_party/blink/web_tests/TestLists/chrome.filter\n[2]: /third_party/blink/web_tests/TestLists/content_shell.filter\n[3]: writing_web_tests.md#Relying-on-Blink_Specific-Testing-APIs\n"
  },
  {
    "path": "development/testing/resultdb",
    "title": "Chromium Integration with ResultDB",
    "content": "[TOC]\n\n# Chromium Integration with ResultDB\n\nResultDB is a LUCI service for storing and retrieving test results. See also\nits [public source] and [Google internal documentation]. As of Q4 2021, all\ntests on Chrome/Chromium builders have their test results uploaded to ResultDB,\nand nearly all pass-fail decisions on the builders are based on these results.\nConsequently, any JSON output from a test no longer has little to no impact on\nthe results of the bots. (For example, if a test's JSON say that there was a\nfailure but this isn't similarly reflected in ResultDB, then the build _will not\nfail._\n\n## ResultDB API & ResultSink\n\nAll test harnesses are responsible for uploading their results to ResultDB. This\nis most easily done via ResultSink (see [internal documentation]). ResultSink is\na local HTTP server that proxies requests to ResultDB's backend. It can be\nlaunched via the `rdb stream ...` command and listens on a port specified in a\nfile pointed to by the `LUCI_CONTEXT` environment variable. This server\ncurrently runs in the background on all test bots.\n\nOn a machine with the server running, a test can report its results to ResultDB\nby making JSON-formatted RPCs to the local HTTP ResultSink server. See\n[ResultSink's API] for more details.\n\n## ResultSink integration/wrappers within Chromium\n\nThere are several libraries used within Chromium that have ResultSink\nintegration:\n\n- **Web Tests**: Blink's web tests upload their results through ResultSink via\n  [test_result_sink.py].\n\n- **typ**: Typ is a testing library used by performance benchmarks and GPU\n  tests. It integrates with ResultSink in [result_sink.py].\n\n- **//build/util/lib/results/**: [//build/util/lib/results/] contains a generic\n  wrapper around ResultSink. This is used by both the Android and ChromeOS test\n  runners to upload their results to ResultDB.\n\n- **iOS test runner**: The iOS test runner has its own ResultSink integration in\n  [result_sink_util.py].\n\n- **result_adapter**: Most remaining tests use the `result_adapter` tool. See\n  below for more details.\n\n## result_adapter\n\n[result_adapter] is a command-line tool that wraps a test invocation and will\nautomatically convert the test's output JSON to the ResultSink format and\nuploads those results to ResultDB via ResultSink. Known JSON formats include:\n\n- `gtest` for the JSON generated by running GTests using the support code in\n  //base/test/launcher/, specifically the `SaveSummaryAsJSON` function in\n  [test_results_tracker.h].\n\n- `json` for the JSON format described in [json_test_results_format.md]. Tests\n  that can generate this type of JSON include Blink web tests and typ-supported\n  tests.\n\nThough it eased the migration of results into ResultDB, using result_adapter has\na few drawbacks:\n\n- result_adapter uploads all results at once after the test invocation exits.\n  If the test execution crashes partway through, ResultDB will not track any of\n  the results that were successfully produced.\n\n- result_adapter is limited by the JSON format of the test. It would be unable\n  to use any new or advanced feature in ResultDB.\n\nConsequently, it's preferred when possible for new test types and new test\nharnesses to integrate with ResultSink directly rather than using\nresult_adapter. But if circumstances make integration difficult (e.g. we don't\nhave access to the test harness implementation) result_adapter can be needed.\n\n### Specifying integration in //testing/buildbot/\n\nThe *.pyl spec files in [//testing/buildbot/] control what tests a given bot\nruns. These testing specs also control how result_adapter is used. By default, a\n`isolated_scripts` test will have result_adapter added using the `json`\noutput format, and a `gtest_tests` test will have result_adapter added using the\n`gtest` output format. This can be overwritten using the\n`has_native_resultdb_integration` [mixin] which will disable result_adapter for\nthat test. Additionally, a custom `result_format` can be specified for a test to\noverwrite the expected format of the JSON: [example].\n\n[public source]: https://source.chromium.org/chromium/infra/infra/+/main:go/src/go.chromium.org/luci/resultdb/\n[Google internal documentation]: http://shortn/_bTdqm8VDXz\n[internal documentation]: http://shortn/_zAbl5fa84c\n[ResultSink's API]: https://source.chromium.org/chromium/infra/infra/+/main:go/src/go.chromium.org/luci/resultdb/sink/proto/v1/sink.proto;drc=54f060e7452368ff982d9c66f2c1001bf4fa7394;l=24\n[test_result_sink.py]: https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/tools/blinkpy/web_tests/controllers/test_result_sink.py\n[result_sink.py]: https://source.chromium.org/chromium/chromium/src/+/main:third_party/catapult/third_party/typ/typ/result_sink.py\n[//build/util/lib/results/]: https://source.chromium.org/chromium/chromium/src/+/main:build/util/lib/results/\n[result_sink_util.py]: https://source.chromium.org/chromium/chromium/src/+/main:ios/build/bots/scripts/result_sink_util.py\n[result_adapter]: https://source.chromium.org/chromium/infra/infra/+/main:go/src/infra/tools/result_adapter/\n[test_results_tracker.h]: https://source.chromium.org/chromium/chromium/src/+/main:base/test/launcher/test_results_tracker.h;drc=96020cfd447cb285acfa1a96c37a67ed22fa2499;l=83\n[json_test_results_format.md]: json_test_results_format.md\n[//testing/buildbot/]: https://source.chromium.org/chromium/chromium/src/+/main:testing/buildbot/\n[mixin]: https://source.chromium.org/chromium/chromium/src/+/main:testing/buildbot/mixins.pyl;drc=d0985a69618056e95d64b48803ca90e3ae6a6c77;l=453\n[example]: https://source.chromium.org/chromium/chromium/src/+/main:testing/buildbot/test_suites.pyl;drc=9ef43df31342fb0fc854de5233b7170039028bc1;l=1499\n"
  },
  {
    "path": "development/testing/on_disabling_tests",
    "title": "On disabling tests",
    "content": "# On disabling tests\n\nSometimes you don't want to run a test that you've written (or that\nyou've imported, like conformance tests). The test might not be possible to\nrun in a particular configuration, or be temporarily broken by another\nchange, or be flaky, or simply not work yet. In these cases (and perhaps others),\nyou should disable the test :).\n\nThere are a number of different ways to do so:\n\n*   If the test is an entire binary or test suite, the first (and\n    simplest) way is to simply not build (or build, but not run)\n    the test binary, of course. This makes sense for binaries that\n    are specific to particular build configurations (e.g., Android JUnit\n    tests don't need to be built on Windows).\n\n*   A second way (for tests in C++) is to not compile a test in a\n    given configuration, e.g., `#ifndef WIN`. In this situation, the only\n    way you would know the test existed and was disabled would be to\n    parse the source code. We often do this today for tests that will\n    never be enabled in particular build configurations, but sometimes we do\n    this to temporarily skip tests as well.\n\n*   A third way is to take advantage of features in your testing framework to\n    skip over tests. Examples include involve adding `DISABLED_` to the test\n    method name for GTest-based tests, `@unittest.skip` for Python-based tests,\n    or using the\n    [DisabledTest](../../base/test/android/javatests/src/org/chromium/base/test/DisabledTest.java)\n    annotation for JUnit-based Java tests (this works in both instrumentation\n    and Robolectric tests). In these cases, you don't run the test by default,\n    but you can determine the list of disabled tests at runtime because the\n    tests are present in the executable, and you may still be able to force the\n    test to be run via a command-line flag.\n\n*   Fourth, for test frameworks that support\n    [expectations files or filter files](https://bit.ly/chromium-test-list-format),\n    you can use them to decide what to run and what to skip. This moves\n    the mechanisms out of the source code and into separate files; there are\n    advantages and disadvantages to this. The main advantage is that it\n    can make it easier to write tooling to disable tests, and the main\n    disadvantage is that it moves the mechanism away from the code it affects,\n    potentially making it harder to understand what's going on.\n\n*   Finally, the test harness can run the test, but the test itself\n    might detect at runtime that it should exit early for some reason\n    rather than actually executing the code paths you'd normally want to\n    test. For example, if you have a test for some code path that requires\n    a GPU, but there's no GPU on the machine, the test might check for a\n    GPU and exit early with \"success\".\n\nIf you want to be able to determine a global picture of which tests\nwere disabled, you can either parse BUILD files, expectations and filter\nfiles, and source code to try and figure that out, or require the tests be\npresent in test binaries (i.e., not compiled out) and then run the test\nbinaries in order to collect the lists of disabled tests and report them\nto a central system.\n\nParsing code can be straightforward for some types of tests, but\ndifficult-to-impractical to do correctly for others.\n"
  },
  {
    "path": "development/testing/navbar",
    "title": "Testing in Chromium",
    "content": "# Testing in Chromium\n\n* [Chromium Docs Home](/docs/README.md)\n* [Testing in Chromium](/docs/testing/testing_in_chromium.md)\n* [Web Tests](/docs/testing/web_tests.md)\n* [Android](/docs/testing/android_test_instructions.md)\n* [iOS](/docs/ios/testing.md)\n* [Code Coverage](/docs/testing/code_coverage.md)\n\n[home]: /docs/testing/testing_in_chromium.md\n"
  },
  {
    "path": "development/testing/linux_running_asan_tests",
    "title": "Running Chrome tests with AddressSanitizer (asan) and LeakSanitizer (lsan)",
    "content": "# Running Chrome tests with AddressSanitizer (asan) and LeakSanitizer (lsan)\n\nRunning asan/lsan tests requires changing the build and setting a few\nenvironment variables.\n\nChanges to args.gn (ie, `out/Release/args.gn`):\n\n```python\nis_asan = true\nis_lsan = true\n```\n\nSetting up environment variables and running the test:\n\n```sh\n$ export ASAN_OPTIONS=\"symbolize=1 external_symbolizer_path=./third_party/llvm-build/Release+Asserts/bin/llvm-symbolizer detect_leaks=1 detect_odr_violation=0\"\n$ export LSAN_OPTIONS=\"\"\n$ out/Release/browser_tests\n```\n\nStack traces (such as those emitted by `base::debug::StackTrace().Print()`) may\nnot be fully symbolized. The following snippet can symbolize them:\n\n```sh\n$ out/Release/browser_tests 2>&1 | ./tools/valgrind/asan/asan_symbolize.py\n```\n"
  },
  {
    "path": "development/testing/life_of_increasing_code_coverage",
    "title": "Life of Increasing Code Coverage",
    "content": "# Life of Increasing Code Coverage\n\nThe goal of this doc is to provide guidance on how to write better tests using\ncode coverage information rather than increase code coverage itself.\n\n1. Pay attention to **untested** code in both the\n[coverage dashboard](https://analysis.chromium.org/coverage/p/chromium) and\n[code coverage in Gerrit](code_coverage_in_gerrit.md) during code review.\n\n2. Is this dead code? If yes, draft a CL\n([example](https://chromium-review.googlesource.com/c/chromium/src/+/1550769))\nto remove it, otherwise, please go to step 3.\n\n3. Think about why the code is not covered by any test. Is it because it's\ntoo complicated to be testable? If yes, draft a CL to refactor the code and add\ntests ([example](https://chromium-review.googlesource.com/c/chromium/src/+/1558233)),\notherwise, please go to step 4.\n\n4. If the code is testable, but a test was forgotten, draft a CL to add\ntests for it ([example](https://chromium-review.googlesource.com/c/chromium/src/+/1447030)).\n\nAnytime you upload a CL to refactor or add tests, you can use\n[code coverage in Gerrit](code_coverage_in_gerrit.md) to help you verify the\npreviously untested code is now tested by your CL.\n\nPlease refer to [code_coverage.md](code_coverage.md) for how code coverage works\nin Chromium in general.\n\n### Contacts\n\nFor any breakage report and feature requests, please\n[file a bug](https://bugs.chromium.org/p/chromium/issues/entry?components=Infra%3ETest%3ECodeCoverage).\n\nFor questions and general discussions, please join\n[code-coverage group](https://groups.google.com/a/chromium.org/forum/#!forum/code-coverage).\n"
  },
  {
    "path": "development/testing/json_test_results_format",
    "title": "The JSON Test Results Format",
    "content": "# The JSON Test Results Format\n\n*** note\nWarning: The JSON test result format no longer affects the pass-fail decisions\nmade by Chrome's bots. All results are now fetched from ResultDB. For more\ninfo, see [resultdb.md](resultdb.md).\n***\n\nThe JSON Test Results Format is a generic file format we use to record the\nresults of each individual test in test run (whether the test is run on a bot,\nor run locally).\n\n[TOC]\n\n## Introduction\n\nWe use these files on the bots in order to determine whether a test step had\nany failing tests (using a separate file means that we don't need to parse the\noutput of the test run, and hence the test can be tailored for human readability\nas a result). We also upload the test results to dashboards like the\n[Flakiness Dashboard](http://test-results.appspot.com).\n\nThe test format originated with the Blink web tests, but has since been\nadopted by GTest-based tests and Python unittest-based tests, so we've\nstandardized on it for anything related to tracking test flakiness.\n\n### Example\n\nHere's a very simple example for one Python test:\n\n    % python mojo/tools/run_mojo_python_tests.py --write-full-results-to results.json mojom_tests.parse.ast_unittest.ASTTest.testNodeBase\n    Running Python unit tests under mojo/public/tools/bindings/pylib ...\n    .\n    ----------------------------------------------------------------------\n    Ran 1 test in 0.000s\n\n    OK\n    % cat results.json\n    {\n      \"tests\": {\n        \"mojom_tests\": {\n          \"parse\": {\n            \"ast_unittest\": {\n              \"ASTTest\": {\n                \"testNodeBase\": {\n                  \"expected\": \"PASS\",\n                  \"actual\": \"PASS\",\n                  \"artifacts\": {\n                    \"screenshot\": [\"screenshots/page.png\"],\n                  }\n                }\n              }\n            }\n          }\n        }\n      },\n      \"interrupted\": false,\n      \"path_delimiter\": \".\",\n      \"version\": 3,\n      \"seconds_since_epoch\": 1406662283.764424,\n      \"num_failures_by_type\": {\n        \"FAIL\": 0,\n        \"PASS\": 1\n      },\n      \"artifact_types\": {\n        \"screenshot\": \"image/png\"\n      }\n    }\n\n\n\nAs you can see, the format consists of a one top level dictionary containing a\nset of metadata fields describing the test run, plus a single `tests` key that\ncontains the results of every test run, structured in a hierarchical trie format\nto reduce duplication of test suite names (as you can see from the deeply\nhierarchical Python test name).\n\nThe file is strictly JSON-compliant. As a part of this, the fields in each\nobject may appear in any order.\n\n## Top-level field names\n\n| Field Name | Data Type | Description |\n|------------|-----------|-------------|\n| `interrupted` | boolean | **Required.** Whether the test run was interrupted and terminated early (either via the runner bailing out or the user hitting ctrl-C, etc.) If true, this indicates that not all of the tests in the suite were run and the results are at best incomplete and possibly totally invalid. |\n| `num_failures_by_type` |  dict | **Required.** A summary of the totals of each result type. If a test was run more than once, only the first invocation's result is included in the totals. Each key is one of the result types listed below. A missing result type is the same as being present and set to zero (0). |\n| `path_delimiter` | string | **Optional, will be mandatory.** The separator string to use in between components of a tests name; normally \".\" for GTest- and Python-based tests and \"/\" for web tests; if not present, you should default to \"/\" for backwards-compatibility.  |\n| `seconds_since_epoch` | float | **Required.** The start time of the test run expressed as a floating-point offset in seconds from the UNIX epoch. |\n| `tests` | dict | **Required.** The actual trie of test results. Each directory or module component in the test name is a node in the trie, and the leaf contains the dict of per-test fields as described below. |\n| `version` | integer | **Required.** Version of the file format. Current version is 3. |\n| `artifact_types` | dict | **Optional. Required if any artifacts are present for any tests.** MIME Type information for artifacts in this json file. All artifacts with the same name must share the same MIME type.  |\n| `artifact_permanent_location` | string | **Optional.** The URI of the root location where the artifacts are stored. If present, any artifact locations are taken to be relative to this location. Currently only the `gs://` scheme is supported. |\n| `build_number` | string | **Optional.** If this test run was produced on a bot, this should be the build number of the run, e.g., \"1234\". |\n| `builder_name` | string | **Optional.** If this test run was produced on a bot, this should be the builder name of the bot, e.g., \"Linux Tests\". |\n| `metadata` | dict | **Optional.** It maps to a dictionary that contains all the key value pairs used as metadata. This dictionary also includes the tags, test name prefix and test expectations file paths used during a test run. |\n| `chromium_revision` | string | **Optional.** The revision of the current Chromium checkout, if relevant, e.g. \"356123\". |\n| `has_pretty_patch` | bool | **Optional, layout test specific, deprecated.** Whether the web tests' output contains PrettyDiff-formatted diffs for test failures. |\n| `has_wdiff` | bool | **Optional, layout test specific, deprecated.** Whether the web tests' output contains wdiff-formatted diffs for test failures. |\n| `layout_tests_dir` | string | **Optional, layout test specific.** Path to the web_tests directory for the test run (used so that we can link to the tests used in the run). |\n| `pixel_tests_enabled` | bool | **Optional, layout test specific.** Whether the web tests' were run with the --pixel-tests flag.  |\n| `flag_name` | string | **Optional, layout test specific.** The flags used when running tests|\n| `fixable` | integer | **Optional, deprecated.** The number of tests that were run but were expected to fail. |\n| `num_flaky` | integer | **Optional, deprecated.** The number of tests that were run more than once and produced different results each time. |\n| `num_passes` | integer | **Optional, deprecated.** The number of successful tests; equivalent to `num_failures_by_type[\"Pass\"]` |\n| `num_regressions` | integer | **Optional, deprecated.** The number of tests that produced results that were unexpected failures. |\n| `skips` | integer | **Optional, deprecated.** The number of tests that were found but not run (tests should be listed in the trie with \"expected\" and \"actual\" values of `SKIP`). |\n\n## Per-test fields\n\nEach leaf of the `tests` trie contains a dict containing the results of a\nparticular test name. If a test is run multiple times, the dict contains the\nresults for each invocation in the `actual` field. Unless otherwise noted,\nif the test is run multiple times, all of the other fields represent the\noverall / final / last value. For example, if a test unexpectedly fails and\nthen is retried and passes, both `is_regression` and `is_unexpected` will be false).\n\n|  Field Name | Data Type | Description |\n|-------------|-----------|-------------|\n|  `actual` | string | **Required.** An ordered space-separated list of the results the test actually produced. `FAIL PASS` means that a test was run twice, failed the first time, and then passed when it was retried. If a test produces multiple different results, then it was actually flaky during the run. |\n|  `expected` | string | **Required.** An unordered space-separated list of the result types expected for the test, e.g. `FAIL PASS` means that a test is expected to either pass or fail. A test that contains multiple values is expected to be flaky. |\n|  `artifacts` | dict | **Optional.** A dictionary describing test artifacts generated by the execution of the test. The dictionary maps the name of the artifact (`screenshot`, `crash_log`) to a list of relative locations of the artifact (`screenshot/page.png`, `logs/crash.txt`). Any '/' characters in the file paths are meant to be platform agnostic; tools will replace them with the appropriate per platform path separators. There is one entry in the list per test execution. If `artifact_permanent_location` is specified, then this location is relative to that path. Otherwise, the path is assumed to be relative to the location of the json file which contains this (i.e., `$ISOLATED_OUTDIR`). The actual locations of artifacts are implementation-defined by the test program and can follow any convention, since these entries will allow them to be looked up easily. |\n|  `bugs` | string | **Optional.** A comma-separated list of URLs to bug database entries associated with each test. |\n|  `shard` | int | **Optional.** The 0-based index of the shard that the test ran on, if the test suite was sharded across multiple bots. |\n|  `is_flaky` | bool | **Optional.** If present and true, the test was run multiple times and produced more than one kind of result. If false (or if the key is not present at all), the test either only ran once or produced the same result every time. |\n|  `is_regression` | bool | **Optional.** If present and true, the test failed unexpectedly. If false (or if the key is not present at all), the test either ran as expected or passed unexpectedly. |\n|  `is_unexpected` | bool | **Optional.** If present and true, the test result was unexpected. This might include an unexpected pass, i.e., it is not necessarily a regression. If false (or if the key is not present at all), the test produced the expected result. |\n|  `time` | float | **Optional.** If present, the time it took in seconds to execute the first invocation of the test. |\n|  `times` | array of floats | **Optional.** If present, the times in seconds of each invocation of the test. |\n|  `has_repaint_overlay` | bool | **Optional, web test specific.** If present and true, indicates that the test output contains the data needed to draw repaint overlays to help explain the results (only used in layout tests). |\n|  `is_missing_audio` | bool | **Optional, we test specific.** If present and true, the test was supposed to have an audio baseline to compare against, and we didn't find one. |\n|  `is_missing_text` | bool | **Optional, web test specific.** If present and true, the test was supposed to have a text baseline to compare against, and we didn't find one.  |\n|  `is_missing_video` | bool | **Optional, web test specific.** If present and true, the test was supposed to have an image baseline to compare against and we didn't find one. |\n|  `is_testharness_test` | bool | **Optional, web test specific.** If present, indicates that the layout test was written using the w3c's test harness and we don't necessarily have any baselines to compare against. |\n|  `is_slow_test` | bool | **Optional, web test specific.** If present and true, the test is expected to take longer than normal to run. |\n|  `reftest_type` | string | **Optional, web test specific.** If present, one of `==` or `!=` to indicate that the test is a \"reference test\" and the results were expected to match the reference or not match the reference, respectively (only used in layout tests). |\n\n## Test result types\n\nAny test may fail in one of several different ways. There are a few generic\ntypes of failures, and the web tests contain a few additional specialized\nfailure types.\n\n|  Result type | Description |\n|--------------|-------------|\n|  `CRASH` | The test runner crashed during the test. |\n|  `FAIL` | The test did not run as expected. |\n|  `PASS` | The test ran as expected. |\n|  `SKIP` | The test was not run. |\n|  `TIMEOUT` | The test hung (did not complete) and was aborted. |\n|  `AUDIO` | **Web test specific, deprecated.** The test is expected to produce audio output that doesn't match the expected result. Normally you will see `FAIL` instead. |\n|  `IMAGE` | **Web test specific, deprecated.** The test produces image (and possibly text output). The image output doesn't match what we'd expect, but the text output, if present, does. Normally you will see `FAIL` instead. |\n|  `IMAGE+TEXT` | **Web test specific, deprecated.** The test produces image and text output, both of which fail to match what we expect. Normally you will see `FAIL` instead. |\n|  `LEAK` | **Web test specific, deprecated.** Memory leaks were detected during the test execution. |\n|  `MISSING` | **Web test specific, deprecated.** The test completed but we could not find an expected baseline to compare against. |\n|  `NEEDSREBASELINE` | **Web test specific, deprecated.** The expected test result is out of date and will be ignored (as above); the auto-rebaseline-bot will look for tests of this type and automatically update them. This should never show up as an `actual` result. |\n|  `REBASELINE`  | **Web test specific, deprecated.** The expected test result is out of date and will be ignored (any result other than a crash or timeout will be considered as passing). This test result should only ever show up on local test runs, not on bots (it is forbidden to check in a TestExpectations file with this expectation). This should never show up as an \"actual\" result. |\n|  `SLOW` | **Web test specific, deprecated.** The test is expected to take longer than normal to run. This should never appear as an `actual` result, but may (incorrectly) appear in the expected fields. |\n|  `TEXT` | **Web test specific, deprecated.** The test is expected to produce a text-only failure (the image, if present, will match). Normally you will see `FAIL` instead. |\n\nUnexpected results, failures, and regressions are different things.\n\nAn unexpected result is simply a result that didn't appear in the `expected`\nfield. It may be used for tests that _pass_ unexpectedly, i.e. tests that\nwere expected to fail but passed. Such results should _not_ be considered\nfailures.\n\nAnything other than `PASS`, `SKIP`, `SLOW`, or one of the REBASELINE types is\nconsidered a failure.\n\nA regression is a result that is both unexpected and a failure.\n\n## `full_results.json` and `failing_results.json`\n\nThe web tests produce two different variants of the above file. The\n`full_results.json` file matches the above definition and contains every test\nexecuted in the run. The `failing_results.json` file contains just the tests\nthat produced unexpected results, so it is a subset of the `full_results.json`\ndata. The `failing_results.json` file is also in the JSONP format, so it can be\nread via as a `<script>` tag from an html file run from the local filesystem\nwithout falling prey to the same-origin restrictions for local files.  The\n`failing_results.json` file is converted into JSONP by containing the JSON data\npreceded by the string \"ADD_RESULTS(\" and followed by the string \");\", so you\ncan extract the JSON data by stripping off that prefix and suffix.\n"
  },
  {
    "path": "development/testing/ipc_fuzzer",
    "title": "IPC Fuzzer",
    "content": "# IPC Fuzzer\n\nA Chromium IPC fuzzer is under development by aedla and tsepez. The fuzzer lives\nunder `src/tools/ipc_fuzzer/` and is running on ClusterFuzz. A previous version\nof the fuzzer was a simple bitflipper, which caught around 10 bugs. A new\nversion is doing smarter mutations and generational fuzzing. To do so, each\n`ParamTraits<Type>` needs a corresponding `FuzzTraits<Type>`. Feel free to\ncontribute.\n\n[TOC]\n\n## Working with the fuzzer\n\n### Build instructions\n\n*   Run `gn args` and add `enable_ipc_fuzzer = true` to your args.gn.\n*   build `ipc_fuzzer_all` target\n*   component builds are currently broken, sorry\n*   Debug builds are broken; only Release mode works.\n\n### Replaying ipcdumps\n\n*   `tools/ipc_fuzzer/scripts/play_testcase.py path/to/testcase.ipcdump`\n*   more help: `tools/ipc_fuzzer/scripts/play_testcase.py -h`\n\n### Listing messages in ipcdump\n\n*   `out/<Build>/ipc_message_util --dump path/to/testcase.ipcdump`\n\n### Updating fuzzers in ClusterFuzz\n\n*   `tools/ipc_fuzzer/scripts/cf_package_builder.py`\n*   upload `ipc_fuzzer_mut.zip` and `ipc_fuzzer_gen.zip` under build directory\n    to ClusterFuzz\n\n### Contributing FuzzTraits\n\n*   add them to `tools/ipc_fuzzer/fuzzer/fuzzer.cc`\n*   thanks!\n\n## Components\n\n### ipcdump logger\n\n*   add `enable_ipc_fuzzer = true` to `args.gn`\n*   build `chrome` and `ipc_message_dump` targets\n*   run chrome with\n    `--no-sandbox --ipc-dump-directory=/path/to/ipcdump/directory`\n*   ipcdumps will be created in this directory for each renderer using the\n    format `_pid_.ipcdump`\n\n### ipcdump replay\n\nLives under `ipc_fuzzer/replay`. The renderer is replaced with\n`ipc_fuzzer_replay` using `--renderer-cmd-prefix`. This is done automatically\nwith the `ipc_fuzzer/play_testcase.py` convenience script.\n\n### ipcdump mutator / generator\n\nLives under `ipc_fuzzer/fuzzer`. This is the code that runs on ClusterFuzz. It\nuses `FuzzTraits<Type>` to mutate ipcdumps or generate them out of thin air.\n\n## Problems, questions, suggestions\n\nSend them to mbarbella@chromium.org.\n"
  },
  {
    "path": "development/testing/identifying_tests_that_depend_on_order",
    "title": "Fixing web test flakiness",
    "content": "\n# Fixing web test flakiness\n\nWe'd like to stamp out all the tests that have ordering dependencies. This helps\nmake the tests more reliable and, eventually, will make it so we can run tests\nin a random order and avoid new ordering dependencies being introduced. To get\nthere, we need to weed out and fix all the existing ordering dependencies.\n\n## Diagnosing test ordering flakiness\n\nThese are steps for diagnosing ordering flakiness once you have a test that you\nbelieve depends on an earlier test running.\n\n### Bisect test ordering\n\n1. Run the tests such that the test in question fails.\n2. Run `./tools/print_web_test_ordering.py` and save the output to a file. This\n   outputs the tests run in the order they were run on each content_shell\n   instance.\n3. Create a file that contains only the tests run on that worker in the same\n   order as in your saved output file. The last line in the file should be the\n   failing test.\n4. Run\n   `./tools/bisect_web_test_ordering.py --test-list=path/to/file/from/step/3`\n\nThe bisect_web_test_ordering.py script should spit out a list of tests at the\nend that causes the test to fail.\n\n*** promo\nAt the moment bisect_web_test_ordering.py only allows you to find tests that\nfail due to a previous test running. It's a small change to the script to make\nit work for tests that pass due to a previous test running (i.e. to figure out\nwhich test it depends on running before it). Contact ojan@chromium if you're\ninterested in adding that feature to the script.\n***\n\n### Manual bisect\n\nInstead of running `bisect_web_test_ordering.py`, you can manually do the work\nof step 4 above.\n\n1. `run_web_tests.py --child-processes=1 --order=none --test-list=path/to/file/from/step/3`\n2. If the test doesn't fail here, then the test itself is probably just flaky.\n   If it does, remove some lines from the file and repeat step 1. Continue\n   repeating until you've found the dependency. If the test fails when run by\n   itself, but passes on the bots, that means that it depends on another test to\n   pass. In this case, you need to generate the list of tests run by\n   `run_web_tests.py --order=natural` and repeat this process to find which test\n   causes the test in question to *pass* (e.g.\n   [crbug.com/262793](https://crbug.com/262793)).\n3. File a bug and give it the\n   [LayoutTestOrdering](https://crbug.com/?q=label:LayoutTestOrdering) label,\n   e.g. [crbug.com/262787](https://crbug.com/262787) or\n   [crbug.com/262791](https://crbug.com/262791).\n\n### Finding test ordering flakiness\n\n#### Run tests in a random order and diagnose failures\n\n1. Run `run_web_tests.py --order=random --no-retry`\n2. Run `./tools/print_web_test_ordering.py` and save the output to a file. This\n   outputs the tests run in the order they were run on each content_shell\n   instance.\n3. Run the diagnosing steps from above to figure out which tests\n\nRun `run_web_tests.py --run-singly --no-retry`. This starts up a new\ncontent_shell instance for each test. Tests that fail when run in isolation but\npass when run as part of the full test suite represent some state that we're not\nproperly resetting between test runs or some state that we're not properly\nsetting when starting up content_shell. You might want to run with\n`--timeout-ms=60000` to weed out tests that timeout due to waiting on\ncontent_shell startup time.\n\n#### Diagnose especially flaky tests\n\n1. Load\n   https://test-results.appspot.com/dashboards/overview.html#group=%40ToT%20Blink&flipCount=12\n2. Tweak the flakiness threshold to the desired level of flakiness.\n3. Click on *webkit_tests* to get that list of flaky tests.\n4. Diagnose the source of flakiness for that test.\n"
  },
  {
    "path": "development/testing/how_to_repro_bot_failures",
    "title": "How to repro bot failures",
    "content": "# How to repro bot failures\n\nIf you're looking for repro a CI/CQ compile or test bot failures locally, then\nthis is the right doc. This doc tends to getting you to the exact same\nenvironment as bot for repro. Most likely you don't need to follow the exact\nsteps here. But if you have a hard time for repro, then it's good to check each\nsteps. Also keep in mind that even you have the exact same environment, some\nfailures are not reproduceable locally.\n\nWhen trying to repro and fix a bot failure, the easier approach is tryjobs, or\n[debug with swarming](../workflow/debugging-with-swarming.md).\nThis doc is more useful when it requires more local debugging and testing.\n\nThis doc assumes you're familiar with Chrome development on at least 1 platform.\n\n## Identify the platform\n\nUsually this is easy by looking at the builder name. e.g. linux-rel means it's\non linux platform.\n\n## Understand the platform system requirements\n\nUsually you should be able to follow\n[Chromium Get Code](https://www.chromium.org/developers/how-tos/get-the-code/)\nand understand the platform.\n\n## Prepare the compile and test devices\n\nFor compile devices, if you see a \"compilator steps\"(usually for CQ jobs),\n\n![compilator step]\n\nthen\nclick the \"compilator build\" and then bot on the right.\n\n![bot link]\n\nOtherwise just click bot on right.\n\nFor test devices, for most builders, click the shard and then click on bot.\n\n![test shard]\n\nThe bot page would look like this\n\n![bot page]\n\nYou would want to prepare the exact same environment when possible. e.g.\nmany tests are using e2-standard-8 GCE bot, you will want to create a\ne2-standard-8 GCE VM. If you see python 3.8, then install python 3.8.\n\n## Checkout code\n\nFollow the platform specific workflow to checkout the code.\n\n## Revision\n\nOn bot, you can see revision like this.\n\n![revision]\n\nFirst sync to that revision. For tryjobs, you also need to cherry-pick the\nchangelist with correct patchset after sync to correct revision.\n\nNote: Don't just download a patch from gerrit to repro tryjob failures.\nOn bot, it will first checkout the branch HEAD, then patch the patchset.\nBut on gerrit, what you see is your local revision when you upload with\nthe patchset. On gerrit you see code at an older base revision.\n\n## Change gclient config\nOn bot, there is a step named 'gclient config'.\n\n![gclient]\n\nApply the same gclient config and then rerun 'gclient sync'.\n\n## gn args\n\nOn bot, there is a step named 'lookup GN args'.\n\n![gn args]\n\nApply the same gn args locally.\nNote: most of the time, some path based flags can be omitted. Examples:\n* coverage_instrumentation_input_file\n\nAt the time of writing, some gn args can't be applied locally, like\nuse_remoteexec.\n\nNote: ChromeOS builders(or any platform that involves 'import' gn rules)\nmay show complicated gn args. For these builders, you need to search the\nbuilder name in //tools/mb or //internal/tools/mb for internal builders to\nfind out the gn args.\n\n## Compile\n\nOn bot, there is a step named 'compile'. Go to 'execution_details' and compile\nall the targets listed in there.\n\n## Run test\n\nMost of the time, on bot the compile device is not the same as the test device.\nBut locally for desktop platforms, they are the same device and in the same\n//out folder. There is no easy way to mimic bot behavior. A common issue on bot\nis you see some missing files but can't repro locally. This is because on bot\nwe calculate what files are needed for testing using build maps and copy them\nfrom compile machine to test machine. But locally you don't have this step.\nUsually the fix is to add some 'data' or 'data_deps' in BUILD.gn.\n\nOpen the bot test result page\n\n![test page]\n\nYou should see all the information you need including system environment and\ncommandlines.\n\nNote: Set system environment. Especially important for gtest GTEST_SHARD_INDEX\nGTEST_TOTAL_SHARDS.\n\nNote: On bot, the CURRENT_DIR is in the //out dir. Say you compile test target\nin //out/Default, then enter //out/Default and then run tests.\n\nNote: For gtest, if you're not using the same machine type, you might want to\noverride some flags like --test-launcher-jobs.\n\n[bot link]: images/bot_repro_bot_link.png\n[bot page]: images/bot_repro_bot_page.png\n[compilator step]: images/bot_repro_compilator_step.png\n[gclient]: images/bot_repro_gclient.png\n[gn args]: images/bot_repro_gn_args.png\n[revision]: images/bot_repro_revision.png\n[test page]: images/bot_repro_test_page.png\n[test shard]: images/bot_repro_test_shard.png\n"
  },
  {
    "path": "development/testing/gtest_flake_tips",
    "title": "Addressing Flaky GTests",
    "content": "# Addressing Flaky GTests\n\n## Understanding builder results\n\n[LUCI Analysis](https://luci-analysis.appspot.com/p/chromium/clusters) lists the\ntop flake clusters of tests along with any associated bug and failure counts in\ndifferent contexts.\n\n## Reproducing the flaky test\n\nIf debugging via bot is too slow or you otherwise need to drill further into the\ncause of the flake, you can try to reproduce the flake locally. Reproducing the\nflake can be difficult, so it can help to try and replicate the test environment\nas closely as possible.\n\nCopy the gn args from one of the bots where the flake occurs, and try to choose\na bot close to your system, i.e. linux-rel if you're building on linux. To get\nthe gn args, you can again click on the timestamp in the flake portal to view\nthe bot run details, and search for the \"lookup GN args\" build step to copy the\nargs.\n\n![bot_gn_args]\n\nBuild and run the test locally. Depending on the frequency of the flake, it may\ntake some time to reproduce. Some helpful flags:\n - --gtest_repeat=100\n - --gtest_also_run_disabled_tests (if the flaky test(s) you're looking at have\nbeen disabled)\n\nIf you're unable to reproduce the flake locally, you can also try uploading your\npatch with the debug logging and flaky test enabled to try running the bot to\nreproduce the flake with more information.\n\nAnother good solution is to use\n*Swarming* -- which will let you mimic bot conditions to better reproduce flakes\nthat actually occur on CQ bots.\n\n### Swarming\nFor a more detailed dive into swarming you can follow this\n[link](https://chromium.googlesource.com/chromium/src/+/master/docs/workflow/debugging-with-swarming.md#authenticating).\n\nAs an example, suppose we have built Chrome using the GN args from\nabove into a directory `out/linux-rel`, then we can simply run this command\nwithin the `chromium/src` directory:\n\n```\ntools/run-swarmed.py out/linux-rel browser_tests -- --gtest_filter=\"*<YOUR_TEST_NAME_HERE>*\" --gtest_repeat=20 --gtest_also_run_disabled_tests\n```\n\nThis allows us to quickly iterate over errors using logs to reproduce flakes and\neven fix them!\n\n>TODO: Add more tips for reproducing flaky tests\n\n## Debugging the flaky test\n\nIf the test is flakily timing out, consider any asynchronous code that may cause\nrace conditions, where the test subject may early exit and miss a callback, or\nreturn faster than the test can start waiting for it (i.e. make sure event\nlisteners are spawned before invoking the event). Make sure event listeners are\nfor the proper event instead of a proxy (e.g. [Wait for the correct event in\ntest](https://chromium.googlesource.com/chromium/src/+/6da09f7510e94d2aebbbed13b038d71c511d6cbc)).\n\nConsider possible bugs in the system or test infrastructure (e.g. [races in\nglibc](https://bugs.chromium.org/p/chromium/issues/detail?id=1010318)).\n\nFor browsertest flakes, consider possible inter-process issues, such as the\nrenderer taking too long or returning something unexpected (e.g. [flaky\nRenderFrameHostImplBrowserTest](https://bugs.chromium.org/p/chromium/issues/detail?id=1120305)).\n\nFor browsertest flakes that check EvalJs results, make sure test objects are not\ndestroyed before JS may read their values (e.g. [flaky\nPaymentAppBrowserTest](https://chromium.googlesource.com/chromium/src/+/6089f3480c5036c73464661b3b1b6b82807b56a3)).\n\nFor browsertest flakes that involve dialogs or widgets, make sure that test\nobjects are not destroyed because focus is lost on the dialog (e.g [flaky AccessCodeCastHandlerBrowserTest](https://chromium-review.googlesource.com/c/chromium/src/+/3951132)).\n\n## Preventing similar flakes\n\nOnce you understand the problem and have a fix for the test, think about how the\nfix may apply to other tests, or if documentation can be improved either in the\nrelevant code or this flaky test documentation.\n\n\n[bot_gn_args]: images/bot_gn_args.png\n"
  },
  {
    "path": "development/testing/expectation_files",
    "title": "Expectation Files",
    "content": "# Expectation Files\n\nA number of test suites in Chromium use expectation files to handle test\nfailures in order to have more granular control compared to the usual approach\nof entirely disabling failing tests. This documentation goes into the general\nusage of expecation files, while suite-specific details are handled in other\nfiles.\n\n[TOC]\n\nCurrently, the test suites that use expectation files can be broadly categorized\nas Blink tests and GPU tests. Blink-specific documentation can be found\n[here][blink_expectation_doc], while GPU-specific documentation can be found\n[here][gpu_expectation_doc].\n\n[blink_expectation_doc]: https://source.chromium.org/chromium/chromium/src/+/main:docs/testing/web_test_expectations.md\n[gpu_expectation_doc]: https://source.chromium.org/chromium/chromium/src/+/main:docs/gpu/gpu_expectation_files.md\n\n## Design\n\nThe full design for the format can be found [here][chromium_test_list_format] if\nthe overview in this documentation is not sufficient.\n\n[chromium_test_list_format]: http://bit.ly/chromium-test-list-format\n\n## Code\n\nThe parser implementation used by Chromium can be found [here][typ_parser]. This\nhandles the parsing of the text files into Python objects usable by Chromium's\ntest harnesses.\n\n[typ_parser]: https://source.chromium.org/chromium/chromium/src/+/main:third_party/catapult/third_party/typ/typ/expectations_parser.py\n\n## Syntax\n\nAn expectation file can be broadly broken up into two sections: the header and\ntest expectations.\n\n### Header\n\nThe header consists of specially formatted comments that define what tags and\nexpected results are usable in expectations later in the file. All header\ncontent must be before any expectation content. Failure to do so will result in\nthe parser raising errors. An example header is:\n\n```\n# tags: [ linux ubuntu jammy\n#         mac mac10 mac11 mac12 mac13\n#         win win7 win10 ]\n# tags: [ release debug ]\n# results: [ Failure Skip Slow ]\n````\n\nSpecifically, the header consists of one or more tag sets and exactly one\nexpected result set.\n\n#### Tag Sets\n\nEach tag set begins with a `# tags:` comment followed by a space-separated list\nof tags between `[ ]`. Order does not matter to the parser, and tags are\ncase-insensitive. Tag sets can span multiple lines as long as each line starts\nwith `#` and all tags are within the brackets.\n\nEach tag set contains all the tags that can be used in expectations for a\nparticular aspect of a test configuration. In the example header, the first tag\nset contains values for operating systems, while the second tag set contains\nvalues for browser build type. Grouping tags together into different sets\ninstead of having a monolithic set with all possible tag values is necessary\nin order to handle conflicting expectation detection (explained later in\n[the conflict section](#Conflicts)).\n\nOne important note about tag sets is that unless a test harness is implementing\ncustom conflict detection logic, all tags within a set should be mutually\nexclusive, i.e. only one tag from each tag set should be produced when running a\ntest. Failure to do so can result in conflict detection false negatives, the\nspecifics of which are explained in [the conflict section](#Conflicts).\n\n#### Expected Result Set\n\nThe expected result set begins with a `# results:` comment followed by a\nspace-separated list of expected results between `[ ]`. Order does not matter to\nthe parser, but expected results are case sensitive. Additionally, only values\n[known to the parser][typ_known_results] can be used. The expected results can\nspan multiple lines as long as each line starts with `#` and all values are\nwithin the brackets.\n\nThe expected result set contains all the expected results that can be used in\nexpectations. The specifics of how each expected result affects test behavior\ncan differ slightly between test suites, but generally do the following:\n\n* Pass - The default expected result for all tests. Let the test run, and expect\n  it to run without issue.\n* Failure - Let the test run, but treat failures as a pass.\n* Crash - Let the test run, but treat test failures due to crashes as a pass.\n* Timeout - Let the test run, but treat test failures due to timeouts as a pass.\n* Skip - Do not run the test.\n* RetryOnFailure - Re-enable automatic retries of a test if a suite has them\n  disabled by default.\n* Slow - Indicate that the test is expected to take longer than normal, usually\n  as a signal to increase timeouts.\n\n[typ_known_results]: https://source.chromium.org/chromium/chromium/src/+/main:third_party/catapult/third_party/typ/typ/expectations_parser.py;l=40\n\n### Expectations\n\nAfter the header, the rest of the file consists of test expectations which\nspecify what non-standard test behavior is expected on specific test machine\nconfigurations. An expectation is a single line in the following format:\n\n```\nbug_identifier [ tags ] test_name [ expected_results ]\n```\n\nAs an example, the following would be an expectation specifying that the\n`foo.html` test is expected to fail on Windows machines with Debug browsers:\n\n```\ncrbug.com/1234 [ win debug ] foo.html [ Failure ]\n```\n\nThe bug identifier and tags are both optional and can be omitted. Not specifying\nany tags means that the expectation applies to the test regardless of where it\nis run. When omitting tags, the brackets are also omitted. Additionally,\nmultiple bug identifiers are allowed as long as they are space-separated. The\nparser looks for certain prefixes, e.g. `crbug.com/` to determine what is\nconsidered a bug. This allows the parser to properly disambiguate one or more\nbug identifiers from the test name in the event that an expectation does not\nhave any tags.\n\nMultiple expected results are allowed and are space-separated like tags. As an\nexample, `[ Failure Crash ]` would specify that the test is expected to either\nfail or crash.\n\nAdditionally, the test name is allowed to have up to one wildcard at the very\nend to match any tests that begin with the specified name. As an example, the\nfollowing would be an expectation specifying that any test starting with `foo`\nis expected to fail on Windows machines with Debug browsers.\n\n```\ncrbug.com/1234 [ win debug ] foo* [ Failure ]\n```\n\n#### Priority\n\nWhen using wildcards, it is possible for multiple expectations to apply to a\ntest at runtime. For example, given the following:\n\n```\n[ win ] foo* [ Slow ]\n[ win ] foo/bar* [ Failure ]\n[ win ] foo/bar/specific_test.html [ Skip ]\n```\n\n`foo/bar/specific_test.html` running on a Windows machine would have three\napplicable expectations. In these cases, the most specific (i.e. the\nlongest-named) expectation will be used.\n\nThe order in which expectations are defined is *not* considered when determining\npriority.\n\n## Conflicts\n\nWhen more than one expectation exists for a test, it is possible that there will\nbe a conflict where a test run on a particular test machine could have more than\none expectation apply to it. Whether these conflicts are treated as errors and\nhow conflicts get resolved are both configurable options via annotations found\nunder [the annotations section](#Annotations).\n\n### Detection\n\nTwo expectations for the same test conflict with each other if they do not use\ndifferent tags from at least one shared tag set. As an example, look at the\nfollowing expectations:\n\n```\n# Group 1\n[ win ] foo.html [ Failure ]\n[ mac ] foo.html [ Skip ]\n\n# Group 2\n[ win ] bar.html [ Failure ]\n[ debug ] bar.html [ Skip ]\n\n# Group 3\n[ linux ] foo.html [ Failure ]\n[ linux debug ] foo.html [ Skip ]\n```\n\nGroup 1 would not result in a conflict since both `win` and `mac` are from the\nsame tag set and are different values. Thus, the parser would be able to\ndetermine that at most one expectation will apply when running a test.\n\nGroup 2 would result in a conflict since there are no tag sets that both\nexpectations use, and thus there could be a test configuration that causes both\nexpectations to apply. In this case, a configuration that produces both the\n`win` and `debug` tags is possible. This conflict could be resolved by adding\na browser type tag to the first expectation or an operating system tag to the\nsecond expectation.\n\nGroup 3 would result in a conflict since there is a tag set that both\nexpectations use (operating system), but the exact tag is the same. Thus, a\ntest running on Linux with a Debug browser would have both expectations apply.\nThis conflict could be resolved by changing the first expectation to use\n`[ linux release ]`.\n\nIt is important to be aware of the following when it comes to conflicts:\n\n1. The expectation file has no knowledge of which tag combinations are actually\n   possible in the real world, only what is theoretically possible given the\n   defined tag sets. A real world example of this would be the use of the Metal\n   API, which is Mac-specific. While a human would be able to reason that\n   `[ metal ]` implies `[ mac metal ]`, the latter is necessary for the\n   conflict detection to work properly.\n2. If tag sets include non-mutually-exclusive values and the test suite has not\n   implemented custom conflict checking logic, there can be false negatives when\n   checking for conflicts. For example, if `win` and `win10` were both in the OS\n   tag set, `[ win ] foo.html [ Failure ]` and `[ win10 ] foo.html [ Skip ]`\n   would not be found to conflict even though they can in the real world due to\n   `win10` being a more specific version of `win`.\n3. Expectations that use wildcards can result in conflict detection false\n   negatives. Conflict detection is only run on expectations with identical test\n   names. Thus, while `[ win ] foo* [ Failure ]` and `[ debug ] foo* [ Skip ]`\n   would be found to conflict since the test name is `foo*` in both cases,\n   `[ win ] f* [ Failure ]` and `[ debug ] foo* [ Skip ]` would not be found to\n   conflict.\n\n### Annotations\n\nBy default, conflicts result in a parsing error. However, expectation files\nsupport several annotations to affect how conflicts are handled.\n\n`# conflicts_allowed: true` causes conflicts to no longer cause parsing errors.\nInstead, conflicts will be handled gracefully depending on the conflict\nresolution setting, the default of which is to take the union of expected\nresults.\n\n`# conflict_resolution: ` specifies how conflicts will be handled when they are\nallowed. Supported values are `union` (the default) and `override`. `union`\ncauses all conflicted expectations to be merged together. For example, the\nfollowing:\n\n```\n[ win ] foo.html [ Failure ]\n[ debug ] foo.html [ Slow ]\n```\n\nwould be equivalent to `[ win debug ] foo.html [ Failure Slow ]` when running on\na Windows machine with a Debug browser.\n\n`override` uses whatever expectation was parsed last. Using the above example,\nA Windows machine with a Debug browser would end up using the\n`[ debug ] foo.html [ Slow ]` expectation.\n"
  },
  {
    "path": "development/testing/code_coverage_in_gerrit",
    "title": "Code Coverage in Gerrit",
    "content": "# Code Coverage in Gerrit\n\nTests are critical because they find bugs and regressions, enforce better\ndesigns and make code easier to maintain. **Code coverage helps you ensure your\ntests are thorough**.\n\nChromium CLs can show a line-by-line breakdown of test coverage. **You can use\nit to ensure you only submit well-tested code**.\n\nTo see code coverage for a Chromium CL, **trigger a CQ dry run**, and once the\nbuilds finish and code coverage data is processed successfully, **look at the\nchange view to see absolute and incremental code coverage percentages**:\n\n![code_coverage_percentages]\n\nAbsolute coverage percentage is the percentage of lines covered by tests\nout of **all the lines** in the file, while incremental coverage percentage only\naccounts for **newly added or modified lines**. Both these coverage metrics, are further\nclassified into Unit Tests coverage(coverage from just unit tests) and All Tests coverage(covererd by all tests running in CQ, including unit tests). All Tests coverage is a superset of Unit Tests coverage.\n\nTo further dig into specific lines that are not covered by tests, **look at the\nright column of the side by side diff view, and specifically notice the\nbackground color of each line number**, where a light orange color indicates\nmissing coverage and a light blue color indicates existing coverage. Moreover\nhovering over the line number shows an informative tooltip with\n\"Not covered by tests\" or \"Covered by tests\" text respectively. It only shows\nAll Tests Coverage right now\n\n![code_coverage_annotations]\n\n**Code coverage data is shared between patchsets that are commit-message-edit or\ntrivial-rebase away**, however, if a newly uploaded patchset has\nnon-trivial code change, a new CQ dry run must be triggered before coverage data\nshows up again.\n\nThe code coverage tool supports coverage for C/C++, JAVA and Javascript on all major platforms(Linux, MacOS, Windows, Android, iOS and ChromeOS)\n\n## CLs Blocked Due to Low Coverage\nFor some teams in Chrome, we have turned on a coverage check, which blocks a CL from submission if the incremental coverage is below a preset threshold(default = 70%). CLs with insufficient test coverage have a `CodeCoverage-1` label added to them, which prevents them from being submitted. Also, a descriptive message is added to the CL, notifying developer of why the CL was blocked, and how to resolve it.\n![low_coverage_message]\n\nOnce the tests are added, another run of coverage builders (through CQ+1 or CQ+2) changes the label to `CodeCoverage+1`, allowing CLs to proceed with submission.\n\nTests themselves, as well as test-only files, are generally excluded from coverage checks based on their path or filename. If you are getting coverage warnings for test-related files themselves, check whether the files end in \"test\" or \"tests\" (for example, \"SomethingTest.java\" or \"something_unittests.cc\") or that their path contains a directory named exactly \"test\", \"tests\", or \"testing\". There is no manual list to which files can be added for long-term exclusion.\n\nDevs can also choose to bypass this block, in case they think they are being unfairly punished. They can do so by adding a *Low-Coverage-Reason: reason* footer to the change description. This should follow certain formatting constraints which are mentioned below\n\n### Mention the Bypass Category\n\nThe `reason` string should mention the category the bypass reason belongs to. For e.g. *Low-Coverage-Reason: TRIVIAL_CHANGE This change contains only minor cosmetic changes.* (TRIVIAL_CHANGE is the category)\n\nAvailable category choices are:\n* **TRIVIAL_CHANGE**: CL contains mostly minor changes e.g. renaming, file moves, logging statements, simple interface definitions etc.\n* **TESTS_ARE_DISABLED**: Corresponding tests exist, but they are currently disabled.\n* **TESTS_IN_SEPARATE_CL**: Developer plan to write tests in a separate CL (Should not be exercised often as per best practices)\n* **HARD_TO_TEST**: The code under consideration is hard to test. For e.g. Interfaces with system, real hardware etc.\n* **COVERAGE_UNDERREPORTED**: To be used when the developer thinks that tests exist, but corresponding coverage is missing.\n* **LARGE_SCALE_REFACTOR**: The current change is part of a large scale refactor. Should explain why the refactor shouldn't have tests.\n* **EXPERIMENTAL_CODE**: The current code is experimental and unlikely to be released to users.\n* **OTHER**: None of the above categories are the right fit\n\nIn case the developer doesn't specify the coverage category as prescribed, a warning will be shown in the UI, with details on how to fix\n![impropery_formatted_coverage_footer]\n\n### No empty line after the footer\nIn order for *Low-Coverage-Reason: reason* to work properly, it should occur after the last empty line in CL description, otherwise gerrit recognizes it as part of the commit message, rather than the footer i.e. Following would not work\n![empty_line_after_footer]\n\nRemoving the empty line should fix it\n![no_empty_line_after_footer]\n\n### Be careful with long footer strings\nEither keep the footer message in one line i.e. do not add line breaks; or if you do, add whitespace on new footer lines, otherwise [gerrit doesn’t parse them right]. e.g. a long footer message can be written as\n![long_footer]\nor\n![line_break_footer]\n\n## Contacts\n\n### Reporting problems\nFor any breakage report and feature requests, please [file a bug].\n\n### Mailing list\nFor questions and general discussions, please join [code-coverage group].\n\n## FAQ\n### Why is coverage not shown even though the try job finished successfully?\n\nThere are several possible reasons:\n* A particular source file/test may not be available on a particular project or\nplatform.\n* There is a bug in the pipeline. Please [file a bug] to report the breakage.\n\n### How does it work?\n\nPlease refer to [code_coverage.md] for how code coverage works in Chromium in\ngeneral, and specifically, for per-CL coverage in Gerrit, the\n[clang_code_coverage_wrapper] is used to compile and instrument ONLY the source\nfiles that are affected by the CL for the sake of performance and a\n[chromium-coverage Gerrit plugin] is used to display code coverage information\nin Gerrit.\n\n\n[choose_tryjobs]: images/code_coverage_choose_tryjobs.png\n[code_coverage_annotations]: images/code_coverage_annotations.png\n[code_coverage_percentages]: images/code_coverage_percentages.png\n[low_coverage_message]: images/low_coverage_message.png\n[empty_line_after_footer]: images/empty_line_after_footer.png\n[no_empty_line_after_footer]: images/no_empty_line_after_footer.png\n[long_footer]: images/long_footer.png\n[line_break_footer]: images/line_break_footer.png\n[impropery_formatted_coverage_footer]: images/improperly_formatted_coverage_footer.png\n[file a bug]: https://bugs.chromium.org/p/chromium/issues/entry?components=Infra%3ETest%3ECodeCoverage\n[code-coverage group]: https://groups.google.com/a/chromium.org/forum/#!forum/code-coverage\n[code_coverage.md]: code_coverage.md\n[clang_code_coverage_wrapper]: https://chromium.googlesource.com/chromium/src/+/main/docs/clang_code_coverage_wrapper.md\n[chromium-coverage Gerrit plugin]: https://chromium.googlesource.com/infra/gerrit-plugins/code-coverage/\n[gerrit doesn’t parse them right]: https://bugs.chromium.org/p/chromium/issues/detail?id=1459714#c9\n"
  },
  {
    "path": "development/testing/code_coverage",
    "title": "Code Coverage in Chromium",
    "content": "# Code Coverage in Chromium\n\n### Coverage Dashboard: [link](https://analysis.chromium.org/coverage/p/chromium)\n\nTable of contents:\n\n- [Coverage Infrastructure](#coverage-infra)\n  * [Coverage Builders](#coverage-builders)\n  * [Coverage Service](#coverage-service)\n  * [Coverage Clients](#coverage-clients)\n- [Local Coverage Script](#local-coverage-script)\n  * [Step 0 Download Tooling](#step-0-download-tooling)\n  * [Step 1 Build](#step-1-build)\n  * [Step 2 Create Raw Profiles](#step-2-create-raw-profiles)\n  * [Step 3 Create Indexed Profile](#step-3-create-indexed-profile)\n  * [Step 4 Create Coverage Reports](#step-4-create-coverage-reports)\n- [Read The Artifact](#read-the-artifact)\n  * [HTML report](#html-report)\n  * [lcov report](#lcov-report)\n- [Contacts](#contacts)\n- [FAQ](#faq)\n\n\nThis document is divided into two parts.\n- The first part introduces the code coverage infrastructure that\ncontinuously generates code coverage information for the whole codebase and for\nspecific CLs in Gerrit. For the latter, refer to\n[code\\_coverage\\_in\\_gerrit.md](code_coverage_in_gerrit.md).\n- The second part talks about how to generate code coverage locally for Clang-compiled languages like C++. Refer to [android code coverage instructions] for instructions for java code.\n\n## Coverage Infrastructure\n\n![coverage infra diagram]\n\nThere are 3 layers in the system:\n\n### Coverage Builders\n\nThe first layer is the LUCI builders that\n - build instrumented targets,\n - run the instrumented tests,\n - merge the results into single streams,\n - upload data to cloud storage.\n\nThere are two types of builder:\n\nCI Builder\n\nThe code coverage CI Builders periodically build all the test targets and fuzzer\ntargets for a given platform and instrument all available source files. Then\nsave the coverage data to a dedicated storage bucket.\n\nCQ Builder\n\nThe code coverage CQ builders instrument only the files changed for a given CL.\nMore information about per-cl coverage info in [this\ndoc](code_coverage_in_gerrit.md).\n\n### Coverage Service\n\nThe second layer in the system consists of an AppEngine application that\nconsumes the coverage data from the builders above, structures it and stores it\nin cloud datastore. It then serves the information to the clients below.\n\n### Coverage Clients\n\nIn the last layer we currently have two clients that consume the service:\n\n#### Coverage Dashboard\n\nThe [coverage dashboard] front end is hosted in the same application as the\nservice above.\nIt shows the full-code coverage reports with links to the builds that generated\nthem, as well as per-directory and per-component aggregation, and can be drilled\ndown to the single line of code level of detail.\n\nRefer to the following screenshots:\n\n##### Directory View\n\nSee coverage breakdown by directories (default landing page).\n\n![coverage dashboard directory view]\n\n##### Component View\n\nUse the view dropdown menu to switch between directory and component.\n\n![coverage dashboard component view]\n\n##### Source View\n\nClick on a particular source file in one of the views above to see line-by-line\ncoverage breakdown, and it's useful to identify:\n- Uncovered lines and code blocks that lack test coverage.\n- Potentially dead code. See [dead code example].\n- Hot spots in your code.\n\n![coverage dashboard file view]\n\n##### Project View\n\nClick on \"Previous Reports\" to check out the coverage history of the project.\n\n![coverage dashboard link to previous reports]\n\nList of historical coverage reports are in reverse chronological order.\n\n![coverage dashboard previous reports]\n\n#### Gerrit Coverage View\n\nThe other client supported at the moment is the gerrit plugin for code coverage.\n\n![gerrit coverage view]\n\nSee [this doc](code_coverage_in_gerrit.md) for information about the feature\nthat allows gerrit to display code coverage information generated for a given CL\nby CQ bot. Or see this\n[15-second video tutorial](https://www.youtube.com/watch?v=cxXlYcSgIPE).\n\n## Local Coverage Script\nThis [documentation] explains how to use Clang’s source-based coverage\nfeatures in general. The [coverage script] automates the process described below and provides a\none-stop service to generate code coverage reports locally in just one command.\n\nThis script is currently supported on Android, Linux, Mac, iOS and ChromeOS\nplatforms.\n\nHere is an example usage:\n\n```\n$ gn gen out/coverage \\\n    --args=\"use_clang_coverage=true is_component_build=false\n    dcheck_always_on=true is_debug=false\"\n$ python tools/code_coverage/coverage.py \\\n    crypto_unittests url_unittests \\\n    -b out/coverage -o out/report \\\n    -c 'out/coverage/crypto_unittests' \\\n    -c 'out/coverage/url_unittests --gtest_filter=URLParser.PathURL' \\\n    -f url/ -f crypto/\n```\nThe command above builds `crypto_unittests` and `url_unittests` targets and then\nruns them individually with their commands and arguments specified by the `-c` flag.\nFor `url_unittests`, it only runs the test `URLParser.PathURL`. The coverage report\nis filtered to include only files and sub-directories under `url/` and `crypto/`\ndirectories.\n\nAside from automating the process, this script provides visualization features to\nview code coverage breakdown by directories and by components, similar to the\nviews in the [coverage dashboard](#coverage-dashboard) above.\n\n## Workflow\nThis section presents the workflow of generating code coverage reports using two\nunit test targets in Chromium repo as an example: `crypto_unittests` and\n`url_unittests`, and the following diagram shows a step-by-step overview of the\nprocess.\n\n![code coverage generation workflow](images/code_coverage_workflow.png)\n\n### Step 0 Download Tooling\nGenerating code coverage reports requires llvm-profdata and llvm-cov tools.\nYou can get them by adding `\"checkout_clang_coverage_tools\": True,` to\n`custom_vars` in the `.gclient` config and run `gclient runhooks`. You can also\ndownload the tools manually ([tools link])\n\n### Step 1 Build\nIn Chromium, to compile code with coverage enabled, one needs to add\n`use_clang_coverage=true`, `is_component_build=false` and `is_debug=false` GN\nflags to the args.gn file in the build output directory. Under the hood, they\nensure `-fprofile-instr-generate` and `-fcoverage-mapping` flags are passed to\nthe compiler.\n\n```\n$ gn gen out/coverage \\\n    --args='use_clang_coverage=true is_component_build=false is_debug=false'\n$ gclient runhooks\n$ autoninja -C out/coverage crypto_unittests url_unittests\n```\n\n### Step 2 Create Raw Profiles\nThe next step is to run the instrumented binaries. When the program exits, it\nwrites a raw profile for each process. Because Chromium runs tests in\nmultiple processes, the number of processes spawned can be as many as a few\nhundred, resulting in the generation of a few hundred gigabytes’ raw\nprofiles. To limit the number of raw profiles, `%Nm` pattern in\n`LLVM_PROFILE_FILE` environment variable is used to run tests in multi-process\nmode, where `N` is the number of raw profiles. With `N = 4`, the total size of\nthe raw profiles are limited to a few gigabytes. (If working on Android, the\n.profraw files will be located in ./out/coverage/coverage by default.)\n\nAdditionally, we also recommend enabling the continuous mode by adding the `%c`\npattern to `LLVM_PROFILE_FILE`. The continuous mode updates counters in real\ntime instead of flushing to disk at process exit. This recovers coverage data\nfrom tests that exit abnormally (e.g. death tests). Furthermore, the continuous\nmode is required to recover coverage data for tests that run in sandboxed\nprocesses. For more information, see crbug.com/1468343.\n\n```\n$ export LLVM_PROFILE_FILE=\"out/report/crypto_unittests.%4m%c.profraw\"\n$ ./out/coverage/crypto_unittests\n$ ls out/report/\ncrypto_unittests.3657994905831792357_0.profraw\n...\ncrypto_unittests.3657994905831792357_3.profraw\n```\n\n### Step 3 Create Indexed Profile\nRaw profiles must be indexed before generating code coverage reports, and this\nis done using the `merge` command of `llvm-profdata` tool, which merges multiple\nraw profiles (.profraw) and indexes them to create a single profile (.profdata).\n\nAt this point, all the raw profiles can be thrown away because their information\nis already contained in the indexed profile.\n\n```\n$ llvm-profdata merge -o out/report/coverage.profdata \\\n    out/report/crypto_unittests.3657994905831792357_0.profraw\n...\nout/report/crypto_unittests.3657994905831792357_3.profraw\nout/report/url_unittests.714228855822523802_0.profraw\n...\nout/report/url_unittests.714228855822523802_3.profraw\n$ ls out/report/coverage.profdata\nout/report/coverage.profdata\n```\n\n### Step 4 Create Coverage Reports\nFinally, `llvm-cov` is used to render code coverage reports. There are different\nreport generation modes, and all of them require the following as input:\n- Indexed profile\n- All built target binaries\n- All exercised source files\n\nFor example, the following command can be used to generate per-file line-by-line\ncode coverage report:\n\n```\n$ llvm-cov show -output-dir=out/report -format=html \\\n    -instr-profile=out/report/coverage.profdata \\\n    -compilation-dir=out/coverage \\\n    -object=out/coverage/url_unittests \\\n    out/coverage/crypto_unittests\n```\n\nIf creating a report for Android, the -object arg would be the lib.unstripped\nfile, ie out/coverage/lib.unstripped/libcrypto_unittests__library.so\n\nFor more information on how to use llvm-cov, please refer to the [guide].\n\n## Read The Artifact\n\nThe code coverage tool generates some artifacts, and it is good to\nunderstand the data format to be used by automation tools.\n\n### HTML Report\n\nIf the argument `--format=html` is used in the `llvm-cov export` command, it\ngenerates a report in html format. In this html report, it shows the source\nfiles, lists the functions and coverage metadata on whether the functions are\nexecuted or not.\n\nReading a html report is straightforward: Just open up this html page with a\nChrome browser.\n\n### lcov Report\n\nIf the argument `--format=lcov` is used in the `llvm-cov export` command, it\ngenerates a report in lcov format.\n\nIn the lcov file, the meaning of these keywords are listed below.\n\n* `SF`: source file name (typically beginning of one record)\n* `FN`: mangled function symbol\n* `FNDA`: functions execution\n* `FNF`: functions found\n* `FNH`: functions hit\n* `DA`:  lines executed\n* `BRH`: branches hit\n* `BRF`: branches found\n* `LH`: lines hit\n* `LF`: lines found\n* `end_of_record` end of one record\n\nThe number right after `FN` indicates the starting line number of this function.\nThe number right after `FNDA` indicates the total number of execution of this\nfunction.\n\nIn the following example record, it means that function `_ZN4apps18AppLifetimeMonitorC2EPN7content14BrowserContextE` is defined at line\n21 in file `app_lifetime_monitor.cc` and it is executed once.\n\n```\nSF:../../chromium/src/apps/app_lifetime_monitor.cc\nFN:21,_ZN4apps18AppLifetimeMonitorC2EPN7content14BrowserContextE\nFN:32,_ZN4apps18AppLifetimeMonitorD2Ev\nFNDA:1,_ZN4apps18AppLifetimeMonitorC2EPN7content14BrowserContextE\nFNF:7\nFNH:1\nDA:34,0\nBRF:0\nBRH:0\nLF:5\nLH:1\nend_of_record\n```\n\n## Contacts\n\n### Reporting problems\nFor any breakage report and feature requests, please [file a bug].\n\n### Mailing list\nFor questions and general discussions, please join [code-coverage group].\n\n## FAQ\n\n### Can I use `is_component_build=true` for code coverage build?\n\nYes, code coverage instrumentation works with both component and non-component\nbuilds. Component build is usually faster to compile, but can be up to several\ntimes slower to run with code coverage instrumentation. For more information,\nsee [crbug.com/831939].\n\n### I am getting some warnings while using the script, is that fine?\n\nUsually this is not a critical issue, but in general we tend not to have any\nwarnings. Please check the list of [known issues], and if there is a similar\nbug, leave a comment with the command you run, the output you get, and Chromium\nrevision you use. Otherwise, please [file a bug] providing the same information.\n\n### How do crashes affect code coverage?\n\nIf a crash of any type occurs (e.g. Segmentation Fault or ASan error), the\ncrashing process might not dump coverage information necessary to generate\ncode coverage report. For single-process applications (e.g. fuzz targets), that\nmeans no coverage might be reported at all. For multi-process applications, the\nreport might be incomplete. It is important to fix the crash first. If this is\nhappening only in the coverage instrumented build, please [file a bug].\n\n### How do assertions affect code coverage?\n\nIf a crash is caused by CHECK or DCHECK, the coverage dump will still be written\non the disk ([crrev.com/c/1172932]). However, if a crashing process calls the\nstandard [assert] directly or through a custom wrapper, the dump will not be\nwritten (see [How do crashes affect code coverage?]).\n\n### Is it possible to obtain code coverage from a full Chromium build?\n\nYes, with some important caveats. It is possible to build `chrome` target with\ncode coverage instrumentation enabled. However, there are some inconveniences\ninvolved:\n\n* Linking may take a while, especially if you use a non-component build.\n* The binary is huge (2-4GB).\n* The browser may be noticeably slow and laggy.\n\nFor more information, please see [crbug.com/834781].\n\n### Why do we see significantly different coverage reported on different revisions?\n\nThere can be two possible scenarios:\n\n* It can be a one time flakiness due to a broken build or failing tests.\n* It can be caused by extension of the test suite used for generating code\ncoverage reports. When we add new tests to the suite, the aggregate coverage\nreported usually grows after that.\n\n### How can I improve [coverage dashboard]?\n\nThe code for the service and dashboard currently lives along with findit at\n[this location](https://chromium.googlesource.com/infra/infra/+/main/appengine/findit/)\nbecause of significant shared logic.\n\nThe code used by the bots that generate the coverage data lives (among other\nplaces) in the\n[code coverage recipe module](https://chromium.googlesource.com/chromium/tools/build/+/main/scripts/slave/recipe_modules/code_coverage/).\n\n### Why is coverage for X not reported or unreasonably low, even though there is a test for X?\n\nThere are several reasons why coverage reports can be incomplete or incorrect:\n\n* A particular test is not used for code coverage report generation. Please\n[file a bug].\n* A test may have a build failure or a runtime crash. Please check the build\nfor that particular report (rightmost column on the [coverage dashboard]).\nIf there is any failure, please upload a CL with the fix. If you can't fix it,\nfeel free to [file a bug].\n* A particular test may not be available on a particular platform. As of now,\nonly reports generated on Linux and CrOS are available on the\n[coverage dashboard].\n\n### Is coverage reported for the code executed inside the sandbox?\n\nYes!\n\n\n[assert]: http://man7.org/linux/man-pages/man3/assert.3.html\n[code-coverage group]: https://groups.google.com/a/chromium.org/forum/#!forum/code-coverage\n[code-coverage repository]: https://chrome-internal.googlesource.com/chrome/tools/code-coverage\n[coverage dashboard]: https://analysis.chromium.org/coverage/p/chromium\n[coverage script]: https://cs.chromium.org/chromium/src/tools/code_coverage/coverage.py\n[coverage infra diagram]: images/code_coverage_infra_diagram.png\n[coverage dashboard file view]: images/code_coverage_dashboard_file_view.png\n[coverage dashboard component view]: images/code_coverage_dashboard_component_view.png\n[coverage dashboard directory view]: images/code_coverage_dashboard_directory_view.png\n[coverage dashboard link to previous reports]: images/code_coverage_dashboard_link_to_previous_reports.png\n[coverage dashboard previous reports]: images/code_coverage_dashboard_previous_reports.png\n[crbug.com/821617]: https://crbug.com/821617\n[crbug.com/831939]: https://crbug.com/831939\n[crbug.com/834781]: https://crbug.com/834781\n[crrev.com/c/1172932]: https://crrev.com/c/1172932\n[clang roll]: https://crbug.com/841908\n[dead code example]: https://chromium.googlesource.com/chromium/src/+/ac6e09311fcc7e734be2ef21a9ccbbe04c4c4706\n[documentation]: https://clang.llvm.org/docs/SourceBasedCodeCoverage.html\n[file a bug]: https://bugs.chromium.org/p/chromium/issues/entry?components=Infra%3ETest%3ECodeCoverage\n[gerrit coverage view]: images/code_coverage_annotations.png\n[guide]: http://llvm.org/docs/CommandGuide/llvm-cov.html\n[How do crashes affect code coverage?]: #how-do-crashes-affect-code-coverage\n[known issues]: https://bugs.chromium.org/p/chromium/issues/list?q=component:Infra%3ETest%3ECodeCoverage\n[tools link]: https://storage.googleapis.com/chromium-browser-clang-staging/\n[android code coverage instructions]: https://chromium.googlesource.com/chromium/src/+/HEAD/build/android/docs/coverage.md\n"
  },
  {
    "path": "development/testing/chromeos_debugging_tips",
    "title": "Chrome OS Debugging Instructions",
    "content": "# Chrome OS Debugging Instructions\nChrome on Chrome OS is tested using a handful of frameworks, each of which\nyou'll find running on Chrome's CQ and waterfalls. If you're investigating\nfailures in these tests, below are some tips for debugging and identifying the\ncause.\n\n*** note\n\nThis doc outlines tests running in true Chrome OS environments (ie: on virtual\nmachines or real devices). [linux-chromeos] tests, on the other hand, can be\ndebugged like any other linux test.\n***\n\n## Tast\n\n[Tast] is Chrome OS's integration testing framework. Since Chrome itself is\ninstrumental to the Chrome OS system, it's equally important that we run some\nof these integration tests on Chrome's waterfalls. If you find one of these\ntests failing (likely in the `chrome_all_tast_tests` step), you can:\n\n- **Inspect the failed test's log snippet**: There should be a log snippet for\neach failed test in the `Test Results` tab in the build UI. eg: For this\n[failed build], clicking on the `policy.IncognitoModeAvailability` expands to\ninclude stack traces and error messages.\n\n- **View browser & system logs**: A common cause of failure on Chrome's builders\nare browser crashes. When this happens, each test's log snippets will simply\ncontain warnings like \"[Chrome probably crashed]\". To debug these crashes,\nexpand the list of attached artifacts for the test by clicking the `Artifacts`\nlink under the failed test in the `Test Results` tab. There you'll find an\nextended log for the test under `log.txt`. Additionally, you can find system\nlogs included in that list. To find a system log for a particular test, match\nthe timestamps printed in the test's log with the timestamps present in the\nsystem log filename. For instance, the previous `example.ChromeFixture` failure\nmatches the [chrome/chrome_20210920-051805] browser log, which contains the\nculprit Chrome crash and backtrace.\n\n- **Symbolizing a browser crash dump**: See [below](#symbolizing-a-crash-dump).\n\n### Disabling a test\n\nIf you are a Chrome Sheriff, please read the sheriff documentation\n[here](http://go/chrome-sheriff-tast) before disabling any tests.\n\nTast tests are run under both Chrome's builders and CrOS's builders. They can be\ndisabled either completely (in all builders), or in Chrome's builders alone. The\nlatter should be used only for changes which are not expected to occur on CrOS's\nbuilders.\n\n- **Disabling in all builders**: If you have a full CrOS checkout, you can add\nthe `informational` [attribute] to the test's definition. (You may be able to\nbypass the need for a full CrOS checkout by using the `Edit code` button in\ncodesearch UI, but this flow is unverified.) This can take time (ie: many hours)\nto land and propagate onto Chrome's builders. So if you need the test disabled\nASAP, consult the next option.\n- **Disabling in only Chrome's builders**: You can add the test to the list of\ndisabled tests for the step's GN target. For example, to disable a test in the\n`chrome_all_tast_tests` step, add it to [this list]. **Note**: If the test is\nfailing consistently, and you only disable it here, it will likely start to fail\nin the next [Chrome uprev] on CrOS's builders, which can lead to further\nproblems down the road. So please make sure you pursue the first option as well\nin that case.\n\nIn both cases, please make sure a bug is filed for the test, and route it to\nthe appropriate owners.\n\n### Symbolizing a crash dump\n\nIf a test fails due to a browser crash, there should be a Minidump crash report\npresent in the test's isolated out under the prefix `crashes/chrome...`. These\nreports aren't very useful by themselves, but with a few commands you can\nsymbolize the report locally to get insight into what conditions caused Chrome\nto crash.\n\nIf you are running a locally compiled [Simple Chrome] binary on a device or VM,\nyou can can build `minidump_stackwalk` and download the\n`/home/chronos/crash/chrome*.dmp` file.\n```\nautoninja -C out/Release minidump_stackwalk dump_syms\n\nrsync -r -e \"ssh -p 9222\" root@localhost:/home/chronos/crash /tmp\n```\n\nFor a crash on a bot, download both the task's input files (this provides the\nsymbols and the symbolizing tools) as well as the task's output results (this\nprovides the crash reports). See the commands listed under the *Reproducing the\ntask locally* section on the task page. For example, to download them for\n[this task](https://chrome-swarming.appspot.com/task?id=5cc272e0a839b311), `cd`\ninto a tmp directory and run:\n```\ncipd install \"infra/tools/luci/cas/\\${platform}\" -root bar\n./bar/cas login\n./bar/cas download -cas-instance projects/chrome-swarming/instances/default_instance -digest 1ad29e201e4ae7e3056a8b17935edbcd62fb54befdfeba221f2e82e54f150c86/812 -dir foo\n\ncipd install \"infra/tools/luci/swarming/\\${platform}\" -root bar\n./bar/swarming login\n./bar/swarming collect -S chrome-swarming.appspot.com -output-dir=foo 5cc272e0a839b311\n```\n\nGenerate the breakpad symbols by pointing the `generate_breakpad_symbols.py` script to\nyour local binary, or the downloaded input build dir:\n```\ncd foo\nvpython3 components/crash/content/tools/generate_breakpad_symbols.py --symbols-dir symbols --build-dir out/Release/ --binary out/Release/chrome --platform chromeos\n```\n\nThat will generate the symbols in the `symbols/` dir. Then to symbolize a Chrome\ncrash report (either in the tasks's output, or the `/tmp/crash` dir):\n```\n./out/Release/minidump_stackwalk 5cc272e0a839b311/crashes/chrome.20220816.214251.44917.24579.dmp symbols/\n```\n\n\n### Running a test locally\n\nTo run a Tast test the same way it's ran on Chrome's builders:\n\n- Decide which Chrome OS device type or VM to test on.\n\n- Build Chrome via the [Simple Chrome] workflow for that board.\n\n- Deploy your Chrome to the device via the [deploy_chrome.py] tool.\n\n- Finally, run the Tast test on the device via the `cros_run_test` tool under\n  `//third_party/chromite/bin/`. eg:\n  `cros_run_test --device $DEVICE_IP --tast login.Chrome`. See [here] for more\n  info on cros_run_test.\n\n## Telemetry\n\n>TODO: Add instructions for debugging telemetry failures.\n\n## GTest\n\n>TODO: Add instructions for debugging GTest failures.\n\n## Rerunning these tests locally\n\n>TODO: Add instructions for rerunning these tests locally.\n\n\n[linux-chromeos]: https://chromium.googlesource.com/chromium/src/+/HEAD/docs/chromeos_build_instructions.md\n[Tast]: https://chromium.googlesource.com/chromiumos/platform/tast/+/HEAD/README.md\n[failed build]: https://ci.chromium.org/ui/p/chromium/builders/ci/chromeos-kevin-rel/37300/test-results\n[Chrome probably crashed]: https://luci-milo.appspot.com/ui/inv/build-8835572137562508161/test-results?q=example.ChromeFixture\n[chrome/chrome_20210920-051805]: https://luci-milo.appspot.com/ui/artifact/raw/invocations/task-chromium-swarm.appspot.com-561bed66572a9411/artifacts/chrome%2Fchrome_20210920-051805\n[attribute]: https://chromium.googlesource.com/chromiumos/platform/tast/+/HEAD/docs/test_attributes.md\n[this list]: https://codesearch.chromium.org/chromium/src/chromeos/tast_control.gni\n[Chrome uprev]: https://chromium.googlesource.com/chromiumos/docs/+/HEAD/chrome_commit_pipeline.md#the-chrome-os-commit-pipeline-for-chrome-changes\n[Simple Chrome]: https://chromium.googlesource.com/chromiumos/docs/+/HEAD/simple_chrome_workflow.md\n[deploy_chrome.py]: https://chromium.googlesource.com/chromiumos/docs/+/HEAD/simple_chrome_workflow.md#Deploying-Chrome-to-the-device\n[here]: https://chromium.googlesource.com/chromiumos/docs/+/HEAD/cros_vm.md#in-simple-chrome\n"
  },
  {
    "path": "development/testing/batching_instrumentation_tests",
    "title": "Instrumentation Test Batching Guide",
    "content": "# Instrumentation Test Batching Guide\n\n## What is Test Batching?\n\nOutside of Chromium, it is most common to run all tests of a test suite using a\nsingle `adb shell am instrument` command (a single execution / OS process).\nHowever, Chromium's test runner runs each test using a separate command, which\nmeans tests cannot interfere with one another, but also that tests take much\nlonger to run. Test batching is a way to make our tests run faster by not\nrestarting the process between every test.\n\nAll on-device tests would ideally be annotated with one of:\n\n* `@Batch(Batch.UNIT_TESTS)`: For tests the do not rely on global application\n  state.\n* `@Batch(Batch.PER_CLASS)`: For test classes where the process does not need\n   to be restarted between `@Test`s within the class, but should be restarted\n   before and after the suite runs.\n* `@DoNotBatch(reason = \"...\"`: For tests classes that require the process to be\n  restarted for each test or are infeasible to batch.\n\nTests that are not annotated are treated as `@DoNotBatch` and are assumed to\nhave not yet been assessed.\n\n## How to Batch a Test\n\nAdd the `@Batch` annotation to the test class, and ensure that each test within\nthe chosen batch doesn't leave behind state that could cause other tests in the\nbatch to fail.\n\nFor some tests, batching won’t be as useful (tests that test Activity\nstartup, for example), and tests that test process startup shouldn’t be batched\nat all.\n\nIf a few tests within a larger batched suite cannot be batched (eg. it tests\nprocess initialization), you may add the\n[@RequiresRestart](https://source.chromium.org/chromium/chromium/src/+/main:base/test/android/javatests/src/org/chromium/base/test/util/RequiresRestart.java;bpv=1;bpt=1;l=19?q=RequiresRestart&ss=chromium%2Fchromium%2Fsrc&originalUrl=https:%2F%2Fcs.chromium.org%2F&gsn=RequiresRestart&gs=kythe%3A%2F%2Fchromium.googlesource.com%2Fchromium%2Fsrc%3Flang%3Djava%3Fpath%3Dorg.chromium.base.test.util.RequiresRestart%23b5e85d5c8071e18f350b7f2c5014310bd2cabd0e0d3d176949c991ea18403f55)\nannotation to test methods to exclude them from the batch.\n\n## Types of Batched tests\n\n### [UNIT_TESTS](https://source.chromium.org/chromium/chromium/src/+/main:base/test/android/javatests/src/org/chromium/base/test/util/Batch.java;bpv=1;bpt=1;l=51?q=Batch.java&ss=chromium%2Fchromium%2Fsrc&originalUrl=https:%2F%2Fcs.chromium.org%2F&gsn=UNIT_TESTS&gs=kythe%3A%2F%2Fchromium.googlesource.com%2Fchromium%2Fsrc%3Flang%3Djava%3Fpath%3Dorg.chromium.base.test.util.Batch%2319ebd2758adfaed0bda0e97542f70ca5b1564e7c1fa0f8c2bcb9e8170b75684d)\n\nTests that belong in this category are tests that are effectively unit tests.\nThey may be written as instrumentation tests rather than junit tests for a\nvariety of reasons such as needing to use real Android APIs, or needing to\nuse the native library.\n\nBatching Unit Test style tests is usually fairly simple\n([example](https://chromium-review.googlesource.com/c/chromium/src/+/2216044)).\nIt requires adding the `@Batch(Batch.UNIT_TESTS)` annotation, and ensuring no\nglobal state, like test overrides, persists across tests. Unit Tests should also\nnot start the browser process, but may load the native library. Note that even\nwith Batched tests, the test fixture (the class) is recreated for each test.\n\nNote that since the browser isn't initialized for unit tests, if you would like\nto take advantage of feature annotations in your test you will have to use\n`Features.JUnitProcessor` instead of `Features.InstrumentationProcessor`.\n\n\n### [PER_CLASS](https://source.chromium.org/chromium/chromium/src/+/main:base/test/android/javatests/src/org/chromium/base/test/util/Batch.java;bpv=1;bpt=1;l=39?q=Batch.java&ss=chromium%2Fchromium%2Fsrc&originalUrl=https:%2F%2Fcs.chromium.org%2F&gsn=PER_CLASS&gs=kythe%3A%2F%2Fchromium.googlesource.com%2Fchromium%2Fsrc%3Flang%3Djava%3Fpath%3Dorg.chromium.base.test.util.Batch%23780b702db42a1901f05647fd29f75d443bc4efd2db588848b4aedf826ddf9e21)\n\nThis batching type is typically for larger and more complex test suites, and\nwill run the suite in its own batch. This will limit side-effects and reduce\nthe complexity of managing state from these tests as you only have to think\nabout tests within the suite.\n\nTests with different `@Features` annotations (`@EnableFeatures` and\n`@DisableFeatures`) or `@CommandLineFlags` will be run in separate batches.\n\n### Custom\n\nThis batching type is best for smaller and less complex test suites, that\nrequire browser initialization, or something else that prevents them from being\nunit tests. Custom batches allow you to pay the process startup cost once per\nbatch instead of once per test suite. To put multiple test suites into the same\nbatch, you will have to use a shared custom batch name\n([example](https://chromium-review.googlesource.com/c/chromium/src/+/2307650)).\nWhen batching across suites you’ll want to use something like\n[BlankCTATabInitialStateRule](https://source.chromium.org/chromium/chromium/src/+/main:chrome/test/android/javatests/src/org/chromium/chrome/test/batch/BlankCTATabInitialStateRule.java?q=BlankCTATabInitialStateRule&ss=chromium&originalUrl=https:%2F%2Fcs.chromium.org%2F)\nto persist static state (like the Activity) between test suites and perform any\nnecessary state cleanup between tests.\n\nNote that there is an inherent tradeoff here between batch size and\ndebuggability - the larger your batch, the harder it will be to diagnose one\ntest causing a different test to fail/flake. I would recommend grouping tests\nsemantically to make it easier to understand relationships between the tests and\nwhich shared state is relevant.\n\n### Running Test Batches\n\nRun all tests with `@Batch=UnitTests`:\n\n```shell\nout/<dir>/bin/run_chrome_public_unit_test_apk -A Batch=UnitTests\n\nout/<dir>/bin/run_chrome_public_test_apk -A Batch=UnitTests\n```\n\nRun all tests in a custom batch:\n```shell\n./tools/autotest.py -C out/Debug BluetoothChooserDialogTest \\\n--gtest_filter=\"*\" -A Batch=device_dialog\n```\n\n## Things worth noting\n\n* Activities won’t be automatically finished for you, if your test requires\nthat. Other common state like SharedPreferences\n[issue 1086663](https://crbug.com/1086663) also won’t be automatically reset.\n* @ClassRule and @BeforeClass/@AfterClass run during test listing, so don’t do\nany heavy work in them (and will run twice for parameterized tests). See\n[issue 1090043](https://crbug.com/1090043).\n* Sometimes it can be very difficult to figure out which test in a batch is\ncausing another test to fail. A good first step is to minimize [_TEST_BATCH_MAX_GROUP_SIZE](https://source.chromium.org/chromium/chromium/src/+/main:build/android/pylib/local/device/local_device_instrumentation_test_run.py;drc=3ab9a142091516aa57f10feebc46dee649ae4589;l=109)\nto minimize the number of tests within the batch while still reproducing the\nfailure. Then, you can use multiple gtest filter patterns to control which tests\nrun together. Ex:\n  ```shell\n  ./tools/autotest.py -C out/Debug ExternalNavigationHandlerTest \\\n  --gtest_filter=\"*#testOrdinaryIncognitoUri:*#testChromeReferrer\"\n  ```\n"
  },
  {
    "path": "development/testing/android_test_instructions",
    "title": "Android Testing in Chromium",
    "content": "# Android Testing in Chromium\n\n[TOC]\n\n## Test Types\n\n-   **[gtests]**: For writing unit tests in C++. Most tests are cross-platform.\n-   **Browser Tests**: Built on top of gtest. Used to write integration tests.\n-   **[Robolectric]**: JUnit tests that run on the host machine by emulating (or\n    mocking) Android APIs.\n-   **[Instrumentation Tests]**: JUnit tests that run on devices / emulators.\n    - **Unit Instrumentation Tests**: Unit tests that do not require\n      initializing the Content layer. These use [BaseActivityTestRule] often\n      using [BlankUiTestActivity].\n    - **Integration Instrumentation Tests**: Instrumentation tests that require\n      initializing the Content layer to test a certain feature in the end-to-end\n      flow. These typically use more specialized test rules such as\n      [ContentShellActivityTestRule] or [ChromeActivityTestRule].\n\n[gtests]: android_gtests.md\n[Robolectric]: android_robolectric_tests.md\n[Instrumentation Tests]: android_instrumentation_tests.md\n[BaseActivityTestRule]: https://source.chromium.org/chromium/chromium/src/+/main:base/test/android/javatests/src/org/chromium/base/test/BaseActivityTestRule.java\n[BlankUiTestActivity]: https://source.chromium.org/chromium/chromium/src/+/main:ui/android/javatests/src/org/chromium/ui/test/util/BlankUiTestActivity.java\n[ContentShellActivityTestRule]: https://source.chromium.org/chromium/chromium/src/+/main:content/shell/android/javatests/src/org/chromium/content_shell_apk/ContentShellActivityTestRule.java\n[ChromeActivityTestRule]: https://source.chromium.org/chromium/chromium/src/+/main:chrome/test/android/javatests/src/org/chromium/chrome/test/ChromeActivityTestRule.java\n\n## Device Setup\n\n### Physical Device Setup\n\n#### Root Access\n\nRunning tests requires root access, which requires using a userdebug build on\nyour device.\n\nTo use a userdebug build, see\n[Running Builds](https://source.android.com/setup/build/running.html). Googlers\ncan refer to [this page](https://goto.google.com/flashdevice).\n\nIf you can't run \"adb root\", you will get an error when trying to install\nthe test APKs like \"adb: error: failed to copy\" and\n\"remote secure_mkdirs failed: Operation not permitted\" (use \"adb unroot\" to\nreturn adb to normal).\n\n#### ADB Debugging\n\nThe adb executable exists within the Android SDK:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb\n```\n\nIn order to allow the ADB to connect to the device, you must enable USB\ndebugging:\n\n* Developer options are hidden by default. To unhide them:\n    *   Go to \"About phone\"\n    *   Tap 10 times on \"Build number\"\n    *   The \"Developer options\" menu will now be available.\n    *   Check \"USB debugging\".\n    *   Un-check \"Verify apps over USB\".\n\n#### Screen\n\nYou **must** ensure that the screen stays on while testing: `adb shell svc power\nstayon usb` Or do this manually on the device: Settings -> Developer options ->\nStay Awake.\n\nIf this option is greyed out, stay awake is probably disabled by policy. In that\ncase, get another device or log in with a normal, unmanaged account (because the\ntests will break in exciting ways if stay awake is off).\n\n#### Disable Verify Apps\n\nYou may see a dialog like [this\none](http://www.samsungmobileusa.com/simulators/ATT_GalaxyMega/mobile/screens/06-02_12.jpg),\nwhich states, _Google may regularly check installed apps for potentially harmful\nbehavior._ This can interfere with the test runner. To disable this dialog, run:\n\n```shell\nadb shell settings put global package_verifier_enable 0\n```\n\n### Using Emulators\n\nRunning tests on emulators is the same as [on device](#Running-Tests). Refer to\n[android_emulator.md](/docs/android_emulator.md) for setting up emulators.\n\n## Building Tests\n\nIf you're adding a new test file, you'll need to explicitly add it to a gn\ntarget. If you're adding a test to an existing file, you won't need to make gn\nchanges, but you may be interested in where your test winds up. In either case,\nhere are some guidelines for where a test belongs:\n\n### C++\n\nC++ test files typically belong in `<top-level directory>_unittests` (e.g.\n`base_unittests` for `//base`). There are a few exceptions -- browser tests are\ntypically their own target (e.g. `content_browsertests` for `//content`, or\n`browser_tests` for `//chrome`), and some unit test suites are broken at the\nsecond directory rather than the top-level one.\n\n### Java\n\nJava test files vary a bit more widely than their C++ counterparts:\n\n-   Instrumentation test files -- i.e., tests that will run on a device --\n    typically belong in either `<top-level directory>_javatests` or `<top-level\n    directory>_test_java`. Regardless, they'll wind up getting packaged into one\n    of a few test APKs:\n    -   `webview_instrumentation_test_apk` for anything in `//android_webview`\n    -   `content_shell_test_apk` for anything in `//content` or below\n    -   `chrome_public_test_apk` for most things in `//chrome`\n-   JUnit or Robolectric test files -- i.e., tests that will run on the host --\n    typically belong in `<top-level directory>_junit_tests` (e.g.\n    `base_junit_tests` for `//base`), though here again there are cases\n    (particularly in `//components`) where suites are split at the second\n    directory rather than the top-level one.\n\nOnce you know what to build, just do it like you normally would build anything\nelse, e.g.: `ninja -C out/Release chrome_public_test_apk`\n\n### Determining Test Target\n\nIf you do not know what target a test file belongs to, you can use\n`//tools/autotest.py` to figure it out fo you:\n\n```sh\n# Builds relevant test target and then runs the test:\ntools/autotest.py -C <output directory> TestClassName\n```\n\n## Running Tests\n\nAll functional tests should be runnable via the wrapper scripts generated at\nbuild time:\n\n```sh\n<output directory>/bin/run_<target_name> [options]\n```\n\nNote that tests are sharded across all attached devices unless explicitly told\nto do otherwise by `-d/--device`.\n\nThe commands used by the buildbots are printed in the logs. Look at\nhttps://build.chromium.org/ to duplicate the same test command as a particular\nbuilder.\n\n### Listing Available Tests\n\nUse `--list-tests` to list what tests are available.\n\n```sh\n# Prints out all available tests:\n<output directory>/bin/run_<target_name> --list-tests\n\n# Prints out all available tests that match a filter:\n<output directory>/bin/run_<target_name> --list-tests -f \"*MyFilter*\"\n```\n\n### INSTALL\\_FAILED\\_CONTAINER\\_ERROR or INSTALL\\_FAILED\\_INSUFFICIENT\\_STORAGE\n\nIf you see this error when the test runner is attempting to deploy the test\nbinaries to the AVD emulator, you may need to resize your userdata partition\nwith the following commands:\n\n```shell\n# Resize userdata partition to be 1G\nresize2fs android_emulator_sdk/sdk/system-images/android-25/x86/userdata.img 1G\n\n# Set filesystem parameter to continue on errors; Android doesn't like some\n# things e2fsprogs does.\ntune2fs -e continue android_emulator_sdk/sdk/system-images/android-25/x86/userdata.img\n```\n\n### AdbCommandFailedError: failed to stat remote object\n\nThere's a known issue (https://crbug.com/1094062) where the unit test binaries can fail on\nAndroid R and later: if you see this error, try rerunning on an Android version\nwith API level <= 29 (Android <= Q).\n\n## Symbolizing Crashes\n\nCrash stacks are logged and can be viewed using `adb logcat`. To symbolize the\ntraces, define `CHROMIUM_OUTPUT_DIR=$OUTDIR` where `$OUTDIR` is the argument you\npass to `ninja -C`, and pipe the output through\n`third_party/android_platform/development/scripts/stack`. If\n`$CHROMIUM_OUTPUT_DIR` is unset, the script will search `out/Debug` and\n`out/Release`. For example:\n\n```shell\n# If you build with\nninja -C out/Debug chrome_public_test_apk\n# You can run:\nadb logcat -d | third_party/android_platform/development/scripts/stack\n\n# If you build with\nninja -C out/android chrome_public_test_apk\n# You can run:\nadb logcat -d | CHROMIUM_OUTPUT_DIR=out/android third_party/android_platform/development/scripts/stack\n# or\nexport CHROMIUM_OUTPUT_DIR=out/android\nadb logcat -d | third_party/android_platform/development/scripts/stack\n```\n\n## Robolectric Tests\n\nJUnit tests are Java unittests running on the host instead of the target device.\nThey are faster to run and therefore are recommended over instrumentation tests\nwhen possible.\n\nThe JUnits tests are usually following the pattern of *target*\\_junit\\_tests,\nfor example, `content_junit_tests` and `chrome_junit_tests`.\n\nWhen adding a new JUnit test, the associated `BUILD.gn` file must be updated.\nFor example, adding a test to `chrome_junit_tests` requires to update\n`chrome/android/BUILD.gn`.\n\n```shell\n# Build the test suite.\nninja -C out/Default chrome_junit_tests\n\n# Run the test suite.\nout/Default/bin/run_chrome_junit_tests\n\n# Run a subset of tests. You might need to pass the package name for some tests.\nout/Default/bin/run_chrome_junit_tests -f \"org.chromium.chrome.browser.media.*\"\n```\n\n### Debugging\n\nSimilar to [debugging apk targets](/docs/android_debugging_instructions.md#debugging-java):\n\n```shell\nout/Default/bin/run_chrome_junit_tests --wait-for-java-debugger\nout/Default/bin/run_chrome_junit_tests --wait-for-java-debugger  # Specify custom port via --debug-socket=9999\n```\n\n## Gtests\n\n```shell\n# Build a test suite\nninja -C out/Release content_unittests\n\n# Run a test suite\nout/Release/bin/run_content_unittests [-vv]\n\n# Run a subset of tests and enable some \"please go faster\" options:\nout/Release/bin/run_content_unittests --fast-local-dev -f \"ByteStreamTest.*\"\n```\n\n## Instrumentation Tests\n\nIn order to run instrumentation tests, you must leave your device screen ON and\nUNLOCKED. Otherwise, the test will timeout trying to launch an intent.\nOptionally you can disable screen lock under Settings -> Security -> Screen Lock\n-> None.\n\nNext, you need to build the app, build your tests, and then run your tests\n(which will install the APK under test and the test APK automatically).\n\nExamples:\n\nContentShell tests:\n\n```shell\n# Build the tests:\nninja -C out/Release content_shell_test_apk\n\n# Run the test suite:\nout/Release/bin/run_content_shell_test_apk [-vv]\n\n# Run a subset of tests and enable some \"please go faster\" options:\nout/Release/bin/run_content_shell_test_apk --fast-local-dev -f \"*TestClass*\"\n```\n\nAndroid WebView tests:\n\nSee [WebView's instructions](/android_webview/docs/test-instructions.md).\n\nIn order to run a subset of tests, use -f to filter based on test class/method\nor -A/-E to filter using annotations.\n\nMore Filtering examples:\n\n```shell\n# Run a specific test class\nout/Debug/bin/run_content_shell_test_apk -f \"AddressDetectionTest.*\"\n\n# Run a specific test method\nout/Debug/bin/run_content_shell_test_apk -f AddressDetectionTest#testAddressLimits\n\n# Run a subset of tests by size (Smoke, SmallTest, MediumTest, LargeTest,\n# EnormousTest)\nout/Debug/bin/run_content_shell_test_apk -A Smoke\n\n# Run a subset of tests by annotation, such as filtering by Feature\nout/Debug/bin/run_content_shell_test_apk -A Feature=Navigation\n```\n\nYou might want to add stars `*` to each as a regular expression, e.g.\n`*`AddressDetectionTest`*`\n\n### Debugging\n\nSimilar to [debugging apk targets](/docs/android_debugging_instructions.md#debugging-java):\n\n```shell\nout/Debug/bin/run_content_shell_test_apk --wait-for-java-debugger\n```\n\n### Deobfuscating Java Stacktraces\n\nIf running with `is_debug=false`, Java stacks from logcat need to be fixed up:\n\n```shell\nbuild/android/stacktrace/java_deobfuscate.py out/Release/apks/ChromePublicTest.apk.mapping < stacktrace.txt\n```\n\nAny stacks produced by test runner output will already be deobfuscated.\n\n\n## Running Blink Web Tests\n\nSee [Web Tests](web_tests.md).\n\n## Running Telemetry (Perf) Tests\n\nSee [Telemetry](https://chromium.googlesource.com/catapult/+/HEAD/telemetry/README.md).\n\n## Running GPU tests\n\n(e.g. the \"Android Debug (Nexus 7)\" bot on the chromium.gpu waterfall)\n\nSee https://www.chromium.org/developers/testing/gpu-testing for details. Use\n`--browser=android-content-shell`. Examine the stdio from the test invocation on\nthe bots to see arguments to pass to `src/content/test/gpu/run_gpu_test.py`.\n"
  },
  {
    "path": "development/testing/android_robolectric_tests",
    "title": "JUnit Tests",
    "content": "# JUnit Tests\n\nJUnit tests are Java unit tests. These tests run locally on your workstation.\n\n[TOC]\n\n## Writing a JUnit test\n\nWhen writing JUnit tests, you must decide whether you need to use Android code.\nIf you want to use Android code you must write a [Robolectric](http://robolectric.org/) test.\n\n### JUnit tests (without Android)\n\nBuild these types of test using the `robolectric_binary` GN template.\n\nIf you don't need to use any Android code in your tests, you can write plain,\nold JUnit tests. Some more documentation about writing JUnit tests can be\nfound [here](https://github.com/junit-team/junit4/wiki/Getting-started).\n\n#### Example Code\n\n```java\npackage org.chromium.sample.test;\n\nimport static org.junit.Assert.assertTrue;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.BlockJUnit4ClassRunner;\n\n@RunWith(BlockJUnit4ClassRunner.class)\npublic class MyJUnitTest {\n\n    @Test\n    public void exampleTest() {\n        boolean shouldWriteMoreJUnitTests = true;\n        assertTrue(shouldWriteMoreJUnitTests);\n    }\n}\n```\n\n#### Example within Chromium\n\nSee the [junit_unit_tests](https://cs.chromium.org/chromium/src/testing/android/junit/BUILD.gn) test suite.\n\n### JUnit tests with Robolectric\n\nBuild these types of test using the `robolectric_binary` GN template.\n\nRobolectric is a unit testing framework that lets you run tests with Android\ncode on your workstation. It does this by providing a special version of the\nAndroid SDK jar that can run in your host JVM. Some more information about\nRobolectric can be found [here](http://robolectric.org/).\n\nOne on the main benefits of using Robolectric framework are [shadow classes](http://robolectric.org/extending/).\nRobolectric comes with many prebuilt shadow classes and also lets you define\nyour own. Whenever an object is instantiated within a Robolectric test,\nRobolectric looks for a corresponding shadow class (marked by\n`@Implements(ClassBeingShadowed.class)`). If found, any time a method is invoked\non the object, the shadow class's implementation of the method is invoked first.\nThis works even for static and final methods.\n\n#### Useful Tips\n\n* Use `@RunWith(BaseRobolectricTestRunner.class)` for all Chromium Robolectric tests.\n* You can specify the Android SDK to run your test with with `@Config(sdk = ??)`.\n\n> Currently, only SDK levels 18, 21, and 25 are supported in Chromium\n> but more can be added on request.\n\n#### Example Code\n\n```java\npackage org.chromium.sample.test;\n\nimport static org.junit.Assert.assertTrue;\n\nimport android.text.TextUtils;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.robolectric.annotation.Config;\n\nimport org.chromium.base.test.BaseRobolectricTestRunner;\n\n// Be sure to specify to run tests with a RobolectricTestRunner. The\n// default JUnit test runner won't load the Robolectric Android code properly.\n// BaseRobolectricTestRunner will do some common initializations. If this is\n// not desired, then RobolectricTestRunner could be used directly.\n@RunWith(BaseRobolectricTestRunner.class)\n// Can specify some Robolectric related configs here.\n// More about configuring Robolectric at http://robolectric.org/configuring/.\n// SDK will default to the latest we support in Chromium.\n@Config(manifest = Config.NONE, sdk = 21)\npublic class MyRobolectricJUnitTest {\n\n    @Test\n    public void exampleTest() {\n        String testString = \"test\";\n\n        // Even though these tests runs on the host, Android classes are\n        // available to use thanks to Robolectric.\n        assertTrue(TextUtils.equals(testString, \"test\"));\n    }\n}\n```\n\n#### Example robolectric_binary build template.\n\n```python\nrobolectric_binary(\"my_robolectric_tests\") {\n\n    sources = [\n        \"java/src/foo/bar/MyJUnitTest.java\"\n    ]\n\n    deps = [\n        \"//my/test:dependency\",\n    ]\n\n    # Sets app's package name in Robolectric tests. You need to specify\n    # this variable in order for Robolectric to be able to find your app's\n    # resources.\n    package_name = manifest_package\n}\n```\n\n#### Example within Chromium\n\nSee the [content_junit_tests](https://cs.chromium.org/chromium/src/content/public/android/BUILD.gn) test suite.\n\n## Running JUnit tests\n\nAfter writing a test, you can run it by:\n\n1. Adding the test file to a `robolectric_binary` GN target.\n2. Rebuild.\n3. GN will generate binary `<out_dir>/bin/run_<suite name>` which\n   can be used to run your test.\n\nFor example, the following can be used to run chrome_junit_tests.\n\n```bash\n# Build the test suite after adding our new test.\nninja -C out/Debug chrome_junit_tests\n\n# Run the test!\nout/Debug/bin/run_chrome_junit_tests\n```\n"
  },
  {
    "path": "development/testing/android_instrumentation_tests",
    "title": "Android Instrumentation Tests",
    "content": "# Android Instrumentation Tests\n\nInstrumentation tests are JUnit 4 tests that run on devices or emulators. They\ncan be either unit tests or integration test.\n\n[TOC]\n\n## Tracing\n\nEnabling tracing during a test run allows all the function calls involved to be\nobserved in a visual display (using Chrome's built-in chrome://tracing feature).\nTo run a test with tracing, add the `--trace-output` flag to the command used to\ncall the instrumentation test (either running the test_runner.py script, or a\ngenerated binary such as `run_chrome_public_test_apk`). The `--trace-output` flag\ntakes a filename, which, after the test run, will contain a JSON file readable\nby chrome://tracing.\n\nBy default, the trace includes only certain function calls important to the test\nrun, both within the Python test runner framework and the Java code running on\nthe device. For a more detailed look, add the (no-argument) `--trace-all` flag.\nThis causes every function called on the Python side to be added to the trace.\n\n## Test Batching Annotations\n\nThe [`@Batch(\"group_name\")`](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/Batch.java)\nannotation is used to run all tests with the same batch group name in the same\ninstrumentation invocation. In other words, the browser process is not\nrestarted between these tests, and so any changes to global state, like\nlaunching an Activity, will persist between tests within a batch group. The\nbenefit of this is that these tests run significantly faster - the per-test cost\nof restarting the process can be as high as 10 seconds (usually around 2\nseconds), and that doesn't count the cost of starting an Activity like\nChromeTabbedActivity.\n\n## Size Annotations\n\nSize annotations are [used by the test runner] to determine the length of time\nto wait before considering a test hung (i.e., its timeout duration).\n\nAnnotations from `androidx.test.filters`:\n\n - [`@SmallTest`](https://developer.android.com/reference/androidx/test/filters/SmallTest.html) (timeout: **10 seconds**)\n - [`@MediumTest`](https://developer.android.com/reference/androidx/test/filters/MediumTest.html) (timeout: **30 seconds**)\n - [`@LargeTest`](https://developer.android.com/reference/androidx/test/filters/LargeTest.html) (timeout: **2 minutes**)\n\nAnnotations from `//base`:\n\n - [`@EnormousTest`](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/EnormousTest.java)\n(timeout: **5 minutes**) Typically used for tests that require WiFi.\n - [`@IntegrationTest`](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/IntegrationTest.java)\n(timeout: **10 minutes**) Used for tests that run against real services.\n - [`@Manual`](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/Manual.java)\n(timeout: **10 hours**) Used for manual tests.\n\n[used by the test runner]: https://source.chromium.org/search?q=file:local_device_instrumentation_test_run.py%20symbol:TIMEOUT_ANNOTATIONS&sq=&ss=chromium\n\n## Annotations that Disable Tests\n\nThere are several annotations that control whether or not a test runs.\nSome are conditional, others are not.\n\n### Unconditional Disabling\n\n[**@DisabledTest**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/DisabledTest.java)\nunconditionally disables a test.\n```java\n@DisabledTest(\n    // Describes why the test is disabled. Typically includes a crbug link.\n    message = \"\"\n)\n```\n\n### Conditional Disabling\n\nThere are two primary annotation categories that conditionally disable tests:\n**@DisableIf** and **@Restriction**. The **@DisableIf** annotations are intended\nto temporarily disable a test in certain scenarios where it *should* work but\ndoesn't. In contrast, the **@Restriction** annotation is intended to\npermanently limit a test to specific configurations. It signifies that the test\nwas not, is not, and will not be intended to run beyond those configurations.\nIn both cases, conditional disabling manifests as a skipped test.\n\n[**@DisableIf.Build**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/DisableIf.java#25)\nallows for conditional test disabling based on values in\n[`android.os.Build`](https://developer.android.com/reference/android/os/Build.html):\n\n```java\n@DisableIf.Build(\n\n    // Describes why the test is disabled.\n    message = \"\",\n\n    // Disables the test on SDK levels that match the given conditions.\n    // Checks against Build.VERSION.SDK_INT.\n    sdk_is_greater_than = 0,\n    sdk_is_less_than = Integer.MAX_VALUE,\n\n    // Disables the test on devices that support the given ABI\n    // (e.g. \"arm64-v8a\"). Checks against:\n    //  - Build.SUPPORTED_ABIS on L+\n    //  - Build.CPU_ABI and Build.CPU_ABI2 otherwise\n    supported_abis_includes = \"\",\n\n    // Disables the test on devices with hardware that matches the given\n    // value. Checks against Build.HARDWARE.\n    hardware_is = \"\",\n\n    // Disables the test on devices with product names that contain the\n    // given value. Checks against Build.PRODUCT.\n    product_name_includes = \"\",\n\n)\n```\n\n[**@DisableIf.Device**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/DisableIf.java#40)\nallows for conditional test disabling based on whether\na device is a phone, a tablet, or a \"large tablet\" as determined by\n[org.chromium.ui.base.DeviceFormFactor](https://chromium.googlesource.com/chromium/src/+/main/ui/android/java/src/org/chromium/ui/base/DeviceFormFactor.java).\nThis is available to tests in\n[//ui](https://chromium.googlesource.com/chromium/src/+/main/ui/)\nor code that uses //ui.\n\n```java\n@DisableIf.Device(\n    // Disables the test on devices that match the given type(s) as described\n    // above.\n    type = {}\n)\n```\n\n[**@Restriction**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/Restriction.java)\ncurrently allows for conditional test disabling based on device\ntype, device performance, internet connectivity, whether Google Play Services is\nup to date, and whether the build was an official one.\n\n```java\n@Restriction(\n    // Possible values include:\n    //\n    // base:\n    //  - Restriction.RESTRICTION_TYPE_LOW_END_DEVICE\n    //    Restricts the test to low-end devices as determined by SysUtils.isLowEndDevice().\n    //\n    //  - Restriction.RESTRICTION_TYPE_NON_LOW_END_DEVICE\n    //    Restricts the test to non-low-end devices as determined by SysUtils.isLowEndDevice().\n    //\n    //  - Restriction.RESTRICTION_TYPE_INTERNET\n    //    Restricts the test to devices that have an internet connection.\n    //\n    // chrome:\n    //  - ChromeRestriction.RESTRICTION_TYPE_GOOGLE_PLAY_SERVICES\n    //    Restricts the test to devices with up-to-date versions of Google Play Services.\n    //\n    //  - ChromeRestriction.RESTRICTION_TYPE_OFFICIAL_BUILD\n    //    Restricts the test to official builds as determined by ChromeVersionInfo.isOfficialBuild().\n    //\n    // ui:\n    //  - UiRestriction.RESTRICTION_TYPE_PHONE\n    //    Restricts the test to phones as determined by DeviceFormFactor.\n    //\n    //  - UiRestriction.RESTRICTION_TYPE_TABLET\n    //    Restricts the test to tablets as determined by DeviceFormFactor.\n    value = {}\n)\n```\n\n[**@MinAndroidSdkLevel**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/MinAndroidSdkLevel.java)\nis similar to **@Restriction** in purpose in that it's\nintended to permanently limit a test to only recent versions of Android.\n\n```java\n@MinAndroidSdkLevel(\n    // The minimum SDK level at which this test should be executed. Checks\n    // against Build.VERSION.SDK_INT.\n    value = 0\n)\n```\n\n## Command-Line Flags Annotations\n\nSeveral annotations affect how a test is run in interesting or nontrivial ways.\n\n[**@CommandLineFlags.Add**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/CommandLineFlags.java#46)\nand\n[**@CommandLineFlags.Remove**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/CommandLineFlags.java#58)\nmanipulate Chrome's\ncommand-line flags on a per-test basis (i.e., the flags handled by\n[`org.chromium.base.CommandLine`](https://chromium.googlesource.com/chromium/src/+/main/base/android/java/src/org/chromium/base/CommandLine.java) and\n[`base::CommandLine`](https://chromium.googlesource.com/chromium/src/+/main/base/command_line.h)).\n\n```java\n@CommandLineFlags.Add(\n    // The flags to add to the command line for this test. These can be\n    // anything and typically should include the leading dashes (e.g. \"--foo\").\n    value = {}\n)\n\n@CommandLineFlags.Remove(\n    // The flags to remove from the command line for this test. These can only\n    // be flags added via @CommandLineFlags.Add. Flags already present in the\n    // command-line file on the device are only present in the native\n    // CommandLine object and cannot be manipulated.\n    value = {}\n)\n```\n\n## Feature Annotations\n\n[**@Feature**](https://chromium.googlesource.com/chromium/src/+/main/base/test/android/javatests/src/org/chromium/base/test/util/Feature.java)\nhas been used inconsistently in Chromium to group tests across\ntest cases according to the feature they're testing.\n\n```java\n@Feature(\n    // The features associated with this test. These can be anything.\n    value = {}\n)\n```\n\n@Feature doesn't have an inherent function, but it can be used to filter tests\nvia the test runner's\n`-A/--annotation` and `-E/--exclude-annotation` flags. For example, this would\nrun only the tests with @Feature annotations containing at least \"Sync\" in\n`chrome_public_test_apk`:\n\n```bash\nout/Debug/bin/run_chrome_public_test_apk -A Feature=Sync\n```\n"
  },
  {
    "path": "development/testing/android_gtests",
    "title": "How GTests work on Android",
    "content": "# How GTests work on Android\n\ngtests are [googletest](https://github.com/google/googletest)-based C++ tests.\nOn Android, they run on a device. In most cases, they're packaged as APKs, but\nthere are a few cases where they're run as raw executables. The latter is\nnecessary in a few cases, particularly when manipulating signal handlers, but\nisn't possible when the suite needs to call back through the JNI into Java code.\n\n[TOC]\n\n## APKs\n\n### GN\n\nGtest APKs are built by default by the\n[test](https://codesearch.chromium.org/chromium/src/testing/test.gni?type=cs&q=file:%5Esrc%5C/testing%5C/test.gni$+template%5C(\"test\"%5C)&sq=package:chromium)\ntemplate, e.g.\n\n```python\ntest(\"sample_gtest\") {\n  # ...\n}\n```\n\nThis uses gn's native\n[shared_library](https://chromium.googlesource.com/chromium/src/+/main/tools/gn/docs/reference.md#shared_library_Declare-a-shared-library-target)\ntarget type along with the\n[unittest_apk](https://codesearch.chromium.org/chromium/src/build/config/android/rules.gni?type=cs&q=file:%5Esrc%5C/build%5C/config%5C/android%5C/rules.gni$+template%5C(%5C\"unittest_apk%5C\"%5C)&sq=package:chromium)\ntemplate to build an APK containing:\n\n - One or more .so files containing the native code on which the test suite\ndepends\n - One or more .dex files containing the Java code on which the test suite\ndepends\n - A [manifest](https://developer.android.com/guide/topics/manifest/manifest-intro.html)\nfile that contains `<instrumentation>` and `<activity>` elements (among others).\n\n### Harness\n\nGTest APKs are packaged with a harness that consists of:\n\n  - [NativeTestInstrumentationTestRunner], an instrumentation entry point that\nhandles running one or more sequential instances of a test Activity. Typically,\nunit test suites will only use one instance of the Activity and will run all of\nthe specified tests in it, while browser test suites will use multiple instances\nand will only run one test per instance.\n  - Three [Activity](https://developer.android.com/reference/android/app/Activity.html)-based\nclasses\n([NativeUnitTestActivity](https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeUnitTestActivity.java),\n[NativeUnitTestNativeActivity](https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeUnitTestNativeActivity.java),\nand\n[NativeBrowserTestActivity](https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeBrowserTestActivity.java))\nthat primarily act as process entry points for individual test shards.\nOnly one is used in any given suite.\n  - [NativeTest] and [NativeUnitTest],\nwhich handle formatting arguments for googletest and transferring control across\nthe JNI.\n  - [testing::android::RunTests](https://codesearch.chromium.org/chromium/src/testing/android/native_test/native_test_launcher.cc?q=file:%5Esrc%5C/testing%5C/android%5C/native_test%5C/native_test_launcher.cc$+RunTests&sq=package:chromium),\nthe function on the native side, which initializes the native command-line,\nredirects stdout either to a FIFO or a regular file, optionally waits for a\ndebugger to attach to the process, sets up the test data directories, and then\ndispatches to googletest's `main` function.\n\n### Runtime\n\n 1. The test runner calls `am instrument` with a bunch of arguments,\n    includes several extras that are arguments to either\n    [NativeTestInstrumentationTestRunner] or [NativeTest]. This results in an\n    intent being sent to [NativeTestInstrumentationTestRunner].\n 2. [NativeTestInstrumentationTestRunner] is created. In its onCreate, it\n    parses its own arguments from the intent and retains all other arguments\n    to be passed to the Activities it'll start later. It also creates a\n    temporary file in the external storage directory for stdout. It finally\n    starts itself.\n 3. [NativeTestInstrumentationTestRunner] is started. In its onStart, it prepares\n    to receive notifications about the start and end of the test run from the\n    Activities it's about to start. It then creates [ShardStarter]\n    that will start the first test shard and adds that to the current\n    [Handler](https://developer.android.com/reference/android/os/Handler.html).\n 4. The [ShardStarter] is executed, starting the test Activity.\n 5. The Activity starts, possibly doing some process initialization, and hands\n    off to the [NativeTest].\n 6. The [NativeTest] handles some initialization and informs the\n    [NativeTestInstrumentationTestRunner] that it has started. On hearing this,\n    the [NativeTestInstrumentationTestRunner] creates a [ShardMonitor] \n    that will monitor the execution of the test Activity.\n 7. The [NativeTest] hands off to testing::android::RunTests. The tests run.\n 8. The [NativeTest] informs the [NativeTestInstrumentationTestRunner] that is has\n    completed. On hearing this, the [ShardMonitor] creates a [ShardEnder].\n 9. The [ShardEnder] is executed, killing the child process (if applicable),\n    parsing the results from the stdout file, and either launching the next\n    shard via [ShardStarter] (in which case the process returns to #4) or sending\n    the results out to the test runner and finishing the instrumentation.\n\n## Executables\n\n### GN\n\nGtest executables are built by passing\n`use_raw_android_executable = True` to the \n[test](https://codesearch.chromium.org/chromium/src/testing/test.gni?type=cs&q=file:%5Esrc%5C/testing%5C/test.gni$+template%5C(\"test\"%5C)&sq=package:chromium)\ntemplate, e.g.\n\n```python\ntest(\"sample_gtest_executable\") {\n  if (is_android) {\n    use_raw_android_executable = true\n  }\n  # ...\n}\n```\n\nThis uses gn's native\n[executable](https://chromium.googlesource.com/chromium/src/+/main/tools/gn/docs/reference.md#executable_Declare-an-executable-target)\ntarget type, then copies the resulting executable and any requisite shared libraries\nto ```${root_out_dir}/${target_name}__dist``` (e.g. ```out/Debug/breakpad_unittests__dist```).\n\n### Harness\n\nUnlike APKs, gtest suites built as executables require no Android-specific harnesses.\n\n### Runtime\n\nThe test runner simply executes the binary on the device directly and parses the\nstdout on its own.\n\n[NativeTest]: https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeTest.java\n[NativeTestInstrumentationTestRunner]: https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeTestInstrumentationTestRunner.java\n[NativeUnitTest]: https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeUnitTest.java\n[ShardEnder]: https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeTestInstrumentationTestRunner.java?q=file:NativeTestInstrumentationTestRunner.java+class:ShardEnder&sq=package:chromium\n[ShardMonitor]: https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeTestInstrumentationTestRunner.java?q=file:NativeTestInstrumentationTestRunner.java+class:ShardMonitor&sq=package:chromium\n[ShardStarter]: https://codesearch.chromium.org/chromium/src/testing/android/native_test/java/src/org/chromium/native_test/NativeTestInstrumentationTestRunner.java?q=file:NativeTestInstrumentationTestRunner.java+class:ShardStarter&sq=package:chromium\n"
  },
  {
    "path": "development/testing/chromeos_integration/README",
    "title": "Crosier: **C**h**r**ome**OS** **I**ntegration/**E**2E-test **R**evamp",
    "content": "# Crosier: **C**h**r**ome**OS** **I**ntegration/**E**2E-test **R**evamp\n\n## Overview\nCrosier is a project for running GTest-based integration tests on a ChromeOS\nhardware Device Under Test (DUT) or in a ChromeOS Virtual Machine (VM). The idea\nis similar to interactive_ui_tests. A runner uses GTest to launch a full browser\non DUT/VM, and the test body runs in the same process as the browser process.\nUtility APIs will be provided so the test body can access Chrome classes and\nobjects like normal browser tests. The test can also access ChromeOS system\nservices/daemons via utilities (D-Bus wrappers, shell commands, etc.).\n\n## Guidelines for using Crosier\nGetting a DUT for local development is not easy for most developers and cros VM\nsetup is harder than using the linux-chromeos \"emulator\" environment. We should\nlimit the usage of Crosier and avoid DUT testing when possible.\nRunning ChromeOS integration tests on DUT is only recommended for tests\nexercising hardware components (e.g. graphics tests) or testing communication\nwith OS daemons (e.g. Bluetooth daemon).\n\nThe preferred order when adding a new test:\n- unit_tests, ash_unittests, browser_tests in linux-chromeos environment\n- unit_tests, chromeos_integration_tests on a ChromeOS VM\n- unit_tests, chromeos_integration_tests on a ChromeOS device (DUT)\n\nContacts: <crosier-team@google.com>\n"
  },
  {
    "path": "development/testing/chromeos_integration/development_guide",
    "title": "Crosier Development Guide",
    "content": "# Crosier Development Guide\n\nThis doc assumes you're already familiar with ChromeOS on-device development.\nIf not, please follow\n[Simple Chrome workflow](https://chromium.googlesource.com/chromiumos/docs/+/HEAD/simple_chrome_workflow.md)\n\n## Demo test\nWe've had several tests.\n\n[NewTab demo](https://source.chromium.org/chromium/chromium/src/+/main:chrome/test/base/chromeos/crosier/demo_integration_test.cc)\nis a test that opens a browser and open a tab which can run DUT or VM.\n\n[Bluetooth test](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/ash/bluetooth/bluetooth_integration_test.cc)\nis a test that toggle to turn on and off bluetooth which can only\nrun on DUT.\n\n## How to run Ash test\nPlease see: [go/crosier-run](http://go/crosier-run)\n\n## Test metadata\n\nEach Crosier test being added should include test metadata in `yaml` format.\n\nSee [Crosier metadata guide](https://source.chromium.org/chromium/chromium/src/+/main:docs/testing/chromeos_integration/crosier_metadata.md)\nfor more information on how to add it.\n\n## Debugging Crosier tests\n\nDebugging a Crosier test is like debugging any Chrome test on device. Following\ndeveloper library resources are available:\n\nhttps://www.chromium.org/chromium-os/developer-library/guides/#debugging\n\n## Continuous builders\n\nhttps://ci.chromium.org/ui/p/chromium/builders/ci/chromeos-amd64-generic-rel\n\nTarget name is 'chromeos_integration_tests'.\n\nWhen you make contributions, please add 'Include-Ci-Only-Tests: true' to the\nchangelist footer so the tests can run on CQ.\n"
  },
  {
    "path": "development/testing/chromeos_integration/crosier_metadata",
    "title": "Metadata for Crosier tests",
    "content": "# Metadata for Crosier tests\n\n## Create a yaml file with test case details\n\nCreate a yaml file in the same location of your test source file. Example of\nsuch yaml file:\n\n```yaml\n---\nname: \"LockScreen\"\nharness: \"crosier\"\ncategory: \"integration\"\nowners:\n  - email: \"test-owner-email@google.com\"\n  - email: \"chromeos-sw-engprod@google.com\"\nhw_agnostic: False\ncriteria: |\n  Tests that dbus messages for lid close trigger screen lock. This only tests\n  the \"lock on lid close\" pref state.\ncases:\n  - id: \"CloseLidDbusIntegration\"\n    tags: [\"crosier:crosierdemosuite\", \"crosier:cq\"]\n  - id: \"CloseLidPref\"\n    tags: [\"crosier:crosierdemosuite\", \"crosier:cq\", \"informational\"]\n...\n```\n\nPay specifal attention to the list of `tags` for each test case. Using these\ntags, tests are being allocated to ChromeOS CI/CQ scheduling suites.\n\nFollowing tags are recognized and supported:\n\n* `crosier:crosierdemosuite` - test case will run daily in a dedicated,\nnon-critical test suite for stability and regression monitoring.\n* `crosier:cq` - test case will run in global CQ and post-submit snapshot\nbuilds.\n* `informational` - test case will be considered as non-critical, i.e. not\nincluded in the critical CQ test suite.\n* `group:hw_agnostic` - test will run on VMs rather than on devices.\n\n## Include new yaml file in Crosier binary build\n\nThe link to all yaml files should be added to the `crosier_metadata` rules that\nis included by the Crosier `chromeos_integration_tests` rule of the\n[BUILD.gn](https://chromium.googlesource.com/chromium/src/+/main/chrome/test/BUILD.gn)\nfile:\n\n```\ncopy(\"crosier_metadata\") {\n    sources = [\n        ...\n        \"../browser/ash/login/lock/lock_screen_integration_test.yaml\",\n        ...\n        ...\n    ]\n    outputs = [ \"$root_out_dir/crosier_metadata/{{source_file_part}}\" ]\n}\n\n```\n"
  },
  {
    "path": "development/build/windows_build_instructions",
    "title": "Checking out and Building Chromium for Windows",
    "content": "# Checking out and Building Chromium for Windows\n\nThere are instructions for other platforms linked from the\n[get the code](get_the_code.md) page.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-chrome-win](https://goto.google.com/building-chrome-win) instead.\n\n[TOC]\n\n## System requirements\n\n* An x86-64 machine with at least 8GB of RAM. More than 16GB is highly\n  recommended.\n* At least 100GB of free disk space on an NTFS-formatted hard drive. FAT32\n  will not work, as some of the Git packfiles are larger than 4GB.\n* An appropriate version of Visual Studio, as described below.\n* Windows 10 or newer.\n\n## Setting up Windows\n\n### Visual Studio\n\nChromium requires [Visual Studio 2022](https://learn.microsoft.com/en-us/visualstudio/releases/2022/release-notes)\n(>=17.0.0) to build. Visual Studio can also be used to debug Chromium.\nThe clang-cl compiler is used but Visual Studio's header files, libraries, and\nsome tools are required. Visual Studio Community Edition should work if its\nlicense is appropriate for you. You must install the \"Desktop development with\nC++\" component and the \"MFC/ATL support\" sub-components. This can be done from\nthe command line by passing these arguments to the Visual Studio installer (see\nbelow for ARM64 instructions):\n```shell\n$ PATH_TO_INSTALLER.EXE ^\n--add Microsoft.VisualStudio.Workload.NativeDesktop ^\n--add Microsoft.VisualStudio.Component.VC.ATLMFC ^\n--includeRecommended\n```\n\nIf you want to build for ARM64 Win32 then some extra arguments are needed. The\nfull set for that case is:\n```shell\n$ PATH_TO_INSTALLER.EXE ^\n--add Microsoft.VisualStudio.Workload.NativeDesktop ^\n--add Microsoft.VisualStudio.Component.VC.ATLMFC ^\n--add Microsoft.VisualStudio.Component.VC.Tools.ARM64 ^\n--add Microsoft.VisualStudio.Component.VC.MFC.ARM64 ^\n--includeRecommended\n```\n\nRequired\n\n* [Windows 11 SDK](https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/)\nversion 10.0.22621.2428. This can be installed separately or by checking the\nappropriate box in the Visual Studio Installer.\n* (Windows 11) SDK Debugging Tools 10.0.22621.755 or higher. This version of the\nDebugging tools is needed in order to support reading the large-page PDBs that\nChrome uses to allow greater-than 4 GiB PDBs. This can be installed after the\nmatching Windows SDK version is installed, from: Control Panel -> Programs and\nFeatures -> Windows Software Development Kit [version] -> Change -> Debugging Tools for\nWindows. If building on ARM64 Windows then you will need to manually copy the\nDebuggers\\x64 directory from another machine because it does not get installed\non ARM64 and is needed, whether you are building Chromium for x64 or ARM64 on\nARM64.\n\nWARNING: On sufficiently old versions of Windows (1909 or earlier), dawn (or\nrelated components) may fail with a D3d-related error when using the 26100 SDK.\nThis is because the d3dcompiler_47.dll file in the new SDK attempts to\ndynamically link versions of the Universal C Runtime which are not present by\ndefault on older systems. If you experience these errors, you can either update\nthe UCRT on your system, or install the 22612 SDK and use the d3dcompiler_47.dll\nfile included there, which statically links the UCRT.\n\nThis problem may also manifest as a DLL failure to load `__CxxFrameHandler4`.\n\n## git installation\n\n### Install git\n\nIf you haven't installed `git` directly before, you can download a standalone\ninstaller for the latest version of Git For Windows from the Git website at\nhttps://git-scm.com/download/win.\n\nFor more information on Git for Windows (which is a separate project from Git),\nsee https://gitforwindows.org.\n\nNote: if you are a Google employee, see git installation instructions at\n[go/building-chrome-win](https://goto.google.com/building-chrome-win#install-updates-and-required-software).\n\n### Update git\n\nNote: this section is about updating a direct installation of `git` because\n`depot_tools` will soon stop bundling `git`.\n\nUpdating to the latest version of `git` will depend on which version you\ncurrently have installed. First, check your `git` version. From a cmd.exe shell,\nrun:\n```shell\n$ git version\n```\n\n| Current version | How to update to latest |\n| --- | --- |\n| `2.14.1` or earlier | You will need to manually uninstall Git, then follow the instructions above to [install git](#install-git) |\n| `2.14.2` to `2.16.1` | In a cmd.exe shell, run: `git update` |\n| `2.16.1(2)` and later | In a cmd.exe shell, run: `git update-git-for-windows` |\n\n## Install `depot_tools`\n\n***\n**Warning:** `depot_tools` will stop bundling Git for Windows from Sep 23, 2024\nonwards. To prepare for this change, Windows users should\n[install Git](#git-installation) directly before then.\n***\n\nDownload the\n[depot_tools bundle](https://storage.googleapis.com/chrome-infra/depot_tools.zip)\nand extract it somewhere (eg: C:\\src\\depot_tools).\n\n***\n**Warning:** **DO NOT** use drag-n-drop or copy-n-paste extract from Explorer,\nthis will not extract the hidden “.git” folder which is necessary for\ndepot_tools to autoupdate itself. You can use “Extract all…” from the\ncontext menu though.\n***\n\nAdd depot_tools to the start of your PATH (must be ahead of any installs of\nPython. Note that environment variable names are case insensitive).\n* Assuming you unzipped the bundle to `C:\\src\\depot_tools`, open:\n  Control Panel → System and Security → System\n* Select which PATH variable to edit.\n  * If you have Administrator access, you can edit the **system** PATH. Click\n  Advanced system settings → Environment Variables. Under \"System variables\",\n  select the Path variable for editing.\n  * If you don't have Administrator access, you can edit your **user-level**\n  PATH. Search for \"Edit environment variables for your account\". Under \"User\n  variables for %USER%\", select the Path variable for editing.\n* Modify the Path variable by adding `C:\\src\\depot_tools` at the front (or at\n  least in front of any directory that might already have a copy of Python).\n  Note: If you can only modify your user-level PATH and the system PATH has a\n  Python in it, you will be out of luck.\n\nAlso, add a DEPOT_TOOLS_WIN_TOOLCHAIN environment variable in the same way, and\nset it to 0. This tells depot_tools to use your locally installed version of\nVisual Studio (by default, depot_tools will try to use a google-internal\nversion).\n\nYou may also have to set variable `vs2022_install` to your installation path of\nVisual Studio 2022, like\n`set vs2022_install=C:\\Program Files\\Microsoft Visual Studio\\2022\\Professional`.\n\nFrom a cmd.exe shell, run:\n```shell\n$ gclient\n```\n\nOn first run, gclient will install all the Windows-specific bits needed to work\nwith the code, including msysgit and python.\n\n* If you run gclient from a non-cmd shell (e.g., cygwin, PowerShell),\n  it may appear to run properly, but msysgit, python, and other tools\n  may not get installed correctly.\n* If you see strange errors with the file system on the first run of gclient,\n  you may want to [disable Windows Indexing](https://tortoisesvn.net/faq.html#cantmove2).\n\n## Check python install\n\nAfter running gclient open a command prompt and type `where python3` and\nconfirm that the depot_tools `python3.bat` comes ahead of any copies of\npython3.exe. Failing to ensure this can lead to overbuilding when\nusing gn - see [crbug.com/611087](https://crbug.com/611087).\n\n[App Execution Aliases](https://docs.microsoft.com/en-us/windows/apps/desktop/modernize/desktop-to-uwp-extensions#alias)\ncan conflict with other installations of python on the system so disable\nthese for 'python.exe' and 'python3.exe' by opening 'App execution aliases'\nsection of Control Panel and unticking the boxes next to both of these\nthat point to 'App Installer'.\n\n## Get the code\n\nFirst, configure Git:\n\n```shell\n$ git config --global user.name \"My Name\"\n$ git config --global user.email \"my-name@chromium.org\"\n$ git config --global core.autocrlf false\n$ git config --global core.filemode false\n$ git config --global core.preloadindex true\n$ git config --global core.fscache true\n$ git config --global branch.autosetuprebase always\n```\n\nWhile not necessarily required it can be helpful to configure git to allow long\npath support (beyond the Windows MAX_PATH limit):\n\n```shell\ngit config --global core.longpaths true\n```\n\nCreate a `chromium` directory for the checkout and change to it. You can call\nthis whatever you like and put it wherever you like, as long as the full path\nhas no spaces. However there are some performance benefits for Googlers in\nplacing the directory under `C:\\src\\`\n(See [Why is my build slow?](https://chromium.googlesource.com/chromium/src/+/main/docs/windows_build_instructions.md#why-is-my-build-slow)).\n\n```shell\n$ mkdir chromium && cd chromium\n```\n\nRun the `fetch` tool from `depot_tools` to check out the code and its\ndependencies.\n\n```shell\n$ fetch chromium\n```\n\nIf you don't want the full repo history, you can save a lot of time by\nadding the `--no-history` flag to `fetch`.\n\nExpect the command to take over an hour on even a fast connection, and many\nhours on slower ones. You should configure your PC so that it doesn't sleep\nor hibernate during the fetch or else errors may occur. If errors occur while\nfetching sub-repos then you can start over, or you may be able to correct them\nby going to the chromium/src directory and running this command:\n\n```shell\n$ gclient sync\n```\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\n$ cd src\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory:\n\n```shell\n$ gn gen out\\Default\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings or using an alternate\n  version of Visual Studio, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build matching the current host\n  operating system and CPU.\n* For more info on GN, run `gn help` on the command line or read the [quick\n  start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n\n### Faster builds\n\n* Reduce file system overhead by excluding build directories from\n  antivirus and indexing software.\n* Store the build tree on a fast disk (preferably SSD).\n* The more cores the better (20+ is not excessive) and lots of RAM is needed\n(64 GB is not excessive).\n\nThere are some gn flags that can improve build speeds. You can specify these\nin the editor that appears when you create your output directory\n(`gn args out\\Default`) or on the gn gen command line\n(`gn gen out\\Default --args=\"is_component_build = true is_debug = true\"`).\nSome helpful settings to consider using include:\n* `is_component_build = true` - this uses more, smaller DLLs, and may avoid\nhaving to relink chrome.dll after every change.\n* `enable_nacl = false` - this disables Native Client which is usually not\nneeded for local builds.\n* `target_cpu = \"x86\"` - x86 builds may be slightly faster than x64 builds. Note\nthat if you set this but don't set `enable_nacl = false` then build times may\nget worse.\n* `blink_symbol_level = 0` - turn off source-level debugging for\nblink to reduce build times, appropriate if you don't plan to debug blink.\n* `v8_symbol_level = 0` - turn off source-level debugging for v8 to reduce\nbuild times, appropriate if you don't plan to debug v8.\n\nIn order to speed up linking you can set `symbol_level = 1` or\n`symbol_level = 0` - these options reduce the work the compiler and linker have\nto do. With `symbol_level = 1` the compiler emits file name and line number\ninformation so you can still do source-level debugging but there will be no\nlocal variable or type information. With `symbol_level = 0` there is no\nsource-level debugging but call stacks still have function names. Changing\n`symbol_level` requires recompiling everything.\n\nWhen invoking ninja, specify 'chrome' as the target to avoid building all test\nbinaries as well.\n\n#### Use Reclient\n\nIn addition, Google employees should use Reclient, a distributed compilation\nsystem. Detailed information is available internally but the relevant gn arg is:\n* `use_remoteexec = true`\n\nGoogle employees can visit\n[go/building-chrome-win#setup-remote-execution](https://goto.google.com/building-chrome-win#setup-remote-execution)\nfor more information. For external contributors, Reclient does not support\nWindows builds.\n\n#### Use SCCACHE\n\nYou might be able to use [sccache](https://github.com/mozilla/sccache) for the\nbuild process by enabling the following arguments:\n\n* `cc_wrapper = \"sccache\"` - assuming the `sccache` binary is in your `%PATH%`\n\n### Why is my build slow?\n\nMany things can make builds slow, with Windows Defender slowing process startups\nbeing a frequent culprit. Have you ensured that the entire Chromium src\ndirectory is excluded from antivirus scanning (on Google machines this means\nputting it in a ``src`` directory in the root of a drive)? Have you tried the\ndifferent settings listed above, including different link settings and -j\nvalues? Have you asked on the chromium-dev mailing list to see if your build is\nslower than expected for your machine's specifications?\n\nIf you suspect that Defender is slowing your build then you can try Microsoft's\n[Performance analyzer for Microsoft Defender Antivirus](https://learn.microsoft.com/en-us/microsoft-365/security/defender-endpoint/tune-performance-defender-antivirus?view=o365-worldwide)\nto investigate in detail.\n\nThe next step is to gather some data. If you set the ``NINJA_SUMMARIZE_BUILD``\nenvironment variable to 1 then ``autoninja`` will do three things. First, it\nwill set the [NINJA_STATUS](https://ninja-build.org/manual.html#_environment_variables)\nenvironment variable so that ninja will print additional information while\nbuilding Chrome. It will show how many build processes are running at any given\ntime, how many build steps have completed, how many build steps have completed\nper second, and how long the build has been running, as shown here:\n\n```shell\n$ set NINJA_SUMMARIZE_BUILD=1\n$ autoninja -C out\\Default base\nninja: Entering directory `out\\Default'\n[1 processes, 86/86 @ 2.7/s : 31.785s ] LINK(DLL) base.dll base.dll.lib base.dll.pdb\n```\n\nThis makes slow process creation immediately obvious and lets you tell quickly\nif a build is running more slowly than normal.\n\nIn addition, setting ``NINJA_SUMMARIZE_BUILD=1`` tells ``autoninja`` to print a\nbuild performance summary when the build completes, showing the slowest build\nsteps and slowest build-step types, as shown here:\n\n```shell\n$ set NINJA_SUMMARIZE_BUILD=1\n$ autoninja -C out\\Default base\nLongest build steps:\n       0.1 weighted s to build obj/base/base/trace_log.obj (6.7 s elapsed time)\n       0.2 weighted s to build nasm.exe, nasm.exe.pdb (0.2 s elapsed time)\n       0.3 weighted s to build obj/base/base/win_util.obj (12.4 s elapsed time)\n       1.2 weighted s to build base.dll, base.dll.lib (1.2 s elapsed time)\nTime by build-step type:\n       0.0 s weighted time to generate 6 .lib files (0.3 s elapsed time sum)\n       0.1 s weighted time to generate 25 .stamp files (1.2 s elapsed time sum)\n       0.2 s weighted time to generate 20 .o files (2.8 s elapsed time sum)\n       1.7 s weighted time to generate 4 PEFile (linking) files (2.0 s elapsed\ntime sum)\n      23.9 s weighted time to generate 770 .obj files (974.8 s elapsed time sum)\n26.1 s weighted time (982.9 s elapsed time sum, 37.7x parallelism)\n839 build steps completed, average of 32.17/s\n```\n\nThe \"weighted\" time is the elapsed time of each build step divided by the number\nof tasks that were running in parallel. This makes it an excellent approximation\nof how \"important\" a slow step was. A link that is entirely or mostly serialized\nwill have a weighted time that is the same or similar to its elapsed time. A\ncompile that runs in parallel with 999 other compiles will have a weighted time\nthat is tiny.\n\nYou can also generate these reports by manually running the script after a\nbuild:\n\n```shell\n$ python depot_tools\\post_build_ninja_summary.py -C out\\Default\n```\n\nFinally, setting ``NINJA_SUMMARIZE_BUILD=1`` tells autoninja to tell Ninja to\nreport on its own overhead by passing \"-d stats\". This can be helpful if, for\ninstance, process creation (which shows up in the StartEdge metric) is making\nbuilds slow, perhaps due to antivirus interference due to clang-cl not being in\nan excluded directory:\n\n```shell\n$ set NINJA_SUMMARIZE_BUILD=1\n$ autoninja -C out\\Default base\nmetric                  count   avg (us)        total (ms)\n.ninja parse            3555    1539.4          5472.6\ncanonicalize str        1383032 0.0             12.7\ncanonicalize path       1402349 0.0             11.2\nlookup node             1398245 0.0             8.1\n.ninja_log load         2       118.0           0.2\n.ninja_deps load        2       67.5            0.1\nnode stat               2516    29.6            74.4\ndepfile load            2       1132.0          2.3\nStartEdge               88      3508.1          308.7\nFinishCommand           87      1670.9          145.4\nCLParser::Parse         45      1889.1          85.0\n```\n\nYou can also get a visual report of the build performance with\n[ninjatracing](https://github.com/nico/ninjatracing). This converts the\n.ninja_log file into a .json file which can be loaded into [chrome://tracing](chrome://tracing):\n\n```shell\n$ python ninjatracing out\\Default\\.ninja_log >build.json\n```\n\n## Build Chromium\n\nBuild Chromium (the \"chrome\" target) with Ninja using the command:\n\n```shell\n$ autoninja -C out\\Default chrome\n```\n\n`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.\n\nYou can get a list of all of the other build targets from GN by running\n`gn ls out\\Default` from the command line. To compile one, pass to Ninja\nthe GN label with no preceding \"//\" (so for `//chrome/test:unit_tests`\nuse `autoninja -C out\\Default chrome/test:unit_tests`).\n\n## Compile a single file\n\nNinja supports a special [syntax `^`][ninja hat syntax] to compile a single\nobject file specifying the source file. For example, `ninja -C\nout/Default ../../base/logging.cc^` compiles `obj/base/base/logging.o`.\n\n[ninja hat syntax]: https://ninja-build.org/manual.html#:~:text=There%20is%20also%20a%20special%20syntax%20target%5E%20for%20specifying%20a%20target%20as%20the%20first%20output%20of%20some%20rule%20containing%20the%20source%20you%20put%20in%20the%20command%20line%2C%20if%20one%20exists.%20For%20example%2C%20if%20you%20specify%20target%20as%20foo.c%5E%20then%20foo.o%20will%20get%20built%20(assuming%20you%20have%20those%20targets%20in%20your%20build%20files)\n\nWith autoninja, you need to add  `^^` to preserve the trailing `^`.\n\n```shell\n$ autoninja -C out\\Default ..\\..\\base\\logging.cc^^\n```\n\nIn addition to `foo.cc^^`, Siso also supports `foo.h^^` syntax to compile\nthe corresponding `foo.o` if it exists.\n\nIf you run a `bash` shell, you can use the following script to ease invocation:\n\n```shell\n#!/bin/sh\nfiles=(\"${@/#/..\\/..\\/}\")\nautoninja -C out/Default ${files[@]/%/^^}\n```\n\nThis script assumes it is run from `src` and your output dir is `out/Default`;\nit invokes `autoninja` to compile all given files. If you place it in your\n`$PATH` and name it e.g. `compile`, you can invoke like this:\n\n```shell\n$ pwd  # Just to illustrate where this is run from\n/c/src\n$ compile base/time/time.cc base/time/time_unittest.cc\n...\n[0/47] 5.56s S CXX obj/base/base/time.obj\n...\n[2/3] 9.27s S CXX obj/base/base_unittests/time_unittest.obj\n...\n```\n\n## Run Chromium\n\nOnce it is built, you can simply run the browser:\n\n```shell\n$ out\\Default\\chrome.exe\n```\n\n(The \".exe\" suffix in the command is actually optional).\n\n## Running test targets\n\nTests are split into multiple test targets based on their type and where they\nexist in the directory structure. To see what target a given unit test or\nbrowser test file corresponds to, the following command can be used:\n\n```shell\n$ gn refs out\\Default --testonly=true --type=executable --all chrome\\browser\\ui\\browser_list_unittest.cc\n//chrome/test:unit_tests\n```\n\nIn the example above, the target is unit_tests. The unit_tests binary can be\nbuilt by running the following command:\n\n```shell\n$ autoninja -C out\\Default unit_tests\n```\n\nYou can run the tests by running the unit_tests binary. You can also limit which\ntests are run using the `--gtest_filter` arg, e.g.:\n\n```shell\n$ out\\Default\\unit_tests.exe --gtest_filter=\"BrowserListUnitTest.*\"\n```\n\nYou can find out more about GoogleTest at its\n[GitHub page](https://github.com/google/googletest).\n\n## Build an Installer\n\nBuild the `mini_installer` target to create a self-contained installer. This\nhas everything needed to install your browser on a machine.\n\n```shell\n$ autoninja -C out\\Default mini_installer\n```\n\nSee [//chrome/installer/setup/README.md](../chrome/installer/setup/README.md)\nand [//chrome/installer/mini_installer/README.md](../chrome/installer/mini_installer/README.md)\nfor more information.\n\n## Update your checkout\n\nTo update an existing checkout, you can run\n\n```shell\n$ git rebase-update\n$ gclient sync -D\n```\n\nThe first command updates the primary Chromium source repository and rebases\nany of your local branches on top of tip-of-tree (aka the Git branch\n`origin/main`). If you don't want to use this script, you can also just use\n`git pull` or other common Git commands to update the repo.\n\nThe second command syncs the subrepositories to the appropriate versions,\ndeleting those that are no longer needed, and re-runs the hooks as needed.\n\n### Editing and Debugging With the Visual Studio IDE\n\nYou can use the Visual Studio IDE to edit and debug Chrome, with or without\nIntellisense support.\n\n#### Using Visual Studio Intellisense\n\nIf you want to use Visual Studio Intellisense when developing Chromium, use the\n`--ide` command line argument to `gn gen` when you generate your output\ndirectory (as described on the [get the code](https://dev.chromium.org/developers/how-tos/get-the-code)\npage). This is an example when your checkout is `C:\\src\\chromium` and your\noutput directory is `out\\Default`:\n\n```shell\n$ gn gen --ide=vs --ninja-executable=C:\\src\\chromium\\src\\third_party\\ninja\\ninja.exe out\\Default\n$ devenv out\\Default\\all.sln\n```\n\nGN will produce a file `all.sln` in your build directory. It will internally\nuse Ninja to compile while still allowing most IDE functions to work (there is\nno native Visual Studio compilation mode). If you manually run \"gen\" again you\nwill need to resupply this argument, but normally GN will keep the build and\nIDE files up to date automatically when you build.\n\nThe generated solution will contain several thousand projects and will be very\nslow to load. Use the `--filters` argument to restrict generating project files\nfor only the code you're interested in. Although this will also limit what\nfiles appear in the project explorer, debugging will still work and you can\nset breakpoints in files that you open manually. A minimal solution that will\nlet you compile and run Chrome in the IDE but will not show any source files\nis:\n\n```\n$ gn gen --ide=vs --ninja-executable=C:\\src\\chromium\\src\\third_party\\ninja\\ninja.exe --filters=//chrome --no-deps out\\Default\n```\n\nYou can selectively add other directories you care about to the filter like so:\n`--filters=//chrome;//third_party/WebKit/*;//gpu/*`.\n\nThere are other options for controlling how the solution is generated, run `gn\nhelp gen` for the current documentation.\n\n#### Using Visual Studio without Intellisense\n\nIt is also possible to debug and develop Chrome in Visual Studio without the\noverhead of a multi-project solution file. Simply \"open\" your chrome.exe binary\nwith `File->Open->Project/Solution`, or from a Visual Studio command prompt like\nso: `devenv /debugexe out\\Debug\\chrome.exe <your arguments>`. Many of Visual\nStudio's code exploration features will not work in this configuration, but by\ninstalling the [VsChromium Visual Studio Extension](https://chromium.github.io/vs-chromium/)\nyou can get the source code to appear in the solution explorer window along\nwith other useful features such as code search. You can add multiple executables\nof interest (base_unittests.exe, browser_tests.exe) to your solution with\n`File->Add->Existing Project...` and change which one will be debugged by\nright-clicking on them in `Solution Explorer` and selecting `Set as Startup\nProject`. You can also change their properties, including command line\narguments, by right-clicking on them in `Solution Explorer` and selecting\n`Properties`.\n\nBy default when you start debugging in Visual Studio the debugger will only\nattach to the main browser process. To debug all of Chrome, install\n[Microsoft's Child Process Debugging Power Tool](https://blogs.msdn.microsoft.com/devops/2014/11/24/introducing-the-child-process-debugging-power-tool/).\nYou will also need to run Visual Studio as administrator, or it will silently\nfail to attach to some of Chrome's child processes.\n\n### Improving performance of git commands\n\n#### Configure git to use an untracked cache\n\nTry running\n\n```shell\n$ git update-index --test-untracked-cache\n```\n\nIf the output ends with `OK`, then the following may also improve performance of\n`git status`:\n\n```shell\n$ git config core.untrackedCache true\n```\n\n#### Configure git to use fsmonitor\n\nYou can significantly speed up git by using [fsmonitor.](https://github.blog/2022-06-29-improve-git-monorepo-performance-with-a-file-system-monitor/)\nYou should enable fsmonitor in large repos, such as Chromium and v8. Enabling\nit globally will launch many processes and consume excess commit/memory and\nprobably isn't worthwhile. The command to enable fsmonitor in the current repo\nis:\n\n```shell\n$ git config core.fsmonitor true\n```\n"
  },
  {
    "path": "development/build/mac_build_instructions",
    "title": "Checking out and building Chromium for Mac",
    "content": "# Checking out and building Chromium for Mac\n\nThere are instructions for other platforms linked from the\n[get the code](get_the_code.md) page.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-chrome](https://goto.google.com/building-chrome) instead.\n\n[TOC]\n\n## System requirements\n\n*   A Mac, Intel or Arm.\n    ([More details about Arm Macs](https://chromium.googlesource.com/chromium/src.git/+/main/docs/mac_arm64.md).)\n*   [Xcode](https://developer.apple.com/xcode/). Xcode comes with...\n*   The macOS SDK. Run\n\n    ```shell\n    $ ls `xcode-select -p`/Platforms/MacOSX.platform/Developer/SDKs\n    ```\n\n    to check whether you have it, and what version you have.\n    `mac_sdk_official_version` in [mac_sdk.gni](../build/config/mac/mac_sdk.gni)\n    is the SDK version used on all the bots and for\n    [official builds](https://source.chromium.org/search?q=MAC_BINARIES_LABEL&ss=chromium),\n    so that version is guaranteed to work. Building with a newer SDK usually\n    works too (please fix or file a bug if it doesn't).\n\n    Building with an older SDK might also work, but if it doesn't then we won't\n    accept changes for making it work.\n\n    The easiest way to get the newest SDK is to use the newest version of Xcode,\n    which often requires using the newest version of macOS. We don't use Xcode\n    itself much, so if you're know what you're doing, you can likely get the\n    build working with an older version of macOS as long as you get a new\n    version of the macOS SDK on it.\n*   An APFS-formatted volume (this is the default format for macOS volumes).\n\n## Install `depot_tools`\n\nClone the `depot_tools` repository:\n\n```shell\n$ git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\n```\n\nAdd `depot_tools` to the end of your PATH (you will probably want to put this in\nyour `~/.bash_profile` or `~/.zshrc`). Assuming you cloned `depot_tools` to\n`/path/to/depot_tools` (note: you **must** use the absolute path or Python will\nnot be able to find infra tools):\n\n```shell\n$ export PATH=\"$PATH:/path/to/depot_tools\"\n```\n\n## Get the code\n\nCreate a `chromium` directory for the checkout and change to it (you can call\nthis whatever you like and put it wherever you like, as long as the full path\nhas no spaces):\n\n```shell\n$ mkdir chromium && cd chromium\n```\n\nRun the `fetch` tool from `depot_tools` to check out the code and its\ndependencies.\n\n```shell\n$ caffeinate fetch chromium\n```\n\nRunning the `fetch` with `caffeinate` is optional, but it will prevent the\nsystem from sleeping for the duration of the `fetch` command, which may run for\na considerable amount of time.\n\nIf you don't need the full repo history, you can save time by using\n`fetch --no-history chromium`. You can call `git fetch --unshallow` to retrieve\nthe full history later.\n\nExpect the command to take 30 minutes on even a fast connection, and many\nhours on slower ones.\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\n$ cd src\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory:\n\n```shell\n$ gn gen out/Default\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build matching the current host\n  operating system and CPU.\n* For more info on GN, run `gn help` on the command line or read the\n  [quick start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n* Building Chromium for arm Macs requires [additional setup](mac_arm64.md).\n\n\n### Faster builds\n\nFull rebuilds are about the same speed in Debug and Release, but linking is a\nlot faster in Release builds.\n\nPut\n\n```\nis_debug = false\n```\n\nin your `args.gn` to do a release build.\n\nPut\n\n```\nis_component_build = true\n```\n\nin your `args.gn` to build many small dylibs instead of a single large\nexecutable. This makes incremental builds much faster, at the cost of producing\na binary that opens less quickly. Component builds work in both debug and\nrelease.\n\nPut\n\n```\nsymbol_level = 0\n```\n\nin your args.gn to disable debug symbols altogether.  This makes both full\nrebuilds and linking faster (at the cost of not getting symbolized backtraces\nin gdb).\n\n#### Use Reclient\n\nIn addition, Google employees should use Reclient, a distributed compilation system.\nDetailed information is available internally but the relevant gn arg is:\n* `use_remoteexec = true`\n\nGoogle employees can visit\n[go/building-chrome-mac#using-remote-execution](https://goto.google.com/building-chrome-mac#using-remote-execution)\nfor more information. For external contributors, Reclient does not support Mac\nbuilds.\n\n#### CCache\n\nYou might also want to [install ccache](ccache_mac.md) to speed up the build.\n\n## Build Chromium\n\nBuild Chromium (the \"chrome\" target) with Ninja using the command:\n\n```shell\n$ autoninja -C out/Default chrome\n```\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.)\n\nYou can get a list of all of the other build targets from GN by running `gn ls\nout/Default` from the command line. To compile one, pass the GN label to Ninja\nwith no preceding \"//\" (so, for `//chrome/test:unit_tests` use `autoninja -C\nout/Default chrome/test:unit_tests`).\n\n## Run Chromium\n\nOnce it is built, you can simply run the browser:\n\n```shell\n$ out/Default/Chromium.app/Contents/MacOS/Chromium\n```\n\n## Avoiding system permissions dialogs after each build\n\nEvery time you start a new developer build, you may get two system dialogs:\n`Chromium wants to use your confidential information stored in \"Chromium Safe\nStorage\" in your keychain.`, and `Do you want the application \"Chromium.app\" to\naccept incoming network connections?`.\n\nTo avoid them, you can run Chromium with these command-line flags (but of\ncourse beware that they will change the behavior of certain subsystems):\n\n```shell\n--use-mock-keychain --disable-features=DialMediaRouteProvider\n```\n\n## Build and run test targets\n\nTests are split into multiple test targets based on their type and where they\nexist in the directory structure. To see what target a given unit test or\nbrowser test file corresponds to, the following command can be used:\n\n```shell\n$ gn refs out/Default --testonly=true --type=executable --all chrome/browser/ui/browser_list_unittest.cc\n//chrome/test:unit_tests\n```\n\nIn the example above, the target is unit_tests. The unit_tests binary can be\nbuilt by running the following command:\n\n```shell\n$ autoninja -C out/Default unit_tests\n```\n\nYou can run the tests by running the unit_tests binary. You can also limit which\ntests are run using the `--gtest_filter` arg, e.g.:\n\n```shell\n$ out/Default/unit_tests --gtest_filter=\"BrowserListUnitTest.*\"\n```\n\nYou can find out more about GoogleTest at its\n[GitHub page](https://github.com/google/googletest).\n\n## Debugging\n\nGood debugging tips can be found [here](mac/debugging.md).\n\n## Update your checkout\n\nTo update an existing checkout, you can run\n\n```shell\n$ git rebase-update\n$ gclient sync\n```\n\nThe first command updates the primary Chromium source repository and rebases\nany of your local branches on top of tip-of-tree (aka the Git branch\n`origin/main`). If you don't want to use this script, you can also just use\n`git pull` or other common Git commands to update the repo.\n\nThe second command syncs dependencies to the appropriate versions and re-runs\nhooks as needed.\n\n## Tips, tricks, and troubleshooting\n\n### Using Xcode-Ninja Hybrid\n\nWhile using Xcode is unsupported, GN supports a hybrid approach of using Ninja\nfor building, but Xcode for editing and driving compilation.  Xcode is still\nslow, but it runs fairly well even **with indexing enabled**.  Most people\nbuild in the Terminal and write code with a text editor, though.\n\nWith hybrid builds, compilation is still handled by Ninja, and can be run from\nthe command line (e.g. `autoninja -C out/gn chrome`) or by choosing the `chrome`\ntarget in the hybrid project and choosing Build.\n\nTo use Xcode-Ninja Hybrid pass `--ide=xcode` to `gn gen`:\n\n```shell\n$ gn gen out/gn --ide=xcode\n```\n\nOpen it:\n\n```shell\n$ open out/gn/all.xcodeproj\n```\n\nYou may run into a problem where http://YES is opened as a new tab every time\nyou launch Chrome. To fix this, open the scheme editor for the Run scheme,\nchoose the Options tab, and uncheck \"Allow debugging when using document\nVersions Browser\". When this option is checked, Xcode adds\n`--NSDocumentRevisionsDebugMode YES` to the launch arguments, and the `YES`\ngets interpreted as a URL to open.\n\nIf you have problems building, join us in `#chromium` on `irc.freenode.net` and\nask there. Be sure that the\n[waterfall](https://build.chromium.org/buildbot/waterfall/) is green and the\ntree is open before checking out. This will increase your chances of success.\n\n### Improving performance of git commands\n\n#### Increase the vnode cache size\n\n`git status` is used frequently to determine the status of your checkout.  Due\nto the large number of files in Chromium's checkout, `git status` performance\ncan be quite variable.  Increasing the system's vnode cache appears to help. By\ndefault, this command:\n\n```shell\n$ sysctl -a | egrep 'kern\\..*vnodes'\n```\n\nOutputs `kern.maxvnodes: 263168` (263168 is 257 * 1024).  To increase this\nsetting:\n\n```shell\n$ sudo sysctl kern.maxvnodes=$((512*1024))\n```\n\nHigher values may be appropriate if you routinely move between different\nChromium checkouts.  This setting will reset on reboot.  To apply it at startup:\n\n```shell\n$ sudo tee /Library/LaunchDaemons/kern.maxvnodes.plist > /dev/null <<EOF\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n  <dict>\n    <key>Label</key>\n      <string>kern.maxvnodes</string>\n    <key>ProgramArguments</key>\n      <array>\n        <string>sysctl</string>\n        <string>kern.maxvnodes=524288</string>\n      </array>\n    <key>RunAtLoad</key>\n      <true/>\n  </dict>\n</plist>\nEOF\n```\n\nOr edit the file directly.\n\n#### Configure git to use an untracked cache\n\nTry running\n\n```shell\n$ git update-index --test-untracked-cache\n```\n\nIf the output ends with `OK`, then the following may also improve performance of\n`git status`:\n\n```shell\n$ git config core.untrackedCache true\n```\n\n#### Configure git to use fsmonitor\n\nYou can significantly speed up git by using [fsmonitor.](https://github.blog/2022-06-29-improve-git-monorepo-performance-with-a-file-system-monitor/)\nYou should enable fsmonitor in large repos, such as Chromium and v8. Enabling\nit globally will launch many processes and probably isn't worthwhile. Be sure\nyou have at least version 2.43 (fsmonitor on the Mac is broken before then). The\ncommand to enable fsmonitor in the current repo is:\n\n```shell\n$ git config core.fsmonitor true\n```\n\n### Xcode license agreement\n\nIf you're getting the error\n\n> Agreeing to the Xcode/iOS license requires admin privileges, please re-run as\n> root via sudo.\n\nthe Xcode license hasn't been accepted yet which (contrary to the message) any\nuser can do by running:\n\n```shell\n$ xcodebuild -license\n```\n\nOnly accepting for all users of the machine requires root:\n\n```shell\n$ sudo xcodebuild -license\n```\n\n### Exclude checkout from Spotlight indexing\n\nChromium's checkout contains a lot of files, and building generates many more.\nSpotlight will try to index all of those files, and uses a lot of CPU time\ndoing so, especially during a build, which can slow things down.\n\nTo prevent the Chromium checkout from being indexed by Spotlight, open System\nPreferences, go to \"Spotlight\" -> \"Privacy\" and add your Chromium checkout\ndirectory to the list of excluded locations.\n"
  },
  {
    "path": "development/build/ios_build_instructions",
    "title": "This document has moved",
    "content": "# This document has moved\n\nNOTE: Please update your link to this file!\n\nThe new file location is [//docs/ios/build_instructions.md](ios/build_instructions.md)"
  },
  {
    "path": "development/build/chromeos_build_instructions",
    "title": "Chrome OS Build Instructions",
    "content": "# Chrome OS Build Instructions\n\nChrome for Chromium OS can be built in a couple different ways. After following\nthe [initial setup](#common-setup), you'll need to choose one of the following\nbuild configurations:\n\n- If you're interested in testing Chrome OS code in Chrome, but not interactions\n  with Chrome OS services, you can build for\n  [linux-chromeos](#Chromium-OS-on-Linux-linux_chromeos) using just a Linux\n  workstation.\n- Otherwise, Chrome's full integration can be covered by building for a real\n  Chrome OS device or VM using [Simple Chrome](#Chromium-OS-Device-Simple-Chrome).\n- Use `is_chromeos_device` in GN and `BUILDFLAG(IS_CHROMEOS_DEVICE)` in C++ code\n  to differentiate between these two modes.\n\n[TOC]\n\n## Common setup\n\nFirst, follow the [normal Linux build\ninstructions](https://chromium.googlesource.com/chromium/src/+/main/docs/linux/build_instructions.md)\nas usual to get a Chromium checkout.\n\nYou'll also need to add `'chromeos'` to the `target_os` list in your `.gclient`\nconfiguration, which will fetch the additional build dependencies required for\nCrOS. This file is located one level up from your Chromium checkout's `src`.\n\nIf you don't already have a `target_os` line present, simply add this to the\nend of the `.gclient` file:\n\n    target_os = ['chromeos']\n\nIf you already have a `target_os` line present in your `.gclient file`, you can\nsimply append `'chromeos'` to the existing list there. For example:\n\n    target_os = ['android', 'chromeos']\n\nOnce your `.gclient` file is updated, you will need to run `gclient sync` once\nbefore proceeding with the rest of these instructions.\n\n## Chromium OS on Linux (linux-chromeos)\n\nChromium on Chromium OS uses Linux Chromium as a base, but adds a large number\nof Chrome OS-specific features to the code. For example, the login UI, window\nmanager and system UI are part of the Chromium code base and built into the\nchrome binary.\n\nFortunately, most Chromium changes that affect Chromium OS can be built and\ntested on a Linux workstation. This build is called \"linux-chromeos\". In this\nconfiguration most system services (like the power manager, bluetooth daemon,\netc.) are stubbed out. The entire system UI runs in a single X11 window on your\ndesktop.\n\nYou can test sign-in/sync in this mode by adding the --login-manager flag, see\nthe [Login notes](#Login-notes) section.\n\n### Building and running Chromium with Chromium OS UI on your local machine\n\nRun the following in your chromium checkout:\n\n    $ gn gen out/Default --args='target_os=\"chromeos\"'\n    $ autoninja -C out/Default chrome\n    $ out/Default/chrome --use-system-clipboard\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`).\n\nSome additional options you may wish to set by passing in `--args` to `gn gen`\nor running `gn args out/Default`:\n\n    # Googlers: Reclient is a distributed compiler service.\n    use_remoteexec = true\n\n    is_component_build = true  # Links faster.\n    is_debug = false           # Release build, runs faster.\n    dcheck_always_on = true    # Enables DCHECK despite release build.\n    enable_nacl = false        # Skips native client build, compiles faster.\n\n    # Builds Chrome instead of Chromium. This requires a src-internal\n    # checkout. Adds internal features and branded art assets.\n    is_chrome_branded = true\n\n    # Enables many optimizations, leading to much slower compiles, links,\n    # and no runtime stack traces.\n    #\n    # Note: not compatible with `is_component_build = true`.\n    is_official_build = true\n\nNOTE: You may wish to replace 'Default' with something like 'Cros' if\nyou switch back and forth between Linux and Chromium OS builds, or 'Debug'\nif you want to differentiate between Debug and Release builds (see below).\n\nSee [GN Build Configuration](https://www.chromium.org/developers/gn-build-configuration)\nfor more information about configuring your build.\n\nYou can also build and run test targets like `unit_tests`, `browser_tests`, etc.\n\n### Flags\n\nSome useful flags:\n\n*    `--ash-debug-shortcuts`: Enable shortcuts such as Ctl+Alt+Shift+T to toggle\n     tablet mode.\n*    `--ash-host-window-bounds=\"0+0-800x600,800+0-800x600\"`: Specify one or more\n     virtual screens, by display position and size.\n*    `--enable-features=Feature1,OtherFeature2`: Enable specified features.\n     Features are often listed in chrome://flags, or in source files such as\n     [chrome_features.cc](https://source.chromium.org/chromium/chromium/src/+/main:chrome/common/chrome_features.cc)\n     or [ash_features.cc](https://source.chromium.org/chromium/chromium/src/+/main:ash/constants/ash_features.cc).\n     Note that changing values in chrome://flags does not work for\n     linux-chromeos, and this flag must be used.\n*    `--enable-ui-devtools[=9223]`: Allow debugging of the system UI through\n     devtools either within linux-chromeos at chrome://inspect, or from a remote\n     browser at\n     devtools://devtools/bundled/devtools_app.html?uiDevTools=true&ws=127.0.0.1:9223/0\n*    `--remote-debugging-port=9222`: Allow debugging through devtools at\n     http://localhost:9222\n*    `--use-system-clipboard`: Integrate clipboard with the host X11 system.\n\n### Login notes\n\nBy default this build signs in with a stub user. To specify a real user:\n\n*   For first run, add the following options to chrome's command line:\n    `--user-data-dir=/tmp/chrome --login-manager`\n*   Go through the out-of-the-box UX and sign in with a real Gmail account.\n*   For subsequent runs, if you want to skip the login manager page, add:\n    `--user-data-dir=/tmp/chrome --login-user=username@gmail.com\n    --login-profile=username@gmail.com-hash`. It's also fine to just keep\n    --login-manager instead.\n*   To run in guest mode instantly, add:\n    `--user-data-dir=/tmp/chrome --bwsi --incognito --login-user='$guest'\n    --login-profile=user`\n\nSigning in as a specific user is useful for debugging features like sync\nthat require a logged in user.\n\n### Graphics notes\n\nThe Chromium OS build requires a functioning GL so if you plan on\ntesting it through Chromium Remote Desktop you might face drawing\nproblems (e.g. Aura window not painting anything). Possible remedies:\n\n*   `--ui-enable-software-compositing --ui-disable-threaded-compositing`\n*   `--use-gl=angle --use-angle=swiftshader`, but it's slow.\n\nTo more closely match the UI used on devices, you can install fonts used\nby Chrome OS, such as Roboto, on your Linux distro.\n\n## Chromium OS Device (Simple Chrome)\n\nThis configuration allows you to build a fully functional Chrome for a real\nChrome OS device or VM. Since Chrome OS uses a different toolchain for each\ndevice model, you'll first need to know the name of the model (or \"board\") you\nwant to build for. For most boards, `amd64-generic` and `arm-generic` will\nproduce a functional binary, though it won't be optimized and may be missing\nfunctionality.\n\n### Additional gclient setup\n\nEach board has its own toolchain and misc. build dependencies. To fetch these,\nlist the board under the `\"cros_boards\"` gclient custom var. If you were using\nthe `amd64-generic` board, your `.gclient` file would look like:\n```\nsolutions = [\n  {\n    \"url\": \"https://chromium.googlesource.com/chromium/src.git\",\n    \"name\": \"src\",\n    \"custom_deps\": {},\n    \"custom_vars\" : {\n        \"cros_boards\": \"amd64-generic\",\n    },\n  },\n]\ntarget_os = [\"chromeos\"]\n```\nOnce your .gclient file is updated, you will need to run `gclient sync` again\nto fetch the toolchain.\n\nNOTE:\n - If you'd like a VM image additionally downloaded for the board, add it to the\n   `\"cros_boards_with_qemu_images\"` gclient custom var. That var downloads the\n   SDK along with a VM image. `cros_boards` downloads only the SDK.\n - If you'd like to fetch multiple boards, add a `:` between each board in the\n   gclient var. For example: `\"cros_boards\": \"amd64-generic:arm-generic\"`.\n\n### Building for the board\n\nAfter the needed toolchain has been downloaded for your ${BOARD}, a build dir\nwill have been conveniently created for you at `out_$BOARD/Release`, which can\nthen be used to build Chrome. For the `amd64-generic` board, this would\nlook like:\n\n    $ gn gen out_amd64-generic/Release\n    $ autoninja -C out_$BOARD/Release chrome\n\nOr if you prefer to use your own build dir, simply add the following line to the\ntop of your GN args: `import(\"//build/args/chromeos/amd64-generic.gni\")`. eg:\n\n    $ gn gen out/Default --args='import(\"//build/args/chromeos/amd64-generic.gni\")'\n    $ autoninja -C out/Default chrome\n\nThat will produce a Chrome OS build of Chrome very similar to what is shipped\nfor that device. You can also supply additional args or even overwrite ones\nsupplied in the imported .gni file after the `import()` line.\n\n### Additional notes\n\nFor more information (like copying the locally-built Chrome to a device, or\nrunning Tast tests), consult Simple Chrome's\n[full documentation](https://chromium.googlesource.com/chromiumos/docs/+/main/simple_chrome_workflow.md).\n"
  },
  {
    "path": "development/build/android_cast_build_instructions",
    "title": "Checking out and building Cast for Android",
    "content": "# Checking out and building Cast for Android\n\n**Note**: it is **not possible** to build a binary functionally\nequivalent to a Chromecast. This is to build a single-page content\nembedder with similar functionality to Cast products.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-android-cast](https://goto.google.com/building-android-cast) instead.\n\n[TOC]\n\n## System requirements\n\n* An x86-64 machine running Linux with at least 8GB of RAM. More than 16GB is\n  highly recommended.\n* At least 100GB of free disk space.\n* You must have Git and Python installed already.\n\nMost development is done on Ubuntu. Other distros may or may not work;\nsee the [Linux instructions](linux/build_instructions.md) for some suggestions.\n\nBuilding the Android client on Windows or Mac is not supported and doesn't work.\n\n## Install `depot_tools`\n\nClone the `depot_tools` repository:\n\n```shell\n$ git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\n```\n\nAdd `depot_tools` to the end of your PATH (you will probably want to put this\nin your `~/.bashrc` or `~/.zshrc`). Assuming you cloned `depot_tools`\nto `/path/to/depot_tools`:\n\n```shell\n$ export PATH=\"$PATH:/path/to/depot_tools\"\n```\n\n## Get the code\n\nCreate a `chromium` directory for the checkout and change to it (you can call\nthis whatever you like and put it wherever you like, as\nlong as the full path has no spaces):\n\n```shell\n$ mkdir ~/chromium && cd ~/chromium\n$ fetch --nohooks android\n```\n\nIf you don't want the full repo history, you can save a lot of time by\nadding the `--no-history` flag to `fetch`.\n\nExpect the command to take 30 minutes on even a fast connection, and many\nhours on slower ones.\n\nIf you've already installed the build dependencies on the machine (from another\ncheckout, for example), you can omit the `--nohooks` flag and `fetch`\nwill automatically execute `gclient runhooks` at the end.\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\n$ cd src\n```\n\n### Converting an existing Linux checkout\n\nIf you have an existing Linux checkout, you can add Android support by\nappending `target_os = ['android']` to your `.gclient` file (in the\ndirectory above `src`):\n\n```shell\n$ echo \"target_os = [ 'android' ]\" >> ../.gclient\n```\n\nThen run `gclient sync` to pull the new Android dependencies:\n\n```shell\n$ gclient sync\n```\n\n(This is the only difference between `fetch android` and `fetch chromium`.)\n\n### Install additional build dependencies\n\nOnce you have checked out the code, run\n\n```shell\n$ build/install-build-deps.sh --android\n```\n\nto get all of the dependencies you need to build on Linux, *plus* all of the\nAndroid-specific dependencies (you need some of the regular Linux dependencies\nbecause an Android build includes a bunch of the Linux tools and utilities).\n\n### Run the hooks\n\nOnce you've run `install-build-deps` at least once, you can now run the\nChromium-specific hooks, which will download additional binaries and other\nthings you might need:\n\n```shell\n$ gclient runhooks\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory which builds Chrome\nfor Android, run:\n\n```shell\n$ gn gen --args='target_os=\"android\" is_cast_android=true' out/Default\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build matching the current host\n  operating system and CPU.\n* For more info on GN, run `gn help` on the command line or read the\n  [quick start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n\nAlso be aware that some scripts (e.g. `tombstones.py`, `adb_gdb.py`)\nrequire you to set `CHROMIUM_OUTPUT_DIR=out/Default`.\n\n### Faster builds\n\nThis section contains some things you can change to speed up your builds,\nsorted so that the things that make the biggest difference are first.\n\n#### Use Reclient\n\n*** note\n**Warning:** If you are a Google employee, do not follow the instructions below.\nSee\n[go/building-android-chrome#initialize-remote-execution-distributed-builds](https://goto.google.com/building-android-chrome#initialize-remote-execution-distributed-builds)\ninstead.\n***\n\nChromium's build can be sped up significantly by using a remote execution system\ncompatible with [REAPI](https://github.com/bazelbuild/remote-apis). This allows\nyou to benefit from remote caching and executing many build actions in parallel\non a shared cluster of workers.\n\nTo use Reclient, follow the corresponding\n[Linux build instructions](linux/build_instructions.md#use-reclient).\n\n## Build cast\\_shell\\_apk\n\nBuild `cast_browser_apk` with Ninja using the command:\n\n```shell\n$ autoninja -C out/Default cast_browser_apk\n```\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.)\n\n## Installing and Running `cast_browser_apk` on a device\n\n### Plug in your Android device\n\nMake sure your Android device is plugged in via USB, and USB Debugging\nis enabled.\n\nTo enable USB Debugging:\n\n*   Navigate to Settings \\> About Phone \\> Build number\n*   Click 'Build number' 7 times\n*   Now navigate back to Settings \\> Developer Options\n*   Enable 'USB Debugging' and follow the prompts\n\nYou may also be prompted to allow access to your PC once your device is\nplugged in.\n\nYou can check if the device is connected by running:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb devices\n```\n\nWhich prints a list of connected devices. If not connected, try\nunplugging and reattaching your device.\n\n### Build the APK\n\n```shell\nautoninja -C out/Release cast_browser_apk\n```\n\nAnd deploy it to your Android device:\n\n```shell\nout/Default/bin/cast_browser_apk install\n# Or to install and run:\nout/Default/bin/cast_browser_apk run \"http://google.com\"\n```\n\nThe app will appear on the device as \"Chromium\".\n\n### Testing\n\nFor information on running tests, see\n[Android Test Instructions](testing/android_test_instructions.md).\n"
  },
  {
    "path": "development/build/android_build_instructions",
    "title": "Checking out and building Chromium for Android",
    "content": "# Checking out and building Chromium for Android\n\nThere are instructions for other platforms linked from the\n[get the code](get_the_code.md) page.\n\n## Instructions for Google Employees\n\nAre you a Google employee? See\n[go/building-android-chrome](https://goto.google.com/building-android-chrome)\ninstead.\n\n[TOC]\n\n## System requirements\n\n* An x86-64 machine running Linux with at least 8GB of RAM. More than 16GB is\n  highly recommended.\n* At least 100GB of free disk space.\n* You must have Git and Python installed already.\n\nMost development is done on Ubuntu. Other distros may or may not work;\nsee the [Linux instructions](linux/build_instructions.md) for some suggestions.\n\nBuilding the Android client on Windows or Mac is not supported and doesn't work.\n\n## Install depot\\_tools\n\nClone the `depot_tools` repository:\n\n```shell\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\n```\n\nAdd `depot_tools` to the end of your PATH (you will probably want to put this\nin your `~/.bashrc` or `~/.zshrc`). Assuming you cloned `depot_tools`\nto `/path/to/depot_tools`:\n\n```shell\nexport PATH=\"$PATH:/path/to/depot_tools\"\n```\n\n## Get the code\n\nCreate a `chromium` directory for the checkout and change to it (you can call\nthis whatever you like and put it wherever you like, as\nlong as the full path has no spaces):\n\n```shell\nmkdir ~/chromium && cd ~/chromium\nfetch --nohooks android\n```\n\nIf you don't want the full repo history, you can save a lot of time by\nadding the `--no-history` flag to `fetch`.\n\nExpect the command to take 30 minutes on even a fast connection, and many\nhours on slower ones.\n\nIf you've already installed the build dependencies on the machine (from another\ncheckout, for example), you can omit the `--nohooks` flag and `fetch`\nwill automatically execute `gclient runhooks` at the end.\n\nWhen `fetch` completes, it will have created a hidden `.gclient` file and a\ndirectory called `src` in the working directory. The remaining instructions\nassume you have switched to the `src` directory:\n\n```shell\ncd src\n```\n\n### Converting an existing Linux checkout\n\nIf you have an existing Linux checkout, you can add Android support by\nappending `target_os = ['linux', 'android']` to your `.gclient` file (in the\ndirectory above `src`):\n\n```shell\necho \"target_os = [ 'linux', 'android' ]\" >> ../.gclient\n```\n\nThen run `gclient sync` to pull the new Android dependencies:\n\n```shell\ngclient sync\n```\n\n(This is the only difference between `fetch android` and `fetch chromium`.)\n\n### Install additional build dependencies\n\nOnce you have checked out the code, run\n\n```shell\nbuild/install-build-deps.sh\n```\n\nto get all of the dependencies you need to build on Linux, *plus* all of the\nAndroid-specific dependencies (you need some of the regular Linux dependencies\nbecause an Android build includes a bunch of the Linux tools and utilities).\n\nNOTE: For 32-bit builds, the `--lib32` command line switch could be used.\nYou may run into issues where `gperf` or `pkgconf` don't get installed,\nwithout it. To remedy this, and potentially other missing packages, you will\nhave to install them manually using:\n\n```shell\nsudo apt-get install {missing_pkg}\n```\n\n### Run the hooks\n\nOnce you've run `install-build-deps` at least once, you can now run the\nChromium-specific hooks, which will download additional binaries and other\nthings you might need:\n\n```shell\ngclient runhooks\n```\n\n*Optional*: You can also [install API\nkeys](https://www.chromium.org/developers/how-tos/api-keys) if you want your\nbuild to talk to some Google services, but this is not necessary for most\ndevelopment and testing purposes.\n\n## Setting up the build\n\nChromium uses [Ninja](https://ninja-build.org) as its main build tool along with\na tool called [GN](https://gn.googlesource.com/gn/+/main/docs/quick_start.md)\nto generate `.ninja` files. You can create any number of *build directories*\nwith different configurations. To create a build directory which builds Chrome\nfor Android, run `gn args out/Default` and edit the file to contain the\nfollowing arguments:\n\n```gn\ntarget_os = \"android\"\ntarget_cpu = \"arm64\"  # See \"Figuring out target_cpu\" below\nuse_remoteexec = true  # Enables distributed builds. See \"Faster Builds\".\nandroid_static_analysis = \"build_server\"  # Does static checks in background. See \"Faster Builds\".\n```\n\n* You only have to run this once for each new build directory, Ninja will\n  update the build files as needed.\n* You can replace `Default` with another name, but\n  it should be a subdirectory of `out`.\n* For other build arguments, including release settings, see [GN build\n  configuration](https://www.chromium.org/developers/gn-build-configuration).\n  The default will be a debug component build.\n* For more info on GN, run `gn help` on the command line or read the\n  [quick start guide](https://gn.googlesource.com/gn/+/main/docs/quick_start.md).\n\nAlso be aware that some scripts (e.g. `tombstones.py`, `adb_gdb.py`)\nrequire you to set `CHROMIUM_OUTPUT_DIR=out/Default`.\n\n### Figuring out target\\_cpu\n\nThe value of\n[`target_cpu`](https://gn.googlesource.com/gn/+/main/docs/reference.md#var_target_cpu)\ndetermines what instruction set to use for native code. Given a device (or\nemulator), you can determine the correct instruction set with `adb shell getprop\nro.product.cpu.abi`:\n\n| `getprop ro.product.cpu.abi` output | `target_cpu` value |\n|-------------------------------------|--------------------|\n| `arm64-v8a`                         | `arm64`            |\n| `armeabi-v7a`                       | `arm`              |\n| `x86`                               | `x86`              |\n| `x86_64`                            | `x64`              |\n\n*** promo\n`arm` and `x86` may optionally be used instead of `arm64` and `x64` for\nnon-WebView targets. This is also allowed for Monochrome, but only when not set\nas the WebView provider.\n***\n\n## Build Chromium\n\nBuild Chromium with Ninja using the command:\n\n```shell\nautoninja -C out/Default chrome_public_apk\n```\n\n(`autoninja` is a wrapper that automatically provides optimal values for the\narguments passed to `ninja`.)\n\nYou can get a list of all of the other build targets from GN by running `gn ls\nout/Default` from the command line. To compile one, pass the GN label to Ninja\nwith no preceding \"//\" (so, for `//chrome/test:unit_tests` use `autoninja -C\nout/Default chrome/test:unit_tests`).\n\n### Multiple Chrome Targets\n\nThe Google Play Store allows apps to send customized bundles (`.aab` files)\ndepending on the version of Android running on a device. Chrome uses this\nfeature to package optimized versions for different OS versions.\n\n1. `monochrome_public_bundle` (`MonochromePublic.aab`)\n   * `minSdkVersion=26` (Oreo).\n   * Contains both Chrome and WebView (to save disk space).\n2. `trichrome_chrome_bundle` (`TrichromeChrome.aab`)\n   * `minSdkVersion=29` (Android 10).\n   * Native code shared with WebView through a \"Static Shared Library APK\": `trichrome_library_apk`\n   * Corresponding WebView target: `trichrome_webview_bundle`\n3. `chrome_public_bundle` & `chrome_public_apk` (`ChromePublic.aab`, `ChromePublic.apk`)\n   * `minSdkVersion=26` (Oreo).\n   * Used for local development (to avoid building WebView).\n   * WebView packaged independently (`system_webview_bundle` / `system_webview_apk`).\n\n*** note\n**Notes:**\n* These instructions use `chrome_public_apk`, but any of the other targets can\n  be substituted.\n* For more about bundles, see [android_dynamic feature modules.md](android_dynamic_feature_modules.md).\n* For more about native library packaging & loading, see [android_native_libraries.md](android_native_libraries.md).\n* There are closed-source equivalents to these targets (for Googlers), which\n  are identical but link in some extra code.\n***\n\n## Updating your checkout\n\nTo update an existing checkout, you can run\n\n```shell\n$ git rebase-update\n$ gclient sync\n```\n\nThe first command updates the primary Chromium source repository and rebases\nany of your local branches on top of tip-of-tree (aka the Git branch\n`origin/main`). If you don't want to use this script, you can also just use\n`git pull` or other common Git commands to update the repo.\n\nThe second command syncs dependencies to the appropriate versions and re-runs\nhooks as needed.\n\n## Installing and Running Chromium on a device\n\n### Plug in your Android device\n\nMake sure your Android device is plugged in via USB, and USB Debugging\nis enabled.\n\nTo enable USB Debugging:\n\n*   Navigate to Settings \\> About Phone \\> Build number\n*   Click 'Build number' 7 times\n*   Now navigate back to Settings \\> Developer Options\n*   Enable 'USB Debugging' and follow the prompts\n\nYou may also be prompted to allow access to your PC once your device is\nplugged in.\n\nYou can check if the device is connected by running:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb devices\n```\n\nWhich prints a list of connected devices. If not connected, try\nunplugging and reattaching your device.\n\n### Enable apps from unknown sources\n\nAllow Android to run APKs that haven't been signed through the Play Store:\n\n*   Enable 'Unknown sources' under Settings \\> Security\n\nIn case that setting isn't present, it may be possible to configure it via\n`adb shell` instead:\n\n```shell\nthird_party/android_sdk/public/platform-tools/adb shell settings put global verifier_verify_adb_installs 0\n```\n\n### Build the full browser\n\n```shell\nautoninja -C out/Default chrome_public_apk\n```\n\nAnd deploy it to your Android device:\n\n```shell\nout/Default/bin/chrome_public_apk install\n```\n\nThe app will appear on the device as \"Chromium\".\n\n### Build Content shell\n\nWraps the content module (but not the /chrome embedder). See\n[https://www.chromium.org/developers/content-module](https://www.chromium.org/developers/content-module)\nfor details on the content module and content shell.\n\n```shell\nautoninja -C out/Default content_shell_apk\nout/Default/bin/content_shell_apk install\n```\n\nthis will build and install an Android apk under\n`out/Default/apks/ContentShell.apk`.\n\n### Build WebView\n\n[Android WebView](https://developer.android.com/reference/android/webkit/WebView.html)\nis a system framework component. Since Android KitKat, it is implemented using\nChromium code (based off the [content module](https://dev.chromium.org/developers/content-module)).\n\nIf you want to build the complete Android WebView framework component and test\nthe effect of your chromium changes in Android apps using WebView, you should\nfollow the [Android AOSP + chromium WebView\ninstructions](https://www.chromium.org/developers/how-tos/build-instructions-android-webview)\n\n### Running\n\nFor Content shell:\n\n```shell\nout/Default/bin/content_shell_apk launch [--args='--foo --bar'] http://example.com\n```\n\nFor Chrome public:\n\n```shell\nout/Default/bin/chrome_public_apk launch [--args='--foo --bar'] http://example.com\n```\n\n### Logging and debugging\n\nLogging is often the easiest way to understand code flow. In C++ you can print\nlog statements using the LOG macro. In Java, refer to\n[android_logging.md](android_logging.md).\n\nYou can see these log via `adb logcat`, or:\n\n```shell\nout/Default/bin/chrome_public_apk logcat\n```\n\nLogcat supports an additional feature of filtering and highlighting user-defined patterns. To use\nthis mechanism, define a shell variable: `CHROMIUM_LOGCAT_HIGHLIGHT` and assign your desired\npattern. The pattern will be used to search for any substring (ie. no need to prefix or suffix it\nwith `.*`), eg:\n\n```shell\nexport CHROMIUM_LOGCAT_HIGHLIGHT='(WARNING|cr_Child)'\nout/Default/bin/chrome_public_apk logcat\n# Highlights messages/tags containing WARNING and cr_Child strings.\n```\n\nNote: both _Message_ and _Tag_ portion of logcat are matched against the pattern.\n\nTo debug C++ code, use one of the following commands:\n\n```shell\nout/Default/bin/content_shell_apk gdb\nout/Default/bin/chrome_public_apk gdb\n```\n\nSee [Android Debugging Instructions](android_debugging_instructions.md)\nfor more on debugging, including how to debug Java code.\n\n### Testing\n\nFor information on running tests, see\n[Android Test Instructions](/docs/testing/android_test_instructions.md)\n\n## Faster Builds\n\n### Use Reclient\n\n*** note\n**Warning:** If you are a Google employee, do not follow the instructions below.\nSee\n[go/building-android-chrome#initialize-remote-execution-distributed-builds](https://goto.google.com/building-android-chrome#initialize-remote-execution-distributed-builds)\ninstead.\n***\n\nChromium's build can be sped up significantly by using a remote execution system\ncompatible with [REAPI](https://github.com/bazelbuild/remote-apis). This allows\nyou to benefit from remote caching and executing many build actions in parallel\non a shared cluster of workers.\n\nTo use Reclient, follow the corresponding\n[Linux build instructions](linux/build_instructions.md#use-reclient).\n\n### GN Args\n\nArgs that affect build speed:\n * `use_remoteexec = true` *(default=false)*\n   * What it does: Enables distributed builds via Reclient\n * `symbol_level = 0` *(default=1)*\n   * What it does: Disables debug information in native code.\n   * Use this when doing primarily Java development.\n   * To disable symbols only in Blink / V8: `blink_symbol_level = 0`, `v8_symbol_level = 0`\n * `is_component_build = true` *(default=`is_debug`)*\n   * What it does: Uses multiple `.so` files instead of just one (faster links)\n * `is_java_debug = true` *(default=`is_debug`)*\n   * What it does: Disables R8 (whole-program Java optimizer)\n * `treat_warnings_as_errors = false` *(default=`true`)*\n   * Causes any compiler warnings or lint checks to not fail the build.\n   * Allows you to iterate without needing to satisfy static analysis checks.\n * `android_static_analysis = \"build_server\"` *(default=`\"on\"`)*\n   * Offloads static analysis steps to the build server. Explained below.\n   * Set this to `\"off\"` if you want to turn off static analysis altogether.\n * `incremental_install = true` *(default=`false`)*\n   * Makes build and install quite a bit faster. Explained in a later section.\n * `enable_chrome_android_internal = false` *(Googlers only)*\n   * Disables non-public code, which exists even when building public targets.\n   * Use this is you do not need to test internal-only things.\n\n### Asynchronous Static Analysis\n\nNormally analysis build steps like Lint and Error Prone will run as normal build\nsteps. The build will then wait for all analysis steps to complete successfully.\nBy offloading analysis build steps to a separate build server to be run lazily at\na low priority, the actual build can complete much faster.\n\n**Note**: Since the build completes before the analysis checks finish, the build\nwill not fail if an analysis check fails.\n\nTo enable this mode, add the gn args:\n\n```gn\nandroid_static_analysis = \"build_server\"\n```\n\nCommand output will show up on the terminal that ran the build, as well as in\n`out/Debug/buildserver.log.0`.\n\nSee the status of the server at any time via:\n```\nbuild/android/fast_local_dev_server.py --print-status-all\n```\n\n### Incremental Install\n[Incremental Install](/build/android/incremental_install/README.md) uses\nreflection and sideloading to speed up the edit & deploy cycle (normally < 10\nseconds). The initial launch of the apk will be a lot slower on older Android\nversions (pre-N) where the OS needs to pre-optimize the side-loaded files, but\nthen be only marginally slower after the first launch.\n\nTo enable Incremental Install, add the gn args:\n\n```gn\nincremental_install = true\n```\n\nSome APKs (e.g. WebView) do not work with `incremental install = true` and are\nalways built as normal APKs. This behavior is controlled via\n`never_incremental = true`.\n\n## Installing and Running Chromium on an Emulator\n\nRunning on an emulator is the same as on a device. Refer to\n[android_emulator.md](android_emulator.md) for setting up emulators.\n\n## Tips, tricks, and troubleshooting\n\n### Rebuilding libchrome.so for a particular release\n\nThese instructions are only necessary for Chrome 51 and earlier.\n\nIn the case where you want to modify the native code for an existing\nrelease of Chrome for Android (v25+) you can do the following steps.\nNote that in order to get your changes into the official release, you'll\nneed to send your change for a codereview using the regular process for\ncommitting code to chromium.\n\n1.  Open Chrome on your Android device and visit chrome://version\n2.  Copy down the id listed next to \"Build ID:\"\n3.  Go to\n    [http://storage.googleapis.com/chrome-browser-components/BUILD\\_ID\\_FROM\\_STEP\\_2/index.html](http://storage.googleapis.com/chrome-browser-components/BUILD_ID_FROM_STEP_2/index.html)\n4.  Download the listed files and follow the steps in the README.\n\n### Building with Docker\n\nTo build Chromium for Android using Docker, please follow the\ninstructions in the [Docker in Linux build instructions](/docs/linux/build_instructions.md#docker).\n\n*** note\n**Note:** You need install the [Android dependencies](#install-additional-build-dependencies) after setting up the [Build dependencies](/docs/linux/build_instructions.md#install-additional-build-dependencies).\n***\n"
  },
  {
    "path": "demo/syntax-highlighting",
    "title": "Code Syntax Highlighting Demo",
    "content": "# Code Syntax Highlighting Demo\r\n\r\nThis page demonstrates the enhanced code syntax highlighting capabilities of the Wanderlust Knowledge Base.\r\n\r\n## Supported Languages\r\n\r\nThe knowledge base now supports syntax highlighting for multiple programming languages with copy-to-clipboard functionality and theme-aware styling.\r\n\r\n### TypeScript/JavaScript\r\n\r\n```typescript\r\ninterface BookmarkFeatures {\r\n  pageBookmarks: boolean;\r\n  sectionBookmarks: boolean;\r\n  persistentStorage: boolean;\r\n  searchAndFilter: boolean;\r\n  importExport: boolean;\r\n  categorization: boolean;\r\n}\r\n\r\nclass BookmarkManager {\r\n  private bookmarks: Map<string, Bookmark> = new Map();\r\n  \r\n  constructor(private storage: Storage) {\r\n    this.loadBookmarks();\r\n  }\r\n  \r\n  async addBookmark(bookmark: Omit<Bookmark, 'id' | 'timestamp'>): Promise<void> {\r\n    const newBookmark: Bookmark = {\r\n      ...bookmark,\r\n      id: this.generateId(),\r\n      timestamp: Date.now(),\r\n    };\r\n    \r\n    this.bookmarks.set(newBookmark.id, newBookmark);\r\n    await this.saveBookmarks();\r\n  }\r\n  \r\n  private generateId(): string {\r\n    return Date.now().toString(36) + Math.random().toString(36).substr(2);\r\n  }\r\n}\r\n```\r\n\r\n### C++ (Chromium Code Example)\r\n\r\n```cpp\r\n#include \"base/memory/weak_ptr.h\"\r\n#include \"content/public/browser/browser_context.h\"\r\n#include \"content/public/browser/render_frame_host.h\"\r\n\r\nnamespace content {\r\n\r\nclass DownloadManagerDelegate {\r\n public:\r\n  virtual ~DownloadManagerDelegate() = default;\r\n  \r\n  // Called when a download is created.\r\n  virtual void OnDownloadCreated(DownloadManager* manager,\r\n                                DownloadItem* item) {}\r\n  \r\n  // Determine the download target path.\r\n  virtual bool DetermineDownloadTarget(\r\n      DownloadItem* download,\r\n      const DownloadTargetCallback& callback) {\r\n    return false;\r\n  }\r\n  \r\n  // Check if the download should proceed.\r\n  virtual bool ShouldCompleteDownload(\r\n      DownloadItem* item,\r\n      const base::Closure& complete_callback) {\r\n    return true;\r\n  }\r\n  \r\n private:\r\n  base::WeakPtrFactory<DownloadManagerDelegate> weak_ptr_factory_{this};\r\n};\r\n\r\n}  // namespace content\r\n```\r\n\r\n### Python (Build Scripts)\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n\"\"\"\r\nChromium build script utilities for the custom browser project.\r\n\"\"\"\r\n\r\nimport os\r\nimport sys\r\nimport subprocess\r\nimport argparse\r\nfrom pathlib import Path\r\nfrom typing import List, Optional, Dict\r\n\r\nclass ChromiumBuilder:\r\n    \"\"\"Handles building Chromium with custom modifications.\"\"\"\r\n    \r\n    def __init__(self, source_dir: Path, build_dir: Path):\r\n        self.source_dir = source_dir\r\n        self.build_dir = build_dir\r\n        self.gn_args = {\r\n            'is_debug': False,\r\n            'is_component_build': False,\r\n            'symbol_level': 1,\r\n            'enable_nacl': False,\r\n            'target_cpu': 'x64',\r\n        }\r\n    \r\n    def configure_build(self, custom_args: Optional[Dict[str, any]] = None) -> bool:\r\n        \"\"\"Configure the build with GN.\"\"\"\r\n        if custom_args:\r\n            self.gn_args.update(custom_args)\r\n        \r\n        gn_command = [\r\n            'gn', 'gen', str(self.build_dir),\r\n            '--args=' + ' '.join(f'{k}={v}' for k, v in self.gn_args.items())\r\n        ]\r\n        \r\n        try:\r\n            result = subprocess.run(gn_command, cwd=self.source_dir, \r\n                                  capture_output=True, text=True)\r\n            if result.returncode != 0:\r\n                print(f\"GN configuration failed: {result.stderr}\")\r\n                return False\r\n            return True\r\n        except Exception as e:\r\n            print(f\"Error running GN: {e}\")\r\n            return False\r\n    \r\n    def build_target(self, target: str = 'chrome') -> bool:\r\n        \"\"\"Build the specified target.\"\"\"\r\n        ninja_command = ['ninja', '-C', str(self.build_dir), target]\r\n        \r\n        try:\r\n            subprocess.run(ninja_command, cwd=self.source_dir, check=True)\r\n            return True\r\n        except subprocess.CalledProcessError as e:\r\n            print(f\"Build failed with exit code {e.returncode}\")\r\n            return False\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description='Build Chromium')\r\n    parser.add_argument('--source-dir', type=Path, required=True)\r\n    parser.add_argument('--build-dir', type=Path, required=True)\r\n    parser.add_argument('--target', default='chrome')\r\n    parser.add_argument('--debug', action='store_true')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    builder = ChromiumBuilder(args.source_dir, args.build_dir)\r\n    \r\n    if args.debug:\r\n        builder.gn_args['is_debug'] = True\r\n        builder.gn_args['symbol_level'] = 2\r\n    \r\n    if not builder.configure_build():\r\n        sys.exit(1)\r\n    \r\n    if not builder.build_target(args.target):\r\n        sys.exit(1)\r\n    \r\n    print(f\"Successfully built {args.target}\")\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n### Bash (Shell Scripts)\r\n\r\n```bash\r\n#!/bin/bash\r\n# Chromium development environment setup script\r\n\r\nset -euo pipefail\r\n\r\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\r\nCHROMIUM_DIR=\"${SCRIPT_DIR}/chromium\"\r\nDEPOT_TOOLS_DIR=\"${SCRIPT_DIR}/depot_tools\"\r\n\r\n# Colors for output\r\nRED='\\033[0;31m'\r\nGREEN='\\033[0;32m'\r\nYELLOW='\\033[1;33m'\r\nNC='\\033[0m' # No Color\r\n\r\nlog_info() {\r\n    echo -e \"${GREEN}[INFO]${NC} $1\"\r\n}\r\n\r\nlog_warn() {\r\n    echo -e \"${YELLOW}[WARN]${NC} $1\"\r\n}\r\n\r\nlog_error() {\r\n    echo -e \"${RED}[ERROR]${NC} $1\"\r\n}\r\n\r\ninstall_depot_tools() {\r\n    if [[ ! -d \"$DEPOT_TOOLS_DIR\" ]]; then\r\n        log_info \"Installing depot_tools...\"\r\n        git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git \"$DEPOT_TOOLS_DIR\"\r\n    else\r\n        log_info \"depot_tools already installed, updating...\"\r\n        cd \"$DEPOT_TOOLS_DIR\"\r\n        git pull\r\n    fi\r\n    \r\n    export PATH=\"$DEPOT_TOOLS_DIR:$PATH\"\r\n}\r\n\r\nfetch_chromium() {\r\n    if [[ ! -d \"$CHROMIUM_DIR\" ]]; then\r\n        log_info \"Fetching Chromium source code...\"\r\n        mkdir -p \"$CHROMIUM_DIR\"\r\n        cd \"$CHROMIUM_DIR\"\r\n        fetch --nohooks chromium\r\n    else\r\n        log_info \"Chromium already fetched, syncing...\"\r\n        cd \"$CHROMIUM_DIR/src\"\r\n        git pull\r\n        gclient sync\r\n    fi\r\n}\r\n\r\nsetup_build_environment() {\r\n    cd \"$CHROMIUM_DIR/src\"\r\n    \r\n    log_info \"Installing build dependencies...\"\r\n    if [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then\r\n        ./build/install-build-deps.sh\r\n    elif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\r\n        log_warn \"Please install Xcode and command line tools manually\"\r\n    fi\r\n    \r\n    log_info \"Running hooks...\"\r\n    gclient runhooks\r\n}\r\n\r\nmain() {\r\n    log_info \"Setting up Chromium development environment...\"\r\n    \r\n    install_depot_tools\r\n    fetch_chromium\r\n    setup_build_environment\r\n    \r\n    log_info \"Setup complete! You can now build Chromium:\"\r\n    log_info \"  cd $CHROMIUM_DIR/src\"\r\n    log_info \"  gn gen out/Default\"\r\n    log_info \"  ninja -C out/Default chrome\"\r\n}\r\n\r\nmain \"$@\"\r\n```\r\n\r\n### JSON Configuration\r\n\r\n```json\r\n{\r\n  \"name\": \"wanderlust-knowledgebase\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"A comprehensive knowledge base for Chromium development\",\r\n  \"main\": \"index.js\",\r\n  \"scripts\": {\r\n    \"dev\": \"vite\",\r\n    \"build\": \"tsc && vite build\",\r\n    \"preview\": \"vite preview\",\r\n    \"generate-search-index\": \"node scripts/generate-search-index.js\",\r\n    \"generate-icons\": \"node scripts/generate-icons.js\"\r\n  },\r\n  \"dependencies\": {\r\n    \"react\": \"^18.2.0\",\r\n    \"react-dom\": \"^18.2.0\",\r\n    \"react-router-dom\": \"^6.8.1\",\r\n    \"react-markdown\": \"^8.0.5\",\r\n    \"react-syntax-highlighter\": \"^15.5.0\"\r\n  },\r\n  \"devDependencies\": {\r\n    \"@types/react\": \"^18.0.27\",\r\n    \"@types/react-dom\": \"^18.0.10\",\r\n    \"@types/react-syntax-highlighter\": \"^15.5.6\",\r\n    \"@vitejs/plugin-react\": \"^3.1.0\",\r\n    \"autoprefixer\": \"^10.4.13\",\r\n    \"postcss\": \"^8.4.21\",\r\n    \"tailwindcss\": \"^3.2.6\",\r\n    \"typescript\": \"^4.9.4\",\r\n    \"vite\": \"^4.1.0\"\r\n  },\r\n  \"browserslist\": {\r\n    \"production\": [\r\n      \">0.2%\",\r\n      \"not dead\",\r\n      \"not op_mini all\"\r\n    ],\r\n    \"development\": [\r\n      \"last 1 chrome version\",\r\n      \"last 1 firefox version\",\r\n      \"last 1 safari version\"\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n### SQL Database Schema\r\n\r\n```sql\r\n-- User management and preferences\r\nCREATE TABLE users (\r\n    id SERIAL PRIMARY KEY,\r\n    username VARCHAR(255) UNIQUE NOT NULL,\r\n    email VARCHAR(255) UNIQUE NOT NULL,\r\n    password_hash VARCHAR(255) NOT NULL,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\r\n);\r\n\r\n-- Content bookmarks\r\nCREATE TABLE bookmarks (\r\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\r\n    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,\r\n    title VARCHAR(500) NOT NULL,\r\n    path VARCHAR(500) NOT NULL,\r\n    url VARCHAR(500) NOT NULL,\r\n    description TEXT,\r\n    section VARCHAR(255),\r\n    anchor VARCHAR(255),\r\n    category VARCHAR(100),\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    INDEX idx_user_bookmarks (user_id, created_at),\r\n    INDEX idx_bookmark_path (path),\r\n    INDEX idx_bookmark_category (category)\r\n);\r\n\r\n-- Search analytics\r\nCREATE TABLE search_queries (\r\n    id SERIAL PRIMARY KEY,\r\n    query TEXT NOT NULL,\r\n    results_count INTEGER DEFAULT 0,\r\n    user_id INTEGER REFERENCES users(id) ON DELETE SET NULL,\r\n    ip_address INET,\r\n    user_agent TEXT,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    INDEX idx_search_analytics (created_at, query),\r\n    INDEX idx_popular_searches (results_count DESC, created_at DESC)\r\n);\r\n\r\n-- Content feedback and ratings\r\nCREATE TABLE content_feedback (\r\n    id SERIAL PRIMARY KEY,\r\n    article_path VARCHAR(500) NOT NULL,\r\n    user_id INTEGER REFERENCES users(id) ON DELETE SET NULL,\r\n    rating INTEGER CHECK (rating BETWEEN 1 AND 5),\r\n    feedback_type ENUM('helpful', 'outdated', 'error', 'suggestion', 'unclear'),\r\n    content TEXT,\r\n    is_resolved BOOLEAN DEFAULT FALSE,\r\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\r\n    INDEX idx_content_feedback (article_path, created_at),\r\n    INDEX idx_unresolved_feedback (is_resolved, created_at)\r\n);\r\n```\r\n\r\n### CSS Styling\r\n\r\n```css\r\n/* Syntax highlighting custom styles */\r\n.code-block-container {\r\n  @apply relative my-4 rounded-lg overflow-hidden;\r\n  box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);\r\n}\r\n\r\n.code-block-header {\r\n  @apply flex items-center justify-between px-4 py-2 text-sm;\r\n  @apply bg-gray-100 dark:bg-gray-800 border-b border-gray-200 dark:border-gray-700;\r\n}\r\n\r\n.code-block-language {\r\n  @apply text-gray-600 dark:text-gray-400 font-medium uppercase tracking-wide;\r\n  font-size: 11px;\r\n  letter-spacing: 0.05em;\r\n}\r\n\r\n.code-block-copy-button {\r\n  @apply flex items-center gap-1.5 px-2 py-1 rounded text-xs transition-all duration-200;\r\n  @apply bg-gray-200 dark:bg-gray-700 text-gray-600 dark:text-gray-400;\r\n  @apply hover:bg-gray-300 dark:hover:bg-gray-600;\r\n}\r\n\r\n.code-block-copy-button.copied {\r\n  @apply bg-green-100 dark:bg-green-900 text-green-700 dark:text-green-300;\r\n}\r\n\r\n/* Custom scrollbar for code blocks */\r\n.code-block-container pre::-webkit-scrollbar {\r\n  height: 8px;\r\n}\r\n\r\n.code-block-container pre::-webkit-scrollbar-track {\r\n  @apply bg-gray-100 dark:bg-gray-800;\r\n}\r\n\r\n.code-block-container pre::-webkit-scrollbar-thumb {\r\n  @apply bg-gray-300 dark:bg-gray-600 rounded;\r\n}\r\n\r\n.code-block-container pre::-webkit-scrollbar-thumb:hover {\r\n  @apply bg-gray-400 dark:bg-gray-500;\r\n}\r\n\r\n/* Inline code styling */\r\n.inline-code {\r\n  @apply bg-gray-100 dark:bg-gray-800 text-red-600 dark:text-red-400;\r\n  @apply px-1.5 py-0.5 rounded text-sm font-mono;\r\n  font-size: 0.875em;\r\n}\r\n\r\n/* Line number styling */\r\n.code-line-numbers {\r\n  @apply select-none;\r\n  border-right: 1px solid rgba(156, 163, 175, 0.3);\r\n  margin-right: 1em;\r\n  padding-right: 1em;\r\n  min-width: 2.5em;\r\n  text-align: right;\r\n}\r\n```\r\n\r\n## Features\r\n\r\n- **Theme-Aware**: Automatically switches between light and dark syntax themes\r\n- **Copy to Clipboard**: One-click copying with visual feedback\r\n- **Language Detection**: Automatic syntax highlighting based on language tags\r\n- **Line Numbers**: For longer code blocks (>5 lines)\r\n- **Responsive Design**: Optimized for all screen sizes\r\n- **Bookmarkable**: Large code blocks can be bookmarked for quick reference\r\n\r\n## Inline Code\r\n\r\nYou can also use `inline code` with proper styling that adapts to the current theme.\r\n\r\n## Supported Languages\r\n\r\nThe syntax highlighter supports over 100 programming languages including:\r\n- JavaScript/TypeScript\r\n- Python\r\n- C/C++\r\n- Java\r\n- Go\r\n- Rust\r\n- Shell/Bash\r\n- SQL\r\n- JSON/YAML\r\n- CSS/SCSS\r\n- HTML\r\n- Markdown\r\n- And many more...\r\n\r\nTry copying any of the code blocks above to see the copy functionality in action!\r\n"
  },
  {
    "path": "demo/progress-tracking",
    "title": "Progress Tracking System",
    "content": "# Progress Tracking System\r\n\r\nWelcome to the comprehensive progress tracking system! This feature helps you monitor your learning journey through the Chromium knowledge base.\r\n\r\n## Features Overview\r\n\r\n### 📊 Reading Progress\r\n- **Automatic tracking**: Progress is tracked based on scroll position and time spent reading\r\n- **Article completion**: Articles are marked as complete when you reach 95% scroll progress\r\n- **Time tracking**: Monitor how much time you spend on each article\r\n- **Cross-session persistence**: Your progress is saved across browser sessions\r\n\r\n### 🎯 Learning Analytics\r\n- **Weekly activity**: Visual charts showing your reading patterns over the past week\r\n- **Category progress**: See how much you've explored in each content category\r\n- **Reading streaks**: Track consecutive days of learning activity\r\n- **Overall progress**: Get a bird's-eye view of your platform progress\r\n\r\n### 🛤️ Learning Paths\r\n- **Custom paths**: Create structured learning journeys for specific topics\r\n- **Progress tracking**: Monitor completion of learning path articles\r\n- **Estimated time**: Get reading time estimates for entire learning paths\r\n- **Difficulty levels**: Organize paths by beginner, intermediate, or advanced levels\r\n\r\n### 📈 Intelligent Recommendations\r\n- **Continue reading**: Smart suggestions for articles you've started but haven't finished\r\n- **Related content**: Discover articles based on your reading history\r\n- **Category exploration**: Find new areas to explore based on your interests\r\n\r\n## How It Works\r\n\r\n### Automatic Progress Tracking\r\n\r\nThe system automatically tracks your reading progress using several methods:\r\n\r\n1. **Scroll-based tracking**: As you scroll through an article, your progress percentage increases\r\n2. **Time-based tracking**: The system records how much time you spend reading each article\r\n3. **Completion detection**: When you scroll to 95% of an article, it's marked as complete\r\n4. **Session management**: Your reading session is tracked from when you start reading until you navigate away\r\n\r\n### Data Storage\r\n\r\nAll progress data is stored locally in your browser using localStorage, ensuring:\r\n- **Privacy**: Your reading data never leaves your device\r\n- **Performance**: Fast access to your progress data\r\n- **Offline support**: Progress tracking works even when offline\r\n- **Data portability**: Export and import your progress data\r\n\r\n### Visual Indicators\r\n\r\nYou'll see progress indicators throughout the platform:\r\n- **Progress bar**: At the top of each article showing reading progress\r\n- **Time estimates**: Reading time estimates for articles and learning paths\r\n- **Completion badges**: Visual indicators for completed articles\r\n- **Streak counters**: Daily reading streak display\r\n\r\n## Getting Started\r\n\r\n### Access Your Dashboard\r\n\r\nClick the progress icon (📊) in the header to access your **Learning Progress Dashboard** where you can:\r\n\r\n- View your overall statistics\r\n- See your reading history\r\n- Manage learning paths\r\n- Analyze your learning patterns\r\n- Export/import your data\r\n\r\n### Create Learning Paths\r\n\r\nOrganize your learning by creating custom learning paths:\r\n\r\n1. Go to the **Learning Paths** tab in your progress dashboard\r\n2. Click **\"Create New Path\"**\r\n3. Add articles to your path\r\n4. Set difficulty level and estimated time\r\n5. Track your progress as you complete articles\r\n\r\n### Monitor Your Progress\r\n\r\nEach article you read will automatically:\r\n- Show a progress indicator at the top\r\n- Track time spent reading\r\n- Update your overall statistics\r\n- Contribute to your learning streaks\r\n\r\n## Example Learning Paths\r\n\r\nHere are some suggested learning paths to get you started:\r\n\r\n### Beginner: Chromium Basics\r\n1. Introduction → Overview\r\n2. Getting Started → Setup & Build\r\n3. Architecture → Overview\r\n4. Architecture → Process Model\r\n\r\n### Intermediate: Browser Architecture\r\n1. Architecture → Module Layering\r\n2. Architecture → IPC Internals\r\n3. Architecture → Render Pipeline\r\n4. Modules → Networking (HTTP)\r\n\r\n### Advanced: Deep Dive Development\r\n1. Architecture → Security → Sandbox Architecture\r\n2. Modules → JavaScript (V8)\r\n3. Architecture → Design Patterns → All patterns\r\n4. Contributing → Contributing Guide\r\n\r\n## Privacy and Data\r\n\r\n### Local Storage Only\r\nYour progress data is stored entirely in your browser's local storage. This means:\r\n- **No server tracking**: Your reading habits are completely private\r\n- **No account required**: Start tracking immediately without signing up\r\n- **Device-specific**: Progress is tied to your specific browser and device\r\n\r\n### Data Export/Import\r\nYou can export your progress data to:\r\n- **Backup your progress**: Save your data before clearing browser storage\r\n- **Transfer between devices**: Move your progress to another browser or device\r\n- **Share learning paths**: Export specific learning paths to share with others\r\n\r\n### Data Management\r\nFull control over your data:\r\n- **Clear all progress**: Reset your progress tracking at any time\r\n- **Selective deletion**: Remove specific articles or learning paths\r\n- **Export anytime**: Get a JSON export of all your progress data\r\n\r\n## Technical Implementation\r\n\r\nThe progress tracking system is built with:\r\n- **React Context**: Centralized state management for progress data\r\n- **TypeScript**: Type-safe interfaces for all progress data structures\r\n- **localStorage API**: Persistent local storage for cross-session data\r\n- **Scroll tracking**: Advanced scroll position monitoring\r\n- **Time tracking**: Accurate reading time measurement\r\n\r\n### Performance Considerations\r\n- **Efficient storage**: Minimal data footprint with optimized storage structure\r\n- **Background tracking**: Non-intrusive progress updates that don't affect reading experience\r\n- **Lazy loading**: Progress dashboard components are loaded only when needed\r\n- **Debounced updates**: Progress updates are batched to prevent excessive localStorage writes\r\n\r\nStart exploring the knowledge base, and watch your progress grow! The system will automatically begin tracking as soon as you start reading articles.\r\n"
  },
  {
    "path": "demo/interactive-diagrams",
    "title": "Interactive Architecture Diagrams",
    "content": "# Interactive Architecture Diagrams\r\n\r\nExplore Chromium's architecture through interactive diagrams. Click on components to learn more about their roles and responsibilities.\r\n\r\n## Chromium Multi-Process Architecture\r\n\r\nThis diagram shows the high-level process architecture of Chromium. Each process runs in its own sandbox for security and stability.\r\n\r\n```interactive-diagram\r\n{\r\n  \"title\": \"Chromium Multi-Process Architecture\",\r\n  \"description\": \"Click on processes to learn about their responsibilities and security boundaries\",\r\n  \"height\": 500,\r\n  \"interactive\": true,\r\n  \"controls\": true,\r\n  \"miniMap\": true,\r\n  \"background\": true,\r\n  \"nodes\": [\r\n    {\r\n      \"id\": \"browser-process\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 100, \"y\": 100 },\r\n      \"data\": {\r\n        \"label\": \"Browser Process\",\r\n        \"description\": \"Main process that manages the UI, network, and coordinates other processes\",\r\n        \"details\": \"The browser process is the main process in Chromium. It handles the browser UI, manages tabs, bookmarks, history, and coordinates communication between other processes.\",\r\n        \"icon\": \"🏠\",\r\n        \"processType\": \"browser\",\r\n        \"links\": [\r\n          { \"title\": \"Browser Process Documentation\", \"url\": \"#/architecture/process-model\" },\r\n          { \"title\": \"Process Model Overview\", \"url\": \"#/architecture/overview\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"renderer-process-1\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 400, \"y\": 50 },\r\n      \"data\": {\r\n        \"label\": \"Renderer Process\",\r\n        \"description\": \"Renders web content, executes JavaScript, handles DOM manipulation\",\r\n        \"details\": \"Each tab typically runs in its own renderer process. This provides isolation between tabs and websites for security and stability.\",\r\n        \"icon\": \"🌐\",\r\n        \"processType\": \"renderer\",\r\n        \"links\": [\r\n          { \"title\": \"Render Pipeline\", \"url\": \"#/architecture/render-pipeline\" },\r\n          { \"title\": \"Blink Architecture\", \"url\": \"#/modules/javascript-v8\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"renderer-process-2\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 400, \"y\": 150 },\r\n      \"data\": {\r\n        \"label\": \"Renderer Process\",\r\n        \"description\": \"Another tab's renderer process (isolated)\",\r\n        \"icon\": \"🌐\",\r\n        \"processType\": \"renderer\"\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"gpu-process\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 100, \"y\": 250 },\r\n      \"data\": {\r\n        \"label\": \"GPU Process\",\r\n        \"description\": \"Handles graphics acceleration and compositing\",\r\n        \"details\": \"The GPU process manages hardware acceleration for graphics rendering, video decoding, and UI compositing.\",\r\n        \"icon\": \"🎮\",\r\n        \"processType\": \"gpu\",\r\n        \"links\": [\r\n          { \"title\": \"GPU Architecture\", \"url\": \"#/architecture/browser-components\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"network-process\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 100, \"y\": 350 },\r\n      \"data\": {\r\n        \"label\": \"Network Process\",\r\n        \"description\": \"Manages network requests, caching, and security\",\r\n        \"details\": \"Handles all network communication, HTTP/HTTPS requests, caching, and network security policies.\",\r\n        \"icon\": \"🌍\",\r\n        \"processType\": \"network\",\r\n        \"links\": [\r\n          { \"title\": \"Network Stack\", \"url\": \"#/modules/networking-http\" },\r\n          { \"title\": \"Storage & Cache\", \"url\": \"#/modules/storage-cache\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"utility-process\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 400, \"y\": 300 },\r\n      \"data\": {\r\n        \"label\": \"Utility Process\",\r\n        \"description\": \"Handles various utility tasks like audio, device access\",\r\n        \"details\": \"Runs services that need sandboxing but don't fit into other process types. Examples include audio service, device service, and storage service.\",\r\n        \"icon\": \"🔧\",\r\n        \"processType\": \"utility\",\r\n        \"links\": [\r\n          { \"title\": \"Process Model\", \"url\": \"#/architecture/process-model\" }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"edges\": [\r\n    {\r\n      \"id\": \"browser-renderer1\",\r\n      \"source\": \"browser-process\",\r\n      \"target\": \"renderer-process-1\",\r\n      \"label\": \"IPC\",\r\n      \"description\": \"Inter-process communication for tab management\",\r\n      \"type\": \"smoothstep\",\r\n      \"animated\": true,\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#374151\" }\r\n    },\r\n    {\r\n      \"id\": \"browser-renderer2\",\r\n      \"source\": \"browser-process\",\r\n      \"target\": \"renderer-process-2\",\r\n      \"label\": \"IPC\",\r\n      \"type\": \"smoothstep\",\r\n      \"animated\": true,\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#374151\" }\r\n    },\r\n    {\r\n      \"id\": \"browser-gpu\",\r\n      \"source\": \"browser-process\",\r\n      \"target\": \"gpu-process\",\r\n      \"label\": \"Graphics Commands\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#374151\" }\r\n    },\r\n    {\r\n      \"id\": \"browser-network\",\r\n      \"source\": \"browser-process\",\r\n      \"target\": \"network-process\",\r\n      \"label\": \"Network Requests\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#374151\" }\r\n    },\r\n    {\r\n      \"id\": \"renderer1-gpu\",\r\n      \"source\": \"renderer-process-1\",\r\n      \"target\": \"gpu-process\",\r\n      \"label\": \"Rendering\",\r\n      \"type\": \"smoothstep\",\r\n      \"style\": { \"stroke\": \"#dc2626\", \"strokeDasharray\": \"5,5\" },\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#dc2626\" }\r\n    },\r\n    {\r\n      \"id\": \"renderer2-gpu\",\r\n      \"source\": \"renderer-process-2\",\r\n      \"target\": \"gpu-process\",\r\n      \"label\": \"Rendering\",\r\n      \"type\": \"smoothstep\",\r\n      \"style\": { \"stroke\": \"#dc2626\", \"strokeDasharray\": \"5,5\" },\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#dc2626\" }\r\n    },\r\n    {\r\n      \"id\": \"browser-utility\",\r\n      \"source\": \"browser-process\",\r\n      \"target\": \"utility-process\",\r\n      \"label\": \"Services\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#374151\" }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n## Blink Rendering Engine Components\r\n\r\nThis diagram shows the internal structure of Blink, the rendering engine used in Chromium's renderer processes.\r\n\r\n```interactive-diagram\r\n{\r\n  \"title\": \"Blink Rendering Engine Architecture\",\r\n  \"description\": \"Explore the components that transform HTML, CSS, and JavaScript into rendered web pages\",\r\n  \"height\": 600,\r\n  \"interactive\": true,\r\n  \"controls\": true,\r\n  \"background\": true,\r\n  \"nodes\": [\r\n    {\r\n      \"id\": \"html-parser\",\r\n      \"type\": \"chromium-component\",\r\n      \"position\": { \"x\": 50, \"y\": 50 },\r\n      \"data\": {\r\n        \"label\": \"HTML Parser\",\r\n        \"description\": \"Parses HTML markup into DOM tree\",\r\n        \"details\": \"Converts HTML text into a Document Object Model (DOM) tree structure that represents the document's structure and content.\",\r\n        \"icon\": \"📄\",\r\n        \"componentType\": \"content\",\r\n        \"links\": [\r\n          { \"title\": \"Render Pipeline\", \"url\": \"#/architecture/render-pipeline\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"css-parser\",\r\n      \"type\": \"chromium-component\",\r\n      \"position\": { \"x\": 50, \"y\": 150 },\r\n      \"data\": {\r\n        \"label\": \"CSS Parser\",\r\n        \"description\": \"Parses CSS stylesheets and computes styles\",\r\n        \"details\": \"Parses CSS stylesheets and creates CSSOM (CSS Object Model). Handles selector matching and style computation.\",\r\n        \"icon\": \"🎨\",\r\n        \"componentType\": \"content\"\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"layout-engine\",\r\n      \"type\": \"chromium-component\",\r\n      \"position\": { \"x\": 300, \"y\": 100 },\r\n      \"data\": {\r\n        \"label\": \"Layout Engine\",\r\n        \"description\": \"Calculates element positions and sizes\",\r\n        \"details\": \"Performs layout calculations to determine the position and size of each element based on CSS rules and content.\",\r\n        \"icon\": \"📐\",\r\n        \"componentType\": \"blink\",\r\n        \"links\": [\r\n          { \"title\": \"Render Pipeline\", \"url\": \"#/architecture/render-pipeline\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"paint-engine\",\r\n      \"type\": \"chromium-component\",\r\n      \"position\": { \"x\": 300, \"y\": 200 },\r\n      \"data\": {\r\n        \"label\": \"Paint Engine\",\r\n        \"description\": \"Converts layout tree to paint instructions\",\r\n        \"details\": \"Generates paint operations (draw commands) that describe how to render each element visually.\",\r\n        \"icon\": \"🖌️\",\r\n        \"componentType\": \"blink\"\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"v8-engine\",\r\n      \"type\": \"chromium-component\",\r\n      \"position\": { \"x\": 50, \"y\": 300 },\r\n      \"data\": {\r\n        \"label\": \"V8 JavaScript Engine\",\r\n        \"description\": \"Executes JavaScript code and manages DOM APIs\",\r\n        \"details\": \"High-performance JavaScript engine that compiles and executes JavaScript, manages memory, and provides DOM/Web APIs.\",\r\n        \"icon\": \"⚡\",\r\n        \"componentType\": \"v8\",\r\n        \"links\": [\r\n          { \"title\": \"JavaScript Integration\", \"url\": \"#/modules/javascript-v8\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"compositor\",\r\n      \"type\": \"chromium-component\",\r\n      \"position\": { \"x\": 550, \"y\": 150 },\r\n      \"data\": {\r\n        \"label\": \"Compositor\",\r\n        \"description\": \"Composites layers for GPU acceleration\",\r\n        \"details\": \"Manages compositing layers and coordinates with the GPU process for hardware-accelerated rendering.\",\r\n        \"icon\": \"🔧\",\r\n        \"componentType\": \"blink\",\r\n        \"links\": [\r\n          { \"title\": \"GPU Architecture\", \"url\": \"#/architecture/browser-components\" }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"dom\",\r\n      \"type\": \"default\",\r\n      \"position\": { \"x\": 300, \"y\": 50 },\r\n      \"data\": {\r\n        \"label\": \"DOM Tree\",\r\n        \"description\": \"Document Object Model representation\",\r\n        \"icon\": \"🌳\"\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"render-tree\",\r\n      \"type\": \"default\",\r\n      \"position\": { \"x\": 550, \"y\": 100 },\r\n      \"data\": {\r\n        \"label\": \"Render Tree\",\r\n        \"description\": \"Combined DOM + Style tree for rendering\",\r\n        \"icon\": \"🌲\"\r\n      }\r\n    }\r\n  ],\r\n  \"edges\": [\r\n    {\r\n      \"id\": \"html-dom\",\r\n      \"source\": \"html-parser\",\r\n      \"target\": \"dom\",\r\n      \"label\": \"creates\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"css-layout\",\r\n      \"source\": \"css-parser\",\r\n      \"target\": \"layout-engine\",\r\n      \"label\": \"styles\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"dom-layout\",\r\n      \"source\": \"dom\",\r\n      \"target\": \"layout-engine\",\r\n      \"label\": \"structure\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"layout-render\",\r\n      \"source\": \"layout-engine\",\r\n      \"target\": \"render-tree\",\r\n      \"label\": \"generates\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"layout-paint\",\r\n      \"source\": \"layout-engine\",\r\n      \"target\": \"paint-engine\",\r\n      \"label\": \"layout info\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"paint-compositor\",\r\n      \"source\": \"paint-engine\",\r\n      \"target\": \"compositor\",\r\n      \"label\": \"paint ops\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"render-compositor\",\r\n      \"source\": \"render-tree\",\r\n      \"target\": \"compositor\",\r\n      \"label\": \"layers\",\r\n      \"type\": \"smoothstep\",\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"v8-dom\",\r\n      \"source\": \"v8-engine\",\r\n      \"target\": \"dom\",\r\n      \"label\": \"DOM API\",\r\n      \"type\": \"smoothstep\",\r\n      \"style\": { \"stroke\": \"#f59e0b\", \"strokeDasharray\": \"3,3\" },\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#f59e0b\" }\r\n    },\r\n    {\r\n      \"id\": \"v8-layout\",\r\n      \"source\": \"v8-engine\",\r\n      \"target\": \"layout-engine\",\r\n      \"label\": \"style changes\",\r\n      \"type\": \"smoothstep\",\r\n      \"style\": { \"stroke\": \"#f59e0b\", \"strokeDasharray\": \"3,3\" },\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#f59e0b\" }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n## IPC (Inter-Process Communication) Flow\r\n\r\nUnderstanding how different processes communicate is crucial for Chromium development.\r\n\r\n```interactive-diagram\r\n{\r\n  \"title\": \"IPC Message Flow Example\",\r\n  \"description\": \"Follow a typical user interaction from browser UI to web content rendering\",\r\n  \"height\": 400,\r\n  \"interactive\": true,\r\n  \"controls\": true,\r\n  \"nodes\": [\r\n    {\r\n      \"id\": \"user\",\r\n      \"type\": \"default\",\r\n      \"position\": { \"x\": 50, \"y\": 200 },\r\n      \"data\": {\r\n        \"label\": \"User\",\r\n        \"description\": \"User clicks a link in the browser\",\r\n        \"icon\": \"👤\"\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"browser-ui\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 200, \"y\": 200 },\r\n      \"data\": {\r\n        \"label\": \"Browser UI\",\r\n        \"description\": \"Browser process handles UI events\",\r\n        \"processType\": \"browser\",\r\n        \"icon\": \"🖥️\"\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"renderer\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 400, \"y\": 200 },\r\n      \"data\": {\r\n        \"label\": \"Renderer\",\r\n        \"description\": \"Renderer process handles the navigation\",\r\n        \"processType\": \"renderer\",\r\n        \"icon\": \"🌐\"\r\n      }\r\n    },\r\n    {\r\n      \"id\": \"network\",\r\n      \"type\": \"chromium-process\",\r\n      \"position\": { \"x\": 600, \"y\": 200 },\r\n      \"data\": {\r\n        \"label\": \"Network\",\r\n        \"description\": \"Network process fetches the resource\",\r\n        \"processType\": \"network\",\r\n        \"icon\": \"🌍\"\r\n      }\r\n    }\r\n  ],\r\n  \"edges\": [\r\n    {\r\n      \"id\": \"user-browser\",\r\n      \"source\": \"user\",\r\n      \"target\": \"browser-ui\",\r\n      \"label\": \"1. Click\",\r\n      \"type\": \"smoothstep\",\r\n      \"animated\": true,\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"browser-renderer\",\r\n      \"source\": \"browser-ui\",\r\n      \"target\": \"renderer\",\r\n      \"label\": \"2. Navigate IPC\",\r\n      \"type\": \"smoothstep\",\r\n      \"animated\": true,\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"renderer-network\",\r\n      \"source\": \"renderer\",\r\n      \"target\": \"network\",\r\n      \"label\": \"3. Fetch Request\",\r\n      \"type\": \"smoothstep\",\r\n      \"animated\": true,\r\n      \"markerEnd\": { \"type\": \"arrowclosed\" }\r\n    },\r\n    {\r\n      \"id\": \"network-renderer-back\",\r\n      \"source\": \"network\",\r\n      \"target\": \"renderer\",\r\n      \"label\": \"4. Response\",\r\n      \"type\": \"smoothstep\",\r\n      \"style\": { \"stroke\": \"#16a34a\" },\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#16a34a\" }\r\n    },\r\n    {\r\n      \"id\": \"renderer-browser-back\",\r\n      \"source\": \"renderer\",\r\n      \"target\": \"browser-ui\",\r\n      \"label\": \"5. Update UI\",\r\n      \"type\": \"smoothstep\",\r\n      \"style\": { \"stroke\": \"#16a34a\" },\r\n      \"markerEnd\": { \"type\": \"arrowclosed\", \"color\": \"#16a34a\" }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n## Learning Notes\r\n\r\n### Key Concepts Demonstrated:\r\n\r\n1. **Process Isolation**: Each process runs in its own sandbox for security\r\n2. **IPC Communication**: Processes communicate through well-defined message interfaces\r\n3. **Component Separation**: Different responsibilities are clearly separated\r\n4. **Security Boundaries**: Renderer processes are heavily sandboxed\r\n\r\n### Interactive Features:\r\n\r\n- **Click nodes** to see detailed descriptions and links to relevant documentation\r\n- **Drag nodes** to rearrange the diagram for better understanding\r\n- **Use mouse wheel** to zoom in/out for different levels of detail\r\n- **Minimap** (when enabled) provides overview of large diagrams\r\n\r\n### Next Steps:\r\n\r\nExplore the linked documentation pages to dive deeper into each component's implementation details and design principles.\r\n"
  },
  {
    "path": "demo/enhanced-component-architecture",
    "title": "Enhanced Component Architecture Demo",
    "content": "# Enhanced Component Architecture Demo\r\n\r\nThis page demonstrates the modular, extensible article/component rendering system that supports multiple content types with consistent interaction patterns.\r\n\r\n## Markdown Components\r\n\r\nThis is a standard markdown component that supports rich text formatting, code blocks with syntax highlighting, and interactive section bookmarks.\r\n\r\n### Code Example\r\n\r\nHere's some JavaScript code with syntax highlighting:\r\n\r\n```javascript\r\n// React Component Example\r\nimport React, { useState } from 'react';\r\n\r\nconst ExampleComponent = () => {\r\n  const [count, setCount] = useState(0);\r\n\r\n  const handleClick = () => {\r\n    setCount(prevCount => prevCount + 1);\r\n  };\r\n\r\n  return (\r\n    <div className=\"component\">\r\n      <h2>Count: {count}</h2>\r\n      <button onClick={handleClick}>\r\n        Increment\r\n      </button>\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default ExampleComponent;\r\n```\r\n\r\n### Features\r\n\r\n- ✅ **Syntax Highlighting**: Automatic code highlighting for multiple languages\r\n- ✅ **Section Bookmarks**: Click the bookmark icon next to any heading to save it\r\n- ✅ **Responsive Design**: Optimized for mobile and desktop viewing\r\n- ✅ **Theme Support**: Supports both light and dark themes\r\n\r\n## Interactive Components\r\n\r\nThe system supports various interactive content types:\r\n\r\n### Code Editor\r\n- Live code editing with syntax highlighting\r\n- Multiple programming language support\r\n- Run and reset functionality\r\n\r\n### Simulations\r\n- Interactive simulations with configurable parameters\r\n- Real-time updates and controls\r\n- Educational demonstrations\r\n\r\n### Demos\r\n- Step-by-step interactive demonstrations\r\n- Progress tracking and state management\r\n- Customizable configuration options\r\n\r\n## Media Components\r\n\r\n### Video Content\r\n- HTML5 video player with custom controls\r\n- Interactive captions and transcripts\r\n- Thumbnail previews and metadata display\r\n- Progress tracking and seeking\r\n\r\n### Diagrams\r\n- Multiple diagram types (Mermaid, PlantUML, Flowcharts, Architecture)\r\n- Interactive elements with clickable nodes\r\n- Fullscreen viewing mode\r\n- Zoom and pan controls\r\n\r\n## UI Components\r\n\r\n### Callouts\r\nVarious callout types for important information:\r\n\r\n- **Info**: General information and tips\r\n- **Warning**: Important warnings and cautions\r\n- **Error**: Error messages and troubleshooting\r\n- **Success**: Success messages and confirmations\r\n- **Tip**: Pro tips and best practices\r\n\r\n### Quizzes\r\n- Multiple choice questions\r\n- True/false questions\r\n- Code completion exercises\r\n- Progress tracking and scoring\r\n- Detailed explanations and feedback\r\n\r\n## Architecture Benefits\r\n\r\n### 1. Modularity\r\nEach component type is implemented as a separate, focused renderer that can be developed, tested, and maintained independently.\r\n\r\n### 2. Extensibility\r\nNew component types can be easily added by:\r\n1. Creating a new renderer component\r\n2. Adding the type to the ComponentTypes interface\r\n3. Updating the ComponentRenderer switch statement\r\n\r\n### 3. Consistency\r\nAll components follow the same interaction patterns and design principles, providing a unified user experience.\r\n\r\n### 4. Performance\r\n- Lazy loading of component renderers\r\n- Suspense boundaries for graceful loading states\r\n- Optimized re-rendering with React.memo and proper dependency management\r\n\r\n### 5. Developer Experience\r\n- Type-safe component definitions\r\n- Comprehensive error handling and fallbacks\r\n- Development-time debugging information\r\n- Extensive logging and analytics hooks\r\n\r\n## Technical Implementation\r\n\r\nThe Enhanced Component Architecture consists of:\r\n\r\n- **Core Types**: TypeScript interfaces defining component structure and content types\r\n- **Component Renderer**: Main orchestrator that routes components to appropriate renderers\r\n- **Modular Renderers**: Specialized components for each content type\r\n- **Interaction System**: Consistent event handling and state management\r\n- **Layout System**: Flexible layout and styling options\r\n\r\nThis architecture enables the creation of rich, interactive documentation that goes beyond traditional markdown while maintaining excellent performance and developer experience.\r\n"
  },
  {
    "path": "demo/cpp-chromium-playground",
    "title": "C++ and Chromium Development Playground",
    "content": "# C++ and Chromium Development Playground\r\n\r\nExplore C++ concepts and Chromium-specific patterns in this interactive playground. While full C++ compilation requires a server, you can learn syntax, understand patterns, and experiment with code structure.\r\n\r\n## Basic C++ Concepts\r\n\r\n### Memory Management and Smart Pointers\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <memory>\r\n#include <vector>\r\n#include <string>\r\n\r\n// Modern C++ memory management example\r\nclass ChromiumComponent {\r\nprivate:\r\n    std::string name_;\r\n    int priority_;\r\n    \r\npublic:\r\n    ChromiumComponent(const std::string& name, int priority) \r\n        : name_(name), priority_(priority) {\r\n        std::cout << \"Creating component: \" << name_ << std::endl;\r\n    }\r\n    \r\n    ~ChromiumComponent() {\r\n        std::cout << \"Destroying component: \" << name_ << std::endl;\r\n    }\r\n    \r\n    const std::string& GetName() const { return name_; }\r\n    int GetPriority() const { return priority_; }\r\n    \r\n    void Process() {\r\n        std::cout << \"Processing \" << name_ << \" with priority \" << priority_ << std::endl;\r\n    }\r\n};\r\n\r\n// Factory pattern commonly used in Chromium\r\nclass ComponentFactory {\r\npublic:\r\n    static std::unique_ptr<ChromiumComponent> CreateComponent(\r\n        const std::string& type, const std::string& name) {\r\n        \r\n        int priority = (type == \"ui\") ? 1 : (type == \"network\") ? 2 : 3;\r\n        return std::make_unique<ChromiumComponent>(name, priority);\r\n    }\r\n};\r\n\r\nint main() {\r\n    // Smart pointer usage - automatic memory management\r\n    std::vector<std::unique_ptr<ChromiumComponent>> components;\r\n    \r\n    // Create components using factory\r\n    components.push_back(ComponentFactory::CreateComponent(\"ui\", \"MainWindow\"));\r\n    components.push_back(ComponentFactory::CreateComponent(\"network\", \"HttpClient\"));\r\n    components.push_back(ComponentFactory::CreateComponent(\"storage\", \"CacheManager\"));\r\n    \r\n    // Process all components\r\n    std::cout << \"\\n--- Processing Components ---\\n\";\r\n    for (const auto& component : components) {\r\n        component->Process();\r\n    }\r\n    \r\n    // Memory is automatically cleaned up when vector goes out of scope\r\n    std::cout << \"\\n--- Cleanup ---\\n\";\r\n    return 0;\r\n}\r\n```\r\n\r\n**Learning Points:**\r\n- RAII (Resource Acquisition Is Initialization) principle\r\n- Smart pointers for automatic memory management\r\n- Factory pattern for object creation\r\n- Const correctness in member functions\r\n\r\n---\r\n\r\n## Chromium-Style Code Patterns\r\n\r\n### Observer Pattern Implementation\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <vector>\r\n#include <algorithm>\r\n#include <memory>\r\n\r\n// Observer pattern - widely used in Chromium for event handling\r\nclass Observer {\r\npublic:\r\n    virtual ~Observer() = default;\r\n    virtual void OnNotify(const std::string& event) = 0;\r\n};\r\n\r\nclass Subject {\r\nprivate:\r\n    std::vector<Observer*> observers_;\r\n    \r\npublic:\r\n    void AddObserver(Observer* observer) {\r\n        observers_.push_back(observer);\r\n    }\r\n    \r\n    void RemoveObserver(Observer* observer) {\r\n        observers_.erase(\r\n            std::remove(observers_.begin(), observers_.end(), observer),\r\n            observers_.end()\r\n        );\r\n    }\r\n    \r\n    void NotifyAll(const std::string& event) {\r\n        for (Observer* observer : observers_) {\r\n            observer->OnNotify(event);\r\n        }\r\n    }\r\n};\r\n\r\n// Concrete observers\r\nclass UIObserver : public Observer {\r\nprivate:\r\n    std::string name_;\r\n    \r\npublic:\r\n    UIObserver(const std::string& name) : name_(name) {}\r\n    \r\n    void OnNotify(const std::string& event) override {\r\n        std::cout << \"[UI:\" << name_ << \"] Received event: \" << event << std::endl;\r\n        // Handle UI updates here\r\n    }\r\n};\r\n\r\nclass NetworkObserver : public Observer {\r\npublic:\r\n    void OnNotify(const std::string& event) override {\r\n        std::cout << \"[Network] Handling event: \" << event << std::endl;\r\n        // Handle network-related events\r\n    }\r\n};\r\n\r\n// Example usage\r\nint main() {\r\n    Subject browserEvents;\r\n    \r\n    UIObserver mainWindow(\"MainWindow\");\r\n    UIObserver toolbar(\"Toolbar\");\r\n    NetworkObserver networkManager;\r\n    \r\n    // Register observers\r\n    browserEvents.AddObserver(&mainWindow);\r\n    browserEvents.AddObserver(&toolbar);\r\n    browserEvents.AddObserver(&networkManager);\r\n    \r\n    // Simulate browser events\r\n    std::cout << \"=== Browser Events Simulation ===\\n\";\r\n    browserEvents.NotifyAll(\"page_load_started\");\r\n    browserEvents.NotifyAll(\"navigation_completed\");\r\n    browserEvents.NotifyAll(\"network_error\");\r\n    \r\n    return 0;\r\n}\r\n```\r\n\r\n**Chromium Usage:**\r\n- Content API notifications\r\n- UI event propagation\r\n- Browser process communication\r\n- Preference change notifications\r\n\r\n---\r\n\r\n## C++ Templates and Modern Features\r\n\r\n### Template Metaprogramming Example\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <type_traits>\r\n#include <string>\r\n#include <vector>\r\n\r\n// Template metaprogramming - used extensively in Chromium's base library\r\ntemplate<typename T>\r\nclass Optional {\r\nprivate:\r\n    bool has_value_;\r\n    alignas(T) char storage_[sizeof(T)];\r\n    \r\npublic:\r\n    Optional() : has_value_(false) {}\r\n    \r\n    Optional(const T& value) : has_value_(true) {\r\n        new(storage_) T(value);\r\n    }\r\n    \r\n    ~Optional() {\r\n        if (has_value_) {\r\n            reinterpret_cast<T*>(storage_)->~T();\r\n        }\r\n    }\r\n    \r\n    bool has_value() const { return has_value_; }\r\n    \r\n    const T& value() const {\r\n        if (!has_value_) {\r\n            throw std::runtime_error(\"Optional has no value\");\r\n        }\r\n        return *reinterpret_cast<const T*>(storage_);\r\n    }\r\n    \r\n    T value_or(const T& default_value) const {\r\n        return has_value_ ? value() : default_value;\r\n    }\r\n};\r\n\r\n// SFINAE (Substitution Failure Is Not An Error) example\r\ntemplate<typename T>\r\ntypename std::enable_if<std::is_arithmetic<T>::value, void>::type\r\nPrintValue(const T& value) {\r\n    std::cout << \"Numeric value: \" << value << std::endl;\r\n}\r\n\r\ntemplate<typename T>\r\ntypename std::enable_if<!std::is_arithmetic<T>::value, void>::type\r\nPrintValue(const T& value) {\r\n    std::cout << \"Non-numeric value: \" << value << std::endl;\r\n}\r\n\r\n// Variadic templates - used in Chromium's callback system\r\ntemplate<typename... Args>\r\nvoid LogInfo(const std::string& format, Args&&... args) {\r\n    std::cout << \"[INFO] \" << format;\r\n    // In real implementation, would format with args\r\n    ((std::cout << \" \" << args), ...); // C++17 fold expression\r\n    std::cout << std::endl;\r\n}\r\n\r\nint main() {\r\n    // Optional usage\r\n    std::cout << \"=== Optional Example ===\\n\";\r\n    Optional<std::string> maybe_name(\"Chromium\");\r\n    Optional<std::string> empty_name;\r\n    \r\n    std::cout << \"Has name: \" << maybe_name.has_value() << std::endl;\r\n    std::cout << \"Name: \" << maybe_name.value_or(\"Unknown\") << std::endl;\r\n    std::cout << \"Empty name: \" << empty_name.value_or(\"Default\") << std::endl;\r\n    \r\n    // SFINAE demonstration\r\n    std::cout << \"\\n=== SFINAE Example ===\\n\";\r\n    PrintValue(42);\r\n    PrintValue(std::string(\"Hello\"));\r\n    \r\n    // Variadic templates\r\n    std::cout << \"\\n=== Variadic Templates ===\\n\";\r\n    LogInfo(\"Browser started\", \"version\", \"1.0\", \"build\", 12345);\r\n    \r\n    return 0;\r\n}\r\n```\r\n\r\n**Modern C++ Features:**\r\n- Perfect forwarding with `std::forward`\r\n- SFINAE for template specialization\r\n- Variadic templates for flexible APIs\r\n- Placement new for custom memory management\r\n\r\n---\r\n\r\n## Chromium Base Library Patterns\r\n\r\n### Callback System Simulation\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <functional>\r\n#include <memory>\r\n#include <vector>\r\n\r\n// Simplified version of Chromium's callback system\r\ntemplate<typename Signature>\r\nclass Callback;\r\n\r\ntemplate<typename R, typename... Args>\r\nclass Callback<R(Args...)> {\r\nprivate:\r\n    std::function<R(Args...)> callback_;\r\n    \r\npublic:\r\n    Callback() = default;\r\n    \r\n    template<typename F>\r\n    Callback(F&& f) : callback_(std::forward<F>(f)) {}\r\n    \r\n    R Run(Args... args) const {\r\n        if (callback_) {\r\n            return callback_(args...);\r\n        }\r\n        if constexpr (!std::is_void_v<R>) {\r\n            return R{};\r\n        }\r\n    }\r\n    \r\n    bool is_null() const { return !callback_; }\r\n    \r\n    explicit operator bool() const { return !is_null(); }\r\n};\r\n\r\n// Factory functions for creating callbacks\r\ntemplate<typename F>\r\nauto BindOnce(F&& f) {\r\n    return Callback<std::invoke_result_t<F>()>{std::forward<F>(f)};\r\n}\r\n\r\ntemplate<typename F, typename... Args>\r\nauto BindOnce(F&& f, Args&&... args) {\r\n    return [f = std::forward<F>(f), args...](auto&&... remaining_args) {\r\n        return f(args..., remaining_args...);\r\n    };\r\n}\r\n\r\n// Example usage in a hypothetical browser component\r\nclass NetworkRequest {\r\nprivate:\r\n    std::string url_;\r\n    Callback<void(int, const std::string&)> completion_callback_;\r\n    \r\npublic:\r\n    NetworkRequest(const std::string& url) : url_(url) {}\r\n    \r\n    void SetCompletionCallback(Callback<void(int, const std::string&)> callback) {\r\n        completion_callback_ = std::move(callback);\r\n    }\r\n    \r\n    void Start() {\r\n        std::cout << \"Starting request to: \" << url_ << std::endl;\r\n        \r\n        // Simulate async operation\r\n        // In real code, this would be asynchronous\r\n        int status_code = 200;\r\n        std::string response = \"Response data from \" + url_;\r\n        \r\n        if (completion_callback_) {\r\n            completion_callback_.Run(status_code, response);\r\n        }\r\n    }\r\n};\r\n\r\n// Response handler\r\nvoid HandleResponse(const std::string& context, int status, const std::string& data) {\r\n    std::cout << \"[\" << context << \"] Status: \" << status \r\n              << \", Data: \" << data << std::endl;\r\n}\r\n\r\nint main() {\r\n    std::cout << \"=== Chromium-Style Callback System ===\\n\";\r\n    \r\n    NetworkRequest request(\"https://example.com/api/data\");\r\n    \r\n    // Bind callback with context\r\n    auto callback = BindOnce(&HandleResponse, \"MainFrame\");\r\n    request.SetCompletionCallback(\r\n        Callback<void(int, const std::string&)>{callback}\r\n    );\r\n    \r\n    request.Start();\r\n    \r\n    // Lambda callback example\r\n    std::cout << \"\\n=== Lambda Callback ===\\n\";\r\n    NetworkRequest request2(\"https://api.example.com/users\");\r\n    request2.SetCompletionCallback(\r\n        Callback<void(int, const std::string&)>{\r\n            [](int status, const std::string& data) {\r\n                std::cout << \"Lambda handler - Status: \" << status \r\n                          << \", Data length: \" << data.length() << std::endl;\r\n            }\r\n        }\r\n    );\r\n    \r\n    request2.Start();\r\n    \r\n    return 0;\r\n}\r\n```\r\n\r\n**Key Concepts:**\r\n- Type erasure with `std::function`\r\n- Perfect forwarding for efficient parameter passing\r\n- RAII for automatic resource management\r\n- Callback binding for asynchronous operations\r\n\r\n---\r\n\r\n## Learning Exercises\r\n\r\n### Exercise 1: Implement a Simple Chrome-style Process Manager\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <map>\r\n#include <memory>\r\n#include <string>\r\n\r\n// TODO: Implement a ProcessManager that can:\r\n// 1. Create and track different types of processes (renderer, gpu, network)\r\n// 2. Monitor process health\r\n// 3. Handle process crashes gracefully\r\n\r\nenum class ProcessType {\r\n    BROWSER,\r\n    RENDERER,\r\n    GPU,\r\n    NETWORK\r\n};\r\n\r\nclass Process {\r\nprivate:\r\n    int pid_;\r\n    ProcessType type_;\r\n    bool is_running_;\r\n    \r\npublic:\r\n    Process(int pid, ProcessType type) \r\n        : pid_(pid), type_(type), is_running_(true) {}\r\n    \r\n    // TODO: Add methods for:\r\n    // - GetPID()\r\n    // - GetType()\r\n    // - IsRunning()\r\n    // - Terminate()\r\n    // - GetMemoryUsage()\r\n};\r\n\r\nclass ProcessManager {\r\nprivate:\r\n    std::map<int, std::unique_ptr<Process>> processes_;\r\n    \r\npublic:\r\n    // TODO: Implement these methods:\r\n    // - CreateProcess(ProcessType type)\r\n    // - KillProcess(int pid)\r\n    // - GetProcessCount()\r\n    // - GetProcessesByType(ProcessType type)\r\n    // - HandleProcessCrash(int pid)\r\n};\r\n\r\nint main() {\r\n    ProcessManager manager;\r\n    \r\n    // TODO: Test your implementation\r\n    // Create processes, simulate crashes, clean up resources\r\n    \r\n    return 0;\r\n}\r\n```\r\n\r\n### Exercise 2: Implement Chrome's WeakPtr Pattern\r\n\r\n```cpp\r\n#include <iostream>\r\n#include <memory>\r\n\r\n// TODO: Implement a WeakPtr system similar to Chromium's\r\n// WeakPtr is used to safely reference objects that might be deleted\r\n\r\ntemplate<typename T>\r\nclass WeakPtr;\r\n\r\ntemplate<typename T>\r\nclass WeakPtrFactory {\r\n    // TODO: Implement factory for creating weak pointers\r\n    // Should invalidate all weak pointers when destroyed\r\n};\r\n\r\ntemplate<typename T>\r\nclass WeakPtr {\r\n    // TODO: Implement weak pointer that:\r\n    // - Can safely check if object still exists\r\n    // - Returns nullptr if object was deleted\r\n    // - Can be used like a regular pointer when valid\r\n};\r\n\r\n// Test class\r\nclass BrowserWindow {\r\nprivate:\r\n    WeakPtrFactory<BrowserWindow> weak_factory_{this};\r\n    \r\npublic:\r\n    WeakPtr<BrowserWindow> GetWeakPtr() {\r\n        return weak_factory_.GetWeakPtr();\r\n    }\r\n    \r\n    void DoSomething() {\r\n        std::cout << \"BrowserWindow is doing something\\n\";\r\n    }\r\n};\r\n\r\nint main() {\r\n    // TODO: Test weak pointer behavior\r\n    // Create window, get weak ptr, delete window, test weak ptr\r\n    \r\n    return 0;\r\n}\r\n```\r\n\r\n---\r\n\r\n## Next Steps\r\n\r\nReady to dive deeper into Chromium development? Here are some areas to explore:\r\n\r\n1. **Content API**: Learn the public interface for embedding Chromium\r\n2. **Mojo IPC**: Understand inter-process communication\r\n3. **V8 Integration**: JavaScript engine integration patterns\r\n4. **Blink Rendering**: How web content gets rendered\r\n5. **Network Stack**: HTTP/HTTPS handling and caching\r\n\r\nThe code playground helps you understand these concepts before diving into the actual Chromium codebase!\r\n"
  },
  {
    "path": "demo/code-playground",
    "title": "Code Playground Demo",
    "content": "# Code Playground Demo\r\n\r\nWelcome to the interactive Code Playground! This feature allows you to write, edit, and execute code directly in the browser. Perfect for learning, experimenting, and testing code concepts.\r\n\r\n## JavaScript Playground\r\n\r\nLet's start with a simple JavaScript example. Try modifying the code and click \"Run\" to see the results:\r\n\r\n### Basic JavaScript Example\r\n\r\n```javascript\r\n// Welcome to the JavaScript playground!\r\n// Try modifying this code and clicking \"Run\"\r\n\r\nfunction greetUser(name) {\r\n    return `Hello, ${name}! Welcome to the Wanderlust Knowledge Base.`;\r\n}\r\n\r\nfunction calculateFactorial(n) {\r\n    if (n <= 1) return 1;\r\n    return n * calculateFactorial(n - 1);\r\n}\r\n\r\n// Test the functions\r\nconsole.log(greetUser(\"Developer\"));\r\nconsole.log(\"Factorial of 5:\", calculateFactorial(5));\r\n\r\n// Try some array methods\r\nconst numbers = [1, 2, 3, 4, 5];\r\nconst doubled = numbers.map(n => n * 2);\r\nconsole.log(\"Original:\", numbers);\r\nconsole.log(\"Doubled:\", doubled);\r\n\r\n// Return a value to see it in the output\r\n\"JavaScript playground is working! 🎉\"\r\n```\r\n\r\n**Instructions:** Modify the code above, add your own functions, or try different JavaScript features. The console output will appear below the editor.\r\n\r\n---\r\n\r\n## Advanced JavaScript: DOM and Modern Features\r\n\r\nThis playground demonstrates more advanced JavaScript concepts:\r\n\r\n### ES6+ Features and Async Programming\r\n\r\n```javascript\r\n// Modern JavaScript features demonstration\r\n\r\n// Destructuring and arrow functions\r\nconst user = { name: \"Alice\", age: 30, city: \"New York\" };\r\nconst { name, age } = user;\r\nconsole.log(`User: ${name}, Age: ${age}`);\r\n\r\n// Template literals and spread operator\r\nconst hobbies = [\"reading\", \"coding\"];\r\nconst moreHobbies = [...hobbies, \"hiking\", \"photography\"];\r\nconsole.log(\"Hobbies:\", moreHobbies.join(\", \"));\r\n\r\n// Async/await simulation (using setTimeout)\r\nasync function simulateApiCall() {\r\n    console.log(\"Starting API call...\");\r\n    return new Promise(resolve => {\r\n        setTimeout(() => {\r\n            resolve({ data: \"API response data\", status: \"success\" });\r\n        }, 1000);\r\n    });\r\n}\r\n\r\n// Class syntax\r\nclass Calculator {\r\n    constructor(name) {\r\n        this.name = name;\r\n    }\r\n    \r\n    add(a, b) {\r\n        console.log(`${this.name} calculating: ${a} + ${b} = ${a + b}`);\r\n        return a + b;\r\n    }\r\n    \r\n    multiply(a, b) {\r\n        console.log(`${this.name} calculating: ${a} × ${b} = ${a * b}`);\r\n        return a * b;\r\n    }\r\n}\r\n\r\n// Test the class\r\nconst calc = new Calculator(\"MyCalculator\");\r\ncalc.add(5, 3);\r\ncalc.multiply(4, 7);\r\n\r\n// Call the async function\r\nsimulateApiCall().then(result => {\r\n    console.log(\"API Result:\", result);\r\n});\r\n\r\n\"Advanced JavaScript concepts demo complete! ✨\"\r\n```\r\n\r\n**Instructions:** This example shows ES6+ features, classes, async/await, and more. Try adding your own modern JavaScript code!\r\n\r\n---\r\n\r\n## HTML/CSS Playground\r\n\r\nCreate interactive web content with HTML and CSS:\r\n\r\n### Interactive HTML Example\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html lang=\"en\">\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Interactive Demo</title>\r\n    <style>\r\n        body {\r\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\r\n            max-width: 600px;\r\n            margin: 0 auto;\r\n            padding: 20px;\r\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\r\n            color: white;\r\n            min-height: 100vh;\r\n        }\r\n        \r\n        .card {\r\n            background: rgba(255, 255, 255, 0.1);\r\n            padding: 20px;\r\n            border-radius: 10px;\r\n            margin: 10px 0;\r\n            backdrop-filter: blur(10px);\r\n            border: 1px solid rgba(255, 255, 255, 0.2);\r\n        }\r\n        \r\n        button {\r\n            background: #ff6b6b;\r\n            color: white;\r\n            border: none;\r\n            padding: 10px 20px;\r\n            border-radius: 5px;\r\n            cursor: pointer;\r\n            font-size: 16px;\r\n            transition: all 0.3s ease;\r\n        }\r\n        \r\n        button:hover {\r\n            background: #ff5252;\r\n            transform: translateY(-2px);\r\n            box-shadow: 0 4px 8px rgba(0,0,0,0.2);\r\n        }\r\n        \r\n        .counter {\r\n            font-size: 24px;\r\n            font-weight: bold;\r\n            text-align: center;\r\n            margin: 20px 0;\r\n        }\r\n    </style>\r\n</head>\r\n<body>\r\n    <div class=\"card\">\r\n        <h1>🚀 Interactive HTML Demo</h1>\r\n        <p>This is a live HTML/CSS example running in the browser!</p>\r\n        \r\n        <div class=\"counter\">\r\n            Count: <span id=\"count\">0</span>\r\n        </div>\r\n        \r\n        <button onclick=\"increment()\">Click Me!</button>\r\n        <button onclick=\"reset()\">Reset</button>\r\n    </div>\r\n    \r\n    <div class=\"card\">\r\n        <h2>✨ Features</h2>\r\n        <ul>\r\n            <li>Responsive design</li>\r\n            <li>Modern CSS with gradients and blur effects</li>\r\n            <li>Interactive JavaScript</li>\r\n            <li>Smooth animations</li>\r\n        </ul>\r\n    </div>\r\n\r\n    <script>\r\n        let count = 0;\r\n        const countElement = document.getElementById('count');\r\n        \r\n        function increment() {\r\n            count++;\r\n            countElement.textContent = count;\r\n            \r\n            // Add some fun effects\r\n            if (count % 10 === 0) {\r\n                document.body.style.background = `linear-gradient(135deg, \r\n                    hsl(${count * 10}, 70%, 60%) 0%, \r\n                    hsl(${count * 15}, 70%, 40%) 100%)`;\r\n            }\r\n        }\r\n        \r\n        function reset() {\r\n            count = 0;\r\n            countElement.textContent = count;\r\n            document.body.style.background = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)';\r\n        }\r\n    </script>\r\n</body>\r\n</html>\r\n```\r\n\r\n**Instructions:** This HTML example includes CSS styling and JavaScript interactivity. Try modifying the styles, adding new elements, or changing the JavaScript logic!\r\n\r\n---\r\n\r\n## TypeScript Playground\r\n\r\nExperience TypeScript's powerful type system:\r\n\r\n### TypeScript Interfaces and Generics\r\n\r\n```typescript\r\n// TypeScript playground with interfaces and generics\r\n\r\ninterface User {\r\n    id: number;\r\n    name: string;\r\n    email: string;\r\n    isActive?: boolean;\r\n}\r\n\r\ninterface ApiResponse<T> {\r\n    data: T;\r\n    status: 'success' | 'error';\r\n    message?: string;\r\n}\r\n\r\n// Generic function\r\nfunction createApiResponse<T>(data: T, status: 'success' | 'error'): ApiResponse<T> {\r\n    return { data, status };\r\n}\r\n\r\n// Class with generics\r\nclass DataStore<T> {\r\n    private items: T[] = [];\r\n    \r\n    add(item: T): void {\r\n        this.items.push(item);\r\n        console.log(`Added item:`, item);\r\n    }\r\n    \r\n    getAll(): T[] {\r\n        return [...this.items];\r\n    }\r\n    \r\n    find(predicate: (item: T) => boolean): T | undefined {\r\n        return this.items.find(predicate);\r\n    }\r\n}\r\n\r\n// Usage examples\r\nconst userStore = new DataStore<User>();\r\n\r\nconst users: User[] = [\r\n    { id: 1, name: \"Alice Johnson\", email: \"alice@example.com\", isActive: true },\r\n    { id: 2, name: \"Bob Smith\", email: \"bob@example.com\", isActive: false },\r\n    { id: 3, name: \"Carol Davis\", email: \"carol@example.com\", isActive: true }\r\n];\r\n\r\nusers.forEach(user => userStore.add(user));\r\n\r\nconsole.log(\"All users:\", userStore.getAll());\r\n\r\nconst activeUser = userStore.find(user => user.isActive === true);\r\nconsole.log(\"First active user:\", activeUser);\r\n\r\n// API response example\r\nconst response = createApiResponse(users, 'success');\r\nconsole.log(\"API Response:\", response);\r\n\r\n// Type checking in action\r\n// This would cause a TypeScript error:\r\n// userStore.add(\"invalid user\"); // Error: string is not assignable to User\r\n\r\n\"TypeScript playground working with full type safety! 🛡️\"\r\n```\r\n\r\n**Instructions:** This TypeScript example demonstrates interfaces, generics, and type safety. Try adding new types or modifying the existing ones!\r\n\r\n---\r\n\r\n## Learning Exercises\r\n\r\n### Exercise 1: JavaScript Algorithm Practice\r\n\r\n```javascript\r\n// Algorithm practice: Implement a simple sorting algorithm\r\n// TODO: Complete the bubble sort implementation\r\n\r\nfunction bubbleSort(arr) {\r\n    // Your implementation here\r\n    // Hint: Use nested loops to compare adjacent elements\r\n    \r\n    return arr;\r\n}\r\n\r\n// Test your implementation\r\nconst testArray = [64, 34, 25, 12, 22, 11, 90];\r\nconsole.log(\"Original array:\", testArray);\r\nconsole.log(\"Sorted array:\", bubbleSort([...testArray]));\r\n\r\n// Expected output: [11, 12, 22, 25, 34, 64, 90]\r\n```\r\n\r\n**Expected Output:**\r\n```\r\nOriginal array: [64, 34, 25, 12, 22, 11, 90]\r\nSorted array: [11, 12, 22, 25, 34, 64, 90]\r\n```\r\n\r\n### Exercise 2: Build a Todo List\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n<head>\r\n    <style>\r\n        body { font-family: Arial, sans-serif; max-width: 500px; margin: 50px auto; }\r\n        .todo-item { padding: 10px; margin: 5px 0; background: #f0f0f0; border-radius: 5px; }\r\n        .completed { text-decoration: line-through; opacity: 0.6; }\r\n        input, button { padding: 8px; margin: 5px; }\r\n        button { background: #007bff; color: white; border: none; border-radius: 3px; cursor: pointer; }\r\n    </style>\r\n</head>\r\n<body>\r\n    <h1>📝 Todo List</h1>\r\n    <div>\r\n        <input type=\"text\" id=\"todoInput\" placeholder=\"Enter a new task...\">\r\n        <button onclick=\"addTodo()\">Add Task</button>\r\n    </div>\r\n    <div id=\"todoList\"></div>\r\n\r\n    <script>\r\n        let todos = [];\r\n        let nextId = 1;\r\n\r\n        function addTodo() {\r\n            const input = document.getElementById('todoInput');\r\n            const text = input.value.trim();\r\n            \r\n            if (text) {\r\n                todos.push({ id: nextId++, text, completed: false });\r\n                input.value = '';\r\n                renderTodos();\r\n            }\r\n        }\r\n\r\n        function toggleTodo(id) {\r\n            const todo = todos.find(t => t.id === id);\r\n            if (todo) {\r\n                todo.completed = !todo.completed;\r\n                renderTodos();\r\n            }\r\n        }\r\n\r\n        function deleteTodo(id) {\r\n            todos = todos.filter(t => t.id !== id);\r\n            renderTodos();\r\n        }\r\n\r\n        function renderTodos() {\r\n            const list = document.getElementById('todoList');\r\n            list.innerHTML = todos.map(todo => `\r\n                <div class=\"todo-item ${todo.completed ? 'completed' : ''}\">\r\n                    <span onclick=\"toggleTodo(${todo.id})\" style=\"cursor: pointer;\">\r\n                        ${todo.completed ? '✅' : '⬜'} ${todo.text}\r\n                    </span>\r\n                    <button onclick=\"deleteTodo(${todo.id})\" style=\"float: right; background: #dc3545;\">Delete</button>\r\n                </div>\r\n            `).join('');\r\n        }\r\n\r\n        // Add some sample todos\r\n        todos = [\r\n            { id: 1, text: \"Learn JavaScript\", completed: true },\r\n            { id: 2, text: \"Build a todo app\", completed: false },\r\n            { id: 3, text: \"Master React\", completed: false }\r\n        ];\r\n        nextId = 4;\r\n        renderTodos();\r\n    </script>\r\n</body>\r\n</html>\r\n```\r\n\r\n**Instructions:** This is a fully functional todo list! Try adding new features like editing tasks, filtering by completion status, or saving to localStorage.\r\n\r\n---\r\n\r\n## Code Playground Features\r\n\r\nThe Code Playground includes:\r\n\r\n- ✅ **Multi-language Support**: JavaScript, TypeScript, HTML, CSS, Python, C++\r\n- ✅ **Live Code Execution**: Run JavaScript and TypeScript code instantly\r\n- ✅ **Monaco Editor**: Full VS Code editor experience with IntelliSense\r\n- ✅ **Console Output**: See console.log output and errors\r\n- ✅ **Code Sharing**: Copy code to clipboard easily\r\n- ✅ **Reset Functionality**: Return to original code anytime\r\n- ✅ **Solution Toggle**: Show/hide solutions for learning exercises\r\n- ✅ **Theme Awareness**: Automatically matches your preferred theme\r\n- ✅ **Keyboard Shortcuts**: Ctrl/Cmd + Enter to run code\r\n- ✅ **File Tabs**: Support for multi-file projects\r\n- ✅ **Error Handling**: Graceful error display and debugging\r\n\r\nThis interactive learning environment makes it easy to experiment with code, learn new concepts, and test ideas without leaving the documentation!\r\n"
  },
  {
    "path": "demo/bookmark-features",
    "title": "Bookmark Feature Demo",
    "content": "# Bookmark Feature Demo\r\n\r\nThis page demonstrates the new bookmarking functionality in the Wanderlust Knowledge Base.\r\n\r\n## Features Overview\r\n\r\nThe bookmarking system includes:\r\n\r\n- **Page-level bookmarks**: Bookmark entire articles for quick access\r\n- **Section-level bookmarks**: Bookmark specific sections and code blocks\r\n- **Persistent storage**: Bookmarks are saved in localStorage\r\n- **Search and filter**: Find bookmarks by title, description, or category\r\n- **Import/Export**: Backup and restore your bookmarks\r\n- **Smart categorization**: Automatic categorization based on content location\r\n\r\n## How to Use Bookmarks\r\n\r\n### Bookmarking Pages\r\n\r\n1. Navigate to any page in the knowledge base\r\n2. Look for the bookmark button with label in the top-right corner of the content\r\n3. Click to bookmark the entire page\r\n\r\n### Bookmarking Sections\r\n\r\n1. Hover over any heading (h2, h3) or substantial code block\r\n2. A small bookmark icon will appear on the left side\r\n3. Click to bookmark that specific section\r\n\r\n### Accessing Your Bookmarks\r\n\r\n1. Click the bookmark icon in the header (shows count badge)\r\n2. Use the search box to find specific bookmarks\r\n3. Filter by category using the dropdown\r\n4. Sort by date, title, or category\r\n5. Click any bookmark to navigate directly to it\r\n\r\n### Managing Bookmarks\r\n\r\n- **Remove**: Click the trash icon on individual bookmarks or use the \"Clear\" button\r\n- **Export**: Save your bookmarks as a JSON file\r\n- **Import**: Load bookmarks from a previously exported file\r\n\r\n## Code Example\r\n\r\nHere's a sample code block that can be bookmarked:\r\n\r\n```typescript\r\ninterface BookmarkFeatures {\r\n  pageBookmarks: boolean;\r\n  sectionBookmarks: boolean;\r\n  persistentStorage: boolean;\r\n  searchAndFilter: boolean;\r\n  importExport: boolean;\r\n  categorization: boolean;\r\n}\r\n\r\nconst features: BookmarkFeatures = {\r\n  pageBookmarks: true,\r\n  sectionBookmarks: true,\r\n  persistentStorage: true,\r\n  searchAndFilter: true,\r\n  importExport: true,\r\n  categorization: true,\r\n};\r\n\r\nconsole.log('All bookmark features are implemented!', features);\r\n```\r\n\r\n## Technical Implementation\r\n\r\n### Architecture Components\r\n\r\nThe bookmarking system consists of:\r\n\r\n1. **BookmarkContext**: React context for state management\r\n2. **BookmarkButton**: Reusable bookmark toggle component\r\n3. **BookmarksPanel**: Full-featured bookmark management interface\r\n4. **SectionBookmark**: Wrapper for section-level bookmarking\r\n\r\n### Data Structure\r\n\r\nEach bookmark contains:\r\n- Unique ID and timestamp\r\n- Page title and path\r\n- Full URL for navigation\r\n- Optional section information\r\n- Category for organization\r\n- Custom description\r\n\r\n## Benefits\r\n\r\n- **Quick Navigation**: Jump directly to important content\r\n- **Personal Organization**: Create your own knowledge paths\r\n- **Offline Access**: Bookmarks work even when offline (with PWA)\r\n- **Cross-Session Persistence**: Bookmarks survive browser restarts\r\n- **Shareable**: Export and share bookmark collections with team members\r\n\r\nTry bookmarking this page and some sections to see the system in action!\r\n"
  },
  {
    "path": "debugging/overview",
    "title": "Debugging & Troubleshooting",
    "content": "# Debugging & Troubleshooting\r\n\r\nWelcome to the Debugging section! This comprehensive guide helps developers debug, troubleshoot, and analyze issues in the Wanderlust custom Chromium browser.\r\n\r\n## What You'll Find Here\r\n\r\nThis section provides essential debugging tools and techniques:\r\n\r\n- **[Chrome Internals URLs](chrome-internals-urls.md)**: Access internal Chrome debugging pages and diagnostic tools\r\n- **[Crash Reports](crash-reports.md)**: Understanding, generating, and analyzing crash reports\r\n- **[Debugging Tools](debugging-tools.md)**: Essential tools and techniques for effective debugging\r\n\r\n## Debugging Workflow\r\n\r\nWhen encountering issues in your Chromium development:\r\n\r\n1. **Identify the Problem**: Reproduce the issue and gather initial information\r\n2. **Use Internal Tools**: Leverage Chrome's built-in debugging URLs and tools\r\n3. **Analyze Logs**: Check console output, crash reports, and diagnostic information\r\n4. **Apply Debugging Tools**: Use specialized debugging tools for deeper analysis\r\n\r\n## Common Debugging Scenarios\r\n\r\n- **Browser Crashes**: Use crash report analysis and debugging symbols\r\n- **Performance Issues**: Profile using Chrome DevTools and internal performance pages\r\n- **Network Problems**: Debug using network internals and protocol analysis\r\n- **Rendering Issues**: Inspect the rendering pipeline and GPU processes\r\n- **JavaScript/V8 Issues**: Debug the V8 engine and JavaScript execution\r\n\r\n## Quick Access to Debugging Resources\r\n\r\n- **Chrome Internals**: `chrome://` URLs for instant access to internal debugging pages\r\n- **Crash Analysis**: Tools and techniques for understanding browser crashes\r\n- **Performance Profiling**: Built-in tools for performance analysis and optimization\r\n\r\n## Integration with Development\r\n\r\nThese debugging techniques integrate seamlessly with:\r\n- [Architecture](../architecture/overview.md) understanding for system-level debugging\r\n- [Modules](../modules/overview.md) specific debugging for individual components\r\n- [Getting Started](../getting-started/overview.md) setup for debugging environment configuration\r\n\r\n---\r\n\r\n*Start debugging effectively by exploring our [debugging tools guide](debugging-tools.md) and familiarizing yourself with [Chrome internal URLs](chrome-internals-urls.md).*\r\n"
  },
  {
    "path": "debugging/lldbinit",
    "title": "Usage of tools/lldb/lldbinit.py",
    "content": "# Usage of tools/lldb/lldbinit.py\n\nUsage of Chromium's [lldbinit.py](../tools/lldb/lldbinit.py) is recommended when\ndebugging with lldb. This is necessary for source-level debugging when\n`strip_absolute_paths_from_debug_symbols` is enabled [this is the default].\n\nIf you have not installed LLDB yet, run `sudo apt-get install lldb` to get it.\n\nTo use, add the following to your `~/.lldbinit`\n\n```\n# So that lldbinit.py takes precedence.\nscript sys.path[:0] = ['/<your-path>/chromium/src/tools/lldb']\nscript import lldbinit\n```\n\nMake sure the build configurations include `is_debug=true`, this will set `symbol_level=2` by default, which is required if need to view the content of frame-level local variables.\n\n## How to attach to a process with lldb and start debugging\n\n- Follow the instructions above to create your `~/.lldbinit` file, don't forget to put the correct path to Chromium source in there.\n- Inside of your Chromium checkout, run `lldb out/Default/chrome` (or `out/Debug/chrome`)\n    - On Mac, most likely, `lldb out/Default/Chromium.app/Contents/MacOS/Chromium`\n- Keep lldb running and start Chromium separately with `--no-sandbox` flag:\n    - On Linux, `out/Default/chrome --no-sandbox`\n    - On Mac, `out/Default/Chromium.app/Contents/MacOS/Chromium --no-sandbox`\n    - Note: if you start the process from lldb using `process launch -- --no-sandbox`, you will attach to the main browser process and will not be able to debug tab processes.\n- In Chromium, go to Customize and Control Chromium (three dots) -> More Tools -> Task Manager\n- Depending on what tab or process you want to debug, note the process ID.\n- In the lldb shell:\n    - Execute `process attach -p PID`. PID is the process ID of the tab (process) you want to debug.\n        - Note: it might take a while. Once lldb attaches to the process, you will see a message `Process PID stopped` and some stack traces.\n        - If you an error message such as `attach failed: Operation not permitted`, it is probably due to [ptrace Protection](https://wiki.ubuntu.com/SecurityTeam/Roadmap/KernelHardening#ptrace_Protection). You can disable this feature using `echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope`.\n    - Now you can set breakpoints, for example, `breakpoint set -f inspector_overlay_agent.cc -l 627`.\n    - Execute `cont` to continue the execution of the process.\n    - Perform the actions which would trigger the breakpoint. lldb will stop the execution for you to inspect.\n    - You can pause the execution at any time by pressing Ctrl + C.\n    - Type `help` to learn more about different lldb commands.\n    - More open-source documentation could be found [here](https://developer.apple.com/library/archive/documentation/IDEs/Conceptual/gdb_to_lldb_transition_guide/document/lldb-basics.html#//apple_ref/doc/uid/TP40012917-CH2-SW1).\n"
  },
  {
    "path": "debugging/graphical_debugging_aid_chromium_views",
    "title": "Graphical Debugging Aid for Chromium Views",
    "content": "# Graphical Debugging Aid for Chromium Views\n\n## Introduction\n\nA simple debugging tool exists to help visualize the views tree during\ndebugging. It consists of 4 components:\n\n1.  The function `views::PrintViewGraph()` (in the file\n    `ui/views/debug_utils.h`),\n1.  a custom debugger command\n  - For GDB, see\n    [gdbinit](https://chromium.googlesource.com/chromium/src/+/main/docs/gdbinit.md),\n  - For LLDB, use `tools/lldb/lldb_viewg.py`\n  - For other debuggers, it should be relatively easy to adapt the\n    above scripts.\n1.  the graphViz package (http://www.graphviz.org/ - downloadable for Linux,\n    Windows and Mac), and\n1.  an SVG viewer (_e.g._ Chrome).\n\n## Details\n\nTo use the tool,\n\n1.  Make sure you have 'dot' installed (part of graphViz),\n1.  run gdb/lldb on your build and\n    1. For GDB see\n    [gdbinit](https://chromium.googlesource.com/chromium/src/+/main/docs/gdbinit.md),\n    1. For LLDB `command script import tools/lldb/lldb_viewg.py` (this can\n    be done automatically in `.lldbinit`),\n1.  stop at any breakpoint inside class `View` (or any derived class), and\n1.  type `viewg` at the gdb prompt.\n\nThis will cause the current view, and any descendants, to be described in a\ngraph which is stored as `~/state.svg` (Windows users may need to modify the\nscript slightly to run under CygWin). If `state.svg` is kept open in a browser\nwindow and refreshed each time `viewg` is run, then it provides a graphical\nrepresentation of the state of the views hierarchy that is always up to date.\n\nIt is easy to modify the gdb script to generate PDF in case viewing with evince\n(or other PDF viewer) is preferred.\n\nIf you don't use gdb or lldb, you may be able to adapt the script to work with\nyour favorite debugger. The gdb script invokes\n\n    views::PrintViewGraph(this)\n\non the current object, returning `std::string`, whose contents must then be\nsaved to a file in order to be processed by dot.\n"
  },
  {
    "path": "debugging/gdbinit",
    "title": "Usage of tools/gdb/gdbinit",
    "content": "# Usage of tools/gdb/gdbinit\n\nUsage of Chromium's [gdbinit](../tools/gdb/gdbinit) is recommended when\ndebugging with gdb on any platform. This is necessary for source-level debugging\nwhen `strip_absolute_paths_from_debug_symbols` or `use_custom_libcxx` are\nenabled (they're enabled by default), and also adds extra features like\npretty-printing of custom Chromium types.\n\nTo use, add the following to your `~/.gdbinit`\n\n```\nsource /path/to/chromium/src/tools/gdb/gdbinit\n```\n\n*** promo\nNotice that in components builds, the debug files will be loaded lazily. Because of this, the program needs to run at least once before breakpoints can be set. Alternatively, gdb will ask for confirmation as follows:\n> Make breakpoint pending on future shared library load? (y or [n])\n***\n"
  },
  {
    "path": "debugging/debugging_with_crash_keys",
    "title": "Debugging with Crash Keys",
    "content": "# Debugging with Crash Keys\n\nChrome is client-side software, which means that sometimes there are bugs that\ncan occur only on users' machines (\"in production\") that cannot be reproduced by\ntest or software engineering. When this happens, it's often helpful to gather bug-\nspecific data from production to help pinpoint the cause of the crash. The crash\nkey logging system is a generic method to help do that.\n\n[TOC]\n\n## High-Level Overview\n\nThe core of the crash key logging system is in [//components/crash/core/common/crash_key.h](https://cs.chromium.org/chromium/src/components/crash/core/common/crash_key.h),\nwhich declares a `crash_reporter::CrashKeyString` class. Every crash key has\nan associated value maximum length and a string name to identify it. The maximum\nlength is specified as a template parameter in order to allocate that amount of\nspace for the value up-front. When a process is crashing, memory corruption can\nmake it unsafe to call into the system allocator, so pre-allocating space for\nthe value defends against that.\n\nWhen a crash key is set, the specified value is copied to its internal storage.\nAnd if the process subsequently crashes, the name-value tuple is uploaded as\nPOST form-multipart data when the crash report minidump is uploaded to the\nGoogle crash reporting system. (The data therefore are only accessible to those\nwith access to crash reports internally at Google). For platforms that use\n[Crashpad](https://crashpad.chromium.org) as the crash reporting platform, the\ncrash keys are also stored in the minidump file itself. For platforms that use\nBreakpad, the keys are only available at upload.\n\nThe crash key system is used to report some common pieces of data, not just\nthings that happen in exceptional cases: the URL of the webpage, command line\nswitches, active extension IDs, GPU vendor information, experiment/variations\ninformation, etc.\n\n## Redaction\n\nBeware that certain on certain platforms (e.g. Android WebView) we\n[sanitize the stack in the dump](https://cs.chromium.org/chromium/src/third_party/crashpad/crashpad/snapshot/sanitized/memory_snapshot_sanitized.h)\nand only crash keys on an\n[allowlist](https://cs.chromium.org/chromium/src/android_webview/common/crash_reporter/crash_keys.cc)\nwill be captured.\n\n## Getting Started with a Single Key-Value Pair\n\nImagine you are investigating a crash, and you want to know the value of some\nvariable when the crash occurs; the crash key logging system enables you to do\njust that.\n\n#### 1. Declare the Crash Key\n\nA crash key must be allocated using static storage duration, so that there is\nspace for the value to be set. This can be done as a static variable in the\nglobal or function scope, or in an anonymous namespace:\n\n    static crash_reporter::CrashKeyString<32> crash_key_one(\"one\");\n\n    namespace {\n    crash_reporter::CrashKeyString<64> crash_key_two(\"two\");\n    }\n\n    void DoSomething(const std::string& arg) {\n      static crash_reporter::CrashKeyString<8> three(\"three\");\n      crash_key_two.Set(arg);\n      three.Set(\"true\");\n    }\n\nThe template argument specifies the maximum length a value can be, and it\nshould include space for a trailing NUL byte. Values must be C-strings and\ncannot have embedded NULs. The constructor argument is the name of the\ncrash key, and it is what you will use to identify your data in uploaded\ncrash reports.\n\nNote that crash key names are global and must not conflict with the\nname of any other crash key in Chrome.\n\nIf you need to declare an array of crash keys (e.g., for recording N values\nof an array), you can use a constructor tag to avoid warnings about `explicit`:\n\n    static ArrayItemKey = crash_reporter::CrashKeyString<32>;\n    static ArrayItemKey crash_keys[] = {\n      {\"array-item-1\", ArrayItemKey::Tag::kArray},\n      {\"array-item-2\", ArrayItemKey::Tag::kArray},\n      {\"array-item-3\", ArrayItemKey::Tag::kArray},\n      {\"array-item-4\", ArrayItemKey::Tag::kArray},\n    };\n\nThe crash key system will require your target to have a dependency on\n`//components/crash/core/common:crash_key`. If you encounter link errors for\nunresolved symbols to `crashpad::Annotation::SetSize(unsigned int)`, adding\nthe dependency will resolve them.\n\n#### 2. Set the Crash Key\n\nAfter a key has been allocated, its `Set(base::StringPiece)` and\n`Clear()` methods can be used to record and clear a value. In addition,\ncrash_key.h provides a `ScopedCrashKeyString` class to set the value for the\nduration of a scope and clear it upon exiting.\n\n#### 3. Seeing the Data\n\nUsing <http://go/crash> (internal only), find the crash report signature related\nto your bug, and click on the \"N of M\" reports link to drill down to\nreport-specific information. From there, select a report and go to the\n\"Product Data\" section to view all the crash key-value pairs.\n\n## Dealing with DEPS\n\nNot all targets in the Chromium source tree are permitted to depend on the\n`//components/crash/core/common:crash_key` target due to DEPS file\n`include_rules`.\n\nIf the crash key being added is only a temporary debugging aid to track down a\ncrash, consider adding the dependency temporarily and removing it when done.\nA specific include rule can be added for crash_key.h:\n\n    # DEPS\n    include_rules = [\n      '+components/crash/core/common/crash_key.h',\n    ]\n\nThen simply remove it (and the BUILD.gn dependency) once the crash is resolved\nand the crash key deleted.\n\nIf this crash key is more permanent, then there is an alternate API in //base\nthat can be used. This API is used by the //content module to set its permanent\ncrash key information. Note however that the base-level API is more limited in\nterms of features and flexibility. See the header documentation in\n[//base/debug/crash_logging.h](https://cs.chromium.org/chromium/src/base/debug/crash_logging.h)\nfor usage examples.\n\n## Advanced Topics: Stack Traces\n\nNow imagine a scenario where you have a use-after-free. The crash reports coming\nin do not indicate where the object being used was initially freed, however,\njust where it is later being dereferenced. To make debugging easier, it would be\nnice to have the stack trace of the destructor, and the crash key system works\nfor that, too.\n\n#### 1. Declare the Crash Key\n\nDeclaring the crash key is no different than written above, though special\nattention should be paid to the maximum size argument, which will affect the\nnumber of stack frames that are recorded. Typically a value of _1024_ is\nrecommended.\n\n#### 2. Set the Crash Key\n\nTo set a stack trace to a crash key, use the `SetCrashKeyStringToStackTrace()`\nfunction in crash_logging.h:\n\n    Usemeafterfree::~Usemeafterfree() {\n      static crash_reporter::CrashKeyString<1024> trace_key(\n          \"useme-after-free-uaf-dtor-trace\");\n      crash_reporter::SetCrashKeyStringToStackTrace(&trace_key,\n                                                    base::debug::StackTrace());\n    }\n\n#### 3. Seeing the Data\n\nUnlike with the previous example, a stack trace will just be a string of\nhexadecimal addresses. To turn the addresses back into symbols use,\n<http://go/crsym> (internal instance of <https://github.com/chromium/crsym/>).\nUsing the **Crash Key** input type, give it a crash report ID and the name of\nyour crash key. Crsym will then fetch the symbol data from the internal crash\nprocessing backends and return a formatted, symbolized stack trace.\n"
  },
  {
    "path": "debugging/debugging-tools",
    "title": "Debugging Tools",
    "content": "# Debugging Tools\r\n\r\nWhen working on Chromium you’ll rely on a variety of built-in tools and flags to inspect, profile, and diagnose both browser and renderer behavior. This guide surveys the most useful techniques, commands, and UIs for catching bugs, measuring performance, and analyzing crashes.\r\n\r\n---\r\n\r\n## 1. Logging & Verbose Flags\r\n\r\nChromium uses VLOG and `--v` logging levels throughout. To enable:\r\n\r\n```bash\r\nout/Default/chrome \\\r\n  --enable-logging=stderr \\\r\n  --v=1             # basic INFO-level logs\r\n  --vmodule=\"*.cc=2\"  # more verbose logging for specific source files\r\nLevels\r\n\r\n--v=0 (WARN & above)\r\n\r\n--v=1 (INFO)\r\n\r\n--v=2..5 (DEBUG with increasing detail)\r\n\r\nRedirecting\r\n\r\n--log-net-log=netlog.json to capture network internals\r\n\r\n--log-file=chrome.log to write all logs to a file\r\n\r\n2. Crash Reporting & Breakpad\r\nChromium’s built-in crash handler (Crashpad on Windows/macOS):\r\n\r\nCrash dumps are written under out/Default/crashes/ by default.\r\n\r\nSymbolization:\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/format_symbolized_stacktrace \\\r\n  --symbols-dir=./out/Default \\\r\n  crashes/<dump>.dmp\r\nIntegration\r\n\r\nUnder Git workflows you can upload to a Breakpad server or run locally with minidump_stackwalk.\r\n\r\n3. Chrome Developer Tools\r\nAccessible via F12 or chrome://inspect, DevTools offers:\r\n\r\nElements & Styles\r\n\r\nLive DOM/CSS inspection and editing\r\n\r\nConsole\r\n\r\nRuntime errors, logging APIs, and JS REPL\r\n\r\nSources\r\n\r\nSet breakpoints in JS, step through V8 bytecode/native code\r\n\r\nPerformance\r\n\r\nRecord CPU & heap profiles; “flamethrower” view of main-thread tasks\r\n\r\nNetwork\r\n\r\nInspect HTTP headers, payloads, timing breakdowns\r\n\r\n4. Tracing & Flame Charts\r\n4.1 chrome://tracing\r\nRecord IPC, rendering, and thread-level events.\r\n\r\nFilter by categories (--trace-startup, --trace-mem).\r\n\r\nExport to JSON and view in the Trace Event Profiling Tool.\r\n\r\n4.2 Perfetto (Android & Desktop)\r\n--enable-perfetto and chrome://perfetto for system-wide tracing.\r\n\r\nCaptures kernel, GPU, and user-space events together.\r\n\r\n5. Memory & Heap Analysis\r\n5.1 Heap Profiling\r\nJS heap: use DevTools’ Memory tab → Heap snapshots.\r\n\r\nNative heap:\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/chrome --enable-heap-checking --heap-profiler\r\ngenerates .heap files, viewable with pprof or Chrome’s heap profiler UI.\r\n\r\n5.2 Address Sanitizer (ASan)\r\nEnable with GN args:\r\n\r\ngn\r\nCopy\r\nEdit\r\nis_asan = true\r\nDetects use-after-free, buffer-overflow errors at runtime.\r\n\r\n5.3 Leak Sanitizer (LSan) & Thread Sanitizer (TSan)\r\nSimilar flags (is_lsan, is_tsan) to catch leaks and data races.\r\n\r\n6. CPU Profiling\r\nSampling profiler via DevTools Performance → CPU.\r\n\r\nIn-process profiler:\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/chrome --prof\r\nwrites isolate-0x*.log for V8 CPU sampling.\r\n\r\nExternal tools\r\n\r\nLinux: perf record -g -- out/Default/chrome\r\n\r\nmacOS: Instruments → Time Profiler\r\n\r\n7. GDB & Native Debugging\r\nLaunch Chrome under GDB:\r\n\r\nbash\r\nCopy\r\nEdit\r\ngdb --args out/Default/chrome --enable-logging=stderr\r\nSet breakpoints in C++ (content/browser/..., render_process_main.cc).\r\n\r\nUse thread apply all bt to get stacks from all threads.\r\n\r\n8. Network & Protocol Inspection\r\nchrome://net-export to record HTTP/QUIC traces; then view in NetLog Viewer.\r\n\r\nchrome://webrtc-internals for WebRTC peer-connection stats.\r\n\r\nWireshark: enable --log-net-log and import the JSON trace.\r\n\r\n9. Automated Tests & Debug Builds\r\nDebug Builds (is_debug=true) include assertions and symbol info.\r\n\r\nUnit / Browser Tests:\r\n\r\nbash\r\nCopy\r\nEdit\r\nautoninja -C out/Default content_unittests\r\nout/Default/content_unittests --gtest_filter=YourTest.*\r\nInstrumentation Tests run via run_local_tests.py (Android).\r\n\r\n10. Remote & Headless Debugging\r\nRemote Debugging\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/chrome --remote-debugging-port=9222\r\nthen connect DevTools to localhost:9222.\r\n\r\nHeadless Mode\r\n\r\nbash\r\nCopy\r\nEdit\r\nout/Default/chrome --headless --dump-dom https://example.com\r\n11. Common Pitfalls & Tips\r\nStale binaries: remember to gn gen after changing args.\r\n\r\nCache issues: use --disable-application-cache or --user-data-dir=<tmp> to avoid profile interference.\r\n\r\nDevTools hooks: use --remote-debugging-allow-hosts=* when debugging CI environments.\r\n\r\n12. Next Steps\r\nPair Traces → Flame Charts with CPU profiles to correlate jank.\r\n\r\nUse ASan/TSan in your CI builds to catch low-level bugs early.\r\n\r\nIntegrate --enable-heap-checking into nightly runs for memory leak detection."
  },
  {
    "path": "debugging/crash-reports",
    "title": "Crash Reports in Chromium",
    "content": "# Crash Reports in Chromium\r\n\r\nCrash reports are an essential part of debugging and maintaining Chromium. They provide detailed information about crashes, helping developers identify and resolve issues efficiently.\r\n\r\n---\r\n\r\n## Generating a Crash Report\r\n\r\nTo generate a crash report in Chromium:\r\n\r\n1. Open Chromium and access the URL `http://crash/` to trigger the generation of a crash report.\r\n2. The crash report will be saved in the following locations:\r\n   - **Linux**: `~/.config/google-chrome/Crash Reports/`\r\n   - **Windows/Mac**: `/path/to/profile/Crash Reports`\r\n\r\n### Preventing Crash Reports from Being Sent (Linux)\r\n\r\nOn Linux platforms, you can prevent crash reports from being sent to the server by setting the `CHROME_HEADLESS` environment variable. For example:\r\n\r\n```bash\r\n$ env CHROME_HEADLESS=1 ./out/Debug/chrome-wrapper\r\n```\r\n\r\nThis ensures that crash reports are generated locally without being uploaded to the server.\r\n\r\n## Parsing a Crash Report\r\nCrash reports in Chromium can be parsed using the minidump_stackwalk tool. This tool processes the .dmp files generated during a crash and provides a readable stack trace.\r\n\r\nSteps to Parse a Crash Report:\r\nUse the following command to parse a crash report:\r\n```bash\r\n$ minidump_stackwalk <report-name>.dmp\r\n```\r\nFor crash reports generated on Linux, you may need to remove the file header before parsing. To do this:\r\nOpen the .dmp file in a text editor.\r\nSearch for the MDMP character sequence.\r\nDelete the header before the MDMP sequence.\r\n\r\n## References\r\nFor more information on decoding and handling crash reports in Chromium, refer to the following resources:\r\n\r\n![Decoding Crash Dumps](https://www.chromium.org/developers/decoding-crash-dumps)\r\n![Linux Crash Dumping](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/linux_crash_dumping.md)\r\n\r\n## Additional Notes\r\nCrash Report Locations:\r\nEnsure you have the correct permissions to access the crash report directories.\r\nLicensing:\r\nThe original content referenced in this document is licensed under ![CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/)."
  },
  {
    "path": "debugging/chrome-internals-urls",
    "title": "Chrome Internal URLs (Pseudo-URLs)",
    "content": "# Chrome Internal URLs (Pseudo-URLs)\r\n\r\nChrome provides a set of internal URLs (pseudo-URLs) that allow developers and advanced users to access various debugging, configuration, and diagnostic tools. These URLs are accessible by typing them into the Chrome address bar.\r\n\r\n---\r\n\r\n## Common Chrome Internal URLs\r\n\r\n### 1. Accessibility\r\n- **URL**: `chrome://accessibility/`\r\n- **Description**: Displays accessibility analysis tools. By default, accessibility is off. Clicking \"Show Accessibility Tree\" displays the analysis tree.\r\n\r\n---\r\n\r\n### 2. Application Cache\r\n- **URL**: `chrome://appcache-internals/`\r\n- **Description**: Displays a list of all application cache paths.\r\n\r\n---\r\n\r\n### 3. Installed Apps\r\n- **URL**: `chrome://apps/`\r\n- **Description**: Lists all currently installed Chrome apps.\r\n\r\n---\r\n\r\n### 4. Blob Files\r\n- **URL**: `chrome://blob-internals/`\r\n- **Description**: Displays the current list of internal blob files.\r\n\r\n---\r\n\r\n### 5. Bookmarks\r\n- **URL**: `chrome://bookmarks/`\r\n- **Description**: Opens the Bookmark Manager.\r\n\r\n---\r\n\r\n### 6. Cache\r\n- **URL**: `chrome://cache/`\r\n- **Description**: Displays a list of cached URLs. Clicking a URL shows detailed information about the cache file, including type, encoding, expiration time, and binary content.\r\n\r\n---\r\n\r\n### 7. Chrome About Page\r\n- **URL**: `chrome://chrome/`\r\n- **Description**: Displays the current Chrome version information.\r\n\r\n---\r\n\r\n### 8. Chrome URL List\r\n- **URL**: `chrome://chrome-urls/`\r\n- **Description**: Displays a list of all available Chrome pseudo-URLs.\r\n\r\n---\r\n\r\n### 9. Components\r\n- **URL**: `chrome://components/`\r\n- **Description**: Lists Chrome components. You can check for updates and download new versions if available.\r\n\r\n---\r\n\r\n### 10. Crashes\r\n- **URL**: `chrome://crashes/`\r\n- **Description**: Displays the current Chrome crash reports. Crash reporting must be enabled for this to work. Refer to [Google Support](https://support.google.com/chrome/answer/96817) for enabling crash reporting.\r\n\r\n---\r\n\r\n### 11. Device Logs\r\n- **URL**: `chrome://device-log/`\r\n- **Description**: Displays device logs. Use `chrome://device-log/?refresh=seconds` to enable automatic refresh.\r\n\r\n---\r\n\r\n### 12. Discards\r\n- **URL**: `chrome://discards/`\r\n- **Description**: Displays discarded tab pages, sorted by priority. Tabs with the lowest priority may be discarded if memory usage exceeds available resources.\r\n\r\n---\r\n\r\n### 13. DNS\r\n- **URL**: `chrome://dns/`\r\n- **Description**: Displays DNS pre-resolution and TCP pre-connection data. If disabled, you can enable \"Pre-fetch resources for faster loading of web pages\" in settings.\r\n\r\n---\r\n\r\n### 14. Downloads\r\n- **URL**: `chrome://downloads/`\r\n- **Description**: Opens the Downloads page.\r\n\r\n---\r\n\r\n### 15. Extensions\r\n- **URL**: `chrome://extensions/`\r\n- **Description**: Lists all installed extensions.\r\n\r\n---\r\n\r\n### 16. Experimental Features\r\n- **URL**: `chrome://flags/`\r\n- **Description**: Displays a list of experimental features that can be enabled or disabled.\r\n\r\n---\r\n\r\n### 17. GPU Information\r\n- **URL**: `chrome://gpu/`\r\n- **Description**: Displays GPU information, including hardware acceleration status and GPU memory buffer details.\r\n\r\n---\r\n\r\n### 18. Histograms\r\n- **URL**: `chrome://histograms/`\r\n- **Description**: Displays histogram data for browser performance metrics.\r\n\r\n---\r\n\r\n### 19. IndexedDB Internals\r\n- **URL**: `chrome://indexeddb-internals/`\r\n- **Description**: Displays a list of internal IndexedDB instances, including paths, modification times, and database sizes.\r\n\r\n---\r\n\r\n### 20. Inspect Devices\r\n- **URL**: `chrome://inspect/`\r\n- **Description**: Detects devices, pages, extensions, and apps. Displays all currently open tabs under the \"Pages\" tab, with an option to inspect them using developer tools.\r\n\r\n---\r\n\r\n### 21. Memory Usage\r\n- **URL**: `chrome://memory/`\r\n- **Description**: Redirects to `chrome://memory-redirect/`, showing memory usage for each process, including private, shared, and total memory.\r\n\r\n---\r\n\r\n### 22. Network Internals\r\n- **URL**: `chrome://net-internals/`\r\n- **Description**: Displays network-related information, including proxy settings, DNS cache, and timing data.\r\n\r\n---\r\n\r\n### 23. Plugins\r\n- **URL**: `chrome://plugins/`\r\n- **Description**: Displays information about installed plugins. (Note: This URL is deprecated in newer versions of Chrome.)\r\n\r\n---\r\n\r\n### 24. Print\r\n- **URL**: `chrome://print/`\r\n- **Description**: Opens the browser's print dialog.\r\n\r\n---\r\n\r\n### 25. Sync Internals\r\n- **URL**: `chrome://sync-internals/`\r\n- **Description**: Displays synchronization information for Chrome accounts, including last sync time, token requests, and event logs.\r\n\r\n---\r\n\r\n### 26. System Information\r\n- **URL**: `chrome://system/`\r\n- **Description**: Displays diagnostic data, including Chrome version, OS version, memory usage, and USB device information.\r\n\r\n---\r\n\r\n### 27. Tracing\r\n- **URL**: `chrome://tracing/`\r\n- **Description**: Allows recording and monitoring of Chrome's performance data.\r\n\r\n---\r\n\r\n### 28. User Actions\r\n- **URL**: `chrome://user-actions/`\r\n- **Description**: Displays a list of user actions, including the type of operation and when it occurred.\r\n\r\n---\r\n\r\n### 29. Version Information\r\n- **URL**: `chrome://version/`\r\n- **Description**: Displays detailed version information for Chrome, including the JavaScript engine version, Flash plugin version, and user agent string.\r\n\r\n---\r\n\r\n## References\r\n\r\nFor a complete list of Chrome internal URLs, visit `chrome://chrome-urls/` in your browser.\r\n\r\n---\r\n\r\n*This document consolidates information from various sources, including the Chromium documentation and external references.*"
  },
  {
    "path": "debugging/android_debugging_instructions",
    "title": "Android Debugging Instructions",
    "content": "# Android Debugging Instructions\nChrome on Android has java and c/c++ code. Each \"side\" have its own set of tools\nfor debugging. Here's some tips.\n\n[TOC]\n\n## Instructions for Google Employees\n\nSee also\n[go/clankium/06-debugging-clank](https://goto.google.com/clankium/06-debugging-clank).\n\n## Launching\nYou can run the app by using one of the wrappers.\n\n```shell\n# Installs, launches, and enters logcat.\nout/Default/bin/content_shell_apk run --args='--disable-fre' 'data:text/html;utf-8,<html>Hello World!</html>'\n# Launches without first installing. Does not show logcat.\nout/Default/bin/chrome_public_apk launch --args='--disable-fre' 'data:text/html;utf-8,<html>Hello World!</html>'\n```\n\n## Logging\n[Chromium logging from LOG(INFO)](https://chromium.googlesource.com/chromium/src/+/main/docs/android_logging.md)\netc., is directed to the Android logcat logging facility. You can filter the\nmessages, e.g. view chromium verbose logging, everything else at warning level\nwith:\n\n```shell\n# Shows a coloured & filtered logcat.\nout/Default/bin/chrome_public_apk logcat [-v]  # Use -v to show logs for other processes\n```\n\nIf this doesn't display the logs you're looking for, try `adb logcat` with your system `adb`\nor the one in `//third_party/android_sdk/`.\n\n### Warnings for Blink developers\n*   **Do not use fprintf or printf debugging!** This does not\n    redirect to adb logcat. Use `LOG(ERROR)` etc. instead.\n    See also the \"Get Blink code to output to the adb log\" section.\n\n*   Redirecting stdio to logcat, as documented\n    [here](https://developer.android.com/studio/command-line/logcat.html#viewingStd),\n    has a bad side-effect in that it breaks `adb_install.py`. See\n    [here for details](http://stackoverflow.com/questions/28539676/android-adb-fails-to-install-apk-to-nexus-5-on-windows-8-1).\n\n## Take a Screenshot\n```shell\nbuild/android/screenshot.py /tmp/screenshot.png\n```\n\n## Inspecting the View Hierarchy\nGenerate an [Android Studio](android_studio.md) project, and then use\n[Layout Inspector](https://developer.android.com/studio/debug/layout-inspector).\n\n## Debugging Java\nFor both apk and test targets, pass `--wait-for-java-debugger` to the wrapper\nscripts.\n\nExamples:\n\n```shell\n# Install, launch, and wait:\nout/Default/bin/chrome_public_apk run --wait-for-java-debugger\n\n# Launch, and have GPU process wait rather than Browser process:\nout/Default/bin/chrome_public_apk launch --wait-for-java-debugger --debug-process-name privileged_process0\n\n# Have Renderers wait:\nout/Default/bin/chrome_public_apk launch --args=\"--renderer-wait-for-java-debugger\"\n\n# Have tests wait:\nout/Default/bin/run_chrome_public_test_apk --wait-for-java-debugger\nout/Default/bin/run_chrome_junit_tests --wait-for-java-debugger  # Specify custom port via --debug-socket=9999\n```\n\n### Android Studio\n*   Open Android Studio ([instructions](android_studio.md))\n*   Click \"Run\"->\"Attach debugger to Android process\" (see\n[here](https://developer.android.com/studio/debug/index.html) for more).\n*   Click \"Run\"->\"Attach to Local Process...\" for Robolectric junit tests.\n    * If this fails, you likely need to follow [these instructions](https://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process).\n\n### Eclipse\n*   In Eclipse, make a debug configuration of type \"Remote Java Application\".\n    Choose a \"Name\" and set \"Port\" to `8700`.\n\n*   Make sure Eclipse Preferences > Run/Debug > Launching > \"Build (if required)\n    before launching\" is unchecked.\n\n*   Run Android Device Monitor:\n\n    ```shell\n    third_party/android_sdk/public/tools/monitor\n    ```\n\n*   Now select the process you want to debug in Device Monitor (the port column\n    should now mention 8700 or xxxx/8700).\n\n*   Run your debug configuration, and switch to the Debug perspective.\n\n## Debugging C/C++\nWhile the app is running, use the wrapper script's `lldb` command to enter into a\nlldb shell.\n\nWhen running with `lldb` attached, the app runs **extremely slowly**.\n\n```shell\n# Attaches to browser process.\nout/Default/bin/content_shell_apk lldb\nout/Default/bin/chrome_public_apk lldb\n\n# Attaches to gpu process.\nout/Default/bin/chrome_public_apk lldb --debug-process-name privileged_process0\n\n# Attach to other processes (\"chrome_public_apk ps\" to show pids).\nout/Default/bin/chrome_public_apk lldb --pid $PID\n```\n\n### Using Visual Studio Code\n\n**NOT WORKING**\n\nThis used to work with GDB, but the LLDB instructions have not been written. If\nyou would like to take this on, please use:\n[crbug/1266055](https://bugs.chromium.org/p/chromium/issues/detail?id=1266055).\n\n### Waiting for Debugger on Early Startup\n```shell\n# Install, launch, and wait:\nout/Default/bin/chrome_public_apk run --args=\"--wait-for-debugger\"\n# Launch, and have GPU process wait rather than Browser process:\nout/Default/bin/chrome_public_apk launch --args=\"--wait-for-debugger-children=gpu-process\"\n# Or for renderers:\nout/Default/bin/chrome_public_apk launch --args=\"--wait-for-debugger-children=renderer\"\n```\n\n#### With Command-line LLDB\nOnce attached, `lldb` will drop into a prompt. Set your breakpoints and run \"c\" to\ncontinue.\n\n## Symbolizing Crash Stacks and Tombstones (C++)\n\nIf a crash has generated a tombstone in your device, use:\n\n```shell\nbuild/android/tombstones.py --output-directory out/Default\n```\n\nIf you have a stack trace (from `adb logcat`) that needs to be symbolized, copy\nit into a text file and symbolize with the following command (run from\n`${CHROME_SRC}`):\n\n```shell\nthird_party/android_platform/development/scripts/stack --output-directory out/Default [tombstone file | dump file]\n```\n\n`stack` can also take its input from `stdin`:\n\n```shell\nadb logcat -d | third_party/android_platform/development/scripts/stack --output-directory out/Default\n```\n\nExample:\n\n```shell\nthird_party/android_platform/development/scripts/stack --output-directory out/Default ~/crashlogs/tombstone_07-build231.txt\n```\n\n## Deobfuscating Stack Traces (Java)\n\nYou will need the ProGuard mapping file that was generated when the application\nthat crashed was built. When building locally, these are found in:\n\n```shell\nout/Default/apks/ChromePublic.apk.mapping\netc.\n```\n\nWhen debugging a failing test on the build waterfall, you can find the mapping\nfile as follows:\n\n1. Open buildbot page for the failing build (e.g.,\n   https://ci.chromium.org/p/chrome/builders/ci/android-go-perf/1234).\n2. Open the swarming page for the failing shard (e.g., shard #3).\n3. Click on \"Isolated Inputs\" to locate the files the shard used to run the\n   test.\n4. Download the `.mapping` file for the APK used by the test (e.g.,\n   `ChromePublic.apk.mapping`). Note that you may need to use the\n   `tools/luci-go/isolated` to download the mapping file if it's too big. The\n   viewer will provide instructions for this.\n\n**Googlers Only**: For official build mapping files, see\n[go/chromejavadeobfuscation](https://goto.google.com/chromejavadeobfuscation).\n\nOnce you have a .mapping file:\n\n```shell\n# For a file:\nbuild/android/stacktrace/java_deobfuscate.py PROGUARD_MAPPING_FILE.mapping < FILE\n# For logcat:\nadb logcat | build/android/stacktrace/java_deobfuscate.py PROGUARD_MAPPING_FILE.mapping\n```\n\n## Get Blink code to output to the adb log\n\nIn your build environment:\n\n```shell\nadb root\nadb shell stop\nadb shell setprop log.redirect-stdio true\nadb shell start\n```\n\nIn the source itself, use `LOG(ERROR),` `LOG(INFO)`, etc. whenever you need to\noutput a message, and it will be automatically redirected to adb logcat.\nRunning `adb logcat chromium:E`, for example, will show all log lines from\n`LOG(ERROR)` (plus others that match \"chromium\").\n\n## Debug unit tests with LLDB\n\nTo run unit tests use the following command:\n\n```shell\nout/Debug/bin/run_test_name -f <test_filter_if_any> --wait-for-debugger -t 6000\n```\n\nThat command will cause the test process to wait until a debugger is attached.\n\nTo attach a debugger:\n\n```shell\nbuild/android/connect_lldb.sh --output-directory=out/Default --package-name=org.chromium.native_test\n```\n\n## Examine app data on a non-rooted device\n\nIf you're developing on a non-rooted device such as a retail phone, security restrictions\nwill prevent directly accessing the application's data. However, as long as the app is\nbuilt with debugging enabled, you can use `adb shell run-as PACKAGENAME` to execute\nshell commands using the app's authorization, roughly equivalent to `su $user`.\n\nNon-Play-Store builds with `is_official_build=false` will by default set\n`android:debuggable=\"true\"` in the app's manifest to allow debugging.\n\nFor exammple, for a Chromium build, run the following:\n\n```\nadb shell run-as org.chromium.chrome\n```\n\nIf successful, this will silently wait for input without printing anything.\nIt acts as a simple shell despite not showing the usual `$ ` shell prompt.\nJust type commands and press RETURN to execute them.\n\nThe starting directory is the app's user data directory where user preferences and other\nprofile data are stored.\n\n```\npwd\n/data/user/0/org.chromium.chrome\n\nfind -type f\n./files/rList\n./shared_prefs/org.chromium.chrome_preferences.xml\n```\n\nIf you need to access the app's application data directory, you need to look up the\nobfuscated installation path since you don't have read access to the */data/app/* directory.\nFor example:\n\n```\npm list packages -f org.chromium.chrome\npackage:/data/app/~~ybTygSP5u72F9GN-3TMKXA==/org.chromium.chrome-zYY5mcB7YgB5pa3vfS3CBQ==/base.apk=org.chromium.chrome\n\nls -l /data/app/~~ybTygSP5u72F9GN-3TMKXA==/org.chromium.chrome-zYY5mcB7YgB5pa3vfS3CBQ==/\ntotal 389079\n-rw-r--r-- 1 system system 369634375 2022-11-05 01:49 base.apk\ndrwxr-xr-x 3 system system      3452 2022-11-05 01:49 lib\n-rw-r--r-- 1 system system    786666 2022-11-05 01:49 split_cablev2_authenticator.apk\n-rw-r--r-- 1 system system  21258500 2022-11-05 01:49 split_chrome.apk\n-rw-r--r-- 1 system system   1298934 2022-11-05 01:49 split_config.en.apk\n-rw-r--r-- 1 system system    413913 2022-11-05 01:49 split_dev_ui.apk\n-rw-r--r-- 1 system system     12432 2022-11-05 01:49 split_weblayer.apk\n```\n"
  },
  {
    "path": "contributing/overview",
    "title": "Contributing to Wanderlust Knowledge Base",
    "content": "# Contributing to Wanderlust Knowledge Base\r\n\r\nWelcome to the Contributing section! This area contains essential information for developers who want to contribute to the Wanderlust project and its custom Chromium browser implementation.\r\n\r\n## What You'll Find Here\r\n\r\nThis section covers the fundamental aspects of contributing to our project:\r\n\r\n- **Contributing Guidelines**: Comprehensive guide on how to contribute code, documentation, and bug reports\r\n- **Development Workflow**: Best practices for development, testing, and submission processes\r\n- **Code Standards**: Coding conventions and quality requirements\r\n- **Review Process**: How pull requests and code reviews are handled\r\n\r\n## Getting Started with Contributing\r\n\r\nIf you're new to contributing to Chromium-based projects, this section will help you understand:\r\n\r\n- How to set up your development environment\r\n- Understanding the project structure and codebase\r\n- Following our coding standards and conventions\r\n- Submitting your first contribution\r\n\r\n## Key Resources\r\n\r\n- Review the main [Contributing Guide](contributing.md) for detailed instructions\r\n- Familiarize yourself with the [Getting Started](../getting-started/overview.md) section for development setup\r\n- Check the [Architecture](../architecture/overview.md) section to understand the codebase structure\r\n\r\n## Community Guidelines\r\n\r\nWe maintain a welcoming and inclusive environment for all contributors. Our contributing guidelines ensure that all team members can effectively collaborate on building our custom Chromium browser.\r\n\r\n---\r\n\r\n*Ready to contribute? Start with our [detailed contributing guide](contributing.md) to learn about our development process and requirements.*\r\n"
  },
  {
    "path": "contributing/contributing",
    "title": "Contributing to C### 1.1. System### 1.1. System Requirements",
    "content": "# Contributing to C### 1.1. System### 1.1. System Requirements\r\n\r\n**Minimum Requirements for v134+:**\r\n- **Python**: 3.8+ (3.11+ recommended for Bazel builds)\r\n- **Git**: 2.35+ with LFS support\r\n- **Memory**: 32GB RAM minimum (64GB for full builds)\r\n- **Storage**: 200GB+ available space (SSD strongly recommended)\r\n- **OS Support**: \r\n  - Windows 11 with WSL2 or na### 3.2. Component Prefixes (Updated for v134)\r\n\r\n**Core Components:**\r\n- `[base]` - Base library changes\r\n- `[content]` - Content layer (renderer, browser processes)\r\n- `[chrome]` - Chrome browser UI and features\r\n- `[net]` - Network stack\r\n- `[security]` - Security-related changes\r\n- `[gpu]` - Graphics and GPU acceleration\r\n- `[mojo]` - IPC and service interfaces\r\n\r\n**New v134+ Components:**\r\n- `[rust]` - Rust integration and components\r\n- `[bazel]` - Build system migration\r\n- `[fuchsia]` - Fuchsia OS support\r\n- `[webgpu]` - WebGPU implementation\r\n- `[origin-trial]` - Origin trial features\r\n\r\n### 3.3. Security-Sensitive Changes\r\n\r\nFor security-related commits, additional requirements apply:\r\n\r\n```\r\n[security] Fix XSS vulnerability in URL parsing\r\n\r\nThis change addresses a cross-site scripting vulnerability where\r\nmalicious URLs could bypass sanitization in the omnibox.\r\n\r\nThe fix implements stricter URL validation using a allowlist\r\napproach rather than blocklist to prevent future bypasses.\r\n\r\nBug: chromium:1234567\r\nSecurity-Review: security-team@chromium.org\r\nSecurity-Severity: high\r\nSecurity-Bug: 1234567\r\nTest: security_tests --gtest_filter=UrlSanitizer.*\r\nChange-Id: I1234567890abcdef1234567890abcdef12345678\r\n```\r\n\r\n### 3.4. Performance-Sensitive Changes\r\n\r\nFor changes affecting performance:\r\n\r\n```\r\n[base] Optimize string concatenation in base::StrCat\r\n\r\nReplace repeated string allocations with a single pre-sized\r\nbuffer allocation, reducing memory pressure and improving\r\nperformance for common string operations.\r\n\r\nBenchmark results:\r\n- 25% reduction in allocations\r\n- 15% improvement in wall-clock time\r\n- No regression in peak memory usage\r\n\r\nBug: chromium:1234567\r\nPerformance-Impact: medium\r\nBenchmark: base_perftests --gtest_filter=StringConcatBenchmark\r\nTest: base_unittests --gtest_filter=StrCat*\r\nChange-Id: I1234567890abcdef1234567890abcdef12345678\r\n```\r\n\r\n### 3.5. Required Footer Tags\r\n\r\n**Mandatory for all commits:**\r\n- `Change-Id:` - Generated by git-cl\r\n- `Test:` - How the change was tested\r\n- `Bug:` - Associated bug (use \"none\" if no bug)\r\n\r\n**Conditional tags:**\r\n- `Security-Review:` - Required for security-sensitive changes\r\n- `Performance-Impact:` - Required for performance-affecting changes\r\n- `Breaking-Change:` - Required for API/behavior changes\r\n- `Security-Severity:` - For security fixes (low|medium|high|critical)\r\n- `Rust-Component:` - For Rust integration changes\r\n- `Origin-Trial:` - For experimental web platform features\r\n\r\n## 4. Testing & ValidationwerShell 7+\r\n  - macOS 12+ (Intel/Apple Silicon)\r\n  - Ubuntu 20.04+ or equivalent Linux distribution\r\n\r\n**Required Accounts & Agreements:**\r\n- Google account with [Chromium CLA][cla] signed\r\n- GitHub account (for security scanning integration)\r\n- Access to [chromium-security mailing list][security-list] (for security-sensitive contributions)irements\r\n\r\n**Minimum Requirements for v134+:**\r\n- **Python**: 3.8+ (3.11+ recommended for Bazel builds)\r\n- **Git**: 2.35+ with LFS support\r\n- **Memory**: 32GB RAM minimum (64GB for full builds)\r\n- **Storage**: 200GB+ available space (SSD strongly recommended)\r\n- **OS Support**: \r\n  - Windows 11 with WSL2 or native PowerShell 7+\r\n  - macOS 12+ (Intel/Apple Silicon)\r\n  - Ubuntu 20.04+ or equivalent Linux distribution\r\n\r\n**Required Accounts & Agreements:**\r\n- Google account with [Chromium CLA][cla] signed\r\n- GitHub account (for security scanning integration)\r\n- Access to [chromium-security mailing list][security-list] (for security-sensitive contributions)um\r\n\r\nWelcome to the Chromium project! This comprehensive guide will walk you through contributing to Chromium v134+, covering everything from initial setup to landing your first security-reviewed patch.\r\n\r\n## What's New in v134+\r\n\r\nChromium v134+ introduces significant architectural improvements:\r\n- **Bazel Build Migration**: Gradual transition from GN to Bazel for improved build performance\r\n- **Enhanced Security Model**: Mandatory security reviews for sensitive components\r\n- **C++23 Support**: Modern language features for improved safety and performance\r\n- **Rust Integration**: Selected components now use Rust for memory safety\r\n- **Advanced Testing**: Enhanced fuzzing and automated security scanning\r\n\r\nWhether you're fixing a bug, implementing a feature, or improving performance, this guide ensures your contribution meets our v134+ standards for security, performance, and maintainability.\r\n\r\n---\r\n\r\n## 1. Prerequisites & Environment Setup\r\n\r\n### 1.1. Prerequisites\r\n\r\n- You’ve [set up and built Chromium][setup-build].  \r\n- You have a Google account and have signed the [Chromium Contributor License Agreement (CLA)][cla].  \r\n- On Linux/macOS you have Python 3 and Git installed; on Windows you’re using PowerShell with Depot Tools in your `PATH`.\r\n\r\n### 1.2. Development Environment Options\r\n\r\n#### Option A: Native Development (Recommended)\r\n\r\n1. **Install Depot Tools**\r\n   ```bash\r\n   # Linux/macOS\r\n   git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\r\n   export PATH=\"$PATH:/path/to/depot_tools\"\r\n   \r\n   # Windows (PowerShell as Administrator)\r\n   git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git C:\\depot_tools\r\n   $env:PATH += \";C:\\depot_tools\"\r\n   ```\r\n\r\n2. **Clone Chromium with v134+ Support**\r\n   ```bash\r\n   mkdir chromium && cd chromium\r\n   fetch --nohooks chromium\r\n   cd src\r\n   \r\n   # Enable Bazel builds (experimental in v134)\r\n   echo \"use_bazel = true\" >> .gclient\r\n   gclient sync --with_branch_heads --with_tags --jobs=16\r\n   ```\r\n\r\n#### Option B: Container Development (Security Isolation)\r\n\r\nFor security-sensitive development, use our official container:\r\n\r\n```bash\r\n# Pull the v134+ development container\r\ndocker pull gcr.io/chromium-container-dev/chromium:v134\r\n\r\n# Run with proper mounts and security\r\ndocker run -it --security-opt seccomp=unconfined \\\r\n  -v $PWD:/workspace \\\r\n  gcr.io/chromium-container-dev/chromium:v134\r\n```\r\n\r\n### 1.3. Git Configuration\r\n\r\n```bash\r\n# Set your identity (must match CLA)\r\ngit config --global user.name \"Your Full Name\"\r\ngit config --global user.email \"your.email@domain.com\"\r\n\r\n# Enable security features\r\ngit config --global commit.gpgsign true\r\ngit config --global tag.forceSignAnnotated true\r\ngit config --global pull.rebase true\r\n\r\n# Configure for large repositories\r\ngit config --global core.preloadindex true\r\ngit config --global core.fscache true\r\ngit config --global gc.auto 256\r\n```\r\n\r\n### 1.4. Authentication Setup\r\n\r\n1. **Gerrit SSH Access**\r\n   ```bash\r\n   # Generate Ed25519 key (more secure than RSA)\r\n   ssh-keygen -t ed25519 -C \"your.email@domain.com\"\r\n   \r\n   # Upload public key to Gerrit settings\r\n   # Test connection\r\n   ssh -p 29418 chromium-review.googlesource.com\r\n   ```\r\n\r\n2. **Enable 2FA** (Required for security contributions)\r\n   - Enable 2FA on your Google account\r\n   - Configure app-specific passwords for depot_tools\r\n\r\n### 1.5. Build Configuration\r\n\r\nCreate your build configuration for v134+:\r\n\r\n```bash\r\n# Generate build files with security hardening\r\ngn gen out/Default --args='\r\n  is_debug = true\r\n  is_component_build = true\r\n  enable_nacl = false\r\n  use_goma = true\r\n  symbol_level = 1\r\n  blink_symbol_level = 0\r\n  enable_security_hardening = true\r\n  use_sanitizer_coverage = true\r\n'\r\n\r\n# For Bazel builds (experimental)\r\nbazel build --config=chromium_dev //chrome:chrome\r\n```\r\n\r\n## 2. Coding Conventions & Style\r\n\r\nChromium v134+ follows modern, security-first coding practices across multiple languages. Our style guides have been updated to leverage the latest language features while maintaining performance and security.\r\n\r\n### 2.1. C++ Guidelines (C++23)\r\n\r\n**Modern C++ Features in v134+:**\r\n```cpp\r\n// Use concepts for better template constraints\r\ntemplate<std::integral T>\r\nclass SafeInteger {\r\n  T value_;\r\npublic:\r\n  constexpr explicit SafeInteger(T val) : value_(val) {}\r\n  \r\n  // Use designated initializers\r\n  struct Config {\r\n    bool enable_bounds_check = true;\r\n    size_t max_value = std::numeric_limits<size_t>::max();\r\n  };\r\n};\r\n\r\n// Prefer ranges and views\r\nauto ProcessItems(const std::vector<Item>& items) {\r\n  return items \r\n    | std::views::filter([](const Item& item) { return item.IsValid(); })\r\n    | std::views::transform([](const Item& item) { return item.Process(); });\r\n}\r\n```\r\n\r\n**Security-First Patterns:**\r\n```cpp\r\n// Always use smart pointers for memory management\r\nstd::unique_ptr<NetworkHandler> CreateHandler() {\r\n  return std::make_unique<NetworkHandler>();\r\n}\r\n\r\n// Use base::span for safe array access\r\nvoid ProcessData(base::span<const uint8_t> data) {\r\n  // Bounds-checked access automatically\r\n  for (auto byte : data) {\r\n    // Process safely\r\n  }\r\n}\r\n\r\n// Prefer std::optional over null pointers\r\nstd::optional<UserSession> GetCurrentSession() {\r\n  if (auto* session = session_manager_->current_session()) {\r\n    return UserSession(*session);\r\n  }\r\n  return std::nullopt;\r\n}\r\n```\r\n\r\n### 2.2. Rust Integration Guidelines\r\n\r\nFor components migrated to Rust in v134+:\r\n\r\n```rust\r\n// Use strict clippy lints\r\n#![deny(clippy::all)]\r\n#![deny(unsafe_code)]\r\n#![warn(clippy::pedantic)]\r\n\r\n// Prefer explicit error handling\r\npub fn parse_config(data: &[u8]) -> Result<Config, ConfigError> {\r\n    serde_json::from_slice(data)\r\n        .map_err(ConfigError::ParseError)\r\n}\r\n\r\n// Use strong typing for security\r\n#[derive(Debug, Clone)]\r\npub struct SanitizedUrl(String);\r\n\r\nimpl SanitizedUrl {\r\n    pub fn new(url: &str) -> Result<Self, UrlError> {\r\n        // Validation logic\r\n        if is_safe_url(url) {\r\n            Ok(Self(url.to_string()))\r\n        } else {\r\n            Err(UrlError::Unsafe)\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n### 2.3. JavaScript/TypeScript Guidelines\r\n\r\n**Modern TypeScript Patterns:**\r\n```typescript\r\n// Use strict mode and modern features\r\ninterface SecurityPolicy {\r\n  readonly cspDirectives: ReadonlyArray<string>;\r\n  readonly allowedOrigins: Set<string>;\r\n}\r\n\r\n// Prefer const assertions and template literals\r\nconst API_ENDPOINTS = {\r\n  users: '/api/v2/users',\r\n  sessions: '/api/v2/sessions',\r\n} as const;\r\n\r\n// Use discriminated unions for type safety\r\ntype ApiResponse<T> = \r\n  | { success: true; data: T }\r\n  | { success: false; error: string };\r\n\r\nasync function fetchUserData(id: string): Promise<ApiResponse<User>> {\r\n  try {\r\n    const response = await fetch(`${API_ENDPOINTS.users}/${id}`);\r\n    const data = await response.json();\r\n    return { success: true, data };\r\n  } catch (error) {\r\n    return { success: false, error: error.message };\r\n  }\r\n}\r\n```\r\n\r\n### 2.4. Style Requirements\r\n\r\n**Formatting & Structure:**\r\n- **Indentation**: 2 spaces (no tabs) for all languages\r\n- **Line Length**: \r\n  - C++/Rust: 80 characters for code, 100 for comments\r\n  - JavaScript/TypeScript: 100 characters\r\n- **File Organization**: Group includes/imports by type (system, third-party, local)\r\n\r\n**Naming Conventions:**\r\n```cpp\r\n// C++ Examples\r\nclass NetworkRequestHandler {};        // PascalCase for types\r\nvoid ProcessHttpRequest();             // PascalCase for functions\r\nconst int kMaxRetryCount = 3;          // k prefix for constants\r\nbool is_secure_connection_ = false;    // snake_case for members\r\n```\r\n\r\n```rust\r\n// Rust Examples\r\nstruct HttpClient {}                   // PascalCase for types\r\nfn process_request() {}               // snake_case for functions\r\nconst MAX_RETRY_COUNT: i32 = 3;       // SCREAMING_SNAKE_CASE for constants\r\n```\r\n\r\n### 2.5. Code Quality Tools\r\n\r\n**Mandatory Pre-submission Checks:**\r\n```bash\r\n# C++ formatting and linting\r\ngit cl presubmit --upload\r\ntools/clang/scripts/run-clang-format.py --style=chromium\r\ntools/clang/scripts/run-clang-tidy.py\r\n\r\n# Rust checks (for Rust components)\r\ncargo clippy --all-targets --all-features -- -D warnings\r\ncargo fmt --check\r\n\r\n# JavaScript/TypeScript\r\nnpm run lint\r\nnpm run type-check\r\n```\r\n\r\n**Security Analysis:**\r\n```bash\r\n# Run security scanners before upload\r\ntools/security/run-security-checks.py\r\ntools/fuzz/check-fuzz-targets.py\r\n\r\n# Memory safety validation\r\ntools/valgrind/chrome_tests.py\r\ntools/asan/run-tests.py\r\n```\r\n\r\n### 2.6. Documentation Standards\r\n\r\n**Required Documentation:**\r\n```cpp\r\n// Header documentation with security considerations\r\n/**\r\n * Handles secure network communication for browser processes.\r\n * \r\n * SECURITY: All input must be validated before processing.\r\n * This class handles sensitive network data and must not\r\n * leak information across security boundaries.\r\n * \r\n * @thread_safety This class is not thread-safe. Use from\r\n * the main thread only.\r\n */\r\nclass SecureNetworkHandler {\r\n public:\r\n  // Document security-relevant parameters\r\n  bool ProcessRequest(const std::string& url,  // Must be validated URL\r\n                     const Headers& headers);   // Trusted headers only\r\n};\r\n```\r\n\r\n## 3. Commit Message Format\r\n\r\nChromium v134+ uses structured commit messages for automated processing, security tracking, and release management. Follow this format precisely:\r\n\r\n### 3.1. Standard Format\r\n\r\n```\r\n[component] Brief summary (50 chars max)\r\n\r\nDetailed description of your change:\r\n- What problem this solves\r\n- Implementation approach\r\n- Performance/security implications\r\n- Breaking changes (if any)\r\n\r\nBug: chromium:1234567\r\nChange-Id: I1234567890abcdef1234567890abcdef12345678\r\nTest: unit_tests --gtest_filter=MyTest.*\r\nSecurity-Review: security-team@ (if security-sensitive)\r\nPerformance-Impact: none|low|medium|high\r\n```\r\nPrefix each message with [area], e.g. [content], [net], [ui].\r\n\r\nInclude a Bug: link if you’re fixing or closing an issue.\r\n\r\nMention tests in the Test: line.\r\n\r\n## 4. Testing & Validation\r\n\r\nChromium v134+ requires comprehensive testing across multiple dimensions: functionality, security, performance, and compatibility. All changes must pass automated validation before review.\r\n\r\n### 4.1. Pre-Upload Validation\r\n\r\n**Mandatory Checks (All Changes):**\r\n```bash\r\n# Format and lint checks\r\ngit cl presubmit --upload\r\ntools/clang/scripts/run-clang-format.py --style=chromium --in-place\r\ntools/clang/scripts/run-clang-tidy.py --checks=-*,readability-*,security-*\r\n\r\n# Build verification (multiple configurations)\r\nautoninja -C out/Default chrome\r\nautoninja -C out/Release chrome\r\nautoninja -C out/Debug chrome\r\n\r\n# Basic functionality tests\r\nout/Default/unit_tests --gtest_filter=*YourComponent*\r\nout/Default/browser_tests --gtest_filter=*YourFeature*\r\n```\r\n\r\n**Security-Sensitive Changes:**\r\n```bash\r\n# Security-specific validation\r\ntools/security/run-security-checks.py --component=your_component\r\ntools/fuzz/check-fuzz-targets.py --validate-new-targets\r\n\r\n# Memory safety verification\r\nautoninja -C out/ASan chrome  # AddressSanitizer build\r\nautoninja -C out/MSan chrome  # MemorySanitizer build\r\nout/ASan/unit_tests --gtest_filter=*YourComponent*\r\n\r\n# Fuzzing validation (if adding new attack surfaces)\r\ntools/fuzz/run-fuzz-tests.py --component=your_component --duration=60s\r\n```\r\n\r\n### 4.2. Test Framework Requirements\r\n\r\n**Unit Testing (Required for all code changes):**\r\n```cpp\r\n// Modern C++ testing with security focus\r\n#include \"testing/gtest/include/gtest/gtest.h\"\r\n#include \"testing/gmock/include/gmock/gmock.h\"\r\n#include \"base/test/scoped_feature_list.h\"\r\n\r\nclass SecureUrlParserTest : public testing::Test {\r\n protected:\r\n  void SetUp() override {\r\n    feature_list_.InitAndEnableFeature(features::kSecureUrlParsing);\r\n  }\r\n\r\n  base::test::ScopedFeatureList feature_list_;\r\n};\r\n\r\nTEST_F(SecureUrlParserTest, RejectsInvalidUrls) {\r\n  // Test security boundary conditions\r\n  EXPECT_FALSE(parser_.IsValid(\"javascript:alert(1)\"));\r\n  EXPECT_FALSE(parser_.IsValid(\"data:text/html,<script>\"));\r\n  \r\n  // Test edge cases that previously caused issues\r\n  EXPECT_FALSE(parser_.IsValid(std::string(10000, 'a')));  // Length attack\r\n  EXPECT_FALSE(parser_.IsValid(\"http://\\x00.example.com\"));  // Null byte injection\r\n}\r\n\r\nTEST_F(SecureUrlParserTest, HandlesValidUrlsCorrectly) {\r\n  EXPECT_TRUE(parser_.IsValid(\"https://example.com/path\"));\r\n  EXPECT_TRUE(parser_.IsValid(\"chrome://settings/\"));\r\n}\r\n```\r\n\r\n**Integration Testing (Required for UI/API changes):**\r\n```cpp\r\n// Browser test with real Chrome instances\r\n#include \"chrome/test/base/in_process_browser_test.h\"\r\n#include \"chrome/test/base/ui_test_utils.h\"\r\n\r\nclass PasswordManagerSecurityTest : public InProcessBrowserTest {\r\n public:\r\n  void SetUpOnMainThread() override {\r\n    // Set up secure test environment\r\n    EnableSecurityFeatures();\r\n  }\r\n};\r\n\r\nIN_PROC_BROWSER_TEST_F(PasswordManagerSecurityTest, BiometricAuthRequired) {\r\n  // Navigate to password manager\r\n  ASSERT_TRUE(ui_test_utils::NavigateToURL(browser(), \r\n    GURL(\"chrome://settings/passwords\")));\r\n  \r\n  // Verify biometric prompt appears\r\n  EXPECT_TRUE(WaitForBiometricPrompt());\r\n  \r\n  // Test successful authentication\r\n  SimulateBiometricAuth(true);\r\n  EXPECT_TRUE(PasswordListVisible());\r\n}\r\n```\r\n\r\n### 4.3. Rust Component Testing\r\n\r\nFor Rust components in v134+:\r\n\r\n```rust\r\n// Comprehensive Rust testing\r\n#[cfg(test)]\r\nmod tests {\r\n    use super::*;\r\n    use std::panic;\r\n    \r\n    #[test]\r\n    fn test_secure_url_parsing() {\r\n        let parser = SecureUrlParser::new();\r\n        \r\n        // Test valid URLs\r\n        assert!(parser.parse(\"https://example.com\").is_ok());\r\n        \r\n        // Test security boundary conditions\r\n        assert!(parser.parse(\"javascript:alert(1)\").is_err());\r\n        assert!(parser.parse(&\"a\".repeat(100_000)).is_err());\r\n    }\r\n    \r\n    #[test]\r\n    fn test_memory_safety() {\r\n        // Rust automatically provides memory safety, but test edge cases\r\n        let large_input = \"x\".repeat(1_000_000);\r\n        let result = panic::catch_unwind(|| {\r\n            SecureUrlParser::new().parse(&large_input)\r\n        });\r\n        assert!(result.is_ok(), \"Parser should handle large inputs gracefully\");\r\n    }\r\n}\r\n\r\n// Property-based testing for security\r\n#[cfg(test)]\r\nmod property_tests {\r\n    use proptest::prelude::*;\r\n    \r\n    proptest! {\r\n        #[test]\r\n        fn url_parser_never_panics(s in \"\\\\PC*\") {\r\n            let parser = SecureUrlParser::new();\r\n            let _ = parser.parse(&s); // Should never panic\r\n        }\r\n        \r\n        #[test]\r\n        fn valid_urls_stay_valid(url in valid_url_strategy()) {\r\n            let parser = SecureUrlParser::new();\r\n            prop_assert!(parser.parse(&url).is_ok());\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n### 4.4. Performance Testing\r\n\r\n**Mandatory for performance-sensitive changes:**\r\n```bash\r\n# Performance regression testing\r\ntools/perf/run_benchmark.py --browser=release \\\r\n  --benchmark=loading.desktop \\\r\n  --story-filter=\"*your_feature*\"\r\n\r\n# Memory usage validation\r\ntools/memory/run_memory_test.py --test-filter=\"*YourComponent*\"\r\n\r\n# Startup time verification (critical for browser changes)\r\ntools/perf/run_benchmark.py --browser=release \\\r\n  --benchmark=start_with_url.cold.startup_pages\r\n\r\n# Custom performance tests\r\nout/Release/performance_tests --gtest_filter=*YourFeature*\r\n```\r\n\r\n### 4.5. Cross-Platform Testing\r\n\r\n**Required test matrix for v134+:**\r\n```bash\r\n# Windows (multiple versions)\r\npython tools/autotest/run_tests.py --platform=win10_x64\r\npython tools/autotest/run_tests.py --platform=win11_x64\r\n\r\n# macOS (Intel and Apple Silicon)\r\npython tools/autotest/run_tests.py --platform=mac_x64\r\npython tools/autotest/run_tests.py --platform=mac_arm64\r\n\r\n# Linux (multiple distributions)\r\npython tools/autotest/run_tests.py --platform=linux_x64\r\npython tools/autotest/run_tests.py --platform=linux_arm64\r\n\r\n# ChromeOS\r\npython tools/autotest/run_tests.py --platform=chromeos_x64\r\n```\r\n\r\n### 4.6. Security Testing Requirements\r\n\r\n**Fuzzing (Required for new attack surfaces):**\r\n```bash\r\n# Create fuzzing target for new components\r\ncat > fuzz_your_component.cc << 'EOF'\r\n#include \"base/logging.h\"\r\n#include \"your_component/your_component.h\"\r\n\r\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {\r\n  if (size < 1) return 0;\r\n  \r\n  std::string input(reinterpret_cast<const char*>(data), size);\r\n  YourComponent component;\r\n  \r\n  // Fuzz all public methods safely\r\n  component.ProcessInput(input);\r\n  return 0;\r\n}\r\nEOF\r\n\r\n# Build and run fuzzer\r\nautoninja -C out/Fuzzer your_component_fuzzer\r\nout/Fuzzer/your_component_fuzzer -max_total_time=300\r\n```\r\n\r\n**Security Boundary Testing:**\r\n```cpp\r\n// Test process isolation boundaries\r\nTEST_F(SecurityTest, ProcessIsolationMaintained) {\r\n  // Ensure renderer cannot access browser memory\r\n  EXPECT_FALSE(CanAccessBrowserMemory());\r\n  \r\n  // Test IPC validation\r\n  EXPECT_FALSE(SendInvalidIPC());\r\n}\r\n\r\n// Test against known attack patterns\r\nTEST_F(SecurityTest, ResistantToKnownAttacks) {\r\n  // Test against XXS attempts\r\n  EXPECT_FALSE(ExecutesArbitraryScript(\"javascript:alert(1)\"));\r\n  \r\n  // Test against buffer overflow attempts\r\n  std::string overflow_attempt(10000, 'A');\r\n  EXPECT_FALSE(CausesBufferOverflow(overflow_attempt));\r\n}\r\n```\r\n\r\n### 4.7. Manual Testing Checklist\r\n\r\n**Smoke Testing (All Changes):**\r\n- [ ] Browser starts without crashes\r\n- [ ] Basic navigation works (loading pages, back/forward)\r\n- [ ] No obvious visual regressions\r\n- [ ] Feature works as designed\r\n- [ ] No new console errors or warnings\r\n\r\n**Security-Sensitive Changes:**\r\n- [ ] No information leakage across security boundaries\r\n- [ ] Authentication/authorization works correctly  \r\n- [ ] Input validation rejects malicious inputs\r\n- [ ] No new attack surfaces introduced\r\n\r\n**Performance-Sensitive Changes:**\r\n- [ ] No significant startup time regression\r\n- [ ] Memory usage remains stable\r\n- [ ] No new performance cliffs or bottlenecks\r\n- [ ] Benchmark numbers meet expectations\r\n\r\n### 4.8. Automated Testing Integration\r\n\r\n**CI/CD Pipeline Integration:**\r\n```yaml\r\n# Example .gn configuration for automated testing\r\ntest_targets = [\r\n  \"//your_component:unit_tests\",\r\n  \"//your_component:integration_tests\", \r\n  \"//your_component:security_tests\",\r\n  \"//your_component:performance_tests\",\r\n]\r\n\r\n# Security-specific CI requirements\r\nsecurity_tests = [\r\n  \"//security:url_sanitizer_tests\",\r\n  \"//security:xss_prevention_tests\",\r\n  \"//security:csrf_protection_tests\",\r\n]\r\n```\r\n\r\n## 5. Code Review Process\r\nCommit & Upload\r\n\r\nbash\r\nCopy\r\nEdit\r\ngit checkout -b my-feature-branch\r\ngit commit -a\r\ngit cl upload\r\nReviewer Feedback\r\n\r\nAddress comments by amending your commit:\r\n\r\nbash\r\nCopy\r\nEdit\r\ngit commit --amend\r\ngit cl upload --replace\r\nGetting LGTM\r\nOnce two reviewers give LGTM and CQ+1 passes, your change will land automatically.\r\n\r\n6. Working with Gerrit\r\nView your changes:\r\nhttps://chromium-review.googlesource.com/q/status:open+owner:self\r\n\r\nCherry-picking / rebasing:\r\n\r\nbash\r\nCopy\r\nEdit\r\ngit fetch https://chromium.googlesource.com/chromium/src refs/changes/54/12354/2 && git cherry-pick FETCH_HEAD\r\nUndoing a landed CL:\r\nUpload a revert by clicking “Revert Change” in the Gerrit UI.\r\n\r\n7. Troubleshooting & Common Pitfalls\r\nPresubmit failures\r\n\r\nCheck the Buildbucket logs for style or test errors.\r\n\r\nMerge conflicts\r\n\r\nRebase onto main:\r\n\r\nbash\r\nCopy\r\nEdit\r\ngit fetch origin main && git rebase origin/main\r\nSlow builds\r\n\r\nUse autoninja -j<N> matching your CPU cores.\r\n\r\n8. Beyond Code\r\nDesign Documents\r\n\r\nFor large features, submit a design doc under docs/ and get early feedback.\r\n\r\nLocalization\r\n\r\nUI strings live in chrome/app/resources/; use l10n tools to extract/update translations.\r\n\r\nDocumentation\r\n\r\nKeep inline /README.md files up to date in each directory you touch.\r\n\r\n9. Getting Help & Community\r\nMailing Lists: chromium-dev@chromium.org\r\n\r\nIRC/Slack: channels listed on the Chromium Community wiki\r\n\r\nWeekly Office Hours: check the Chromium Calendar\r\n\r\n10. Next Steps\r\nRead Getting Started → Project Layout to orient yourself in the tree.\r\n\r\nPick a Good First Issue and try submitting a small fix.\r\n\r\nCelebrate landing your first commit—welcome to the Chromium community!\r\n\r\nHappy coding!\r\n\r\narduino\r\nCopy\r\nEdit\r\n\r\nThis guide will give newcomers a clear path: set up their environment, follow style and commit guidelines, run tests, and navigate Chromium’s Gerrit-based review process. Let me know if you’d like any tweaks or additions!"
  },
  {
    "path": "architecture/threading_and_tasks",
    "title": "Threading and Tasks in Chrome",
    "content": "# Threading and Tasks in Chrome\n\n[TOC]\n\nNote: See [Threading and Tasks FAQ](threading_and_tasks_faq.md) for more\nexamples.\n\n## Overview\n\nChrome has a [multi-process\narchitecture](https://www.chromium.org/developers/design-documents/multi-process-architecture)\nand each process is heavily multi-threaded. In this document we will go over the\nbasic threading system shared by each process. Our primary goal is to keep the\nbrowser highly responsive. Absent external requirements about latency or\nworkload, Chrome attempts to be a [highly concurrent, but not necessarily\nparallel](https://stackoverflow.com/questions/1050222/what-is-the-difference-between-concurrency-and-parallelism#:~:text=Concurrency%20is%20when%20two%20or,e.g.%2C%20on%20a%20multicore%20processor.),\nsystem.\n\nA basic intro to the way Chromium does concurrency (especially Sequences) can be\nfound\n[here](https://docs.google.com/presentation/d/1ujV8LjIUyPBmULzdT2aT9Izte8PDwbJi).\n\nThis documentation assumes familiarity with computer science\n[threading concepts](https://en.wikipedia.org/wiki/Thread_(computing)).\n\n### Quick start guide\n\n * Do not perform expensive computation or blocking IO on the main thread\n   (a.k.a. “UI” thread in the browser process) or IO thread (each\n   process's thread for receiving IPC). A busy UI / IO thread can cause\n   user-visible latency, so prefer running that work on the\n   [thread pool](#direct-posting-to-the-thread-pool).\n * Always avoid reading/writing to the same place in memory from separate\n   threads or sequences. This will lead to\n   [data races](https://en.wikipedia.org/wiki/Race_condition#Data_race)!\n   Prefer passing messages across sequences instead. Alternatives to message\n   passing like using locks is discouraged.\n * If you need to orchestrate multiple objects that live on different\n   sequences, be careful about object lifetimes.\n    * To prevent accidental data races, prefer for most classes to be used\n      exclusively on a single sequence. You should use utilities like\n      [SEQUENCE_CHECKER][4] or [base::SequenceBound][5] to help enforce this\n      constraint.\n    * As a rule of thumb, avoid [base::Unretained][1]. [weak pointers][2] can\n      usually be substituted.\n    * Explicit ownership via `std::unique_ptr` is preferred.\n    * [scoped_refptrs][3] can be used for objects that have multiple owners\n      across multiple sequences. This is usually the wrong design pattern and is\n      discouraged for new code.\n\n[1]: https://source.chromium.org/chromium/chromium/src/+/main:base/functional/bind.h;l=169;drc=ef1375f2c9fffa0d9cd664b43b0035c09fb70e99\n[2]: https://source.chromium.org/chromium/chromium/src/+/main:base/memory/weak_ptr.h\n[3]: https://source.chromium.org/chromium/chromium/src/+/main:base/memory/scoped_refptr.h\n[4]: https://source.chromium.org/chromium/chromium/src/+/main:base/sequence_checker.h\n[5]: https://source.chromium.org/chromium/chromium/src/+/main:base/threading/sequence_bound.h\n\n### Nomenclature\n\n## Core Concepts\n * **Task**: A unit of work to be processed. Effectively a function pointer with\n   optionally associated state. In Chrome this is `base::OnceCallback` and\n   `base::RepeatingCallback` created via `base::BindOnce` and\n   `base::BindRepeating`, respectively.\n   ([documentation](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/callback.md)).\n * **Task queue**: A queue of tasks to be processed.\n * **Physical thread**: An operating system provided thread (e.g. pthread on\n   POSIX or CreateThread() on Windows). The Chrome cross-platform abstraction\n   is `base::PlatformThread`. You should pretty much never use this directly.\n * **`base::Thread`**: A physical thread forever processing messages from a\n   dedicated task queue until Quit(). You should pretty much never be creating\n   your own `base::Thread`'s.\n * **Thread pool**: A pool of physical threads with a shared task queue. In\n   Chrome, this is `base::ThreadPoolInstance`. There's exactly one instance per\n   Chrome process, it serves tasks posted through\n   [`base/task/thread_pool.h`](https://cs.chromium.org/chromium/src/base/task/thread_pool.h)\n   and as such you should rarely need to use the `base::ThreadPoolInstance` API\n   directly (more on posting tasks later).\n * **Sequence** or **Virtual thread**: A chrome-managed thread of execution.\n   Like a physical thread, only one task can run on a given sequence / virtual\n   thread at any given moment and each task sees the side-effects of the\n   preceding tasks. Tasks are executed sequentially but may hop physical\n   threads between each one.\n * **Task runner**: An interface through which tasks can be posted. In Chrome\n   this is `base::TaskRunner`.\n * **Sequenced task runner**: A task runner which guarantees that tasks posted\n   to it will run sequentially, in posted order. Each such task is guaranteed to\n   see the side-effects of the task preceding it. Tasks posted to a sequenced\n   task runner are typically processed by a single thread (virtual or physical).\n   In Chrome this is `base::SequencedTaskRunner` which is-a\n   `base::TaskRunner`.\n * **Single-thread task runner**: A sequenced task runner which guarantees that\n   all tasks will be processed by the same physical thread. In Chrome this is\n   `base::SingleThreadTaskRunner` which is-a `base::SequencedTaskRunner`. We\n   [prefer sequences to threads](#prefer-sequences-to-physical-threads) whenever\n   possible.\n\n## Threading Lexicon\nNote to the reader: the following terms are an attempt to bridge the gap between\ncommon threading nomenclature and the way we use them in Chrome. It might be a\nbit heavy if you're just getting started. Should this be hard to parse, consider\nskipping to the more detailed sections below and referring back to this as\nnecessary.\n\n * **Thread-unsafe**: The vast majority of types in Chrome are thread-unsafe\n   (by design). Access to such types/methods must be externally synchronized.\n   Typically thread-unsafe types require that all tasks accessing their state be\n   posted to the same `base::SequencedTaskRunner` and they verify this in debug\n   builds with a `SEQUENCE_CHECKER` member. Locks are also an option to\n   synchronize access but in Chrome we strongly\n   [prefer sequences to locks](#Using-Sequences-Instead-of-Locks).\n * **Thread-affine**: Such types/methods need to be always accessed from the\n   same physical thread (i.e. from the same `base::SingleThreadTaskRunner`) and\n   typically have a `THREAD_CHECKER` member to verify that they are. Short of\n   using a third-party API or having a leaf dependency which is thread-affine:\n   there's pretty much no reason for a type to be thread-affine in Chrome.\n   Note that `base::SingleThreadTaskRunner` is-a `base::SequencedTaskRunner` so\n   thread-affine is a subset of thread-unsafe. Thread-affine is also sometimes\n   referred to as **thread-hostile**.\n * **Thread-safe**: Such types/methods can be safely accessed in parallel.\n * **Thread-compatible**: Such types provide safe parallel access to const\n   methods but require synchronization for non-const (or mixed const/non-const\n   access). Chrome doesn't expose reader-writer locks; as such, the only use\n   case for this is objects (typically globals) which are initialized once in a\n   thread-safe manner (either in the single-threaded phase of startup or lazily\n   through a thread-safe static-local-initialization paradigm a la\n   `base::NoDestructor`) and forever after immutable.\n * **Immutable**: A subset of thread-compatible types which cannot be modified\n   after construction.\n * **Sequence-friendly**: Such types/methods are thread-unsafe types which\n   support being invoked from a `base::SequencedTaskRunner`. Ideally this would\n   be the case for all thread-unsafe types but legacy code sometimes has\n   overzealous checks that enforce thread-affinity in mere thread-unsafe\n   scenarios. See [Prefer Sequences to\n   Threads](#prefer-sequences-to-physical-threads) below for more details.\n\n### Threads\n\nEvery Chrome process has\n\n* a main thread\n   * in the browser process (BrowserThread::UI): updates the UI\n   * in renderer processes (Blink main thread): runs most of Blink\n* an IO thread\n   * in all processes: all IPC messages arrive on this thread. The application\n     logic to handle the message may be in a different thread (i.e., the IO\n     thread may route the message to a [Mojo\n     interface](/docs/README.md#Mojo-Services) which is bound to a\n     different thread).\n   * more generally most async I/O happens on this thread (e.g., through\n     base::FileDescriptorWatcher).\n   * in the browser process: this is called BrowserThread::IO.\n* a few more special-purpose threads\n* and a pool of general-purpose threads\n\nMost threads have a loop that gets tasks from a queue and runs them (the queue\nmay be shared between multiple threads).\n\n### Tasks\n\nA task is a `base::OnceClosure` added to a queue for asynchronous execution.\n\nA `base::OnceClosure` stores a function pointer and arguments. It has a `Run()`\nmethod that invokes the function pointer using the bound arguments. It is\ncreated using `base::BindOnce`. (ref. [Callback<> and Bind()\ndocumentation](callback.md)).\n\n```\nvoid TaskA() {}\nvoid TaskB(int v) {}\n\nauto task_a = base::BindOnce(&TaskA);\nauto task_b = base::BindOnce(&TaskB, 42);\n```\n\nA group of tasks can be executed in one of the following ways:\n\n* [Parallel](#Posting-a-Parallel-Task): No task execution ordering, possibly all\n  at once on any thread\n* [Sequenced](#Posting-a-Sequenced-Task): Tasks executed in posting order, one\n  at a time on any thread.\n* [Single Threaded](#Posting-Multiple-Tasks-to-the-Same-Thread): Tasks executed\n  in posting order, one at a time on a single thread.\n   * [COM Single Threaded](#Posting-Tasks-to-a-COM-Single_Thread-Apartment-STA_Thread-Windows):\n     A variant of single threaded with COM initialized.\n\n### Prefer Sequences to Physical Threads\n\nSequenced execution (on virtual threads) is strongly preferred to\nsingle-threaded execution (on physical threads). Except for types/methods bound\nto the main thread (UI) or IO threads: thread-safety is better achieved via\n`base::SequencedTaskRunner` than through managing your own physical threads\n(ref. [Posting a Sequenced Task](#posting-a-sequenced-task) below).\n\nAll APIs which are exposed for \"current physical thread\" have an equivalent for\n\"current sequence\"\n([mapping](threading_and_tasks_faq.md#How-to-migrate-from-SingleThreadTaskRunner-to-SequencedTaskRunner)).\n\nIf you find yourself writing a sequence-friendly type and it fails\nthread-affinity checks (e.g., `THREAD_CHECKER`) in a leaf dependency: consider\nmaking that dependency sequence-friendly as well. Most core APIs in Chrome are\nsequence-friendly, but some legacy types may still over-zealously use\nThreadChecker/SingleThreadTaskRunner when they could instead rely on the\n\"current sequence\" and no longer be thread-affine.\n\n## Posting a Parallel Task\n\n### Direct Posting to the Thread Pool\n\nA task that can run on any thread and doesn’t have ordering or mutual exclusion\nrequirements with other tasks should be posted using one of the\n`base::ThreadPool::PostTask*()` functions defined in\n[`base/task/thread_pool.h`](https://cs.chromium.org/chromium/src/base/task/thread_pool.h).\n\n```cpp\nbase::ThreadPool::PostTask(FROM_HERE, base::BindOnce(&Task));\n```\n\nThis posts tasks with default traits.\n\nThe `base::ThreadPool::PostTask*()` functions allow the caller to provide\nadditional details about the task via TaskTraits (ref. [Annotating Tasks with\nTaskTraits](#Annotating-Tasks-with-TaskTraits)).\n\n```cpp\nbase::ThreadPool::PostTask(\n    FROM_HERE, {base::TaskPriority::BEST_EFFORT, MayBlock()},\n    base::BindOnce(&Task));\n```\n\n### Posting via a TaskRunner\n\nA parallel\n[`base::TaskRunner`](https://cs.chromium.org/chromium/src/base/task/task_runner.h) is\nan alternative to calling `base::ThreadPool::PostTask*()` directly. This is\nmainly useful when it isn’t known in advance whether tasks will be posted in\nparallel, in sequence, or to a single-thread (ref. [Posting a Sequenced\nTask](#Posting-a-Sequenced-Task), [Posting Multiple Tasks to the Same\nThread](#Posting-Multiple-Tasks-to-the-Same-Thread)). Since `base::TaskRunner`\nis the base class of `base::SequencedTaskRunner` and\n`base::SingleThreadTaskRunner`, a `scoped_refptr<TaskRunner>` member can hold a\n`base::TaskRunner`, a `base::SequencedTaskRunner` or a\n`base::SingleThreadTaskRunner`.\n\n```cpp\nclass A {\n public:\n  A() = default;\n\n  void PostSomething() {\n    task_runner_->PostTask(FROM_HERE, base::BindOnce(&A, &DoSomething));\n  }\n\n  void DoSomething() {\n  }\n\n private:\n  scoped_refptr<base::TaskRunner> task_runner_ =\n      base::ThreadPool::CreateTaskRunner({base::TaskPriority::USER_VISIBLE});\n};\n```\n\nUnless a test needs to control precisely how tasks are executed, it is preferred\nto call `base::ThreadPool::PostTask*()` directly (ref. [Testing](#Testing) for\nless invasive ways of controlling tasks in tests).\n\n## Posting a Sequenced Task\n\nA sequence is a set of tasks that run one at a time in posting order (not\nnecessarily on the same thread). To post tasks as part of a sequence, use a\n[`base::SequencedTaskRunner`](https://cs.chromium.org/chromium/src/base/task/sequenced_task_runner.h).\n\n### Posting to a New Sequence\n\nA `base::SequencedTaskRunner` can be created by\n`base::ThreadPool::CreateSequencedTaskRunner()`.\n\n```cpp\nscoped_refptr<SequencedTaskRunner> sequenced_task_runner =\n    base::ThreadPool::CreateSequencedTaskRunner(...);\n\n// TaskB runs after TaskA completes.\nsequenced_task_runner->PostTask(FROM_HERE, base::BindOnce(&TaskA));\nsequenced_task_runner->PostTask(FROM_HERE, base::BindOnce(&TaskB));\n```\n\n### Posting to the Current (Virtual) Thread\n\nThe preferred way of posting to the current (virtual) thread is via\n`base::SequencedTaskRunner::GetCurrentDefault()`.\n\n```cpp\n// The task will run on the current (virtual) thread's default task queue.\nbase::SequencedTaskRunner::GetCurrentDefault()->PostTask(\n    FROM_HERE, base::BindOnce(&Task));\n```\n\nNote that `SequencedTaskRunner::GetCurrentDefault()` returns the default queue for the\ncurrent virtual thread. On threads with multiple task queues (e.g.\nBrowserThread::UI) this can be a different queue than the one the current task\nbelongs to. The \"current\" task runner is intentionally not exposed via a static\ngetter. Either you know it already and can post to it directly or you don't and\nthe only sensible destination is the default queue. See https://bit.ly/3JvCLsX\nfor detailed discussion.\n\n## Using Sequences Instead of Locks\n\nUsage of locks is discouraged in Chrome. Sequences inherently provide\nthread-safety. Prefer classes that are always accessed from the same\nsequence to managing your own thread-safety with locks.\n\n**Thread-safe but not thread-affine; how so?** Tasks posted to the same sequence\nwill run in sequential order. After a sequenced task completes, the next task\nmay be picked up by a different worker thread, but that task is guaranteed to\nsee any side-effects caused by the previous one(s) on its sequence.\n\n```cpp\nclass A {\n public:\n  A() {\n    // Do not require accesses to be on the creation sequence.\n    DETACH_FROM_SEQUENCE(sequence_checker_);\n  }\n\n  void AddValue(int v) {\n    // Check that all accesses are on the same sequence.\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\n    values_.push_back(v);\n}\n\n private:\n  SEQUENCE_CHECKER(sequence_checker_);\n\n  // No lock required, because all accesses are on the\n  // same sequence.\n  std::vector<int> values_;\n};\n\nA a;\nscoped_refptr<SequencedTaskRunner> task_runner_for_a = ...;\ntask_runner_for_a->PostTask(FROM_HERE,\n                      base::BindOnce(&A::AddValue, base::Unretained(&a), 42));\ntask_runner_for_a->PostTask(FROM_HERE,\n                      base::BindOnce(&A::AddValue, base::Unretained(&a), 27));\n\n// Access from a different sequence causes a DCHECK failure.\nscoped_refptr<SequencedTaskRunner> other_task_runner = ...;\nother_task_runner->PostTask(FROM_HERE,\n                            base::BindOnce(&A::AddValue, base::Unretained(&a), 1));\n```\n\nLocks should only be used to swap in a shared data structure that can be\naccessed on multiple threads.  If one thread updates it based on expensive\ncomputation or through disk access, then that slow work should be done without\nholding the lock.  Only when the result is available should the lock be used to\nswap in the new data.  An example of this is in PluginList::LoadPlugins\n([`content/browser/plugin_list.cc`](https://cs.chromium.org/chromium/src/content/browser/plugin_list.cc).\nIf you must use locks,\n[here](https://www.chromium.org/developers/lock-and-condition-variable) are some\nbest practices and pitfalls to avoid.\n\nIn order to write non-blocking code, many APIs in Chrome are asynchronous.\nUsually this means that they either need to be executed on a particular\nthread/sequence and will return results via a custom delegate interface, or they\ntake a `base::OnceCallback<>` (or `base::RepeatingCallback<>`) object that is\ncalled when the requested operation is completed.  Executing work on a specific\nthread/sequence is covered in the PostTask sections above.\n\n## Posting Multiple Tasks to the Same Thread\n\nIf multiple tasks need to run on the same thread, post them to a\n[`base::SingleThreadTaskRunner`](https://cs.chromium.org/chromium/src/base/task/single_thread_task_runner.h).\nAll tasks posted to the same `base::SingleThreadTaskRunner` run on the same thread in\nposting order.\n\n### Posting to the Main Thread or to the IO Thread in the Browser Process\n\nTo post tasks to the main thread or to the IO thread, use\n`content::GetUIThreadTaskRunner({})` or `content::GetIOThreadTaskRunner({})`\nfrom\n[`content/public/browser/browser_thread.h`](https://cs.chromium.org/chromium/src/content/public/browser/browser_thread.h)\n\nYou may provide additional BrowserTaskTraits as a parameter to those methods\nthough this is generally still uncommon in BrowserThreads and should be reserved\nfor advanced use cases.\n\nThere's an ongoing migration ([task APIs v3]) away from the previous\nbase-API-with-traits which you may still find throughout the codebase (it's\nequivalent):\n\n```cpp\nbase::PostTask(FROM_HERE, {content::BrowserThread::UI}, ...);\n\nbase::CreateSingleThreadTaskRunner({content::BrowserThread::IO})\n    ->PostTask(FROM_HERE, ...);\n```\n\nNote: For the duration of the migration, you'll unfortunately need to continue\nmanually including\n[`content/public/browser/browser_task_traits.h`](https://cs.chromium.org/chromium/src/content/public/browser/browser_task_traits.h).\nto use the browser_thread.h API.\n\nThe main thread and the IO thread are already super busy. Therefore, prefer\nposting to a general purpose thread when possible (ref.\n[Posting a Parallel Task](#Posting-a-Parallel-Task),\n[Posting a Sequenced task](#Posting-a-Sequenced-Task)).\nGood reasons to post to the main thread are to update the UI or access objects\nthat are bound to it (e.g. `Profile`). A good reason to post to the IO thread is\nto access the internals of components that are bound to it (e.g. IPCs, network).\nNote: It is not necessary to have an explicit post task to the IO thread to\nsend/receive an IPC or send/receive data on the network.\n\n### Posting to the Main Thread in a Renderer Process\nTODO(blink-dev)\n\n### Posting to a Custom SingleThreadTaskRunner\n\nIf multiple tasks need to run on the same thread and that thread doesn’t have to\nbe the main thread or the IO thread, post them to a\n`base::SingleThreadTaskRunner` created by\n`base::Threadpool::CreateSingleThreadTaskRunner`.\n\n```cpp\nscoped_refptr<SingleThreadTaskRunner> single_thread_task_runner =\n    base::Threadpool::CreateSingleThreadTaskRunner(...);\n\n// TaskB runs after TaskA completes. Both tasks run on the same thread.\nsingle_thread_task_runner->PostTask(FROM_HERE, base::BindOnce(&TaskA));\nsingle_thread_task_runner->PostTask(FROM_HERE, base::BindOnce(&TaskB));\n```\n\nRemember that we [prefer sequences to physical\nthreads](#prefer-sequences-to-physical-threads) and that this thus should rarely\nbe necessary.\n\n### Posting to the Current Thread\n\n*** note\n**IMPORTANT:** To post a task that needs mutual exclusion with the current\nsequence of tasks but doesn’t absolutely need to run on the current physical\nthread, use `base::SequencedTaskRunner::GetCurrentDefault()` instead of\n`base::SingleThreadTaskRunner::GetCurrentDefault()` (ref. [Posting to the Current\nSequence](#Posting-to-the-Current-Virtual_Thread)). That will better document\nthe requirements of the posted task and will avoid unnecessarily making your API\nphysical thread-affine. In a single-thread task,\n`base::SequencedTaskRunner::GetCurrentDefault()` is equivalent to\n`base::SingleThreadTaskRunner::GetCurrentDefault()`.\n***\n\nIf you must post a task to the current physical thread nonetheless, use\n[`base::SingleThreadTaskRunner::CurrentDefaultHandle`](https://source.chromium.org/chromium/chromium/src/+/main:base/task/single_thread_task_runner.h).\n\n```cpp\n// The task will run on the current thread in the future.\nbase::SingleThreadTaskRunner::GetCurrentDefault()->PostTask(\n    FROM_HERE, base::BindOnce(&Task));\n```\n\n## Posting Tasks to a COM Single-Thread Apartment (STA) Thread (Windows)\n\nTasks that need to run on a COM Single-Thread Apartment (STA) thread must be\nposted to a `base::SingleThreadTaskRunner` returned by\n`base::ThreadPool::CreateCOMSTATaskRunner()`. As mentioned in [Posting Multiple\nTasks to the Same Thread](#Posting-Multiple-Tasks-to-the-Same-Thread), all tasks\nposted to the same `base::SingleThreadTaskRunner` run on the same thread in\nposting order.\n\n```cpp\n// Task(A|B|C)UsingCOMSTA will run on the same COM STA thread.\n\nvoid TaskAUsingCOMSTA() {\n  // [ This runs on a COM STA thread. ]\n\n  // Make COM STA calls.\n  // ...\n\n  // Post another task to the current COM STA thread.\n  base::SingleThreadTaskRunner::GetCurrentDefault()->PostTask(\n      FROM_HERE, base::BindOnce(&TaskCUsingCOMSTA));\n}\nvoid TaskBUsingCOMSTA() { }\nvoid TaskCUsingCOMSTA() { }\n\nauto com_sta_task_runner = base::ThreadPool::CreateCOMSTATaskRunner(...);\ncom_sta_task_runner->PostTask(FROM_HERE, base::BindOnce(&TaskAUsingCOMSTA));\ncom_sta_task_runner->PostTask(FROM_HERE, base::BindOnce(&TaskBUsingCOMSTA));\n```\n\n## Memory ordering guarantees for posted Tasks\n\nThis task system guarantees that all the memory effects of sequential execution\nbefore posting a task are _visible_ to the task when it starts running. More\nformally, a call to `PostTask()` and the execution of the posted task are in the\n[happens-before\nrelationship](https://preshing.com/20130702/the-happens-before-relation/) with\neach other. This is true for all variants of posting a task in `::base`,\nincluding `PostTaskAndReply()`. Similarly the happens-before relationship is\npresent for tasks running in a sequence as part of the same SequencedTaskRunner.\n\nThis guarantee is important to know about because Chrome tasks commonly access\nmemory beyond the immediate data copied into the `base::OnceCallback`, and this\nhappens-before relationship allows to avoid additional synchronization within\nthe tasks themselves. As a very specific example, consider a callback that binds\na pointer to memory which was just initialized in the thread posting the task.\n\nA more constrained model is also worth noting. Execution can be split into tasks\nrunning on different task runners, where each task _exclusively_ accesses\ncertain objects in memory without explicit synchronization. Posting another task\ntransfers this 'ownership' (of the objects) to the next task. With this the\nnotion of object ownership can often be extended to the level of task runners,\nwhich provides useful invariants to reason about. This model allows to avoid\nrace conditions while also avoiding locks and atomic operations. Because of its\nsimplicity this model is commonly used in Chrome.\n\n## Annotating Tasks with TaskTraits\n\n[`base::TaskTraits`](https://cs.chromium.org/chromium/src/base/task/task_traits.h)\nencapsulate information about a task that helps the thread pool make better\nscheduling decisions.\n\nMethods that take `base::TaskTraits` can be be passed `{}` when default traits\nare sufficient. Default traits are appropriate for tasks that:\n\n- Don’t block (ref. MayBlock and WithBaseSyncPrimitives);\n- Pertain to user-blocking activity;\n  (explicitly or implicitly by having an ordering dependency with a component\n   that does)\n- Can either block shutdown or be skipped on shutdown (thread pool is free to\n  choose a fitting default).\n\nTasks that don’t match this description must be posted with explicit TaskTraits.\n\n[`base/task/task_traits.h`](https://cs.chromium.org/chromium/src/base/task/task_traits.h)\nprovides exhaustive documentation of available traits. The content layer also\nprovides additional traits in\n[`content/public/browser/browser_task_traits.h`](https://cs.chromium.org/chromium/src/content/public/browser/browser_task_traits.h)\nto facilitate posting a task onto a BrowserThread.\n\nBelow are some examples of how to specify `base::TaskTraits`.\n\n```cpp\n// This task has no explicit TaskTraits. It cannot block. Its priority is\n// USER_BLOCKING. It will either block shutdown or be skipped on shutdown.\nbase::ThreadPool::PostTask(FROM_HERE, base::BindOnce(...));\n\n// This task has the highest priority. The thread pool will schedule it before\n// USER_VISIBLE and BEST_EFFORT tasks.\nbase::ThreadPool::PostTask(\n    FROM_HERE, {base::TaskPriority::USER_BLOCKING},\n    base::BindOnce(...));\n\n// This task has the lowest priority and is allowed to block (e.g. it\n// can read a file from disk).\nbase::ThreadPool::PostTask(\n    FROM_HERE, {base::TaskPriority::BEST_EFFORT, base::MayBlock()},\n    base::BindOnce(...));\n\n// This task blocks shutdown. The process won't exit before its\n// execution is complete.\nbase::ThreadPool::PostTask(\n    FROM_HERE, {base::TaskShutdownBehavior::BLOCK_SHUTDOWN},\n    base::BindOnce(...));\n```\n\n## Keeping the Browser Responsive\n\nDo not perform expensive work on the main thread, the IO thread or any sequence\nthat is expected to run tasks with a low latency. Instead, perform expensive\nwork asynchronously using `base::ThreadPool::PostTaskAndReply*()` or\n`base::SequencedTaskRunner::PostTaskAndReply()`. Note that\nasynchronous/overlapped I/O on the IO thread are fine.\n\nExample: Running the code below on the main thread will prevent the browser from\nresponding to user input for a long time.\n\n```cpp\n// GetHistoryItemsFromDisk() may block for a long time.\n// AddHistoryItemsToOmniboxDropDown() updates the UI and therefore must\n// be called on the main thread.\nAddHistoryItemsToOmniboxDropdown(GetHistoryItemsFromDisk(\"keyword\"));\n```\n\nThe code below solves the problem by scheduling a call to\n`GetHistoryItemsFromDisk()` in a thread pool followed by a call to\n`AddHistoryItemsToOmniboxDropdown()` on the origin sequence (the main thread in\nthis case). The return value of the first call is automatically provided as\nargument to the second call.\n\n```cpp\nbase::ThreadPool::PostTaskAndReplyWithResult(\n    FROM_HERE, {base::MayBlock()},\n    base::BindOnce(&GetHistoryItemsFromDisk, \"keyword\"),\n    base::BindOnce(&AddHistoryItemsToOmniboxDropdown));\n```\n\n## Posting a Task with a Delay\n\n### Posting a One-Off Task with a Delay\n\nTo post a task that must run once after a delay expires, use\n`base::ThreadPool::PostDelayedTask*()` or `base::TaskRunner::PostDelayedTask()`.\n\n```cpp\nbase::ThreadPool::PostDelayedTask(\n  FROM_HERE, {base::TaskPriority::BEST_EFFORT}, base::BindOnce(&Task),\n  base::Hours(1));\n\nscoped_refptr<base::SequencedTaskRunner> task_runner =\n    base::ThreadPool::CreateSequencedTaskRunner(\n        {base::TaskPriority::BEST_EFFORT});\ntask_runner->PostDelayedTask(\n    FROM_HERE, base::BindOnce(&Task), base::Hours(1));\n```\n\n*** note\n**NOTE:** A task that has a 1-hour delay probably doesn’t have to run right away\nwhen its delay expires. Specify `base::TaskPriority::BEST_EFFORT` to prevent it\nfrom slowing down the browser when its delay expires.\n***\n\n### Posting a Repeating Task with a Delay\nTo post a task that must run at regular intervals,\nuse [`base::RepeatingTimer`](https://cs.chromium.org/chromium/src/base/timer/timer.h).\n\n```cpp\nclass A {\n public:\n  ~A() {\n    // The timer is stopped automatically when it is deleted.\n  }\n  void StartDoingStuff() {\n    timer_.Start(FROM_HERE, Seconds(1),\n                 this, &A::DoStuff);\n  }\n  void StopDoingStuff() {\n    timer_.Stop();\n  }\n private:\n  void DoStuff() {\n    // This method is called every second on the sequence that invoked\n    // StartDoingStuff().\n  }\n  base::RepeatingTimer timer_;\n};\n```\n\n## Cancelling a Task\n\n### Using base::WeakPtr\n\n[`base::WeakPtr`](https://cs.chromium.org/chromium/src/base/memory/weak_ptr.h)\ncan be used to ensure that any callback bound to an object is canceled when that\nobject is destroyed.\n\n```cpp\nint Compute() { … }\n\nclass A {\n public:\n  void ComputeAndStore() {\n    // Schedule a call to Compute() in a thread pool followed by\n    // a call to A::Store() on the current sequence. The call to\n    // A::Store() is canceled when |weak_ptr_factory_| is destroyed.\n    // (guarantees that |this| will not be used-after-free).\n    base::ThreadPool::PostTaskAndReplyWithResult(\n        FROM_HERE, base::BindOnce(&Compute),\n        base::BindOnce(&A::Store, weak_ptr_factory_.GetWeakPtr()));\n  }\n\n private:\n  void Store(int value) { value_ = value; }\n\n  int value_;\n  base::WeakPtrFactory<A> weak_ptr_factory_{this};\n};\n```\n\nNote: `WeakPtr` is not thread-safe: `~WeakPtrFactory()` and\n`Store()` (bound to a `WeakPtr`) must all run on the same sequence.\n\n### Using base::CancelableTaskTracker\n\n[`base::CancelableTaskTracker`](https://cs.chromium.org/chromium/src/base/task/cancelable_task_tracker.h)\nallows cancellation to happen on a different sequence than the one on which\ntasks run. Keep in mind that `CancelableTaskTracker` cannot cancel tasks that\nhave already started to run.\n\n```cpp\nauto task_runner = base::ThreadPool::CreateTaskRunner({});\nbase::CancelableTaskTracker cancelable_task_tracker;\ncancelable_task_tracker.PostTask(task_runner.get(), FROM_HERE,\n                                 base::DoNothing());\n// Cancels Task(), only if it hasn't already started running.\ncancelable_task_tracker.TryCancelAll();\n```\n\n## Posting a Job to run in parallel\n\nThe [`base::PostJob`](https://cs.chromium.org/chromium/src/base/task/post_job.h)\nis a power user API to be able to schedule a single base::RepeatingCallback\nworker task and request that ThreadPool workers invoke it in parallel.\nThis avoids degenerate cases:\n* Calling `PostTask()` for each work item, causing significant overhead.\n* Fixed number of `PostTask()` calls that split the work and might run for a\n  long time. This is problematic when many components post “num cores” tasks and\n  all expect to use all the cores. In these cases, the scheduler lacks context\n  to be fair to multiple same-priority requests and/or ability to request lower\n  priority work to yield when high priority work comes in.\n\nSee [`base/task/job_perftest.cc`](https://cs.chromium.org/chromium/src/base/task/job_perftest.cc)\nfor a complete example.\n\n```cpp\n// A canonical implementation of |worker_task|.\nvoid WorkerTask(base::JobDelegate* job_delegate) {\n  while (!job_delegate->ShouldYield()) {\n    auto work_item = TakeWorkItem(); // Smallest unit of work.\n    if (!work_item)\n      return:\n    ProcessWork(work_item);\n  }\n}\n\n// Returns the latest thread-safe number of incomplete work items.\nvoid NumIncompleteWorkItems(size_t worker_count) {\n  // NumIncompleteWorkItems() may use |worker_count| if it needs to account for\n  // local work lists, which is easier than doing its own accounting, keeping in\n  // mind that the actual number of items may be racily overestimated and thus\n  // WorkerTask() may be called when there's no available work.\n  return GlobalQueueSize() + worker_count;\n}\n\nbase::PostJob(FROM_HERE, {},\n              base::BindRepeating(&WorkerTask),\n              base::BindRepeating(&NumIncompleteWorkItems));\n```\n\nBy doing as much work as possible in a loop when invoked, the worker task avoids\nscheduling overhead. Meanwhile `base::JobDelegate::ShouldYield()` is\nperiodically invoked to conditionally exit and let the scheduler prioritize\nother work. This yield-semantic allows, for example, a user-visible job to use\nall cores but get out of the way when a user-blocking task comes in.\n\n### Adding additional work to a running job\n\nWhen new work items are added and the API user wants additional threads to\ninvoke the worker task in parallel,\n`JobHandle/JobDelegate::NotifyConcurrencyIncrease()` *must* be invoked shortly\nafter max concurrency increases.\n\n## Testing\n\nFor more details see [Testing Components Which Post\nTasks](threading_and_tasks_testing.md).\n\nTo test code that uses `base::SingleThreadTaskRunner::CurrentDefaultHandle`,\n`base::SequencedTaskRunner::CurrentDefaultHandle` or a function in\n[`base/task/thread_pool.h`](https://cs.chromium.org/chromium/src/base/task/thread_pool.h),\ninstantiate a\n[`base::test::TaskEnvironment`](https://cs.chromium.org/chromium/src/base/test/task_environment.h)\nfor the scope of the test. If you need BrowserThreads, use\n`content::BrowserTaskEnvironment` instead of\n`base::test::TaskEnvironment`.\n\nTests can run the `base::test::TaskEnvironment`'s message pump using a\n`base::RunLoop`, which can be made to run until `Quit()` (explicitly or via\n`RunLoop::QuitClosure()`), or to `RunUntilIdle()` ready-to-run tasks and\nimmediately return.\n\nTaskEnvironment configures RunLoop::Run() to GTEST_FAIL() if it hasn't been\nexplicitly quit after TestTimeouts::action_timeout(). This is preferable to\nhaving the test hang if the code under test fails to trigger the RunLoop to\nquit. The timeout can be overridden with base::test::ScopedRunLoopTimeout.\n\n```cpp\nclass MyTest : public testing::Test {\n public:\n  // ...\n protected:\n   base::test::TaskEnvironment task_environment_;\n};\n\nTEST_F(MyTest, FirstTest) {\n  base::SingleThreadTaskRunner::GetCurrentDefault()->PostTask(FROM_HERE, base::BindOnce(&A));\n  base::SequencedTaskRunner::GetCurrentDefault()->PostTask(FROM_HERE,\n                                                   base::BindOnce(&B));\n  base::SingleThreadTaskRunner::GetCurrentDefault()->PostDelayedTask(\n      FROM_HERE, base::BindOnce(&C), base::TimeDelta::Max());\n\n  // This runs the (SingleThread|Sequenced)TaskRunner::CurrentDefaultHandle queue until it is empty.\n  // Delayed tasks are not added to the queue until they are ripe for execution.\n  // Prefer explicit exit conditions to RunUntilIdle when possible:\n  // bit.ly/run-until-idle-with-care2.\n  base::RunLoop().RunUntilIdle();\n  // A and B have been executed. C is not ripe for execution yet.\n\n  base::RunLoop run_loop;\n  base::SingleThreadTaskRunner::GetCurrentDefault()->PostTask(FROM_HERE, base::BindOnce(&D));\n  base::SingleThreadTaskRunner::GetCurrentDefault()->PostTask(FROM_HERE, run_loop.QuitClosure());\n  base::SingleThreadTaskRunner::GetCurrentDefault()->PostTask(FROM_HERE, base::BindOnce(&E));\n\n  // This runs the (SingleThread|Sequenced)TaskRunner::CurrentDefaultHandle queue until QuitClosure is\n  // invoked.\n  run_loop.Run();\n  // D and run_loop.QuitClosure() have been executed. E is still in the queue.\n\n  // Tasks posted to thread pool run asynchronously as they are posted.\n  base::ThreadPool::PostTask(FROM_HERE, {}, base::BindOnce(&F));\n  auto task_runner =\n      base::ThreadPool::CreateSequencedTaskRunner({});\n  task_runner->PostTask(FROM_HERE, base::BindOnce(&G));\n\n  // To block until all tasks posted to thread pool are done running:\n  base::ThreadPoolInstance::Get()->FlushForTesting();\n  // F and G have been executed.\n\n  base::ThreadPool::PostTaskAndReplyWithResult(\n      FROM_HERE, {}, base::BindOnce(&H), base::BindOnce(&I));\n\n  // This runs the (SingleThread|Sequenced)TaskRunner::CurrentDefaultHandle queue until both the\n  // (SingleThread|Sequenced)TaskRunner::CurrentDefaultHandle queue and the ThreadPool queue are\n  // empty. Prefer explicit exit conditions to RunUntilIdle when possible:\n  // bit.ly/run-until-idle-with-care2.\n  task_environment_.RunUntilIdle();\n  // E, H, I have been executed.\n}\n```\n\n## Using ThreadPool in a New Process\n\nThreadPoolInstance needs to be initialized in a process before the functions in\n[`base/task/thread_pool.h`](https://cs.chromium.org/chromium/src/base/task/thread_pool.h)\ncan be used. Initialization of ThreadPoolInstance in the Chrome browser process\nand child processes (renderer, GPU, utility) has already been taken care of. To\nuse ThreadPoolInstance in another process, initialize ThreadPoolInstance early\nin the main function:\n\n```cpp\n// This initializes and starts ThreadPoolInstance with default params.\nbase::ThreadPoolInstance::CreateAndStartWithDefaultParams(\"process_name\");\n// The base/task/thread_pool.h API can now be used with base::ThreadPool trait.\n// Tasks will be scheduled as they are posted.\n\n// This initializes ThreadPoolInstance.\nbase::ThreadPoolInstance::Create(\"process_name\");\n// The base/task/thread_pool.h API can now be used with base::ThreadPool trait. No\n// threads will be created and no tasks will be scheduled until after Start() is\n// called.\nbase::ThreadPoolInstance::Get()->Start(params);\n// ThreadPool can now create threads and schedule tasks.\n```\n\nAnd shutdown ThreadPoolInstance late in the main function:\n\n```cpp\nbase::ThreadPoolInstance::Get()->Shutdown();\n// Tasks posted with TaskShutdownBehavior::BLOCK_SHUTDOWN and\n// tasks posted with TaskShutdownBehavior::SKIP_ON_SHUTDOWN that\n// have started to run before the Shutdown() call have now completed their\n// execution. Tasks posted with\n// TaskShutdownBehavior::CONTINUE_ON_SHUTDOWN may still be\n// running.\n```\n## TaskRunner ownership (encourage no dependency injection)\n\nTaskRunners shouldn't be passed through several components. Instead, the\ncomponent that uses a TaskRunner should be the one that creates it.\n\nSee [this example](https://codereview.chromium.org/2885173002/) of a\nrefactoring where a TaskRunner was passed through a lot of components only to be\nused in an eventual leaf. The leaf can and should now obtain its TaskRunner\ndirectly from\n[`base/task/thread_pool.h`](https://cs.chromium.org/chromium/src/base/task/thread_pool.h).\n\nAs mentioned above, `base::test::TaskEnvironment` allows unit tests to\ncontrol tasks posted from underlying TaskRunners. In rare cases where a test\nneeds to more precisely control task ordering: dependency injection of\nTaskRunners can be useful. For such cases the preferred approach is the\nfollowing:\n\n```cpp\nclass Foo {\n public:\n\n  // Overrides |background_task_runner_| in tests.\n  void SetBackgroundTaskRunnerForTesting(\n      scoped_refptr<base::SequencedTaskRunner> background_task_runner) {\n    background_task_runner_ = std::move(background_task_runner);\n  }\n\n private:\n  scoped_refptr<base::SequencedTaskRunner> background_task_runner_ =\n      base::ThreadPool::CreateSequencedTaskRunner(\n          {base::MayBlock(), base::TaskPriority::BEST_EFFORT});\n}\n```\n\nNote that this still allows removing all layers of plumbing between //chrome and\nthat component since unit tests will use the leaf layer directly.\n\n## FAQ\nSee [Threading and Tasks FAQ](threading_and_tasks_faq.md) for more examples.\n\n[task APIs v3]: https://docs.google.com/document/d/1tssusPykvx3g0gvbvU4HxGyn3MjJlIylnsH13-Tv6s4/edit?ts=5de99a52#heading=h.ss4tw38hvh3s\n\n## Internals\n\n### SequenceManager\n\n[SequenceManager](https://cs.chromium.org/chromium/src/base/task/sequence_manager/sequence_manager.h)\nmanages TaskQueues which have different properties (e.g. priority, common task\ntype) multiplexing all posted tasks into a single backing sequence. This will\nusually be a MessagePump. Depending on the type of message pump used other\nevents such as UI messages may be processed as well. On Windows APC calls (as\ntime permits) and signals sent to a registered set of HANDLEs may also be\nprocessed.\n\n### MessagePump\n\n[MessagePumps](https://cs.chromium.org/chromium/src/base/message_loop/message_pump.h)\nare responsible for processing native messages as well as for giving cycles to\ntheir delegate (SequenceManager) periodically. MessagePumps take care to mixing\ndelegate callbacks with native message processing so neither type of event\nstarves the other of cycles.\n\nThere are different [MessagePumpTypes](https://cs.chromium.org/chromium/src/base/message_loop/message_pump_type.h),\nmost common are:\n\n* DEFAULT: Supports tasks and timers only\n\n* UI: Supports native UI events (e.g. Windows messages)\n\n* IO: Supports asynchronous IO (not file I/O!)\n\n* CUSTOM: User provided implementation of MessagePump interface\n\n### RunLoop\n\nRunLoop is a helper class to run the RunLoop::Delegate associated with the\ncurrent thread (usually a SequenceManager). Create a RunLoop on the stack and\ncall Run/Quit to run a nested RunLoop but please avoid nested loops in\nproduction code!\n\n### Task Reentrancy\n\nSequenceManager has task reentrancy protection. This means that if a\ntask is being processed, a second task cannot start until the first task is\nfinished. Reentrancy can happen when processing a task, and an inner\nmessage pump is created. That inner pump then processes native messages\nwhich could implicitly start an inner task. Inner message pumps are created\nwith dialogs (DialogBox), common dialogs (GetOpenFileName), OLE functions\n(DoDragDrop), printer functions (StartDoc) and *many* others.\n\n```cpp\nSample workaround when inner task processing is needed:\n  HRESULT hr;\n  {\n    CurrentThread::ScopedAllowApplicationTasksInNativeNestedLoop allow;\n    hr = DoDragDrop(...); // Implicitly runs a modal message loop.\n  }\n  // Process |hr| (the result returned by DoDragDrop()).\n```\n\nPlease be SURE your task is reentrant (nestable) and all global variables\nare stable and accessible before using\nCurrentThread::ScopedAllowApplicationTasksInNativeNestedLoop.\n\n## APIs for general use\n\nUser code should hardly ever need to access SequenceManager APIs directly as\nthese are meant for code that deals with scheduling. Instead you should use the\nfollowing:\n\n* base::RunLoop: Drive the SequenceManager from the thread it's bound to.\n\n* base::Thread/SequencedTaskRunner::CurrentDefaultHandle: Post back to the SequenceManager TaskQueues from a task running on it.\n\n* SequenceLocalStorageSlot : Bind external state to a sequence.\n\n* base::CurrentThread : Proxy to a subset of Task related APIs bound to the current thread\n\n* Embedders may provide their own static accessors to post tasks on specific loops (e.g. content::BrowserThreads).\n\n### SingleThreadTaskExecutor and TaskEnvironment\n\nInstead of having to deal with SequenceManager and TaskQueues code that needs a\nsimple task posting environment (one default task queue) can use a\n[SingleThreadTaskExecutor](https://cs.chromium.org/chromium/src/base/task/single_thread_task_executor.h).\n\nUnit tests can use [TaskEnvironment](https://cs.chromium.org/chromium/src/base/test/task_environment.h)\nwhich is highly configurable.\n\n## MessageLoop and MessageLoopCurrent\n\nYou might come across references to MessageLoop or MessageLoopCurrent in the\ncode or documentation. These classes no longer exist and we are in the process\nor getting rid of all references to them. `base::MessageLoopCurrent` was\nreplaced by `base::CurrentThread` and the drop in replacements for\n`base::MessageLoop` are `base::SingleThreadTaskExecutor` and\n`base::Test::TaskEnvironment`.\n"
  },
  {
    "path": "architecture/threading",
    "title": "Threading",
    "content": "# Threading\n\nSuperseded by new [`Threading and Tasks in Chrome`](../threading_and_tasks.md)\nand [`Callback<> and Bind()`](../callback.md) documentation.\n"
  },
  {
    "path": "architecture/startup",
    "title": "Startup",
    "content": "# Startup\n\nChrome is (mostly) shipped as a single executable that knows how to run as all\nthe interesting sorts of processes we use.\n\nHere's an overview of how that works.\n\n1. First there's the platform-specific entry point: `wWinMain()` on Windows,\n   `main()` on Linux.  This lives in `chrome/app/chrome_exe_main_*`.  On Mac and\n   Windows, that function loads modules as described later, while on Linux it\n   does very little, and all of them call into:\n2. `ChromeMain()`, which is the place where cross-platform code that needs to\n   run in all Chrome processes lives.  It lives in `chrome/app/chrome_main*`.\n   For example, here is where we call initializers for modules like logging and\n   ICU.  We then examine the internal `--process-type` switch and dispatch to:\n3. A process-type-specific main function such as `BrowserMain()` (for the outer\n   browser process) or `RendererMain()` (for a tab-specific renderer process).\n\n## Platform-specific entry points\n\n### Windows\n\nOn Windows we build the bulk of Chrome as a DLL.  (XXX: why?)  `wWinMain()`\nloads `chrome.dll`, does some other random stuff (XXX: why?) and calls\n`ChromeMain()` in the DLL.\n\n### Mac\n\nMac is also packaged as a framework and an executable, but they're linked\ntogether: `main()` calls `ChromeMain()` directly.  There is also a second entry\npoint, in\n[`chrome_main_app_mode_mac.mm`](https://cs.chromium.org/chromium/src/chrome/app_shim/chrome_main_app_mode_mac.mm),\nfor app mode shortcuts: \"On Mac, one can't make shortcuts with command-line\narguments. Instead, we produce small app bundles which locate the Chromium\nframework and load it, passing the appropriate\ndata.\"  This executable also calls `ChromeMain()`.\n\n### Linux\n\nOn Linux due to the sandbox we launch subprocesses by repeatedly forking from a\nhelper process.  This means that new subprocesses don't enter through main()\nagain, but instead resume from clones in the middle of startup.  The initial\nlaunch of the helper process still executes the normal startup path, so any\ninitialization that happens in `ChromeMain()` will have been run for all\nsubprocesses but they will all share the same initialization.\n"
  },
  {
    "path": "architecture/sandbox_faq",
    "title": "Sandbox FAQ",
    "content": "# Sandbox FAQ\n\n[TOC]\n\n### What is the sandbox?\n\nThe sandbox is a C++ library that allows the creation of sandboxed processes —\nprocesses that execute within a very restrictive environment. The only resources\nsandboxed processes can freely use are CPU cycles and memory. For example,\nsandboxes processes cannot write to disk or display their own windows. What\nexactly they can do is controlled by an explicit policy. Chromium renderers are\nsandboxed processes.\n\n### What does and doesn't it protect against?\n\nThe sandbox limits the severity of bugs in code running inside the sandbox. Such\nbugs cannot install persistent malware in the user's account (because writing to\nthe filesystem is banned). Such bugs also cannot read and steal arbitrary files\nfrom the user's machine.\n\n(In Chromium, the renderer processes are sandboxed and have this\nprotection. After the NPAPI removal, all remaining plugins are also\nsandboxed. Also note that Chromium renderer processes are isolated from the\nsystem, but not yet from the web. Therefore, domain-based data isolation is not\nyet provided.).\n\nThe sandbox cannot provide any protection against bugs in system components such\nas the kernel it is running on.\n\n### Is the sandbox like what you get with the Java VM?\n\nYeah, kind of... except that to take advantage of the Java sandbox, you must\nrewrite your code to use Java. With our sandbox you can add sandboxing to your\nexisting C/C++ applications. Because the code is not executed inside a virtual\nmachine, you get native speed and direct access to the Windows API.\n\n### Do I need to install a driver or kernel module? Does the user need to be Administrator?\n\nNo and no. The sandbox is a pure user-mode library, and any user can run\nsandboxed processes.\n\n### How can you do this for C++ code if there is no virtual machine?\n\nWe leverage the Windows security model. In Windows, code cannot perform any form\nof I/O (be it disk, keyboard, or screen) without making a system call. In most\nsystem calls, Windows performs some sort of security check. The sandbox sets\nthings up so that these security checks fail for the kinds of actions that you\ndon’t want the sandboxed process to perform. In Chromium, the sandbox is such\nthat all access checks should fail.\n\n### So how can a sandboxed process such as a renderer accomplish anything?\n\nCertain communication channels are explicitly open for the sandboxed processes;\nthe processes can write and read from these channels. A more privileged process\ncan use these channels to do certain actions on behalf of the sandboxed\nprocess. In Chromium, the privileged process is usually the browser process.\n\n### Doesn't Vista have similar functionality? \n\nYes. It's called integrity levels (ILs). The sandbox detects Vista and uses\nintegrity levels, as well. The main difference is that the sandbox also works\nwell under Windows XP. The only application that we are aware of that uses ILs\nis Internet Explorer 7. In other words, leveraging the new Vista security\nfeatures is one of the things that the sandbox library does for you.\n\n### This is very neat. Can I use the sandbox in my own programs?\n\nYes. The sandbox does not have any hard dependencies on the Chromium browser and\nwas designed to be used with other Internet-facing applications. The main hurdle\nis that you have to split your application into at least two interacting\nprocesses. One of the processes is privileged and does I/O and interacts with\nthe user; the other is not privileged at all and does untrusted data processing.\n\n### Isn't that a lot of work?\n\nPossibly. But it's worth the trouble, especially if your application processes\narbitrary untrusted data. Any buffer overflow or format decoding flaw that your\ncode might have won't automatically result in malicious code compromising the\nwhole computer. The sandbox is not a security silver bullet, but it is a strong\nlast defense against nasty exploits.\n\n### Should I be aware of any gotchas?\n\nWell, the main thing to keep in mind is that you should only sandbox code that\nyou fully control or that you fully understand. Sandboxing third-party code can\nbe very difficult. For example, you might not be aware of third-party code's\nneed to create temporary files or display warning dialogs; these operations will\nnot succeed unless you explicitly allow them. Furthermore, third-party\ncomponents could get updated on the end-user machine with new behaviors that you\ndid not anticipate.\n\n### How about COM, Winsock, or DirectX — can I use them?\n\nFor the most part, no. We recommend against using them before lock-down. Once a\nsandboxed process is locked down, use of Winsock, COM, or DirectX will either\nmalfunction or outright fail.\n\n### What do you mean by before _lock-down_? Is the sandboxed process not locked down from the start?\n\nNo, the sandboxed process does not start fully secured. The sandbox takes effect\nonce the process makes a call to the sandbox method `LowerToken()`. This allows\nfor a period during process startup when the sandboxed process can freely get\nhold of critical resources, load libraries, or read configuration files. The\nprocess should call `LowerToken()` as soon as feasible and certainly before it\nstarts interacting with untrusted data.\n\n**Note:** Any OS handle that is left open after calling `LowerToken()` can be\nabused by malware if your process gets infected. That's why we discourage\ncalling COM or other heavyweight APIs; they leave some open handles around for\nefficiency in later calls.\n\n### So what APIs can you call?\n\nThere is no master list of safe APIs. In general, structure your code such that\nthe sandboxed code reads and writes from pipes or shared memory and just does\noperations over this data. In the case of Chromium, the entire WebKit code runs\nthis way, and the output is mostly a rendered bitmap of the web pages. You can\nuse Chromium as inspiration for your own memory- or pipe-based IPC.\n\n### But can't malware just infect the process at the other end of the pipe or shared memory?\n\nYes, it might, if there's a bug there. The key point is that it's easier to\nwrite and analyze a correct IPC mechanism than, say, a web browser\nengine. Strive to make the IPC code as simple as possible, and have it reviewed\nby others.\n"
  },
  {
    "path": "architecture/sandbox",
    "title": "Sandbox",
    "content": "# Sandbox\n\n[TOC]\n\n## Overview\n\nSecurity is one of the most important goals for Chromium. The key to security is\nunderstanding: we can only truly secure a system if we fully understand its\nbehaviors with respect to the combination of all possible inputs in all possible\nstates. For a codebase as large and diverse as Chromium, reasoning about the\ncombined behavior of all its parts is nearly impossible. The sandbox objective\nis to provide hard guarantees about what ultimately a piece of code can or\ncannot do no matter what its inputs are.\n\nSandbox leverages the OS-provided security to allow code execution that cannot\nmake persistent changes to the computer or access information that is\nconfidential. The architecture and exact assurances that the sandbox provides\nare dependent on the operating system. This document covers the Windows\nimplementation as well as the general design. The Linux implementation is\ndescribed [here](../../linux/sandbox/linux/README.md), the OSX implementation\n[here](https://dev.chromium.org/developers/design-documents/sandbox/osx-sandboxing-design).\n\nIf you don't feel like reading this whole document you can read the\n[Sandbox FAQ](sandbox_faq.md) instead. A description of what the sandbox does\nand doesn't protect against may also be found in the FAQ.\n\n## Design principles\n\n*   **Do not re-invent the wheel:** It is tempting to extend the OS kernel with\n    a better security model. Don't. Let the operating system apply its security\n    to the objects it controls. On the other hand, it is OK to create\n    application-level objects (abstractions) that have a custom security model.\n*   **Principle of least privilege:** This should be applied both to the\n    sandboxed code and to the code that controls the sandbox. In other words,\n    the sandbox should work even if the user cannot elevate to super-user.\n*   **Assume sandboxed code is malicious code:** For threat-modeling purposes,\n    we consider the sandbox compromised (that is, running malicious code) once\n    the execution path reaches past a few early calls in the `main()` function.\n    In practice, it could happen as soon as the first external input is\n    accepted, or right before the main loop is entered.\n*   **Be nimble:** Non-malicious code does not try to access resources it cannot\n    obtain. In this case the sandbox should impose near-zero performance impact.\n    It's ok to have performance penalties for exceptional cases when a sensitive\n    resource needs to be touched once in a controlled manner. This is usually\n    the case if the OS security is used properly.\n*   **Emulation is not security:** Emulation and virtual machine solutions do\n    not by themselves provide security. The sandbox should not rely on code\n    emulation, code translation, or patching to provide security.\n\n## Sandbox Windows architecture\n\nThe Windows sandbox is a user-mode only sandbox. There are no special kernel\nmode drivers, and the user does not need to be an administrator in order for the\nsandbox to operate correctly. The sandbox is designed for both 32-bit and 64-bit\nprocesses and has been tested on all Windows OS flavors from Windows 10.\n\nSandbox operates at process-level granularity. Anything that needs to be\nsandboxed needs to live on a separate process. The minimal sandbox configuration\nhas two processes: one that is a privileged controller known as the *broker*,\nand one or more sandboxed processes known as the *target*. Throughout the\ndocumentation and the code these two terms are used with that precise\nconnotation. The sandbox is provided as a static library that must be linked to\nboth the broker and the target executables.\n\n### The broker process\n\nIn Chromium, the broker is always the browser process. The broker, is in broad\nterms, a privileged controller/supervisor of the activities of the sandboxed\nprocesses. The responsibilities of the broker process are:\n\n1.  Specify the policy for each target process\n1.  Spawn the target processes\n1.  Host the sandbox policy engine service\n1.  Host the sandbox interception manager\n1.  Host the sandbox IPC service (to the target processes)\n1.  Perform the policy-allowed actions on behalf of the target process\n\nThe broker should always outlive all the target processes that it spawned. The\nsandbox IPC is a low-level mechanism (different from Chromium's IPC) that is\nused to transparently forward certain Windows API calls from the target to the\nbroker: these calls are evaluated against the policy. The policy-allowed calls\nare then executed by the broker and the results returned to the target process\nvia the same IPC. The job of the interceptions manager is to patch the Windows\nAPI calls that should be forwarded via IPC to the broker.\n\n### The target process\n\nIn Chromium, the renderers are always target processes, unless the\n`--no-sandbox` command line has been specified for the browser process. The\ntarget process hosts all the code that is going to run inside the sandbox, plus\nthe sandbox infrastructure client side:\n\n1.  All code to be sandboxed\n1.  The sandbox IPC client\n1.  The sandbox policy engine client\n1.  The sandbox interceptions\n\nItems 2,3 and 4 are part of the sandbox library that is linked with the code to\nbe sandboxed.\n\nThe interceptions (also known as hooks) are how Windows API calls are forwarded\nvia the sandbox IPC to the broker. It is up to the broker to re-issue the API\ncalls and return the results or simply fail the calls. The interception + IPC\nmechanism does not provide security; it is designed to provide compatibility\nwhen code inside the sandbox cannot be modified to cope with sandbox\nrestrictions. To save unnecessary IPCs, policy is also evaluated in the target\nprocess before making an IPC call, although this is not used as a security\nguarantee but merely a speed optimization.\n\nIt is the expectation that in the future most plugins will run inside a target\nprocess.\n\n![Sandbox Top Level Box Diagram](sandbox_top_diagram.png)\n\n## Sandbox restrictions\n\nAt its core, the sandbox relies on the protection provided by four Windows\nmechanisms:\n\n*   A restricted token\n*   The Windows *job* object\n*   The Windows *desktop* object\n*   Integrity levels\n\nThese mechanisms are highly effective at protecting the OS, its configuration,\nand the user's data provided that:\n\n*   All the securable resources have a better than null security descriptor. In\n    other words, there are no critical resources with misconfigured security.\n*   The computer is not already compromised by malware.\n*   Third party software does not weaken the security of the system.\n\n** Note that extra mitigations above and beyond this base/core will be described\nin the \"Process Mitigations\" section below.\n\n### The token\n\nOne issue that other similar sandbox projects face is how restricted can the\ntoken and job be while still having a properly functioning process. For the\nChromium sandbox, the most restrictive token takes the following form:\n\n#### Regular Groups\n\n*   Logon SID : mandatory\n*   All other SIDs : deny only, mandatory\n\n#### Restricted Groups\n\n*   S-1-0-0 : mandatory\n\n#### Privileges\n\n*   None\n\n#### Integrity\n\n*   Untrusted integrity level label (S-1-16-0x0)\n\nWith the caveats described above, it is near impossible to find an existing\nresource that the OS will grant access with such a token. As long as the disk\nroot directories have non-null security, even files with null security cannot be\naccessed. The Chromium renderer runs with this token, which means that almost\nall resources that the renderer process uses have been acquired by the Browser\nand their handles duplicated into the renderer process.\n\nNote that the token is not derived from anonymous or from the guest token; it is\nderived from the user's token and thus associated to the user logon. As a\nresult, any auditing that the system or the domain has in place can still be\nused.\n\nBy design, the sandbox token cannot protect the non-securable resources such as:\n\n*   Mounted FAT or FAT32 volumes: The security descriptor on them is effectively\n    null. Malware running in the target can read and write to these volumes as\n    long it can guess or deduce their paths.\n*   TCP/IP: The security of TCP/IP sockets in Windows 2000 and Windows XP (but\n    not in Vista) is effectively null. It might be possible for malicious code\n    in the target to send and receive network packets to any host.\n*   Some unlabelled objects, such as anonymous shared memory sections (e.g.\n    [bug 338538](https://crbug.com/338538))\n\nSee NULL DACLs and Other Dangerous ACE Types, *Secure Coding Techniques*,\n195-199 for more information.\n\n### The Job object\n\nThe target process also runs under a Job object. Using this Windows mechanism,\nsome interesting global restrictions that do not have a traditional object or\nsecurity descriptor associated with them are enforced:\n\n*   Forbid per-use system-wide changes using `SystemParametersInfo()`, which can\n    be used to swap the mouse buttons or set the screen saver timeout\n*   Forbid the creation or switch of Desktops\n*   Forbid changes to the per-user display configuration such as resolution and\n    primary display\n*   No read or write to the clipboard\n*   Forbid Windows message broadcasts\n*   Forbid setting global Windows hooks (using `SetWindowsHookEx()`)\n*   Forbid access to the global atoms table\n*   Forbid access to USER handles created outside the Job object\n*   One active process limit (disallows creating child processes)\n\nChromium renderers normally run with all these restrictions active. Each\nrenderer runs in its own Job object. Using the Job object, the sandbox can (but\ncurrently does not) prevent:\n\n*   Excessive use of CPU cycles\n*   Excessive use of memory\n*   Excessive use of IO\n\nMore information about Windows Job Objects can be found\n[here](https://docs.microsoft.com/en-us/windows/desktop/procthread/job-objects).\n\n### The alternate desktop\n\nThe token and the job object define a security boundary: that is, all processes\nwith the same token and in the same job object are effectively in the same\nsecurity context. However, one not-well-understood fact is that applications\nthat have windows on the same desktop are also effectively in the same security\ncontext because the sending and receiving of window messages is not subject to\nany security checks. Sending messages across desktops is not allowed. This is\nthe source of the infamous \"shatter\" attacks, which is why services should not\nhost windows on the interactive desktop. A Windows desktop is a regular kernel\nobject that can be created and assigned a security descriptor.\n\nIn a standard Windows installation, at least two desktops are attached to the\ninteractive window station; the regular (default) desktop, and the logon\ndesktop. The sandbox creates a third desktop that is associated to all target\nprocesses. This desktop is never visible or interactive and effectively isolates\nthe sandboxed processes from snooping the user's interaction and from sending\nmessages to windows operating at more privileged contexts.\n\nThe only disadvantage of an alternate desktop is that it uses approximately 4MB\nof RAM from a separate pool, possibly more on Vista.\n\nMore information about Window Stations\n\n### The integrity levels\n\nIntegrity levels are available on Windows Vista and later versions. They don't\ndefine a security boundary in the strict sense, but they do provide a form of\nmandatory access control (MAC) and act as the basis of Microsoft's Internet\nExplorer sandbox.\n\nIntegrity levels are implemented as a special set of SID and ACL entries\nrepresenting five levels of increasing privilege: untrusted, low, medium, high,\nsystem. Access to an object may be restricted if the object is at a higher\nintegrity level than the requesting token. Integrity levels also implement User\nInterface Privilege Isolation, which applies the rules of integrity levels to\nwindow messages exchanged between different processes on the same desktop.\n\nBy default, a token can read an object of a higher integrity level, but not\nwrite to it. Most desktop applications run at medium integrity (MI), while less\ntrusted processes like Internet Explorer's protected mode and our GPU sandbox\nrun at low integrity (LI), while our renderer processes run at the lowest\nUntrusted integrity level.\n\nA low integrity level token can access only the following shared resources:\n\n*   Read access to most files\n*   Write access to `%USER PROFILE%\\AppData\\LocalLow`\n*   Read access to most of the registry\n*   Write access to `HKEY_CURRENT_USER\\Software\\AppDataLow`\n*   Clipboard (copy and paste for certain formats)\n*   Remote procedure call (RPC)\n*   TCP/IP Sockets\n*   Window messages exposed via `ChangeWindowMessageFilter`\n*   Shared memory exposed via LI (low integrity) labels\n*   COM interfaces with LI (low integrity) launch activation rights\n*   Named pipes exposed via LI (low integrity) labels\n\nWhile an Untrusted integrity level can only write to resources which have a null\nDACL or an explicit Untrusted Mandatory Level.\n\nYou'll notice that the previously described attributes of the token, job object,\nand alternate desktop are more restrictive, and would in fact block access to\neverything allowed in the above list. So, the integrity level is a bit redundant\nwith the other measures, but it can be seen as an additional degree of\ndefense-in-depth, and its use has no visible impact on performance or resource\nusage.\n\nThe integrity level of different Chrome components will change over time as\nfunctionality is split into smaller services. At M75 the browser, crash handler,\nand network utility processes run at Medium integrity, the GPU process at Low\nand most remaining services including isolated renderers at Untrusted.\n\nMore information on integrity levels can be found\n[here](http://msdn.microsoft.com/en-us/library/bb625963.aspx) and in Chapter 7\nof *Windows Internals, Part 1, 7th Ed.*.\n\n### Process mitigation policies\n\nMost process mitigation policies can be applied to the target process by means\nof SetProcessMitigationPolicy. The sandbox uses this API to set various policies\non the target process for enforcing security characteristics.\n\n#### Relocate Images:\n\n*   &gt;= Win8\n*   Address-load randomization (ASLR) on all images in process (and must be\n    supported by all images).\n\n#### Heap Terminate:\n\n*   &gt;= Win8\n*   Terminates the process on Windows heap corruption.\n\n#### Bottom-up ASLR:\n\n*   &gt;= Win8\n*   Sets random lower bound as minimum user address for the process.\n\n#### High-entropy ASLR:\n\n*   &gt;= Win8\n*   Increases randomness range for bottom-up ASLR to 1TB.\n\n#### Strict Handle Checks:\n\n*   &gt;= Win8\n*   Immediately raises an exception on a bad handle reference.\n\n#### Win32k.sys lockdown:\n\n*   &gt;= Win8\n*   `ProcessSystemCallDisablePolicy`, which allows selective disabling of system\n    calls available from the target process.\n*   Renderer processes now have this set to `DisallowWin32kSystemCalls` which\n    means that calls from user mode that are serviced by `win32k.sys` are no\n    longer permitted. This significantly reduces the kernel attack surface\n    available from a renderer. See\n    [here](https://docs.google.com/document/d/1gJDlk-9xkh6_8M_awrczWCaUuyr0Zd2TKjNBCiPO_G4)\n    for more details.\n\n#### Disable Extension Points (legacy hooking):\n\n*   &gt;= Win8\n*   `ProcessExtensionPointDisablePolicy`\n*   The following injection vectors are blocked:\n    *   AppInit DLLs Winsock Layered Service Providers (LSPs)\n    *   Global Window Hooks (not thread-targeted hooks)\n    *   Legacy Input Method Editors (IMEs)\n\n#### Control Flow Guard (CFG):\n\n*   &gt;= Win8.1 Update 3 (KB3000850)\n*   Enabled in all chrome.exe processes. Not compiled into all chrome binaries.\n*   Takes advantage of CFG security in Microsoft system DLLs in our processes.\n*   Compiler/Linker opt-in, not a run-time policy opt-in. See\n    [MSDN](https://msdn.microsoft.com/en-us/library/windows/desktop/mt637065\\(v=vs.85\\).aspx).\n\n#### CET Shadow Stack:\n\n*   Available in Windows 10 2004 December Update.\n*   Is not enabled in the renderer. See\n    [ticket](https://bugs.chromium.org/p/chromium/issues/detail?id=1136224),\n    [MSDN](https://docs.microsoft.com/en-us/cpp/build/reference/cetcompat?view=vs-2019).\n\n#### Disable Font Loading:\n\n*   &gt;= Win10\n*   `ProcessFontDisablePolicy`\n\n#### Disable Loading of Unsigned Code (CIG):\n\n*   &gt;= Win10 TH2\n*   `ProcessSignaturePolicy`\n*   Prevents loading unsigned code into our processes. This means attackers\n    can't just LoadLibrary a DLL after gaining execution (which shouldn't be\n    possible anyway due to other sandbox mitigations), but more importantly,\n    prevents third party DLLs from being injected into our processes, which can\n    affect stability and our ability to enable other security mitigations.\n*   Enabled (post-startup) for all sandboxed child processes.\n*   Enabled (pre-startup) for sandboxed renderer processes. This eliminates a\n    process launch time gap where local injection of improperly signed DLLs into\n    a renderer process could occur.\n*   See\n    [msedgedev blog](https://blogs.windows.com/msedgedev/2017/02/23/mitigating-arbitrary-native-code-execution/)\n    for more background on this mitigation.\n\n#### Disable Image Load from Remote Devices:\n\n*   &gt;= Win10 TH2\n*   `ProcessImageLoadPolicy`\n*   E.g. UNC path to network resource.\n\n#### Disable Image Load of \"mandatory low\" (low integrity level):\n\n*   &gt;= Win10 TH2\n*   `ProcessImageLoadPolicy`\n*   E.g. temporary internet files.\n\n#### Extra Disable Child Process Creation:\n\n*   &gt;= Win10 TH2\n*   If the Job level <= `JOB_LIMITED_USER`, set\n    `PROC_THREAD_ATTRIBUTE_CHILD_PROCESS_POLICY` to\n    `PROCESS_CREATION_CHILD_PROCESS_RESTRICTED` via\n    `UpdateProcThreadAttribute()`.\n*   This is an extra layer of defense, given that Job levels can be broken out\n    of. See also:\n    [ticket](https://bugs.chromium.org/p/project-zero/issues/detail?id=213&redir=1),\n    [Project Zero blog](http://googleprojectzero.blogspot.co.uk/2015/05/in-console-able.html).\n\n#### Disable Dynamic Code (ACG):\n\n*   &gt;= Windows 10 RS1\n*   `ProcessDynamicCodePolicy` - Also known as Arbitrary Code Guard (ACG).\n*   With ACG enabled, the Windows kernel prevents a process from creating and\n    modifying code pages in memory by enforcing that all code pages are\n    immutable and new unsigned code pages cannot be created. This will cause\n    code that attempts to modify or inject into these processes to fail, such as\n    certain attempts to corrupt browser memory, and some third party DLLs.\n*   This is enabled by default for sandboxed service utility processes, and for\n    sandboxed renderer processes that perform no JIT (just-in-time) compilation,\n    and can be enabled for the browser process via the\n    `BrowserDynamicCodeDisabled` feature.\n\n### App Container (low box token):\n\n*   In Windows this is implemented at the kernel level by a Low Box token which\n    is a stripped version of a normal token with limited privilege (normally\n    just `SeChangeNotifyPrivilege` and `SeIncreaseWorkingSetPrivilege`), running\n    at Low integrity level and an array of \"Capabilities\" which can be mapped to\n    allow/deny what the process is allowed to do (see\n    [MSDN](https://msdn.microsoft.com/en-us/library/windows/apps/hh464936.aspx)\n    for a high level description). The capability most interesting from a\n    sandbox perspective is denying is access to the network, as it turns out\n    network checks are enforced if the token is a Low Box token and the\n    `INTERNET_CLIENT` Capability is not present.\n*   The sandbox therefore takes the existing restricted token and adds the Low\n    Box attributes, without granting any Capabilities, so as to gain the\n    additional protection of no network access from the sandboxed process.\n\n### Less Privileged App Container (LPAC)\n\n*   An extension of the App Container (see above) available on later versions of\n    Windows 10 (RS2 and greater), the Less Privileged App Container (LPAC) runs\n    at a lower privilege level than normal App Container, with access granted by\n    default to only those kernel, filesystem and registry objects marked with\n    the `ALL RESTRICTED APPLICATION PACKAGES` or a specific package SID. This is\n    opposed to App Container which uses `ALL APPLICATION PACKAGES`.\n*   A key characteristic of the LPAC is that specific named capabilities can be\n    added such as those based on well known SIDs (defined in\n    [`base/win/sid.h`](https://cs.chromium.org/chromium/src/base/win/sid.h)) or\n    via 'named capabilities' resolved through call to\n    [DeriveCapabilitySidsFromName](https://docs.microsoft.com/en-us/windows/win32/api/securitybaseapi/nf-securitybaseapi-derivecapabilitysidsfromname)\n    which are not really strictly defined anywhere but can be found in various\n    [places](https://social.technet.microsoft.com/Forums/scriptcenter/en-US/3e7d85e3-d0e1-4e79-8141-0bbf8faf3644/windows-10-anniversary-update-the-case-of-the-mysterious-account-sid-causing-the-flood-of-dcom?forum=win10itprosetup)\n    and include capabilities such as:\n    *   `lpacCom`\n    *   `registryRead`\n    *   `lpacWebPlatform`\n    *   `lpacClipboard`\n    *   etc...\n    *   Each LPAC process can have a process-specific SID created for it and\n        this can be used to protect files specific to that particular sandbox,\n        and there can be multiple different overlapping sets of access rights\n        depending on the interactions between services running in different\n        sandboxes.\n\n#### LPAC File System Permissions\n\n*   Importantly, all locations in the filesystem and registry that the LPAC\n    process will access during its lifetime need to have the right ACLs on them.\n    `registryRead` is important for registry read access, and Windows system\n    files have `ALL RESTRICTED APPLICATION PACKAGES` ACE on them already, but\n    other files that the sandbox process needs access to including the binaries\n    (e.g. chrome.exe, chrome.dll) and also any data files need ACLs to be laid\n    down. This is typically done by the installer, and also done automatically\n    for tests. However, if the LPAC sandbox is to be used in other environments\n    then these filesystem permissions need to be manually laid down using\n    `icacls`, the installer, or a similar tool. An example of a ACE that could\n    be used can be found in\n    [`testing/scripts/common.py`](https://cs.chromium.org/chromium/src/testing/scripts/common.py)\n    however in high security environments a more restrictive SID should be used\n    such as one from the\n    [installer](https://source.chromium.org/chromium/chromium/src/+/main:chrome/installer/setup/install_worker.cc;l=74).\n\n### Other caveats\n\nThe operating system might have bugs. Of interest are bugs in the Windows API\nthat allow the bypass of the regular security checks. If such a bug exists,\nmalware will be able to bypass the sandbox restrictions and broker policy and\npossibly compromise the computer. Under Windows, there is no practical way to\nprevent code in the sandbox from calling a system service.\n\nIn addition, third party software, particularly anti-malware solutions, can\ncreate new attack vectors. The most troublesome are applications that inject\ndlls in order to enable some (usually unwanted) capability. These dlls will also\nget injected in the sandbox process. In the best case they will malfunction, and\nin the worst case can create backdoors to other processes or to the file system\nitself, enabling specially crafted malware to escape the sandbox.\n\n## Sandbox policy\n\nThe actual restrictions applied to a target process are configured by a policy.\nThe policy is just a programmatic interface that the broker calls to define the\nrestrictions and allowances. Four functions control the restrictions, roughly\ncorresponding to the four Windows mechanisms:\n\n*   `TargetPolicy::SetTokenLevel()`\n*   `TargetPolicy::SetJobLevel()`\n*   `TargetPolicy::SetIntegrityLevel()`\n*   `TargetPolicy::SetDesktop()`\n\nThe first three calls take an integer level parameter that goes from very strict\nto very loose; for example, the token level has 7 levels and the job level has 5\nlevels. Chromium renderers are typically run with the most strict level in all\nfour mechanisms. Finally, the last (desktop) policy is binary and can only be\nused to indicate if a target is run on an alternate desktop or not.\n\nThe restrictions are by design coarse in that they affect all securable\nresources that the target can touch, but sometimes a more finely-grained\nresolution is needed. The policy interface allows the broker to specify\nexceptions. An exception is a way to take a specific Windows API call issued in\nthe target and proxy it over to the broker. The broker can inspect the\nparameters and re-issue the call as is, re-issue the call with different\nparameters, or simply deny the call. To specify exceptions there is a single\ncall: `AddRule`. The following kinds of rules for different Windows subsystems\nare supported at this time:\n\n*   Files\n*   Named pipes\n*   Process creation\n*   Registry\n*   Synchronization objects\n\nThe exact form of the rules for each subsystem varies, but in general rules are\ntriggered based on a string pattern. For example, a possible file rule is:\n\n```\nAddRule(SUBSYS_FILES, FILES_ALLOW_READONLY, L\"c:\\\\temp\\\\app_log\\\\d*.dmp\")\n```\n\nThis rule specifies that access will be granted if a target wants to open a\nfile, for read-only access as long as the file matches the pattern expression;\nfor example `c:\\temp\\app_log\\domino.dmp` is a file that satisfies the pattern.\nConsult the header files for an up-to-date list of supported objects and\nsupported actions.\n\nRules can only be added before each target process is spawned, and cannot be\nmodified while a target is running, but different targets can have different\nrules.\n\n### Diagnostics\n\nIn Chromium, the policies associated with active processes can be viewed at\nchrome://sandbox. Tracing of the `sandbox` category will output the policy used\nwhen a process is launched. Tracing can be enabled using chrome://tracing or by\nusing the `--trace-startup=-*,disabled-by-default-sandbox` command line flag.\nTrace output can be investigated with `//tools/win/trace-sandbox-viewer.py`.\n\n## Target bootstrapping\n\nTargets do not start executing with the restrictions specified by policy. They\nstart executing with a token that is very close to the token the regular user\nprocesses have. The reason is that during process bootstrapping the OS loader\naccesses a lot of resources, most of them are actually undocumented and can\nchange at any time. Also, most applications use the standard CRT provided with\nthe standard development tools; after the process is bootstrapped the CRT needs\nto initialize as well and there again the internals of the CRT initialization\nare undocumented.\n\nTherefore, during the bootstrapping phase the process actually uses two tokens:\nthe lockdown token which is the process token as is and the initial token which\nis set as the impersonation token of the initial thread. In fact the actual\n`SetTokenLevel` definition is:\n\n```\nSetTokenLevel(TokenLevel initial, TokenLevel lockdown)\n```\n\nAfter all the critical initialization is done, execution continues at `main()`\nor `WinMain()`, here the two tokens are still active, but only the initial\nthread can use the more powerful initial token. It is the target's\nresponsibility to discard the initial token when ready. This is done with a\nsingle call:\n\n```\nLowerToken()\n```\n\nAfter this call is issued by the target the only token available is the lockdown\ntoken and the full sandbox restrictions go into effect. The effects of this call\ncannot be undone. Note that the initial token is a impersonation token only\nvalid for the main thread, other threads created in the target process use only\nthe lockdown token and therefore should not attempt to obtain any system\nresources subject to a security check.\n\nThe fact that the target starts with a privileged token simplifies the explicit\npolicy since anything privileged that needs to be done once, at process startup\ncan be done before the `LowerToken()` call and does not require to have rules in\nthe policy.\n\n**Important**\n\nMake sure any sensitive OS handles obtained with the initial token are closed\nbefore calling LowerToken(). Any leaked handle can be abused by malware to\nescape the sandbox.\n"
  },
  {
    "path": "architecture/render-pipeline",
    "title": "Render Pipeline",
    "content": "# Render Pipeline\r\n\r\nChromium's render pipeline transforms HTML, CSS and JavaScript into pixels on your screen. In this article we'll cover each major stage, the threads involved, and how Chromium optimizes for smooth, high-performance rendering in modern versions (v134+).\r\n\r\n---\r\n\r\n## 1. Overview & Motivation\r\n\r\n- **Goals**  \r\n  - **Speed**: maintain 60 FPS (or higher) on modern devices with 120Hz+ displays  \r\n  - **Smoothness**: avoid jank by minimizing main-thread work per frame  \r\n  - **Efficiency**: only repaint and composite what changed using advanced optimization techniques  \r\n  - **Responsiveness**: prioritize user interactions and critical rendering paths\r\n\r\n- **Key Processes**  \r\n  - **Browser Process**: coordinates navigation, input, and process management  \r\n  - **Renderer Process**: handles parsing, style computation, layout, and paint preparation  \r\n  - **GPU Process**: manages compositing, rasterization, and hardware acceleration via Viz  \r\n\r\n- **Modern Architecture (v134+)**  \r\n  - **Viz Display Compositor**: unified GPU-accelerated compositing architecture  \r\n  - **SkiaRenderer**: advanced Skia-based rendering backend  \r\n  - **Out-of-Process Rasterization (OOP-R)**: rasterization moved to GPU process  \r\n  - **Canvas2D in GPU Process**: hardware-accelerated canvas rendering\r\n\r\n*(Link back to [Architecture → Process Model](process-model.md) for IPC & sandbox context.)*\r\n\r\n---\r\n\r\n## 2. Stage 1 – Document Parsing & DOM Construction\r\n\r\n1. **HTML Tokenizer**  \r\n   - Splits raw bytes into tokens using streaming parser  \r\n   - Supports incremental parsing for progressive rendering  \r\n\r\n2. **DOM Tree Builder**  \r\n   - Builds a tree of `Node` objects from tokens  \r\n   - Handles `<script>` tags: may pause parsing for execution  \r\n   - Uses **script streaming** for async script compilation  \r\n\r\n3. **Preload Scanner**  \r\n   - Speculatively discovers resources during parsing  \r\n   - Enables early resource loading for better performance  \r\n\r\n4. **Progressive Rendering**  \r\n   - Allows rendering to begin before full document parse  \r\n   - Critical for perceived performance on large pages  \r\n\r\n```text\r\nHTML Bytes → [Preload Scanner] → Resource Discovery\r\n     ↓\r\n[Tokenizer] → Tokens → [Parser] → DOM Tree\r\n```\r\n\r\n---\r\n\r\n## 3. Stage 2 – CSS Style Resolution & Computed Styles\r\n\r\n### CSSOM Construction\r\n- **CSS Tokenizer**: Parses stylesheets into CSSOM tree  \r\n- **Rule Matching**: Optimized selector matching against DOM nodes  \r\n- **Cascade Resolution**: Handles specificity, inheritance, and !important rules  \r\n\r\n### Style Computation (Blink StyleEngine)\r\n- **Computed Style Calculation**: Resolves all CSS properties to computed values  \r\n- **CSS Container Queries**: Modern layout feature support (v134+)  \r\n- **CSS Cascade Layers**: Advanced cascade control mechanisms  \r\n- **Style Invalidation**: Efficiently updates styles when changes occur  \r\n\r\n### Modern CSS Features (v134+)\r\n- **CSS Grid subgrid**: Advanced grid layout capabilities  \r\n- **CSS View Transitions**: Smooth page transitions  \r\n- **CSS Color Level 4**: Extended color spaces and functions  \r\n- **CSS Logical Properties**: Writing-mode aware properties  \r\n\r\n```text\r\nCSS → [Parser] → CSSOM → [Style Engine] → Computed Styles\r\n                    ↓\r\nDOM + CSSOM → [Style Resolver] → Styled Elements\r\n```\r\n\r\n---\r\n\r\n## 4. Stage 3 – Layout (Reflow) & Modern Layout Engines\r\n\r\n### Box Tree Construction\r\n- **LayoutObject Tree**: Wraps styled nodes into layout boxes  \r\n- **Modern Layout**: Support for Flexbox, Grid, and Container Queries  \r\n- **NG Layout Engine**: Next-generation layout system for better performance  \r\n\r\n### Layout Computation\r\n- **Flow & Positioning**: Box model, floats, positioning schemes  \r\n- **Fragmentation**: Multi-column, CSS regions, and printing support  \r\n- **Intrinsic Sizing**: Content-based sizing calculations  \r\n- **Layout Containment**: Performance optimizations through CSS containment  \r\n\r\n### Threading & Performance\r\n- **Main Thread**: Primary layout computation  \r\n- **Layout Shift Prevention**: Core Web Vitals optimization  \r\n- **Incremental Layout**: Only relayout affected subtrees  \r\n\r\n**Output**: LayoutObject tree with precise geometry and positioning\r\n\r\n```text\r\nStyled Elements → [NG Layout] → LayoutObject Tree with Geometry\r\n```\r\n\r\n---\r\n\r\n## 5. Stage 4 – Paint Preparation & Recording\r\n\r\n### Paint Operation Generation\r\n- **Paint Records**: Translates LayoutObjects into Skia drawing operations  \r\n- **Display Lists**: Serializes paint commands for efficient replay  \r\n- **Paint Worklets**: CSS Paint API support for custom rendering  \r\n\r\n### Layer Creation & Compositing Decisions\r\n- **Compositing Triggers**: CSS transforms, opacity, filters, will-change  \r\n- **Layer Tree**: Organizes content into compositable layers  \r\n- **Paint Invalidation**: Tracks which regions need repainting  \r\n- **Backdrop Filters**: Advanced filter effects on layer content  \r\n\r\n### Modern Paint Features (v134+)\r\n- **Variable Fonts**: Advanced typography support  \r\n- **CSS Color Spaces**: P3, Rec2020, and other wide-gamut colors  \r\n- **Advanced Filters**: CSS filter effects and backdrop-filter  \r\n\r\n**Key Classes**: `PaintController`, `DisplayItemList`, `PaintLayer`\r\n\r\n```text\r\nLayoutObjects → [Paint] → Display Lists → Layer Tree\r\n```\r\n\r\n---\r\n\r\n## 6. Stage 5 – Rasterization & GPU Acceleration\r\n\r\n### Out-of-Process Rasterization (OOP-R)\r\n- **GPU Process Raster**: Rasterization moved from renderer to GPU process  \r\n- **Vulkan Backend**: Modern graphics API support on supported platforms  \r\n- **Metal Backend**: macOS hardware acceleration  \r\n- **Performance Benefits**: Reduced memory usage and improved parallelism  \r\n\r\n### Tile-Based Rendering\r\n- **Raster Tiles**: Large layers split into manageable tiles  \r\n- **Tile Prioritization**: Visible tiles rendered first  \r\n- **Tile Caching**: Intelligent reuse of unchanged tile content  \r\n- **GPU Texture Management**: Efficient GPU memory allocation  \r\n\r\n### Modern Rasterization Features\r\n- **SkiaRenderer**: Advanced Skia-based rendering backend  \r\n- **Hardware-accelerated Canvas**: Canvas2D operations in GPU process  \r\n- **WebGL Integration**: Seamless 3D content integration  \r\n\r\n**Output**: GPU textures and rasterized tile bitmaps\r\n\r\n```text\r\nDisplay Lists → [OOP Raster] → GPU Textures → Tile Cache\r\n```\r\n\r\n---\r\n\r\n## 7. Stage 6 – Compositing & Display via Viz\r\n\r\n### Viz Display Compositor Architecture\r\n- **Unified Compositing**: Single compositor for all content types  \r\n- **Surface Aggregation**: Combines surfaces from multiple sources  \r\n- **Damage Tracking**: Precise tracking of changed regions  \r\n- **Frame Synchronization**: Coordinated frame submission across processes  \r\n\r\n### GPU Process Composition\r\n- **CompositorFrameSink**: Interface for submitting compositor frames  \r\n- **Surface Hierarchy**: Nested surface management for complex layouts  \r\n- **Display Transform**: Handle device rotation and scaling  \r\n- **HDR Support**: High dynamic range content rendering  \r\n\r\n### Present & VSync Integration\r\n- **Frame Scheduling**: Intelligent frame timing based on display capabilities  \r\n- **Variable Refresh Rate**: Support for adaptive sync displays  \r\n- **Frame Pacing**: Optimized frame submission for smooth animation  \r\n- **Multi-Display**: Synchronized rendering across multiple screens  \r\n\r\n**Output**: Smooth, synchronized frames displayed to screen\r\n\r\n```text\r\nLayer Tree → [Viz Compositor] → GPU → Display Hardware\r\n```\r\n\r\n---\r\n\r\n## 8. Threading & Modern Pipeline Architecture\r\n\r\n### Thread Responsibilities (v134+)\r\n\r\n| Thread | Work | Modern Enhancements |\r\n|--------|------|-------------------|\r\n| **Main** | DOM, CSSOM, style, layout, paint commands | Script streaming, lazy loading |\r\n| **Compositor** | Layer tree updates, IPC to GPU process | Viz integration, surface management |\r\n| **Raster** | Display list → GPU textures (OOP-R) | GPU process rasterization |\r\n| **GPU** | Texture uploads, draw calls, compositing | SkiaRenderer, Vulkan/Metal support |\r\n| **IO** | Network, file operations | Parallel resource loading |\r\n| **Worker** | Web Workers, Service Workers | Off-main-thread execution |\r\n\r\n### Performance Optimizations\r\n- **Frame Pipelining**: Chromium overlaps raster & GPU work across frames to maximize throughput\r\n- **Predictive Loading**: Anticipate user actions for better responsiveness  \r\n- **Priority-based Scheduling**: Critical rendering path optimization\r\n- **Concurrent Processing**: Multi-threaded execution where possible\r\n\r\n---\r\n\r\n## 9. Modern Optimizations & Techniques (v134+)\r\n\r\n### Rendering Optimizations\r\n- **Partial Invalidation**: Only repaint precisely changed regions using damage tracking\r\n- **Occlusion Culling**: Skip rendering completely hidden content\r\n- **Content Visibility**: CSS content-visibility for performance gains\r\n- **Container Queries**: Efficient responsive design without layout thrashing\r\n\r\n### GPU & Memory Optimizations  \r\n- **Zero-Copy Paths**: Direct GPU texture for video, WebGL, and canvas content\r\n- **Memory Pressure Handling**: Intelligent texture eviction under memory constraints\r\n- **Shared GPU Memory**: Efficient cross-process texture sharing\r\n- **Tile Prioritization**: Render visible content first, defer off-screen tiles\r\n\r\n### Scrolling & Animation\r\n- **Compositor-Only Scrolling**: Smooth scrolling without main thread involvement\r\n- **Transform Animations**: GPU-accelerated CSS transforms and animations\r\n- **Scroll Anchoring**: Prevent layout shifts during dynamic content loading\r\n- **Paint Holding**: Minimize flash of unstyled content (FOUC)\r\n\r\n### Modern Web Features\r\n- **Canvas OffscreenCanvas**: Multi-threaded canvas rendering\r\n- **WebAssembly Integration**: Optimized WASM execution in rendering pipeline\r\n- **WebGPU Support**: Next-generation graphics API integration\r\n- **CSS Containment**: Isolation boundaries for performance optimization\r\n\r\n---\r\n\r\n## 10. Debugging & Performance Analysis (v134+)\r\n\r\n### Chrome DevTools Integration\r\n- **Performance Panel**: Detailed flame graphs with modern metrics\r\n- **Rendering Tab**: Layer visualization, paint flashing, and layout shift detection\r\n- **Core Web Vitals**: LCP, FID, CLS measurement and optimization guidance\r\n- **Memory Panel**: GPU memory usage and texture analysis\r\n\r\n### Command Line Debugging\r\n```bash\r\n# Modern GPU debugging flags\r\n--enable-gpu-rasterization          # Force GPU rasterization\r\n--enable-vulkan                     # Use Vulkan backend (where supported)\r\n--disable-gpu-sandbox               # Disable GPU process sandbox (debug only)\r\n--show-composited-layer-borders     # Visualize compositing layers\r\n--show-paint-rects                  # Highlight repainted regions\r\n--enable-logging=stderr             # Detailed logging output\r\n\r\n# Performance analysis\r\n--trace-startup                     # Profile startup performance\r\n--no-sandbox                       # Disable sandboxing (debug builds only)\r\n```\r\n\r\n### Chrome Internal Pages\r\n- **chrome://gpu/**: GPU capabilities and feature status\r\n- **chrome://tracing/**: Advanced performance tracing with timeline visualization\r\n- **chrome://histograms/**: Detailed performance metrics and histograms\r\n- **chrome://memory-internals/**: Memory usage breakdown by process\r\n- **chrome://discards/**: Tab lifecycle and memory pressure information\r\n\r\n### Blink Rendering Metrics\r\n- **First Contentful Paint (FCP)**: Time to first visible content\r\n- **Largest Contentful Paint (LCP)**: Time to largest content element\r\n- **Cumulative Layout Shift (CLS)**: Visual stability measurement\r\n- **First Input Delay (FID)**: Input responsiveness metric\r\n\r\n---\r\n\r\n## 11. Next Steps & Further Reading\r\n\r\n### Advanced Topics\r\n- **[Browser Components](browser-components.md)**: Cross-process services and architecture\r\n- **[Storage & Cache](../modules/storage-cache.md)**: How caching integrates with rendering pipeline\r\n- **[Security Model](../security/security-model.md)**: Sandboxing and process isolation details\r\n\r\n### Experimental Features (v134+)\r\n- **Document Transition API**: Smooth page transitions with shared element animations\r\n- **CSS Anchor Positioning**: Advanced positioning relative to other elements\r\n- **WebGPU Integration**: Next-generation graphics API for web applications\r\n- **Advanced Typography**: Variable fonts and OpenType features\r\n\r\n### Performance Optimization Resources\r\n- **Web Performance Working Group**: Latest standards and best practices\r\n- **Chrome Platform Status**: Track new rendering features and their implementation status\r\n- **Lighthouse CI**: Automated performance testing and Core Web Vitals monitoring\r\n\r\n### Hands-On Experiments\r\n```bash\r\n# Try modern GPU acceleration\r\n--enable-gpu-rasterization --enable-vulkan\r\n\r\n# Profile rendering performance\r\nchrome://tracing/ with \"Rendering\" category enabled\r\n\r\n# Measure Core Web Vitals\r\nDevTools → Lighthouse → Performance audit\r\n\r\n# Visualize rendering pipeline\r\nDevTools → Rendering → Paint flashing + Layer borders\r\n```\r\n\r\n---\r\n\r\n**End of Modern Render Pipeline Guide**\r\n\r\n### Key Changes in v134+\r\n- **Viz Display Compositor**: Unified GPU-accelerated compositing\r\n- **Out-of-Process Rasterization**: Improved performance and stability\r\n- **SkiaRenderer**: Advanced graphics rendering backend\r\n- **Modern CSS Support**: Container queries, cascade layers, color spaces\r\n- **Enhanced Performance Tools**: Better debugging and optimization capabilities\r\n\r\n**Notes for Developers:**\r\n- Monitor Chrome Platform Status for latest rendering features\r\n- Use modern CSS containment for performance optimization\r\n- Leverage GPU acceleration through proper layer promotion\r\n- Profile regularly with DevTools Performance panel and Core Web Vitals metrics\r\n"
  },
  {
    "path": "architecture/README",
    "title": "Chromium Design Docs",
    "content": "# Chromium Design Docs\n\nThis directory contains chromium project documentation in\n[Gitiles-flavored Markdown](https://gerrit.googlesource.com/gitiles/+/master/Documentation/markdown.md).\nIt is automatically\n[rendered by Gitiles](https://chromium.googlesource.com/chromium/src/+/main/docs/).\n\nDocuments here have been imported\nfrom [the Project site](https://www.chromium.org/developers/design-documents).\nAs of this writing, the vast majority of docs have not been imported yet.\n\n* [Sandboxing](sandbox.md) - The Sandboxing architecture, and Windows\n  implementation of sandboxing.\n* [Sandboxing FAQ](sandbox_faq.md) - Frequently asked questions about Chromium\n  sandboxing.\n* [Startup](startup.md) - How browser processes starts up, on different\n  platforms.\n* [Threading](threading.md) - Preferred ways to use threading, and library\n  support for concurrency.\n* [GPU Synchronization](gpu_synchronization.md) - Mechanisms for sequencing\n  GPU drawing operations across contexts or processes.\n"
  },
  {
    "path": "architecture/process_model_and_site_isolation",
    "title": "Process Model and Site Isolation",
    "content": "# Process Model and Site Isolation\n\nAs the early Web matured, web sites evolved from simple documents to active\nprograms, changing the web browser's role from a simple document renderer to an\noperating system for programs. Modern browsers like Chromium use multiple\noperating system processes to manage this workload, improving stability,\nsecurity, and performance.\n\nChromium's **process model** determines how documents, workers, and other web\ncontent are divided into processes. First, the process model must identify\nwhich parts of a \"program\" on the web need to coexist in a single process.\nSomewhat surprisingly, a program on the web is not a single document plus its\nsubresources, but rather a group of same (or similar) origin documents that can\nfully access each other's contents. Once these atomic groups are defined, the\nprocess model can then decide which groups will share a process. These\ndecisions can be tuned based on platform, available resources, etc, to achieve\nthe right level of isolation for different scenarios.\n\nThis document outlines the goals and design of Chromium's process model and the\nvarious ways it is used today, including its support for Site Isolation.\n\n[TOC]\n\n\n## Goals\n\nAt a high level, Chromium aims to use separate processes for different instances\nof web sites when possible. A **web site instance** is a group of documents or\nworkers that must share a process with each other to support their needs, such\nas cross-document scripting. (This roughly corresponds to an \"[agent\ncluster](https://html.spec.whatwg.org/multipage/webappapis.html#integration-with-the-javascript-agent-cluster-formalism)\"\nfrom the HTML Standard, as described below.)\n\nFor stability, putting web site instances in separate processes limits the\nimpact of a renderer process crash or hang, allowing other content to continue\nworking. For performance, this allows different web site instances to run in\nparallel with better responsiveness, at the cost of some memory overhead for\neach process.\n\nFor security, strictly using separate processes for different web sites allows\nsignificantly stronger defenses against malicious web sites. In addition to\nrunning web content within a low-privilege\n[sandbox](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/design/sandbox.md)\nthat limits an attacker's access to the user's machine, Chromium's\nmulti-process architecture can support [Site\nIsolation](https://www.chromium.org/Home/chromium-security/site-isolation),\nwhere each renderer process is only allowed to access data from a single site.\nSite Isolation involves:\n\n* **Locked Renderer Processes**: A renderer process can be limited to documents\n    and workers from a single web site or origin, even if such documents are in\n    iframes.\n* **Browser-Enforced Restrictions**: The privileged browser process can monitor\n    IPC messages from renderer processes to limit their actions or access to\n    site data (e.g., using ChildProcessSecurityPolicy::CanAccessDataForOrigin).\n    This [prevents compromised renderer\n    processes](https://chromium.googlesource.com/chromium/src/+/main/docs/security/compromised-renderers.md)\n    from asking for cross-site data, using permissions granted to other sites,\n    etc. These restrictions take two main forms:\n  * _\"Jail\" checks_: Ensure that a process locked to a particular site can only\n      access data belonging to that site. If all processes are locked, this is\n      sufficient protection.\n  * _\"Citadel\" checks_: Ensure that unlocked processes cannot access data\n      for sites that require a dedicated process. This adds protection in cases\n      where full Site Isolation is not available, such as Android.\n* **Network Response Limitations**: Chromium can ensure that locked renderer\n    processes are only allowed to receive sensitive data (e.g., HTML, XML,\n    JSON) from their designated site or origin, while still allowing\n    cross-origin subresource requests (e.g., images, media) as needed for\n    compatibility. This is achieved using [Cross-Origin Read\n    Blocking](https://www.chromium.org/Home/chromium-security/corb-for-developers)\n    (CORB) or [Opaque Response Blocking](https://github.com/annevk/orb) (ORB).\n\n\n## Abstractions and Implementations\n\nChromium uses several abstractions to track which documents and workers need\nsynchronous access to each other, as a constraint for process model decisions.\n\n* **Security Principal** (implemented by\n    [SiteInfo](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/site_info.h;drc=c79153d6f931dbe2ce2c992962512eaca6766623;l=22)):\n    In security terminology, a **principal** is an entity with certain\n    privileges. Chromium associates a security principal with execution\n    contexts (e.g., documents, workers) to track which data their process is\n    allowed to access. This principal is typically a\n    \"[site](https://html.spec.whatwg.org/multipage/origin.html#site)\" (i.e.,\n    scheme plus eTLD+1, such as `https://example.com`), because web pages can\n    modify their document.domain value to access other same-site documents, and\n    not just same-origin documents. In some cases, though, the principal may be\n    an origin or have a coarser granularity (e.g., `file:`). The SiteInfo class\n    tracks all values that identify a security principal.\n\n* **Principal Instance** (implemented by\n    [SiteInstance](https://source.chromium.org/chromium/chromium/src/+/main:content/public/browser/site_instance.h;drc=858df4ab8b73f2418f51385954760f2154512029;l=32)):\n    A principal instance is the core unit of Chromium's process model. Any two\n    documents with the same principal in the same browsing context group\n    (see below) must live in the same process, because they have synchronous\n    access to each other's content. This access includes cross-document\n    scripting and synchronous communication through shared memory (e.g.,\n    SharedArrayBuffer). If such documents were in different processes, data\n    races or deadlocks would occur if they concurrently accessed objects in\n    their shared DOM or JavaScript heaps.\n\n    This roughly corresponds to the [agent\n    cluster](https://html.spec.whatwg.org/multipage/webappapis.html#integration-with-the-javascript-agent-cluster-formalism)\n    concept in the spec, although they do not match exactly: multiple agent\n    clusters may sometimes share a principal instance (e.g., with `data:` URLs\n    in the same principal instance as their creator), and principals may keep\n    track of more factors than [agent cluster\n    keys](https://html.spec.whatwg.org/multipage/webappapis.html#agent-cluster-key)\n    (e.g., whether the StoragePartition differs).\n\n    Note that the user may visit multiple instances of a given principal in the\n    browser, sometimes in unrelated tabs (i.e., separate browsing context\n    groups). These separate instances do not need synchronous access to each\n    other and can safely run in separate processes.\n\n* **Browsing Context Group** (implemented by\n    [BrowsingInstance](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/browsing_instance.h;drc=df269acf8de952b68b2fbec49365457ff1f6266b;l=34)):\n    A browsing context group is a group of tabs and frames (i.e., containers of\n    documents) that have references to each other (e.g., frames within the same\n    page, popups with window.opener references, etc). Any two documents within\n    a browsing context group may find each other by name, so it is important\n    that any same-principal documents in the group live in the same process. In\n    other words, there is only one principal instance per principal in a given\n    browsing context group. Note that a tab may change its browsing context\n    group on some types of navigations (e.g., due to a\n    Cross-Origin-Opener-Policy header, browser-initiated cross-site\n    navigations, and other reasons).\n\nFrom an implementation perspective, Chromium keeps track of the SiteInstance of\neach RenderFrameHost, to determine which renderer process to use for the\nRenderFrameHost's documents. SiteInstances are also tracked for workers, such\nas ServiceWorker or SharedWorkerHost.\n\n\n## Modes and Availability\n\n### Full Site Isolation (site-per-process)\n\n_Used on: Desktop platforms (Windows, Mac, Linux, ChromeOS)._\n\nIn (one-)site-per-process mode, each process is locked to documents from a\nsingle site. Sites are defined as scheme plus eTLD+1, since different origins\nwithin a given site may have synchronous access to each other if they each\nmodify their document.domain. This mode provides all sites protection against\ncompromised renderers and Spectre-like attacks, without breaking backwards\ncompatibility.\n\nThis mode can be enabled on Android using\n`chrome://flags/#enable-site-per-process`.\n\n\n### Partial Site Isolation\n\n_Used on: Chrome for Android (2+ GB RAM)._\n\nOn platforms like Android with more significant resource constraints, Chromium\nonly uses dedicated (locked) processes for some sites, putting the rest in\nunlocked processes that can be used for any web site. (Note that there is a\nthreshold of about 2 GB of device RAM required to support any level of Site\nIsolation on Android.)\n\nLocked processes are only allowed to access data from their own site. Unlocked\nprocesses can generally access data from any site that does not require a\nlocked process. Chromium usually creates one unlocked process per browsing\ncontext group.\n\nCurrently, several heuristics are used to isolate the sites that are most likely\nto have user-specific information. As on all platforms, privileged pages like\nWebUI are always isolated. Chromium also isolates sites that users tend to log\ninto in general, as well as sites on which a given user has entered a password,\nlogged in via an OAuth provider, or encountered a Cross-Origin-Opener-Policy\n(COOP) header.\n\n\n### No Site Isolation\n\n_Used on: Low-memory Chrome for Android (<2 GB RAM), Android WebView, Chrome for\niOS._\n\nOn some platforms, Site Isolation is not available, due to implementation or\nresource constraints.\n\n* On Android devices with less than 2 GB of RAM, Site Isolation is disabled to\n  avoid requiring multiple renderer processes in a given tab (for out-of-process\n  iframes). Cross-process navigations in the main frame are still possible\n  (e.g., for browser-initiated cross-site navigations with no other pages in the\n  browsing context group, when a new browsing context group may be created).\n* Android WebView does not yet support multiple renderer processes or\n  out-of-process iframes.\n* Chrome for iOS uses WebKit, which does not currently have support for\n  out-of-process iframes or Site Isolation.\n\n\n### Origin Isolation\n\n_Available on: Desktop platforms, Chrome for Android (2+ GB RAM)._\n\nThere are several optional ways to lock processes at an origin granularity\nrather than a site granularity, with various tradeoffs for compatibility\n(e.g., breaking pages that modify document.domain). These are available on\nplatforms that support some level of Site Isolation.\n\n* **Built-in**: //content embedders can designate particular origins that\n    require isolation from the rest of their site, using\n    ContentBrowserClient::GetOriginsRequiringDedicatedProcess.\n* **Configurable**: Users and administrators can list particular origins that\n    should be isolated from the rest of their site, using the command line\n    (`--isolate-origins=`...), `chrome://flags#isolate-origins`, or\n    [enterprise policy](https://support.google.com/chrome/a/answer/7581529)\n    ([IsolateOrigins](https://chromeenterprise.google/policies/#IsolateOrigins)\n    or\n    [IsolateOriginsAndroid](https://chromeenterprise.google/policies/#IsolateOriginsAndroid)).\n    It is also possible to isolate all origins (except those that opt-out) using\n    `chrome://flags/#origin-keyed-processes-by-default`.\n* **Opt-in**: The [Origin-Agent-Cluster](https://web.dev/origin-agent-cluster)\n    HTTP response header can be used by web developers to hint to the browser\n    that an origin locked process can be used. This is not a security guarantee\n    and may not always be honored (e.g., to keep all same-origin documents\n    consistent within a given browsing context group), though it allows finer\n    grained isolation in the common case. Note that\n    [Origin-Agent-Cluster is now enabled by default](https://github.com/mikewest/deprecating-document-domain),\n    effectively disabling changes to document.domain unless an OAC opt-out\n    header is used.\n\n\n### CrossOriginIsolated\n\nCertain powerful web platform features now require an opt-in\n[CrossOriginIsolated](https://web.dev/coop-coep/) mode, which ensures that all\ncross-origin content (e.g., documents and workers, as well as subresources like\nmedia or scripts) consents to being loaded in the same process as an origin\nusing these features. This opt-in is required because these powerful features\n(e.g., SharedArrayBuffers) can be used for very precise timing, which can make\nattacks that leak data from the process (e.g., using Spectre or other transient\nexecution attacks) more effective. This mode is important because not all\nbrowsers support out-of-process iframes for cross-origin documents, and not all\ncross-origin subresources can be put in a separate process.\n\nCrossOriginIsolated mode requires the main document to have\nCross-Origin-Opener-Policy and Cross-Origin-Embedder-Policy headers. These\nheaders impose restrictions on all content that may load within the page or\nprocess (e.g., requiring similar headers on subframes, and CORS, CORP, or a\ncredentialless mode for subresources).\n\n\n### Historical Modes\n\nBefore Site Isolation was introduced, Chromium initially supported a few other\nprocess models that affected the number of renderer processes.\n\n* **Process-per-site-instance**: This model was the default when Chromium first\n    launched. It used a new process when navigating to a different site in some\n    scenarios (e.g., via the address bar but not link clicks), as well as when\n    visiting different instances of the same site in different tabs. At the\n    time, cross-site subframes stayed in the same process as their parent\n    frames.\n* **Process-per-site**: This model consolidated all instances of a given site\n    into a single process (per profile), to reduce the process count. It\n    generally led to poor usability when a single process was used for too many\n    tabs. This mode is still used for certain limited cases (e.g., the New Tab\n    Page) to reduce the process count and process creation latency. It is also\n    used for extensions to allow synchronous scripting from a background page.\n    Note that having a single process for a site might not be guaranteed (e.g.,\n    due to multiple profiles, or races).\n* **Process-per-tab**: This model used a separate process for each browsing\n    context group (i.e., possibly multiple related tabs), but did not attempt\n    to switch processes on cross-site navigations. In practice, though, this\n    model still needed to swap processes for privileged pages like `chrome://`\n    URLs.\n* **Single process**: Chromium also allows a single process model which runs all\n    of the browser and renderer code in a single OS process. This is generally\n    not a safe or robust process model, since it prevents the use of the\n    sandbox and cannot survive any crash in renderer process code. It is mainly\n    used for older low-resource Android WebView scenarios, and for debugging or\n    testing.\n\n\n## Visualizations\n\nChromium provides several ways to view the current state of the process model:\n\n* **Chromium's Task Manager**: This can be found under \"More Tools\" in the menu,\n    and shows live resource usage for each of Chromium's processes. The Task\n    Manager also shows which documents and workers are grouped together in a\n    given process: only the first row of a given group displays process ID and\n    most statistics, and all rows of a group are highlighted when one is\n    clicked. Note that double clicking any row attempts to switch to the tab it\n    is associated with. In the default sort order (i.e., when clicking the Task\n    column header until the up/down triangle disappears), processes for\n    subframes are listed under the process for their tab when possible,\n    although this may not be possible if subframes from multiple tabs are in a\n    given process.\n* **`chrome://process-internals/#web-contents`**: This is an internal diagnostic\n    page which shows information about the SiteInstances and processes for each\n    open document.\n* **`chrome://discards/graph`**: This is an internal diagnostic page that\n    includes a visualization of how the open documents and workers map to\n    processes. Clicking on any node provides more details.\n\n\n## Process Reuse\n\nFor performance, Chromium attempts to strike a balance between using more\nprocesses to improve parallelism and using fewer processes to conserve memory.\nThere are some cases where a new process is always required (e.g., for a\ncross-site page when Site Isolation is enabled), and other cases where\nheuristics can determine whether to create a new process or reuse an old one.\nGenerally, process reuse can only happen in suitable cases, such as within a\ngiven profile or respecting a process lock.  Several factors go into this\ndecision.\n\n* **Suitability**: Several properties are global to a given renderer process:\n    profile (including Incognito), StoragePartition (which may differ between\n    tabs and Chrome Apps), and crossOriginIsolated status. For example, two\n    documents from different profiles or StoragePartitions can never share the\n    same renderer process. The ProcessLock (described below) also restricts\n    which documents are allowed in a process.\n* **Soft Process Limit**: On desktop platforms, Chromium sets a \"soft\" process\n    limit based on the memory available on a given client. While this can be\n    exceeded (e.g., if Site Isolation is enabled and the user has more open\n    sites than the limit), Chromium makes an attempt to start randomly reusing\n    same-site processes when over this limit. For example, if the limit is 100\n    processes and the user has 50 open tabs to `example.com` and 50 open tabs to\n    `example.org`, then a new `example.com` tab will share a process with a\n    random existing `example.com` tab, while a `chromium.org` tab will create a\n    101st process. Note that Chromium on Android does not set this soft process\n    limit, and instead relies on the OS to discard processes.\n* **Aggressive Reuse**: For some cases (including on Android), Chromium will\n    aggressively look for existing same-site processes to reuse even before\n    reaching the process limit. Out-of-process iframes (OOPIFs) and [fenced\n    frames](https://developer.chrome.com/en/docs/privacy-sandbox/fenced-frame/)\n    use this approach, such that an `example.com` iframe in a cross-site page\n    will be placed in an existing `example.com` process (in any browsing context\n    group), even if the process limit has not been reached. This keeps the\n    process count lower, based on the assumption that most iframes/fenced frames\n    are less resource demanding than top-level documents. Similarly,\n    ServiceWorkers are generally placed in the same process as a document that\n    is likely to rely on them.\n* **Extensions**: Chromium ensures that extensions do not share a process with\n    each other or with web pages, but also that a large number of extensions\n    will not consume the entire soft process limit, forcing same-site web pages\n    into too few processes. Chromium only allows extensions to consume [one\n    third](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/extensions/chrome_content_browser_client_extensions_part.cc;drc=8d6a246c9be4f6b731dc7f6e680b7d5e13a512b5;l=454-458)\n    of the process limit before disregarding further extension processes from\n    the process limit computation.\n* **Process-per-site**: As noted above, pages like the New Tab Page (NTP) and\n    extensions use a model where all instances of the page are placed in the\n    same process.\n\n\n## Process Locks\n\nChromium assigns a\n[ProcessLock](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/process_lock.h;drc=47457a6923c0527261d0503998cbeb7de9bab489;l=19)\nto some or all RenderProcessHosts, to restrict which sites are allowed to load\nin the process and which data the process has access to. A RenderProcessHost is\nan object in the browser process that represents a given renderer process,\nthough it can be reused if that renderer process crashes and is restarted. Some\nProcessLock cases are used on all platforms (e.g., `chrome://` URLs are never\nallowed to share a process with other sites), while other cases may depend on\nthe mode (e.g., Full Site Isolation requires all processes to be locked, once\ncontent has been loaded in the process).\n\nProcessLocks may have varying granularity, such as a single site\n(e.g., `https://example.com`), a single origin\n(e.g., `https://accounts.example.com`), an entire scheme (e.g., `file://`), or\na special \"allow-any-site\" value for processes allowed to host multiple sites\n(which may have other restrictions, such as whether they are\ncrossOriginIsolated). RenderProcessHosts begin with an \"invalid\" or unlocked\nProcessLock before one is assigned.\n\nProcessLocks are always assigned before any content is loaded in a renderer\nprocess, either at the start of a navigation or at OnResponseStarted time, just\nbefore a navigation commits. Note that a process may initially receive\nan \"allow-any-site\" lock for some empty document schemes (e.g., `about:blank`),\nwhich may later be refined to a site-specific lock when the first actual\ncontent commits. Once a site-specific lock is assigned, it remains constant for\nthe lifetime of the RenderProcessHost, even if the renderer process itself\nexits and is recreated.\n\nNote that content may be allowed in a locked process based on its origin\n(e.g., an `about:blank` page with an inherited `https://example.com` origin is\nallowed in a process locked to `https://example.com`). Also, some opaque origin\ncases are allowed into a locked process as well, such as `data:` URLs created\nwithin that process.\n\n\n## Special Cases\n\nThere are many special cases to consider in Chromium's process model, which may\naffect invariants or how features are designed.\n\n* **WebUI**: Pages like `chrome://settings` are considered part of Chromium and\n    are highly privileged, usually hosted in the `chrome://` scheme. They are\n    strictly isolated from non-WebUI pages as well as other types of WebUI\n    pages (based on \"site\"), on all platforms. They are also generally not\n    allowed to load content from the network (apart from a shrinking\n    [list](https://source.chromium.org/chromium/chromium/src/+/main:chrome/browser/ui/webui/chrome_web_ui_controller_factory.cc;drc=3344b61f7c7f06cf96069751c3bd64d8ec3e3428;l=1405)\n    of allowlisted pages), unless it is from a separate unprivileged\n    `chrome-untrusted://` document. Additionally, normal web pages are not\n    allowed to navigate to WebUI pages, which makes privilege escalation\n    attacks more difficult.\n* **New Tab Page**: On desktop platforms, the default \"local\" NTP is a WebUI\n    page using process-per-site mode, which loads content from the network via\n    `chrome-untrusted://` iframes. Third party NTPs are also possible, which\n    load a \"remote\" non-WebUI web page with limited privileges. On Android, the\n    NTP is instead a native Android surface with no privileged renderer\n    process. Chrome on Android creates an unused renderer process in the\n    background while the NTP surface is visible, so that the next page can use\n    it.\n* **Extensions**: On desktop platforms, extension documents and workers are\n    semi-privileged and run in dedicated renderer processes. In contrast,\n    extension content scripts run within the same unprivileged web renderer\n    process as the pages they modify, and thus Chrome Extensions need to\n    [treat content scripts as less\n    trustworthy](https://groups.google.com/a/chromium.org/g/chromium-extensions/c/0ei-UCHNm34/m/lDaXwQhzBAAJ).\n    The browser process makes an effort to enforce that renderer processes have\n    access to any extension APIs or capabilities that they attempt to use.\n* **Hosted Apps**: A hosted app is a deprecated type of extension which allows a\n    normal web site to have a special type of renderer process. For example, a\n    hosted app for `https://example.com/app/` will have an \"effective URL\" that\n    looks like a `chrome-extension://` URL, causing it to be treated\n    differently in the process model. This support may eventually be removed.\n* **Chrome Web Store**: The [Chrome Web\n    Store](https://chromewebstore.google.com/) is a rare example of a privileged\n    web origin, to which Chrome grants special APIs for installing extensions.\n* **[Isolated Web Apps](https://github.com/WICG/isolated-web-apps/blob/main/README.md)**: Isolated\n    Web Apps (IWAs) are a type of web app that has stricter security and\n    isolation requirements compared to normal web apps. The StoragePartition\n    used for each IWA will be separate from the default StoragePartition used\n    for common browsing and separate from other IWAs. IWAs require strict CSP,\n    [CrossOriginIsolated](#crossoriginisolated), along with other isolation\n    criteria. These contexts are claimed to be\n    \"[IsolatedContext](https://wicg.github.io/isolated-web-apps/isolated-contexts)\"\n    and\n    \"[IsolatedApplication](https://source.chromium.org/chromium/chromium/src/+/main:content/public/browser/web_exposed_isolation_level.h;l=62;drc=998312ac45f85e53257049c5891dff558f203c00)\".\n* **GuestView**: The Chrome Apps `<webview>` tag and similar cases like\n    MimeHandlerView and ExtensionOptionsGuest embed one WebContents within\n    another. All of these cases use strict site isolation for content they\n    embed. Note that Chrome Apps allow `<webview>` tags to load normal web pages\n    and the app's own `data:` or `chrome-extension://` URLs, but not URLs from\n    other extensions or apps. The IWA\n    [&lt;controlledframe&gt;](/chrome/common/controlled_frame/README.md) tag is built\n    on top of the `<webview>` tag's implementation and exposed to contexts\n    that meet the proper security and isolation requirements, such as IWAs that\n    provide IsolatedContexts. See the\n    [Isolated Contexts spec](https://wicg.github.io/isolated-web-apps/isolated-contexts)\n    for more info.\n* **Sandboxed iframes**: Documents with the sandbox attribute and without\n    `allow-same-origin` (either iframes or popups) may be same-site with their\n    parent or opener but use an opaque origin. Since 127.0.6483.0, Desktop\n    Chromium moves these documents into a separate process from their\n    parent or opener. On Android, these documents will only be in a separate\n    process if their parent/opener uses\n    [Partial Site Isolation](#partial-site-isolation). Sandboxed frames embedded\n    in extension pages are in a separate process if they are listed in the\n    \"sandbox\" section of the extension's manifest, otherwise they are in the\n    same process as the parent.\n* **`data:` URLs**: Chromium generally keeps documents with `data:` URLs in the\n    same process as the site that created them, since that site has control\n    over their content. The exception is when restoring a previous session, in\n    which case each document with a `data:` URL ends up in its own process.\n* **File URLs**: Chromium currently treats all `file://` URLs as part of the\n    same site. Normal web pages are not allowed to load `file://` URLs, and\n    renderer processes are only granted access to particular `file://` URLs via\n    file chooser dialogs (e.g., for uploads). These URLs may be further isolated\n    from each other in bug [780770](https://crbug.com/780770).\n* **Error Pages**: Chromium uses a special type of process for error pages\n    provided by the browser (as opposed to error pages provided by a web site,\n    like a 404 page), using process-per-site mode to keep all such pages in the\n    same process. Currently this only applies to error pages in a main frame.\n* **Spare Process**: Chromium often creates a spare RenderProcessHost with a\n    live but unlocked renderer process, which is used the next time a renderer\n    process is needed. This avoids the need to wait for a new process to\n    start.\n* **Android WebView**: While Android WebView uses much of the same code as\n    Chromium, it currently only supports a single renderer process in most\n    cases.\n\n\n## Further Reading\n\nSeveral academic papers have covered topics about Chromium's process model.\n\n[**Security Architecture of the Chromium\nBrowser**](https://crypto.stanford.edu/websec/chromium/)\n\nAdam Barth, Collin Jackson, Charles Reis, and The Google Chrome Team. Stanford\nTechnical Report, September 2008.\n\n_Abstract:_\n\nMost current web browsers employ a monolithic architecture that combines \"the\nuser\" and \"the web\" into a single protection domain. An attacker who exploits\nan arbitrary code execution vulnerability in such a browser can steal sensitive\nfiles or install malware. In this paper, we present the security architecture\nof Chromium, the open-source browser upon which Google Chrome is built.\nChromium has two modules in separate protection domains: a browser kernel,\nwhich interacts with the operating system, and a rendering engine, which runs\nwith restricted privileges in a sandbox. This architecture helps mitigate\nhigh-severity attacks without sacrificing compatibility with existing web\nsites. We define a threat model for browser exploits and evaluate how the\narchitecture would have mitigated past vulnerabilities.\n\n[**Isolating Web Programs in Modern Browser\nArchitectures**](https://research.google.com/pubs/archive/34924.pdf)\n\nCharles Reis, Steven D. Gribble (both authors at UW + Google). Eurosys,\nApril 2009.\n\n_Abstract:_\n\nMany of today's web sites contain substantial amounts of client-side code, and\nconsequently, they act more like programs than simple documents. This creates\nrobustness and performance challenges for web browsers. To give users a robust\nand responsive platform, the browser must identify program boundaries and\nprovide isolation between them.\n\nWe provide three contributions in this paper. First, we present abstractions of\nweb programs and program instances, and we show that these abstractions clarify\nhow browser components interact and how appropriate program boundaries can be\nidentified. Second, we identify backwards compatibility tradeoffs that\nconstrain how web content can be divided into programs without disrupting\nexisting web sites. Third, we present a multi-process browser architecture that\nisolates these web program instances from each other, improving fault\ntolerance, resource management, and performance. We discuss how this\narchitecture is implemented in Google Chrome, and we provide a quantitative\nperformance evaluation examining its benefits and costs.\n\n[**Site Isolation: Process Separation for Web Sites within the\nBrowser**](https://www.usenix.org/conference/usenixsecurity19/presentation/reis)\n\nCharles Reis, Alexander Moshchuk, and Nasko Oskov, Google. Usenix Security,\nAugust 2019.\n\n_Abstract:_\n\nCurrent production web browsers are multi-process but place different web sites\nin the same renderer process, which is not sufficient to mitigate threats\npresent on the web today. With the prevalence of private user data stored on\nweb sites, the risk posed by compromised renderer processes, and the advent of\ntransient execution attacks like Spectre and Meltdown that can leak data via\nmicroarchitectural state, it is no longer safe to render documents from\ndifferent web sites in the same process. In this paper, we describe our\nsuccessful deployment of the Site Isolation architecture to all desktop users\nof Google Chrome as a mitigation for process-wide attacks. Site Isolation locks\neach renderer process to documents from a single site and filters certain\ncross-site data from each process. We overcame performance and compatibility\nchallenges to adapt a production browser to this new architecture. We find that\nthis architecture offers the best path to protection against compromised\nrenderer processes and same-process transient execution attacks, despite\ncurrent limitations. Our performance results indicate it is practical to deploy\nthis level of isolation while sufficiently preserving compatibility with\nexisting web content. Finally, we discuss future directions and how the current\nlimitations of Site Isolation might be addressed.\n"
  },
  {
    "path": "architecture/process-model",
    "title": "Process Model",
    "content": "# Process Model\r\n\r\nChromium's multi-process architecture has evolved significantly in recent versions (v134+) to provide enhanced stability, security, and performance through sophisticated process isolation and modern service-based design. This article explores each process type, communication patterns, and the advanced sandboxing model that protects users in the modern web.\r\n\r\n![](../../img/architecture/multiprocess-architecture.png)\r\n\r\n---\r\n\r\n## 1. Why Multi-Process Architecture? (v134+ Enhancements)\r\n\r\n- **Enhanced Isolation**  \r\n  - Crashes in renderers, GPU, or service processes don't affect the browser\r\n  - **Site Isolation**: Strict per-origin process boundaries for security\r\n  - **Process-per-frame**: Fine-grained isolation for embedded content\r\n\r\n- **Advanced Security (v134+)**  \r\n  - **Spectre/Meltdown Mitigations**: Process boundaries prevent cross-origin data leaks\r\n  - **Enhanced Sandboxing**: Improved platform-specific security restrictions  \r\n  - **Control Flow Integrity (CFI)**: Hardware-assisted exploit prevention\r\n\r\n- **Modern Performance Optimizations**  \r\n  - **Intelligent Process Management**: Dynamic process allocation based on memory pressure\r\n  - **Background Tab Throttling**: Aggressive resource management for inactive content\r\n  - **GPU Process Optimization**: Unified Viz compositor for better performance\r\n  - **Service-based Architecture**: Microservice design for better scalability\r\n\r\n---\r\n\r\n## 2. Browser Process (Enhanced in v134+)\r\n\r\nThe **Browser Process** serves as the central coordinator and control hub for Chromium's architecture, managing all other processes and providing the primary user interface.\r\n\r\n- **Core Responsibilities**  \r\n  - **UI Management**: Address bar, tabs, menus, extensions UI\r\n  - **Navigation Coordination**: Cross-origin navigation and security decisions\r\n  - **Storage Management**: Cookies, cache, local storage, IndexedDB coordination\r\n  - **Process Lifecycle**: Launching, monitoring, and terminating child processes\r\n  - **Security Policy Enforcement**: Site isolation decisions and sandbox configuration\r\n\r\n- **Modern Components (v134+)**  \r\n  - `BrowserMainLoop` (enhanced main event loop)\r\n  - `SiteIsolationPolicy` (per-origin process management)\r\n  - `ProcessManager` (intelligent process allocation)\r\n  - `ServiceManager` (microservice coordination)\r\n  - `FeaturePolicyManager` (permissions and feature controls)\r\n\r\n- **Advanced Features**  \r\n  - **Memory Pressure Management**: Intelligent tab discarding and process prioritization\r\n  - **Extension Security**: Enhanced extension process isolation\r\n  - **DevTools Integration**: Advanced debugging and profiling capabilities\r\n  - **Update Management**: Background update coordination and rollback support\r\n\r\nThe browser process uses sophisticated **Mojo IPC** channels to communicate with all child processes while maintaining strict security boundaries.\r\n\r\n---\r\n\r\n## 3. Renderer Processes (Modern Site Isolation in v134+)\r\n\r\n**Renderer Processes** are responsible for web content rendering, JavaScript execution, and DOM management, with enhanced security through strict site isolation policies.\r\n\r\n- **Core Responsibilities**  \r\n  - **Blink Engine**: Modern HTML/CSS parsing, layout, and painting\r\n  - **V8 JavaScript Engine**: High-performance JavaScript execution with JIT compilation\r\n  - **DOM Management**: Document tree construction and manipulation\r\n  - **Web APIs**: Implementation of modern web platform features\r\n\r\n- **Enhanced Isolation (v134+)**  \r\n  - **Site-per-Process**: Strict per-origin process boundaries (default since v67, enhanced in v134+)\r\n  - **Cross-Origin Isolation**: Prevention of Spectre-style attacks through process boundaries\r\n  - **Origin Agent Clusters**: Fine-grained process allocation for related origins\r\n  - **CORB/CORP Protection**: Cross-Origin Read Blocking and Resource Policy enforcement\r\n\r\n- **Threading Architecture**  \r\n  - **Main Thread**: DOM operations, style computation, layout, JavaScript execution\r\n  - **Compositor Thread**: Hardware-accelerated scrolling and animations\r\n  - **Worker Threads**: Web Workers, Service Workers, Worklets\r\n  - **Raster Threads**: Painting and texture generation (when not using GPU process)\r\n\r\n- **Modern Security Features**  \r\n  - **Process Isolation**: Complete memory isolation between different origins\r\n  - **Sandbox Restrictions**: Severely limited file system and network access\r\n  - **Control Flow Integrity**: Hardware-assisted exploit mitigation\r\n  - **Memory Protection**: Advanced heap protection and UAF detection\r\n\r\n- **Performance Optimizations**  \r\n  - **Code Caching**: Optimized JavaScript bytecode caching\r\n  - **Background Processing**: Intelligent prioritization of background tabs\r\n  - **Memory Management**: Advanced garbage collection and memory pressure handling\r\n\r\n**Crash Recovery**: If a renderer crashes, only affected tabs show error pages while other content continues working normally.\r\n\r\n![](../../img/architecture/chromium-process-model.png)\r\n\r\n---\r\n\r\n## 4. GPU Process (Viz Compositor Architecture in v134+)\r\n\r\nThe **GPU Process** has been significantly enhanced in v134+ with the Viz Display Compositor, providing unified, high-performance graphics rendering and compositing for all content types.\r\n\r\n- **Core Responsibilities (Enhanced)**  \r\n  - **Viz Display Compositor**: Unified compositing architecture for all surfaces\r\n  - **Out-of-Process Rasterization (OOP-R)**: Rasterization moved from renderer to GPU process\r\n  - **Hardware Acceleration**: Modern graphics APIs (Vulkan, Metal, D3D12)\r\n  - **Canvas2D Acceleration**: Hardware-accelerated 2D canvas rendering\r\n  - **WebGL/WebGPU Management**: 3D graphics context and resource management\r\n\r\n- **Modern Features (v134+)**  \r\n  - **SkiaRenderer**: Advanced Skia-based rendering backend\r\n  - **Surface Aggregation**: Efficient composition of multiple content sources\r\n  - **HDR Support**: High dynamic range content rendering\r\n  - **Variable Refresh Rate**: Adaptive sync display support\r\n  - **Multi-Display**: Coordinated rendering across multiple screens\r\n\r\n- **Performance Optimizations**  \r\n  - **GPU Memory Management**: Intelligent texture allocation and caching\r\n  - **Damage Tracking**: Precise invalidation of changed regions\r\n  - **Frame Pacing**: Optimized frame timing for smooth animations\r\n  - **Tile-based Rendering**: Efficient large surface management\r\n\r\n- **Security & Stability**  \r\n  - **GPU Sandbox**: Restricted access to system resources\r\n  - **Driver Crash Isolation**: GPU crashes don't affect browser stability\r\n  - **Context Recovery**: Automatic recovery from graphics context loss\r\n\r\n---\r\n\r\n## 5. Modern Service Architecture (v134+ Microservices)\r\n\r\nChromium has evolved to a sophisticated microservice architecture with specialized processes for enhanced security, performance, and maintainability:\r\n\r\n| Process Type | Examples | Purpose & Benefits (v134+) |\r\n| --- | --- | --- |\r\n| **Network Service** | DNS resolution, HTTP/3, QUIC stack, certificate validation | Enhanced security isolation, easier updates, cross-platform consistency |\r\n| **Audio Service** | Audio decoding, playback, WebAudio processing | Prevents audio driver issues from affecting other processes |\r\n| **Video Decode Service** | Hardware video acceleration, codec management | Dedicated video processing with hardware acceleration |\r\n| **Storage Service** | IndexedDB, localStorage, cache management | Centralized storage with better security and performance |\r\n| **Device Service** | USB, Bluetooth, serial port access | Secure hardware access with permission management |\r\n| **Printing Service** | Print preview, PDF generation, cloud printing | Isolated printing functionality with enhanced security |\r\n| **PDF Service** | PDF rendering, annotation, form handling | Secure PDF processing separate from renderer |\r\n| **ML Service** | On-device machine learning, TensorFlow Lite | Privacy-preserving ML processing |\r\n| **Utility Processes** | File compression, image decoding, data validation | Sandboxed processing for untrusted data |\r\n\r\n### Modern Service Features (v134+)\r\n- **Service Manager**: Central coordination of all services with dependency management\r\n- **Capability-based Security**: Fine-grained permission system for service access\r\n- **Automatic Recovery**: Services can restart independently without affecting the browser\r\n- **Resource Management**: Intelligent memory and CPU allocation across services\r\n- **Cross-Platform Consistency**: Unified service interface across all supported platforms\r\n\r\n---\r\n\r\n## 6. Advanced Inter-Process Communication (Mojo v134+)\r\n\r\nChromium uses **Mojo IPC**, a sophisticated message-passing framework that has been significantly enhanced for modern multi-process architecture.\r\n\r\n### Core Mojo Features (v134+)\r\n- **Type-Safe Interfaces**: Strongly-typed IDL-based service definitions\r\n- **Capability-Based Security**: Fine-grained permission system for service access\r\n- **Shared Memory**: Efficient large data transfer without copying\r\n- **Associated Interfaces**: Ordered message delivery within interface groups\r\n- **Urgent Messages**: Priority message handling for time-critical operations\r\n\r\n### Communication Patterns\r\n- **Service Interfaces**: Structured RPC-style service definitions with versioning\r\n- **Event Broadcasting**: Efficient multicast for system-wide notifications\r\n- **Stream Processing**: High-throughput data streaming for media and large files\r\n- **Synchronous Calls**: Blocking operations for critical coordination (used sparingly)\r\n\r\n### Modern IPC Optimizations\r\n- **Message Coalescing**: Batching related messages for better performance\r\n- **Zero-Copy Transfers**: Direct memory sharing for large payloads\r\n- **Priority Scheduling**: Critical path message prioritization\r\n- **Connection Pooling**: Efficient reuse of communication channels\r\n- **Dead Process Detection**: Rapid detection and cleanup of crashed processes\r\n\r\n### Security Features\r\n- **Interface Filtering**: Process-specific interface access control\r\n- **Message Validation**: Automatic validation of all incoming messages\r\n- **Capability Delegation**: Secure forwarding of permissions between processes\r\n- **Audit Logging**: Comprehensive IPC activity tracking for security analysis\r\n\r\n**Key Advantage**: Mojo provides type safety, versioning, and security while maintaining high performance across Chromium's complex process hierarchy.\r\n\r\n---\r\n\r\n## 7. Modern Process Lifecycle (v134+ Enhancements)\r\n\r\n### Process Launch & Initialization\r\n1. **Intelligent Process Creation**  \r\n   - **Process Reuse**: Efficient reuse of existing processes when possible\r\n   - **Preemptive Launch**: Speculative process creation for better performance\r\n   - **Resource-Aware Spawning**: Dynamic process allocation based on system resources\r\n\r\n2. **Enhanced Initialization**  \r\n   - **Fast Startup**: Optimized initialization paths with reduced overhead\r\n   - **Service Discovery**: Automatic discovery and connection to required services\r\n   - **Sandbox Setup**: Advanced platform-specific security restrictions\r\n   - **Capability Registration**: Registration of process-specific capabilities and permissions\r\n\r\n### Process Management\r\n3. **Runtime Operations**  \r\n   - **Task Scheduling**: Priority-based task allocation across processes\r\n   - **Memory Pressure Handling**: Intelligent response to system memory constraints\r\n   - **Performance Monitoring**: Real-time process health and performance tracking\r\n   - **Dynamic Resource Allocation**: Adaptive resource management based on workload\r\n\r\n4. **Process Coordination**  \r\n   - **Service Dependencies**: Automatic management of inter-service dependencies\r\n   - **Load Balancing**: Intelligent distribution of work across available processes\r\n   - **Migration Support**: Ability to move work between processes for optimization\r\n\r\n### Advanced Shutdown & Recovery\r\n5. **Graceful Termination**  \r\n   - **Clean Resource Release**: Proper cleanup of GPU contexts, file handles, and memory\r\n   - **State Preservation**: Saving critical state before process termination\r\n   - **Dependency Notification**: Informing dependent processes of shutdown\r\n\r\n6. **Crash Handling & Recovery (v134+)**  \r\n   - **Crashpad Integration**: Advanced crash reporting with detailed stack traces\r\n   - **Automatic Recovery**: Intelligent restart of crashed services\r\n   - **State Restoration**: Recovery of user state after process crashes\r\n   - **Diagnostic Information**: Enhanced crash analytics for debugging and telemetry\r\n\r\n---\r\n\r\n## 8. Advanced Sandboxing & Security (v134+ Enhancements)\r\n\r\nChromium employs sophisticated, platform-specific sandboxes to provide multi-layered security protection:\r\n\r\n### Platform-Specific Sandboxing\r\n- **Windows (v134+)**: \r\n  - **Job Objects**: Process resource and access restrictions\r\n  - **Win32k Lockdown**: Restricted access to legacy Windows APIs\r\n  - **App Container**: Enhanced isolation for renderer processes\r\n  - **Code Integrity Guard (CIG)**: Prevent code injection attacks\r\n\r\n- **Linux (v134+)**: \r\n  - **Namespaces**: Process, network, and filesystem isolation\r\n  - **seccomp-bpf**: System call filtering and restriction\r\n  - **User Namespaces**: Privilege isolation without setuid\r\n  - **Landlock**: Path-based access control for filesystem\r\n\r\n- **macOS (v134+)**: \r\n  - **Seatbelt Profiles**: Granular system access restrictions\r\n  - **System Integrity Protection**: Enhanced system-level protection\r\n  - **Hardened Runtime**: Advanced code signing and runtime protections\r\n  - **App Sandbox**: Additional container-based isolation\r\n\r\n### Advanced Security Features (v134+)\r\n- **Site Isolation Enhancement**: \r\n  - **Cross-Origin Isolation**: Spectre/Meltdown attack prevention\r\n  - **COOP/COEP**: Cross-Origin Opener/Embedder Policy enforcement\r\n  - **Origin Agent Clusters**: Fine-grained process boundaries\r\n\r\n- **Memory Protection**: \r\n  - **Control Flow Integrity (CFI)**: Comprehensive indirect call protection\r\n  - **Stack Canaries**: Buffer overflow detection\r\n  - **Heap Isolation**: Separate heap spaces for different data types\r\n  - **Use-After-Free Detection**: Runtime memory safety validation\r\n\r\n- **Capability-Based Security**: \r\n  - **Service Permissions**: Fine-grained access control for system services\r\n  - **Resource Quotas**: Limits on CPU, memory, and network usage\r\n  - **Dynamic Privilege Adjustment**: Runtime security policy modification\r\n\r\n### Compiler-Level Protections\r\n- **Modern Exploit Mitigations**: \r\n  - **Address Space Layout Randomization (ASLR)**: Enhanced memory layout randomization\r\n  - **Stack Protection**: Comprehensive stack corruption prevention\r\n  - **FORTIFY_SOURCE**: Enhanced runtime bounds checking\r\n  - **-fstack-clash-protection**: Stack clash attack prevention\r\n  - **Retpoline**: Speculative execution attack mitigation\r\n\r\n### Security Monitoring & Response\r\n- **Runtime Security**: \r\n  - **Anomaly Detection**: Behavioral analysis for threat detection\r\n  - **Security Telemetry**: Comprehensive security event logging\r\n  - **Automatic Threat Response**: Dynamic security policy adjustment\r\n  - **Crash Analysis**: Advanced post-mortem security analysis\r\n\r\n---\r\n\r\n## 9. Process Architecture Visualization\r\n\r\nThe following diagrams illustrate Chromium's modern multi-process architecture with service-based design:\r\n\r\n![](../../img/architecture/multiprocess-architecture-detailed.png)\r\n\r\n![](../../img/architecture/multiprocess-architecture-simplified.png)\r\n\r\n---\r\n\r\n## 10. Debugging & Performance Analysis (v134+)\r\n\r\n### Process Monitoring Tools\r\n- **Chrome Task Manager**: `Shift+Esc` - Real-time process resource usage\r\n- **chrome://process-internals/**: Detailed process information and IPC statistics\r\n- **chrome://memory-internals/**: Memory usage breakdown by process type\r\n- **chrome://system/**: Comprehensive system and process information\r\n\r\n### Command Line Debugging\r\n```bash\r\n# Process debugging flags\r\n--enable-logging=stderr             # Detailed process logging\r\n--vmodule=\"*process*=2\"            # Verbose process-related logging\r\n--disable-features=VizDisplayCompositor  # Disable GPU process compositor\r\n--single-process                   # Run in single-process mode (debug only)\r\n--no-sandbox                       # Disable sandboxing (debug builds only)\r\n\r\n# IPC and service debugging\r\n--mojo-core-library-path=path      # Use custom Mojo library\r\n--enable-service-manager-tracing   # Trace service manager operations\r\n--trace-to-console                 # Output trace events to console\r\n```\r\n\r\n### Performance Analysis\r\n- **chrome://tracing/**: Advanced process timeline analysis with IPC visualization\r\n- **DevTools → Performance**: Process-aware performance profiling\r\n- **chrome://histograms/**: Process-specific performance metrics\r\n- **chrome://discards/**: Tab lifecycle and memory pressure information\r\n\r\n---\r\n\r\n## 11. Next Steps & Modern Architecture\r\n\r\n### Essential Reading\r\n- **[Render Pipeline](render-pipeline.md)**: How processes coordinate for frame construction\r\n- **[IPC Internals](ipc-internals.md)**: Deep dive into Mojo communication patterns\r\n- **[Security Model](../security/security-model.md)**: Advanced sandboxing and exploit mitigation\r\n\r\n### Modern Development Topics (v134+)\r\n- **Service-Oriented Architecture**: Understanding Chromium's microservice design\r\n- **Site Isolation**: Implementation details and security implications\r\n- **Viz Compositor**: GPU process architecture and performance optimization\r\n- **Memory Management**: Advanced techniques for multi-process memory efficiency\r\n\r\n### Experimental Features\r\n- **Fuchsia Support**: Next-generation OS integration\r\n- **WebAssembly System Interface (WASI)**: Secure system access for WebAssembly\r\n- **Privacy Sandbox**: Enhanced privacy through process isolation\r\n\r\n### Performance Optimization\r\n```bash\r\n# Monitor process performance\r\nchrome://tracing/ with \"Process\" and \"IPC\" categories\r\n\r\n# Analyze memory usage\r\nchrome://memory-internals/ → Process breakdown\r\n\r\n# Profile service interactions\r\nDevTools → Performance → Main thread and GPU process analysis\r\n\r\n# Test site isolation effectiveness\r\nchrome://site-engagement/ → Origin-based process allocation\r\n```\r\n\r\n---\r\n\r\n**End of Modern Process Model Guide**\r\n\r\n### Key Evolution in v134+\r\n- **Enhanced Site Isolation**: Stronger cross-origin protection with Origin Agent Clusters\r\n- **Service Architecture**: Microservice-based design with intelligent coordination\r\n- **Advanced Security**: Hardware-assisted exploit mitigation and capability-based access\r\n- **Performance Optimization**: Intelligent process management and resource allocation\r\n- **Modern IPC**: Type-safe Mojo interfaces with advanced security features\r\n\r\n**Notes for Developers:**\r\n- Process boundaries are security boundaries - design accordingly\r\n- Use Chrome internal pages for debugging multi-process issues\r\n- Monitor memory usage across processes, not just individual tabs\r\n- Leverage site isolation for enhanced security in web applications\r\n"
  },
  {
    "path": "architecture/overview",
    "title": "Chromium Architecture Overview (v134+)",
    "content": "# Chromium Architecture Overview (v134+)\r\n\r\nModern Chromium v134+ represents the pinnacle of browser architectural engineering, embodying decades of evolution in software design principles, security engineering, and performance optimization. This comprehensive overview explores the sophisticated architectural foundations that make Chromium one of the most advanced software systems ever built.\r\n\r\n---\r\n\r\n## 1. Architectural Philosophy & Design Principles\r\n\r\n### Core Design Philosophy\r\nChromium v134+ is built upon a foundation of **Zero-Trust Architecture**, **Service-Oriented Design**, and **Defense in Depth** security principles. The architecture prioritizes user safety, performance excellence, and developer productivity while maintaining the flexibility to adapt to emerging web technologies and security threats.\r\n\r\n### Fundamental Design Principles\r\n\r\n#### SOLID Principles in Modern Practice\r\n```cpp\r\n// Single Responsibility Principle - Enhanced with Service Boundaries\r\nclass SecurityPolicyManager {\r\n public:\r\n  // Single responsibility: Manage security policies only\r\n  void ApplyPolicy(const SecurityPolicy& policy) {\r\n    if (!ValidatePolicy(policy)) {\r\n      LOG(ERROR) << \"Invalid security policy: \" << policy.name();\r\n      return;\r\n    }\r\n    active_policies_[policy.name()] = policy;\r\n    NotifyPolicyChange(policy);\r\n  }\r\n\r\n private:\r\n  bool ValidatePolicy(const SecurityPolicy& policy) const;\r\n  void NotifyPolicyChange(const SecurityPolicy& policy);\r\n  \r\n  std::unordered_map<std::string, SecurityPolicy> active_policies_;\r\n  PolicyChangeNotifier notifier_;\r\n};\r\n\r\n// Open/Closed Principle - Extensible through interfaces\r\nclass WebAPIInterface {\r\n public:\r\n  virtual ~WebAPIInterface() = default;\r\n  virtual void Initialize(const APIConfig& config) = 0;\r\n  virtual bool IsSupported() const = 0;\r\n  virtual void ProcessRequest(const APIRequest& request, \r\n                             APICallback callback) = 0;\r\n};\r\n\r\n// WebGPU implementation extending the interface\r\nclass WebGPUAPI : public WebAPIInterface {\r\n public:\r\n  void Initialize(const APIConfig& config) override {\r\n    gpu_context_ = GPUContextFactory::Create(config);\r\n    compute_pipeline_ = CreateComputePipeline();\r\n  }\r\n\r\n  bool IsSupported() const override {\r\n    return gpu_context_ && gpu_context_->SupportsWebGPU();\r\n  }\r\n\r\n  void ProcessRequest(const APIRequest& request, \r\n                     APICallback callback) override {\r\n    if (!IsSupported()) {\r\n      callback.Run(APIResponse::CreateError(\"WebGPU not supported\"));\r\n      return;\r\n    }\r\n    \r\n    auto response = ProcessWebGPURequest(request);\r\n    callback.Run(std::move(response));\r\n  }\r\n\r\n private:\r\n  std::unique_ptr<GPUContext> gpu_context_;\r\n  std::unique_ptr<ComputePipeline> compute_pipeline_;\r\n};\r\n```\r\n\r\n#### Dependency Inversion & Interface Segregation\r\n```cpp\r\n// Interface segregation - Specific interfaces for different needs\r\nclass NetworkServiceInterface {\r\n public:\r\n  virtual ~NetworkServiceInterface() = default;\r\n  virtual void SendRequest(const NetworkRequest& request,\r\n                          NetworkCallback callback) = 0;\r\n};\r\n\r\nclass SecurityServiceInterface {\r\n public:\r\n  virtual ~SecurityServiceInterface() = default;\r\n  virtual bool ValidateOrigin(const url::Origin& origin) = 0;\r\n  virtual void ApplySecurityHeaders(NetworkResponse* response) = 0;\r\n};\r\n\r\n// Dependency inversion - High-level modules depend on abstractions\r\nclass SecureNetworkManager {\r\n public:\r\n  SecureNetworkManager(\r\n      std::unique_ptr<NetworkServiceInterface> network_service,\r\n      std::unique_ptr<SecurityServiceInterface> security_service)\r\n      : network_service_(std::move(network_service)),\r\n        security_service_(std::move(security_service)) {}\r\n\r\n  void MakeSecureRequest(const SecureNetworkRequest& request,\r\n                        SecureNetworkCallback callback) {\r\n    // Validate security constraints first\r\n    if (!security_service_->ValidateOrigin(request.origin())) {\r\n      callback.Run(SecureNetworkResponse::CreateSecurityError());\r\n      return;\r\n    }\r\n\r\n    // Make network request with security validation\r\n    network_service_->SendRequest(\r\n        request.ToNetworkRequest(),\r\n        base::BindOnce(&SecureNetworkManager::OnNetworkResponse,\r\n                      weak_factory_.GetWeakPtr(),\r\n                      std::move(callback)));\r\n  }\r\n\r\n private:\r\n  void OnNetworkResponse(SecureNetworkCallback callback,\r\n                        NetworkResponse response) {\r\n    // Apply additional security processing\r\n    security_service_->ApplySecurityHeaders(&response);\r\n    callback.Run(SecureNetworkResponse::FromNetworkResponse(response));\r\n  }\r\n\r\n  std::unique_ptr<NetworkServiceInterface> network_service_;\r\n  std::unique_ptr<SecurityServiceInterface> security_service_;\r\n  base::WeakPtrFactory<SecureNetworkManager> weak_factory_{this};\r\n};\r\n```\r\n\r\n---\r\n\r\n## 2. Advanced Architectural Patterns\r\n\r\n### Service-Oriented Architecture (SOA) with Mojo IPC\r\nChromium v134+ implements a sophisticated service-oriented architecture that goes beyond traditional microservices:\r\n\r\n#### Service Definition and Registration\r\n```cpp\r\n// Modern service definition with capability-based security\r\nclass PrivacySandboxService : public service_manager::Service {\r\n public:\r\n  PrivacySandboxService() = default;\r\n  ~PrivacySandboxService() override = default;\r\n\r\n  // Service lifecycle management\r\n  void OnStart() override {\r\n    // Initialize privacy-preserving components\r\n    topics_api_ = std::make_unique<TopicsAPI>(GetPrivacyBudget());\r\n    fledge_api_ = std::make_unique<FLEDGEAPI>(GetAuctionConfig());\r\n    attribution_api_ = std::make_unique<AttributionAPI>(GetReportingConfig());\r\n    \r\n    // Register capability-based interfaces\r\n    registry_.AddInterface(\r\n        base::BindRepeating(&PrivacySandboxService::CreateTopicsBinding,\r\n                           base::Unretained(this)));\r\n    registry_.AddInterface(\r\n        base::BindRepeating(&PrivacySandboxService::CreateFLEDGEBinding,\r\n                           base::Unretained(this)));\r\n  }\r\n\r\n  void OnBindInterface(const service_manager::BindSourceInfo& source_info,\r\n                      const std::string& interface_name,\r\n                      mojo::ScopedMessagePipeHandle interface_pipe) override {\r\n    // Validate capability before binding\r\n    if (!ValidateServiceCapability(source_info, interface_name)) {\r\n      LOG(ERROR) << \"Service capability validation failed for: \" \r\n                 << interface_name;\r\n      return;\r\n    }\r\n    \r\n    registry_.BindInterface(interface_name, std::move(interface_pipe));\r\n  }\r\n\r\n private:\r\n  void CreateTopicsBinding(\r\n      privacy_sandbox::mojom::TopicsAPIRequest request) {\r\n    topics_bindings_.AddBinding(topics_api_.get(), std::move(request));\r\n  }\r\n\r\n  void CreateFLEDGEBinding(\r\n      privacy_sandbox::mojom::FLEDGEAPIRequest request) {\r\n    fledge_bindings_.AddBinding(fledge_api_.get(), std::move(request));\r\n  }\r\n\r\n  bool ValidateServiceCapability(const service_manager::BindSourceInfo& source,\r\n                                const std::string& interface_name) {\r\n    return capability_manager_.HasCapability(source.identity, interface_name);\r\n  }\r\n\r\n  service_manager::BinderRegistry registry_;\r\n  CapabilityManager capability_manager_;\r\n  \r\n  std::unique_ptr<TopicsAPI> topics_api_;\r\n  std::unique_ptr<FLEDGEAPI> fledge_api_;\r\n  std::unique_ptr<AttributionAPI> attribution_api_;\r\n  \r\n  mojo::BindingSet<privacy_sandbox::mojom::TopicsAPI> topics_bindings_;\r\n  mojo::BindingSet<privacy_sandbox::mojom::FLEDGEAPI> fledge_bindings_;\r\n};\r\n```\r\n\r\n#### Advanced Inter-Process Communication\r\n```cpp\r\n// Type-safe Mojo interface with security validation\r\ninterface PrivacySandboxManager {\r\n  // Topics API with differential privacy\r\n  GetTopics(TopicsRequest request) => (TopicsResponse response);\r\n  \r\n  // FLEDGE auction with enhanced security\r\n  RunAuction(AuctionConfig config) => (AuctionResult result);\r\n  \r\n  // Attribution reporting with privacy preservation\r\n  RecordAttribution(AttributionData data) => (AttributionResult result);\r\n  \r\n  // Privacy budget management\r\n  CheckPrivacyBudget(PrivacyBudgetRequest request) => \r\n      (PrivacyBudgetResponse response);\r\n};\r\n\r\n// Implementation with security-first design\r\nclass PrivacySandboxManagerImpl \r\n    : public privacy_sandbox::mojom::PrivacySandboxManager {\r\n public:\r\n  void GetTopics(privacy_sandbox::mojom::TopicsRequestPtr request,\r\n                GetTopicsCallback callback) override {\r\n    // Validate request origin and privacy budget\r\n    if (!ValidateTopicsRequest(*request)) {\r\n      std::move(callback).Run(\r\n          privacy_sandbox::mojom::TopicsResponse::NewError(\r\n              \"Invalid topics request\"));\r\n      return;\r\n    }\r\n\r\n    // Apply differential privacy\r\n    auto topics = topics_calculator_->CalculateTopics(\r\n        request->origin, request->time_range);\r\n    \r\n    auto response = privacy_sandbox::mojom::TopicsResponse::New();\r\n    response->topics = std::move(topics);\r\n    response->privacy_budget_consumed = CalculatePrivacyBudgetUsed(*request);\r\n    \r\n    std::move(callback).Run(std::move(response));\r\n  }\r\n\r\n private:\r\n  bool ValidateTopicsRequest(\r\n      const privacy_sandbox::mojom::TopicsRequest& request) {\r\n    return origin_validator_->IsValidOrigin(request.origin) &&\r\n           privacy_budget_manager_->HasSufficientBudget(\r\n               request.origin, request.privacy_budget_required);\r\n  }\r\n\r\n  std::unique_ptr<TopicsCalculator> topics_calculator_;\r\n  std::unique_ptr<OriginValidator> origin_validator_;\r\n  std::unique_ptr<PrivacyBudgetManager> privacy_budget_manager_;\r\n};\r\n```\r\n\r\n### Modern Component Architecture\r\n```cpp\r\n// Component-based architecture with dependency injection\r\nclass ComponentRegistry {\r\n public:\r\n  template<typename ComponentType, typename... Args>\r\n  void RegisterComponent(Args&&... args) {\r\n    static_assert(std::is_base_of_v<Component, ComponentType>);\r\n    \r\n    auto component = std::make_unique<ComponentType>(\r\n        std::forward<Args>(args)...);\r\n    \r\n    const std::string component_name = ComponentType::GetName();\r\n    \r\n    // Validate component dependencies\r\n    if (!ValidateComponentDependencies<ComponentType>()) {\r\n      LOG(ERROR) << \"Component dependencies not satisfied: \" \r\n                 << component_name;\r\n      return;\r\n    }\r\n    \r\n    // Initialize component with dependency injection\r\n    component->Initialize(CreateDependencyProvider<ComponentType>());\r\n    \r\n    components_[component_name] = std::move(component);\r\n    \r\n    LOG(INFO) << \"Component registered successfully: \" << component_name;\r\n  }\r\n\r\n  template<typename ComponentType>\r\n  ComponentType* GetComponent() {\r\n    const std::string component_name = ComponentType::GetName();\r\n    auto it = components_.find(component_name);\r\n    \r\n    if (it == components_.end()) {\r\n      return nullptr;\r\n    }\r\n    \r\n    return static_cast<ComponentType*>(it->second.get());\r\n  }\r\n\r\n private:\r\n  template<typename ComponentType>\r\n  bool ValidateComponentDependencies() {\r\n    for (const auto& dependency_name : ComponentType::GetDependencies()) {\r\n      if (components_.find(dependency_name) == components_.end()) {\r\n        return false;\r\n      }\r\n    }\r\n    return true;\r\n  }\r\n\r\n  template<typename ComponentType>\r\n  std::unique_ptr<DependencyProvider> CreateDependencyProvider() {\r\n    auto provider = std::make_unique<DependencyProvider>();\r\n    \r\n    for (const auto& dependency_name : ComponentType::GetDependencies()) {\r\n      auto dependency_it = components_.find(dependency_name);\r\n      if (dependency_it != components_.end()) {\r\n        provider->AddDependency(dependency_name, dependency_it->second.get());\r\n      }\r\n    }\r\n    \r\n    return provider;\r\n  }\r\n\r\n  std::unordered_map<std::string, std::unique_ptr<Component>> components_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 3. Multi-Process Architecture Excellence\r\n\r\n### Process Isolation and Security Boundaries\r\n```cpp\r\n// Enhanced process model with security-first design\r\nclass ProcessManager {\r\n public:\r\n  enum class ProcessType {\r\n    kBrowser,           // Main coordinator process\r\n    kRenderer,          // Site-isolated content rendering\r\n    kGPU,              // Graphics and compute acceleration\r\n    kNetwork,          // Network request processing\r\n    kStorage,          // Data persistence and caching\r\n    kAudio,            // Audio processing and effects\r\n    kUtility,          // Sandboxed utility processes\r\n    kML,               // Machine learning inference\r\n    kSecurity          // Security monitoring and analysis\r\n  };\r\n\r\n  struct ProcessSecurityConfig {\r\n    // Sandbox configuration\r\n    SandboxType sandbox_type = SandboxType::kStrictSandbox;\r\n    \r\n    // Capability restrictions\r\n    std::set<std::string> allowed_capabilities;\r\n    \r\n    // Memory protection\r\n    bool enable_cfi = true;\r\n    bool enable_memory_tagging = true;\r\n    bool enable_pointer_authentication = true;\r\n    \r\n    // Network restrictions\r\n    NetworkAccessPolicy network_policy = NetworkAccessPolicy::kDenied;\r\n    \r\n    // File system access\r\n    FileSystemAccessPolicy file_access = FileSystemAccessPolicy::kRestricted;\r\n  };\r\n\r\n  base::Process CreateSecureProcess(ProcessType type,\r\n                                   const ProcessSecurityConfig& config) {\r\n    // Create process with enhanced security\r\n    auto process_launcher = CreateProcessLauncher(type, config);\r\n    \r\n    // Apply sandbox restrictions\r\n    ApplySandboxRestrictions(process_launcher.get(), config);\r\n    \r\n    // Setup IPC channels with capability validation\r\n    auto ipc_channel = CreateSecureIPCChannel(type, config);\r\n    \r\n    // Launch process with security monitoring\r\n    auto process = process_launcher->LaunchProcess();\r\n    \r\n    if (process.IsValid()) {\r\n      RegisterProcessForMonitoring(process.Pid(), type, config);\r\n    }\r\n    \r\n    return process;\r\n  }\r\n\r\n private:\r\n  void ApplySandboxRestrictions(ProcessLauncher* launcher,\r\n                               const ProcessSecurityConfig& config) {\r\n    // Configure sandbox based on process requirements\r\n    auto sandbox_config = CreateSandboxConfig(config);\r\n    launcher->SetSandboxConfiguration(std::move(sandbox_config));\r\n    \r\n    // Apply capability restrictions\r\n    launcher->SetCapabilityRestrictions(config.allowed_capabilities);\r\n    \r\n    // Enable hardware security features\r\n    if (config.enable_cfi) {\r\n      launcher->EnableControlFlowIntegrity();\r\n    }\r\n    \r\n    if (config.enable_memory_tagging) {\r\n      launcher->EnableMemoryTagging();\r\n    }\r\n  }\r\n\r\n  void RegisterProcessForMonitoring(base::ProcessId pid,\r\n                                   ProcessType type,\r\n                                   const ProcessSecurityConfig& config) {\r\n    process_monitor_.RegisterProcess(pid, type, config);\r\n    \r\n    // Setup security event monitoring\r\n    security_monitor_.MonitorProcess(pid, \r\n        base::BindRepeating(&ProcessManager::OnSecurityEvent,\r\n                           base::Unretained(this), pid));\r\n  }\r\n\r\n  void OnSecurityEvent(base::ProcessId pid, const SecurityEvent& event) {\r\n    LOG(WARNING) << \"Security event detected in process \" << pid \r\n                 << \": \" << event.description();\r\n    \r\n    // Take appropriate action based on event severity\r\n    if (event.severity() >= SecurityEventSeverity::kCritical) {\r\n      TerminateProcess(pid, \"Critical security violation\");\r\n    }\r\n  }\r\n\r\n  ProcessMonitor process_monitor_;\r\n  SecurityMonitor security_monitor_;\r\n};\r\n```\r\n\r\n### Site Isolation and Origin-Based Security\r\n```cpp\r\n// Advanced site isolation with enhanced security boundaries\r\nclass SiteIsolationManager {\r\n public:\r\n  struct IsolationPolicy {\r\n    // Origin-based isolation rules\r\n    std::set<url::Origin> high_risk_origins;\r\n    std::set<url::Origin> trusted_origins;\r\n    \r\n    // Cross-origin policy enforcement\r\n    bool enforce_corp = true;              // Cross-Origin Resource Policy\r\n    bool enforce_coep = true;              // Cross-Origin Embedder Policy\r\n    bool enforce_coop = true;              // Cross-Origin Opener Policy\r\n    \r\n    // Enhanced security features\r\n    bool enable_origin_agent_clusters = true;\r\n    bool enable_cross_origin_isolation = true;\r\n    bool enable_shared_array_buffer_isolation = true;\r\n  };\r\n\r\n  bool ShouldIsolateOrigin(const url::Origin& origin,\r\n                          const IsolationPolicy& policy) {\r\n    // Always isolate high-risk origins\r\n    if (policy.high_risk_origins.contains(origin)) {\r\n      return true;\r\n    }\r\n    \r\n    // Check for special isolation requirements\r\n    if (RequiresSpecialIsolation(origin)) {\r\n      return true;\r\n    }\r\n    \r\n    // Apply ML-based risk assessment\r\n    if (security_classifier_.IsHighRiskOrigin(origin)) {\r\n      return true;\r\n    }\r\n    \r\n    // Check for cross-origin isolation requirements\r\n    if (policy.enable_cross_origin_isolation && \r\n        RequiresCrossOriginIsolation(origin)) {\r\n      return true;\r\n    }\r\n    \r\n    return false;\r\n  }\r\n\r\n  void EnforceOriginPolicy(RenderFrameHost* frame,\r\n                          const url::Origin& origin,\r\n                          const IsolationPolicy& policy) {\r\n    // Apply Cross-Origin Resource Policy\r\n    if (policy.enforce_corp) {\r\n      frame->SetCrossOriginResourcePolicy(GetCORPPolicy(origin));\r\n    }\r\n    \r\n    // Apply Cross-Origin Embedder Policy\r\n    if (policy.enforce_coep) {\r\n      frame->SetCrossOriginEmbedderPolicy(GetCOEPPolicy(origin));\r\n    }\r\n    \r\n    // Apply Cross-Origin Opener Policy\r\n    if (policy.enforce_coop) {\r\n      frame->SetCrossOriginOpenerPolicy(GetCOOPPolicy(origin));\r\n    }\r\n    \r\n    // Setup origin agent clusters\r\n    if (policy.enable_origin_agent_clusters) {\r\n      frame->EnableOriginAgentClusters();\r\n    }\r\n  }\r\n\r\n private:\r\n  bool RequiresSpecialIsolation(const url::Origin& origin) {\r\n    // Check for origins that require special handling\r\n    return origin.scheme() == \"chrome-extension\" ||\r\n           origin.scheme() == \"chrome-native\" ||\r\n           IsKnownSensitiveOrigin(origin);\r\n  }\r\n\r\n  bool RequiresCrossOriginIsolation(const url::Origin& origin) {\r\n    // Check if origin uses features requiring cross-origin isolation\r\n    return UsesSharedArrayBuffer(origin) ||\r\n           UsesWASMThreads(origin) ||\r\n           UsesHighResolutionTimer(origin);\r\n  }\r\n\r\n  SecurityClassifier security_classifier_;\r\n  OriginPolicyDatabase origin_policy_db_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 4. Performance Architecture & Optimization\r\n\r\n### Advanced Performance Monitoring\r\n```cpp\r\n// Comprehensive performance tracking with ML-based optimization\r\nclass PerformanceArchitecture {\r\n public:\r\n  struct PerformanceMetrics {\r\n    // Core Web Vitals with enhanced precision\r\n    base::TimeDelta largest_contentful_paint;\r\n    base::TimeDelta interaction_to_next_paint;\r\n    double cumulative_layout_shift;\r\n    \r\n    // Advanced rendering metrics\r\n    base::TimeDelta first_contentful_paint;\r\n    base::TimeDelta time_to_interactive;\r\n    double frame_rate;\r\n    double frame_consistency;\r\n    \r\n    // Resource loading metrics\r\n    base::TimeDelta dns_lookup_time;\r\n    base::TimeDelta tcp_connection_time;\r\n    base::TimeDelta ssl_handshake_time;\r\n    base::TimeDelta resource_download_time;\r\n    \r\n    // JavaScript execution metrics\r\n    base::TimeDelta script_parse_time;\r\n    base::TimeDelta script_execution_time;\r\n    double heap_usage_percentage;\r\n    \r\n    // Memory metrics\r\n    size_t total_memory_usage;\r\n    size_t peak_memory_usage;\r\n    double memory_fragmentation_ratio;\r\n  };\r\n\r\n  class PerformanceOptimizer {\r\n   public:\r\n    void OptimizeBasedOnMetrics(const PerformanceMetrics& metrics,\r\n                               const GURL& url) {\r\n      // Analyze performance bottlenecks\r\n      auto bottlenecks = AnalyzeBottlenecks(metrics);\r\n      \r\n      // Apply targeted optimizations\r\n      for (const auto& bottleneck : bottlenecks) {\r\n        ApplyOptimization(bottleneck, url);\r\n      }\r\n      \r\n      // Update ML model with performance data\r\n      performance_model_.UpdateModel(metrics, url);\r\n    }\r\n\r\n   private:\r\n    enum class PerformanceBottleneck {\r\n      kLargeContentfulPaint,\r\n      kLayoutInstability,\r\n      kSlowInteraction,\r\n      kResourceLoading,\r\n      kJavaScriptExecution,\r\n      kMemoryPressure\r\n    };\r\n\r\n    std::vector<PerformanceBottleneck> AnalyzeBottlenecks(\r\n        const PerformanceMetrics& metrics) {\r\n      std::vector<PerformanceBottleneck> bottlenecks;\r\n      \r\n      if (metrics.largest_contentful_paint > kLCPThreshold) {\r\n        bottlenecks.push_back(PerformanceBottleneck::kLargeContentfulPaint);\r\n      }\r\n      \r\n      if (metrics.cumulative_layout_shift > kCLSThreshold) {\r\n        bottlenecks.push_back(PerformanceBottleneck::kLayoutInstability);\r\n      }\r\n      \r\n      if (metrics.interaction_to_next_paint > kINPThreshold) {\r\n        bottlenecks.push_back(PerformanceBottleneck::kSlowInteraction);\r\n      }\r\n      \r\n      if (metrics.memory_fragmentation_ratio > kMemoryFragmentationThreshold) {\r\n        bottlenecks.push_back(PerformanceBottleneck::kMemoryPressure);\r\n      }\r\n      \r\n      return bottlenecks;\r\n    }\r\n\r\n    void ApplyOptimization(PerformanceBottleneck bottleneck, const GURL& url) {\r\n      switch (bottleneck) {\r\n        case PerformanceBottleneck::kLargeContentfulPaint:\r\n          EnablePreloadOptimizations(url);\r\n          break;\r\n        case PerformanceBottleneck::kLayoutInstability:\r\n          EnableLayoutStabilization(url);\r\n          break;\r\n        case PerformanceBottleneck::kSlowInteraction:\r\n          EnableInteractionOptimizations(url);\r\n          break;\r\n        case PerformanceBottleneck::kMemoryPressure:\r\n          TriggerMemoryOptimization();\r\n          break;\r\n      }\r\n    }\r\n\r\n    PerformanceMLModel performance_model_;\r\n  };\r\n};\r\n```\r\n\r\n### Memory Architecture and Optimization\r\n```cpp\r\n// Advanced memory management with predictive optimization\r\nclass MemoryArchitecture {\r\n public:\r\n  class SmartMemoryManager {\r\n   public:\r\n    struct MemoryConfiguration {\r\n      // Memory allocation strategies\r\n      AllocationStrategy strategy = AllocationStrategy::kAdaptive;\r\n      \r\n      // Garbage collection tuning\r\n      GCStrategy gc_strategy = GCStrategy::kPredictive;\r\n      double gc_threshold = 0.8;\r\n      \r\n      // Memory compression\r\n      bool enable_memory_compression = true;\r\n      CompressionLevel compression_level = CompressionLevel::kBalanced;\r\n      \r\n      // Memory mapping optimizations\r\n      bool enable_memory_mapping = true;\r\n      bool enable_large_pages = true;\r\n    };\r\n\r\n    void OptimizeMemoryUsage(const MemoryConfiguration& config) {\r\n      // Apply memory allocation strategy\r\n      ApplyAllocationStrategy(config.strategy);\r\n      \r\n      // Configure garbage collection\r\n      ConfigureGarbageCollection(config.gc_strategy, config.gc_threshold);\r\n      \r\n      // Enable memory compression if beneficial\r\n      if (config.enable_memory_compression && ShouldCompressMemory()) {\r\n        EnableMemoryCompression(config.compression_level);\r\n      }\r\n      \r\n      // Optimize memory mapping\r\n      if (config.enable_memory_mapping) {\r\n        OptimizeMemoryMapping(config.enable_large_pages);\r\n      }\r\n    }\r\n\r\n    void PredictiveMemoryManagement() {\r\n      // Use ML to predict memory usage patterns\r\n      auto prediction = memory_predictor_.PredictMemoryUsage();\r\n      \r\n      // Proactively adjust memory allocation\r\n      if (prediction.peak_usage > GetAvailableMemory() * 0.9) {\r\n        TriggerPreemptiveGarbageCollection();\r\n        CompressInactivePages();\r\n      }\r\n      \r\n      // Preload frequently accessed data\r\n      if (prediction.confidence > 0.8) {\r\n        PreloadPredictedData(prediction.predicted_pages);\r\n      }\r\n    }\r\n\r\n   private:\r\n    void ApplyAllocationStrategy(AllocationStrategy strategy) {\r\n      switch (strategy) {\r\n        case AllocationStrategy::kAdaptive:\r\n          EnableAdaptiveAllocation();\r\n          break;\r\n        case AllocationStrategy::kPooled:\r\n          EnablePooledAllocation();\r\n          break;\r\n        case AllocationStrategy::kRegion:\r\n          EnableRegionBasedAllocation();\r\n          break;\r\n      }\r\n    }\r\n\r\n    bool ShouldCompressMemory() {\r\n      return GetMemoryPressureLevel() >= MemoryPressureLevel::kModerate &&\r\n             GetCompressionBenefit() > kCompressionThreshold;\r\n    }\r\n\r\n    MemoryPredictor memory_predictor_;\r\n    CompressionEngine compression_engine_;\r\n  };\r\n};\r\n```\r\n\r\n---\r\n\r\n## 5. Security Architecture Excellence\r\n\r\n### Zero-Trust Security Model\r\n```cpp\r\n// Zero-trust security implementation with continuous verification\r\nclass ZeroTrustSecurityArchitecture {\r\n public:\r\n  class SecurityVerifier {\r\n   public:\r\n    struct SecurityContext {\r\n      // Identity and authentication\r\n      UserIdentity user_identity;\r\n      DeviceIdentity device_identity;\r\n      ProcessIdentity process_identity;\r\n      \r\n      // Trust level assessment\r\n      TrustLevel current_trust_level = TrustLevel::kUntrusted;\r\n      base::Time last_verification;\r\n      \r\n      // Security state\r\n      std::set<SecurityCapability> granted_capabilities;\r\n      std::vector<SecurityViolation> recent_violations;\r\n      \r\n      // Cryptographic validation\r\n      CryptographicToken access_token;\r\n      base::Time token_expiry;\r\n    };\r\n\r\n    bool VerifySecurityContext(SecurityContext* context) {\r\n      // Continuous verification of all security aspects\r\n      if (!VerifyUserIdentity(context->user_identity)) {\r\n        context->current_trust_level = TrustLevel::kUntrusted;\r\n        return false;\r\n      }\r\n      \r\n      if (!VerifyDeviceIdentity(context->device_identity)) {\r\n        context->current_trust_level = TrustLevel::kUntrusted;\r\n        return false;\r\n      }\r\n      \r\n      if (!VerifyProcessIntegrity(context->process_identity)) {\r\n        context->current_trust_level = TrustLevel::kUntrusted;\r\n        return false;\r\n      }\r\n      \r\n      // Check for recent security violations\r\n      if (HasRecentSecurityViolations(*context)) {\r\n        ReduceTrustLevel(context);\r\n      }\r\n      \r\n      // Verify cryptographic tokens\r\n      if (!VerifyCryptographicToken(context->access_token)) {\r\n        RefreshSecurityToken(context);\r\n      }\r\n      \r\n      // Update verification timestamp\r\n      context->last_verification = base::Time::Now();\r\n      \r\n      return context->current_trust_level >= TrustLevel::kTrusted;\r\n    }\r\n\r\n   private:\r\n    bool VerifyUserIdentity(const UserIdentity& identity) {\r\n      // Multi-factor authentication verification\r\n      return biometric_verifier_.VerifyBiometric(identity) &&\r\n             token_verifier_.VerifySecurityToken(identity) &&\r\n             behavioral_verifier_.VerifyBehavioralPattern(identity);\r\n    }\r\n\r\n    bool VerifyDeviceIdentity(const DeviceIdentity& identity) {\r\n      // Hardware-based device attestation\r\n      return hardware_attestor_.VerifyHardware(identity) &&\r\n             tpm_verifier_.VerifyTPMAttestation(identity) &&\r\n             secure_boot_verifier_.VerifySecureBoot(identity);\r\n    }\r\n\r\n    bool VerifyProcessIntegrity(const ProcessIdentity& identity) {\r\n      // Code signing and integrity verification\r\n      return code_verifier_.VerifyCodeSignature(identity) &&\r\n             memory_verifier_.VerifyMemoryIntegrity(identity) &&\r\n             execution_verifier_.VerifyExecutionIntegrity(identity);\r\n    }\r\n\r\n    BiometricVerifier biometric_verifier_;\r\n    TokenVerifier token_verifier_;\r\n    BehavioralVerifier behavioral_verifier_;\r\n    HardwareAttestor hardware_attestor_;\r\n    TPMVerifier tpm_verifier_;\r\n    SecureBootVerifier secure_boot_verifier_;\r\n    CodeVerifier code_verifier_;\r\n    MemoryVerifier memory_verifier_;\r\n    ExecutionVerifier execution_verifier_;\r\n  };\r\n};\r\n```\r\n\r\n### Advanced Threat Detection and Response\r\n```cpp\r\n// AI-powered threat detection with automated response\r\nclass ThreatDetectionSystem {\r\n public:\r\n  struct ThreatDetectionConfig {\r\n    // Detection sensitivity levels\r\n    ThreatSensitivity sensitivity_level = ThreatSensitivity::kBalanced;\r\n    \r\n    // AI model configuration\r\n    bool enable_ml_detection = true;\r\n    MLModelVersion model_version = MLModelVersion::kLatest;\r\n    \r\n    // Real-time monitoring\r\n    bool enable_behavioral_monitoring = true;\r\n    bool enable_network_monitoring = true;\r\n    bool enable_memory_monitoring = true;\r\n    \r\n    // Response automation\r\n    bool enable_automated_response = true;\r\n    ResponseAggression response_level = ResponseAggression::kModerate;\r\n  };\r\n\r\n  class ThreatDetector {\r\n   public:\r\n    void StartThreatMonitoring(const ThreatDetectionConfig& config) {\r\n      // Initialize AI-powered detection models\r\n      if (config.enable_ml_detection) {\r\n        threat_model_ = LoadThreatDetectionModel(config.model_version);\r\n      }\r\n      \r\n      // Start behavioral monitoring\r\n      if (config.enable_behavioral_monitoring) {\r\n        behavioral_monitor_.StartMonitoring(\r\n            base::BindRepeating(&ThreatDetector::OnBehavioralAnomaly,\r\n                               base::Unretained(this)));\r\n      }\r\n      \r\n      // Start network monitoring\r\n      if (config.enable_network_monitoring) {\r\n        network_monitor_.StartMonitoring(\r\n            base::BindRepeating(&ThreatDetector::OnNetworkAnomaly,\r\n                               base::Unretained(this)));\r\n      }\r\n      \r\n      // Start memory monitoring\r\n      if (config.enable_memory_monitoring) {\r\n        memory_monitor_.StartMonitoring(\r\n            base::BindRepeating(&ThreatDetector::OnMemoryAnomaly,\r\n                               base::Unretained(this)));\r\n      }\r\n    }\r\n\r\n   private:\r\n    void OnBehavioralAnomaly(const BehavioralAnomaly& anomaly) {\r\n      // Analyze threat using ML model\r\n      auto threat_assessment = threat_model_->AssessThreat(anomaly);\r\n      \r\n      if (threat_assessment.confidence > kThreatThreshold) {\r\n        RespondToThreat(ThreatEvent::FromBehavioralAnomaly(anomaly));\r\n      }\r\n    }\r\n\r\n    void OnNetworkAnomaly(const NetworkAnomaly& anomaly) {\r\n      // Check for known attack patterns\r\n      if (IsKnownAttackPattern(anomaly)) {\r\n        RespondToThreat(ThreatEvent::FromNetworkAnomaly(anomaly));\r\n        return;\r\n      }\r\n      \r\n      // Use ML for unknown pattern detection\r\n      auto threat_assessment = threat_model_->AssessNetworkThreat(anomaly);\r\n      \r\n      if (threat_assessment.severity >= ThreatSeverity::kMedium) {\r\n        RespondToThreat(ThreatEvent::FromNetworkAnomaly(anomaly));\r\n      }\r\n    }\r\n\r\n    void RespondToThreat(const ThreatEvent& threat) {\r\n      // Log threat for analysis\r\n      threat_logger_.LogThreat(threat);\r\n      \r\n      // Take automated response actions\r\n      switch (threat.severity()) {\r\n        case ThreatSeverity::kLow:\r\n          IncreasedMonitoring(threat.source());\r\n          break;\r\n        case ThreatSeverity::kMedium:\r\n          IsolateProcess(threat.source());\r\n          break;\r\n        case ThreatSeverity::kHigh:\r\n          TerminateProcess(threat.source());\r\n          NotifySecurityTeam(threat);\r\n          break;\r\n        case ThreatSeverity::kCritical:\r\n          EmergencyShutdown(threat);\r\n          break;\r\n      }\r\n    }\r\n\r\n    std::unique_ptr<ThreatDetectionModel> threat_model_;\r\n    BehavioralMonitor behavioral_monitor_;\r\n    NetworkMonitor network_monitor_;\r\n    MemoryMonitor memory_monitor_;\r\n    ThreatLogger threat_logger_;\r\n  };\r\n};\r\n```\r\n\r\n---\r\n\r\n## 6. Modern Development Architecture\r\n\r\n### Continuous Integration and Quality Assurance\r\n```cpp\r\n// Comprehensive CI/CD pipeline with quality gates\r\nclass DevelopmentArchitecture {\r\n public:\r\n  class QualityGateSystem {\r\n   public:\r\n    struct QualityMetrics {\r\n      // Code quality metrics\r\n      double code_coverage_percentage;\r\n      int cyclomatic_complexity;\r\n      int technical_debt_hours;\r\n      \r\n      // Security metrics\r\n      int security_vulnerabilities;\r\n      int potential_security_issues;\r\n      double security_score;\r\n      \r\n      // Performance metrics\r\n      base::TimeDelta build_time;\r\n      base::TimeDelta test_execution_time;\r\n      double performance_regression_percentage;\r\n      \r\n      // Maintainability metrics\r\n      double maintainability_index;\r\n      int code_duplication_percentage;\r\n      int documentation_coverage;\r\n    };\r\n\r\n    bool PassesQualityGates(const QualityMetrics& metrics) {\r\n      // Code quality gates\r\n      if (metrics.code_coverage_percentage < kMinCodeCoverage) {\r\n        LOG(ERROR) << \"Code coverage below threshold: \" \r\n                   << metrics.code_coverage_percentage << \"%\";\r\n        return false;\r\n      }\r\n      \r\n      if (metrics.cyclomatic_complexity > kMaxCyclomaticComplexity) {\r\n        LOG(ERROR) << \"Cyclomatic complexity too high: \" \r\n                   << metrics.cyclomatic_complexity;\r\n        return false;\r\n      }\r\n      \r\n      // Security gates\r\n      if (metrics.security_vulnerabilities > 0) {\r\n        LOG(ERROR) << \"Security vulnerabilities detected: \" \r\n                   << metrics.security_vulnerabilities;\r\n        return false;\r\n      }\r\n      \r\n      if (metrics.security_score < kMinSecurityScore) {\r\n        LOG(ERROR) << \"Security score below threshold: \" \r\n                   << metrics.security_score;\r\n        return false;\r\n      }\r\n      \r\n      // Performance gates\r\n      if (metrics.performance_regression_percentage > kMaxPerformanceRegression) {\r\n        LOG(ERROR) << \"Performance regression detected: \" \r\n                   << metrics.performance_regression_percentage << \"%\";\r\n        return false;\r\n      }\r\n      \r\n      return true;\r\n    }\r\n\r\n   private:\r\n    static constexpr double kMinCodeCoverage = 80.0;\r\n    static constexpr int kMaxCyclomaticComplexity = 10;\r\n    static constexpr double kMinSecurityScore = 9.0;\r\n    static constexpr double kMaxPerformanceRegression = 5.0;\r\n  };\r\n};\r\n```\r\n\r\n---\r\n\r\n## 7. Future Architecture Considerations\r\n\r\n### Emerging Technology Integration\r\nChromium v134+ is designed with extensibility for future technologies:\r\n\r\n- **Quantum Computing Integration**: Quantum-resistant cryptography and quantum algorithm support\r\n- **Advanced AI/ML**: On-device large language models and neural processing units\r\n- **Extended Reality (XR)**: Native WebXR support with spatial computing capabilities\r\n- **Edge Computing**: Distributed rendering and computation across edge nodes\r\n- **Blockchain Integration**: Decentralized identity and secure transaction processing\r\n\r\n### Architectural Evolution Roadmap\r\n```cpp\r\n// Future architecture preparation\r\nclass FutureArchitecture {\r\n public:\r\n  // Quantum-resistant security layer\r\n  class QuantumResistantSecurity {\r\n   public:\r\n    void PrepareForQuantumComputing() {\r\n      // Implement post-quantum cryptography\r\n      crypto_manager_.EnablePostQuantumCryptography();\r\n      \r\n      // Update key exchange mechanisms\r\n      key_exchange_.UpgradeToQuantumResistant();\r\n      \r\n      // Prepare quantum-safe storage\r\n      storage_engine_.EnableQuantumSafeEncryption();\r\n    }\r\n  };\r\n  \r\n  // AI-native architecture components\r\n  class AIIntegratedArchitecture {\r\n   public:\r\n    void EnableAINativeFeatures() {\r\n      // On-device ML inference optimization\r\n      ml_accelerator_.OptimizeForOnDeviceInference();\r\n      \r\n      // AI-powered security monitoring\r\n      security_ai_.EnableAIThreatDetection();\r\n      \r\n      // Intelligent performance optimization\r\n      performance_ai_.EnableAIPerformanceOptimization();\r\n    }\r\n  };\r\n};\r\n```\r\n\r\n---\r\n\r\n## Summary\r\n\r\nChromium v134+ represents the pinnacle of browser architectural engineering, featuring:\r\n\r\n### Architectural Excellence\r\n1. **SOLID Principles**: Rigorous application of software engineering best practices\r\n2. **Service-Oriented Design**: Sophisticated microservice architecture with Mojo IPC\r\n3. **Zero-Trust Security**: Continuous verification and capability-based access control\r\n4. **Performance Excellence**: Sub-100ms navigation with AI-powered optimization\r\n5. **Extensibility**: Future-ready architecture for emerging technologies\r\n\r\n### Technical Innovation\r\n- **Advanced Multi-Process Model**: Enhanced security boundaries with site isolation\r\n- **Capability-Based Security**: Least-privilege access with continuous verification\r\n- **AI-Powered Optimization**: Machine learning for performance and security\r\n- **Cross-Platform Excellence**: Consistent behavior across all supported platforms\r\n- **Developer Experience**: Comprehensive tooling and debugging capabilities\r\n\r\n### Future Readiness\r\n- **Quantum-Resistant Security**: Preparation for post-quantum cryptography\r\n- **AI Integration**: Native support for advanced machine learning capabilities\r\n- **Extended Reality**: WebXR and spatial computing readiness\r\n- **Edge Computing**: Distributed architecture for edge deployment\r\n- **Blockchain Integration**: Decentralized technologies and secure transactions\r\n\r\nThis architectural foundation enables Chromium to maintain its position as the world's leading browser engine while continuously evolving to meet future challenges in security, performance, and user experience.\r\n\r\n**Related Documentation**:\r\n- [Module Layering](module-layering.md) - Detailed module architecture\r\n- [Process Model](process-model.md) - Multi-process implementation\r\n- [Security Architecture](security/sandbox-architecture.md) - Security boundaries and sandboxing\r\n- [IPC Internals](ipc-internals.md) - Inter-process communication patterns\r\n- [Performance Optimization](../modules/performance.md) - Performance best practices\r\n\r\n---\r\n\r\n*Last Updated: August 2025 | Chromium v134+ | Advanced Architecture Overview*\r\n"
  },
  {
    "path": "architecture/navigation_concepts",
    "title": "Navigation Concepts",
    "content": "# Navigation Concepts\n\nThis documentation covers a set of important topics to understand related to\nnavigation. For a timeline of how a given navigation proceeds, see [Life of a\nNavigation](navigation.md).\n\n[TOC]\n\n\n## Same-Document and Cross-Document Navigations\n\nChromium defines two types of navigations based on whether the navigation\nresults in a new document or not. A _cross-document_ navigation is one that\nresults in creating a new document to replace an existing document. This is\nthe type of navigation that most users are familiar with. A _same-document_\nnavigation does not create a new document, but rather keeps the same document\nand changes state associated with it. A same-document navigation does create a\nnew session history entry, even though the same document remains active. This\ncan be the result of one of the following cases:\n\n* Navigating to a fragment within an existing document (e.g.\n  `https://foo.com/1.html#fragment`).\n* A document calling the `history.pushState()` or `history.replaceState()` APIs.\n* A new document created via `document.open()`, which may change the URL to\n  match the document that initiated the call (possibly from another frame).\n* A session history navigation that stays in the same document, such as going\n  back/forward to an existing entry for the same document.\n\n\n## Browser-Initiated and Renderer-Initiated Navigations\n\nChromium also defines two types of navigations based on which process started\nthe navigation: _browser-initiated_ and _renderer-initiated_. This distinction\nis useful when making decisions about navigations, for example whether an\nongoing navigation needs to be cancelled or not when a new navigation is\nstarting. It is also used for some security decisions, such as whether to\ndisplay the target URL of the navigation in the address bar or not.\nBrowser-initiated navigations are more trustworthy, as they are usually in\nresponse to a user interaction with the UI of the browser. Renderer-initiated\nnavigations originate in the renderer process, which may be under the control of\nan attacker. Note that some renderer-initiated navigations may be considered\nuser-initiated, if they were performed with a [user\nactivation](https://mustaqahmed.github.io/user-activation-v2/) (e.g., links),\nwhile others are not user-initiated (e.g., script navigations).\n\n\n## Last Committed, Pending, and Visible URLs\n\nMany features care about the URL or Origin of a given document, or about a\npending navigation, or about what is showing in the address bar. These are all\ndifferent concepts with different security implications, so be sure to use the\ncorrect value for your use case.\n\nSee [Origin vs URL](security/origin-vs-url.md) when deciding whether to check\nthe Origin or URL. In many cases that care about the security context, Origin\nshould be preferred.\n\nThe _last committed_ URL or Origin represents the document that is currently in\nthe frame, regardless of what is showing in the address bar. This is almost\nalways what should be used for feature-related state, unless a feature is\nexplicitly tied to the address bar (e.g., padlock icon). This is empty if no\nnavigation is ever committed. e.g. if a tab is newly open for a navigation but\nthen the navigation got cancelled. See\n`RenderFrameHost::GetLastCommittedOrigin` (or URL) and\n`NavigationController::GetLastCommittedEntry`.\n\nThe _pending_ URL exists when a main frame navigation has started but has not\nyet committed. This URL is only sometimes shown to the user in the address bar;\nsee the description of visible URLs below. Features should rarely need to care\nabout the pending URL, unless they are probing for a navigation they expect to\nhave started. See `NavigationController::GetPendingEntry`.\n\nThe _visible_ URL is what the address bar displays. This is carefully managed to\nshow the main frame's last committed URL in most cases, and the pending URL in\ncases where it is safe and unlikely to be abused for a _URL spoof attack_ (where\nan attacker is able to display content as if it came from a victim URL). In\ngeneral, the visible URL is:\n\n * The pending URL for browser-initiated navigations like typed URLs or\n   bookmarks, excluding session history navigations. This becomes empty if the\n   navigation is cancelled.\n * The last committed URL for renderer-initiated navigations, where an attacker\n   might have control over the contents of the document and the pending URL.\n   This is also used when there is no ongoing navigations, and it is empty when\n   no navigation is ever committed.\n * A renderer-initiated navigation's URL is only visible while pending if it\n   opens in a new unmodified tab (so that an unhelpful `about:blank` URL is not\n   displayed), but only until another document tries to access the initial empty\n   document of the new tab. For example, an attacker window might open a new tab\n   to a slow victim URL, then inject content into the initial `about:blank`\n   document as if the slow URL had committed. If that occurs, the visible URL\n   reverts to `about:blank` to avoid a URL spoof scenario. Once the initial\n   navigation commits in the new tab, pending renderer-initiated navigation URLs\n   are no longer displayed.\n\n\n## Virtual URLs\n\nVirtual URLs are a way for features to change how certain URLs are displayed to\nthe user (whether visible or committed). They are generally implemented using\nBrowserURLHandlers. Examples include:\n\n * View Source URLs, where the `view-source:` prefix is not present in the\n   actual committed URL.\n * DOM Distiller URLs, where the original URL is displayed to the user rather\n   than the more complex distiller URL.\n\n\n## Redirects\n\nNavigations can redirect to other URLs in two different ways.\n\nA _server redirect_ happens when the browser receives a 300-level HTTP response\ncode before the document commits, telling it to request a different URL,\npossibly cross-origin. The new request will usually be an HTTP GET request,\nunless the redirect is triggered by a 307 or 308 response code, which preserves\nthe original request method and body. Server redirects are managed by a single\nNavigationRequest. No document is committed to session history, but the original\nURL remains in the redirect chain.\n\nIn contrast, a _client redirect_ happens after a document has committed, when\nthe HTML in the document instructs the browser to request a new document (e.g.,\nvia meta tags or JavaScript). Blink classifies the navigation as a client\nredirect based partly on how much time has passed. In this case, a session\nhistory item is created for the redirecting document, but it gets replaced when\nthe actual destination document commits. A separate NavigationRequest is used\nfor the second navigation.\n\n\n## Concurrent Navigations\n\nMany navigations can be in progress simultaneously. In general, every frame is\nconsidered independent and may have its own navigations(s), with each tracked by\na NavigationRequest. Within a frame, it is possible to have multiple concurrent\nnavigations:\n\n * **A cross-document navigation waiting for its final response (at most one per\n   frame).** The NavigationRequest is owned by FrameTreeNode during this stage,\n   which can take several seconds. Some special case navigations do not use a\n   network request and skip this stage (e.g., `about:blank`, `about:srcdoc`,\n   MHTML).\n * **A queue of cross-document navigations that are between \"ready to commit\"\n   and \"commit,\" while the browser process waits for a commit acknowledgement\n   from the renderer process.** While rare, it is possible for multiple\n   navigations to be in this stage concurrently if the renderer process is slow.\n   The NavigationRequests are owned by the RenderFrameHost during this stage,\n   which is usually short-lived.\n * **Same-document navigations.** These can be:\n    * Renderer-initiated (e.g., `pushState`, fragment link click). In this case,\n      the browser process creates and destroys a NavigationRequest in the same\n      task.\n    * Browser-initiated (e.g., omnibox fragment change). In this case, the\n      browser process creates a NavigationRequest owned by the RenderFrameHost\n      and immediately tells the renderer to commit.\n\nNote that the navigation code is not re-entrant. Callers must not start a new\nnavigation while a call to `NavigateWithoutEntry` or\n`NavigateToExistingPendingEntry` is on the stack, to avoid a CHECK that guards\nagainst use-after-free for `pending_entry_`.\n\n\n## Rules for Canceling Navigations\n\nWe generally do not want an abusive page to prevent the user from navigating\naway, such as by endlessly starting new navigations that interrupt or cancel the\nuser's attempts. Generally, a new navigation will cancel an existing one in a\nframe, but we make the following exception: a renderer-initiated navigation is\nignored iff there is an ongoing browser-initiated navigation and the new\nnavigation lacks a user activation. (This is implemented in\n`Navigator::ShouldIgnoreIncomingRendererRequest`.)\n\nNavigationThrottles also have an ability to cancel navigations when desired by a\nfeature. Keep in mind that it is problematic to simulate a redirect by canceling\na navigation and starting a new one, since this may lose relevant context from\nthe original navigation (e.g., ReloadType, CSP state, Sec-Fetch-Metadata state,\nredirect chain, etc), and it will lead to unexpected observer events and metrics\n(e.g., extra navigation starts, inflated numbers of canceled navigations, etc).\nFeature authors that want to simulate redirects may want to consider using a\nURLLoaderRequestInterceptor instead.\n\n\n## Error Pages\n\nThere are several types of error pages that can be displayed when a navigation\nis not successful.\n\nThe server can return a custom error page, such as a 400 or 500 level HTTP\nresponse code page. These pages are rendered much like a successful navigation\nto the site (and go into an appropriate process for that site), but the error\ncode is available and `NavigationHandle::IsErrorPage()` is true.\n\nIf the navigation fails to get a response from the server (e.g., the DNS lookup\nfails), then Chromium will display an error page. For main frames, this error\npage will be in a special error page process, not affiliated with any site or\ncontaining any untrustworthy content from the web. In these failed cases,\nNetErrorHelperCore may try to reload the URL at a later time (e.g., if a network\nconnection comes back online), to load the document in an appropriate process.\n\nIf instead the navigation is blocked (e.g., by an extension API or a\nNavigationThrottle), then Chromium will similarly display an error page in a\nspecial error page process. However, in blocked cases, Chromium will not attempt\nto reload the URL at a later time.\n\n\n## Interstitial Pages\n\nInterstitial pages are implemented as committed error pages. (Prior to\n[issue 448486](https://crbug.com/448486), they were implemented as overlays.)\nThe original in-progress navigation is canceled when the interstitial is\ndisplayed, and Chromium repeats the navigation if the user chooses to proceed.\n\nNote that some interstitials can be shown after a page has committed (e.g., when\na subresource load triggers a Safe Browsing error). In this case, Chromium\nnavigates away from the original page to the interstitial page, with the intent\nof replacing the original NavigationEntry. However, the original NavigationEntry\nis preserved in `NavigationControllerImpl::entry_replaced_by_post_commit_error_`\nin case the user chooses to dismiss the interstitial and return to the original\npage.\n"
  },
  {
    "path": "architecture/navigation",
    "title": "Life of a Navigation",
    "content": "# Life of a Navigation\n\nNavigation is one of the main functions of a browser. It is the process through\nwhich the user loads documents. This documentation traces the life of a\nnavigation from the time a URL is typed in the URL bar to the time the web page\nis completely loaded. This is one example of many types of navigations, some of\nwhich may start in different places (e.g., in the renderer process).\n\nSee also:\n * [Life of a Navigation tech talk](https://youtu.be/mX7jQsGCF6E) and\n   [slides](https://docs.google.com/presentation/d/1YVqDmbXI0cllpfXD7TuewiexDNZYfwk6fRdmoXJbBlM/edit),\n   for an overview from Chrome University.\n * [Navigation Concepts](navigation_concepts.md), for useful notes on\n   navigation-related concepts in Chromium.\n\n[TOC]\n\n\n## BeforeUnload\n\nOnce a URL is entered, the first step of a navigation is to execute the\nbeforeunload event handler of the previous document, if a document is already\nloaded. This allows the previous document to prompt the user whether they want\nto leave, to avoid losing any unsaved data. In this case, the user can cancel\nthe navigation and no more work will be performed.\n\n\n## Network Request and Response\n\nIf there is no beforeunload handler registered, or the user agrees to proceed,\nthe next step is making a network request to the specified URL to retrieve the\ncontents of the document to be rendered. (Note that not all navigations will go\nto the actual network, for cases like ServiceWorkers, WebUI, cache, data:, etc.)\nAssuming no network error is encountered (e.g. DNS resolution error, socket\nconnection timeout, etc.), the server will respond with data, with the response\nheaders coming first. The parsed headers give enough information to determine\nwhat needs to be done next.\n\nThe HTTP response code allows the browser process to know whether one of the\nfollowing conditions has occurred:\n\n* A successful response follows (2xx)\n* A redirect has been encountered (response 3xx)\n* An HTTP level error has occurred (response 4xx, 5xx)\n\nThere are two cases where a navigation network request can complete without\nresulting in a new document being rendered. The first one is HTTP response code\n204 or 205, which tells the browser that the response was successful, but there\nis no content that follows, and therefore the current document must remain\nactive. The other case is when the server responds with a `Content-Disposition`\nresponse header indicating that the response must be treated as a download\ninstead of a navigation.\n\nIf the server responds with a redirect, Chromium makes another request based on\nthe HTTP response code and the Location header. The browser continues following\nredirects until either an error or a successful response is encountered.\n\nOnce there are no more redirects, the network stack determines if MIME type\nsniffing is needed to detect what type of response the server has sent. This is\nonly needed if the response is not a 204/205 nor a download, doesn't already\nhave a `Content-Type` response header, and doesn’t include a\n`X-Content-Type-Options: nosniff` response header. If MIME type sniffing is\nneeded, the network stack will read a small chunk of the actual response data\nbefore proceeding with the commit.\n\n\n## Commit\n\nAt this point the response is passed from the network stack to the browser\nprocess to be used for rendering a new document. The browser process selects\nan appropriate renderer process for the new document based on the origin and\nheaders of the response as well as the current process model and isolation\npolicy. It then sends the response to the chosen process, waiting for it to\ncreate the document and send an acknowledgement. This acknowledgement from the\nrenderer process marks the _commit_ time, when the browser process changes its\nsecurity state to reflect the new document and creates a session history entry\nfor the previous document.\n\nAs part of creating the new document, the old document needs to be unloaded.\nIn navigations that stay in the same renderer process, the old document is\nunloaded by Blink before the new document is created, including running any\nregistered unload handlers. In the case of a navigation that goes\ncross-process, any unload handlers are executed in the previous document’s\nprocess concurrently with the creation of the new document in the new process.\n\nOnce the creation of the new document is complete and the browser process\nreceives the commit message from the renderer process, the navigation is\ncomplete.\n\n\n## Loading\n\nEven once navigation is complete, the user doesn't actually see the new page\nyet. Most people use the word navigation to describe the act of moving from\none page to another, but in Chromium we separate that process into two phases.\nSo far we have described the _navigation_ phase; once the navigation has been\ncommitted, Chromium moves into the _loading_ phase. Loading consists of\nreading the remaining response data from the server, parsing it, rendering the\ndocument so it is visible to the user, executing any script accompanying it,\nand loading any subresources specified by the document.\n\nThe main reason for splitting into these two phases is that errors are treated\ndifferently before and after a navigation commits. Consider the case where the\nserver responds with an HTTP error code. When this happens, the browser still\ncommits a new document, but that document is an error page. The error page is\neither generated based on the HTTP response code or read as the response data\nfrom the server. On the other hand, if a successful navigation has committed a\nreal document and has moved to the loading phase, it is still possible to\nencounter an error, for example a network connection can be terminated or\ntimes out. In that case the browser displays as much of the new document as it\ncan, without showing an error page.\n\n\n## WebContentsObserver\n\nChromium exposes the various stages of navigation and document loading through\nmethods on the [WebContentsObserver] interface.\n\n### Navigation\n\n* `DidStartNavigation` - invoked after executing the beforeunload event handler\n  and before making the initial network request.\n* `DidRedirectNavigation` - invoked every time a server redirect is encountered.\n* `ReadyToCommitNavigation` - invoked at the time the browser process has\n  determined that it will commit the navigation and has picked a renderer\n  process for it, but before it has sent it to the renderer process. It is not\n  invoked for same-document navigations.\n* `DidFinishNavigation` - invoked once the navigation has committed. The commit\n  can be either an error page if the server responded with an error code or a\n  successful document.\n\n\n### Loading\n\n* `DidStartLoading` - invoked once per WebContents, when a navigation is about\n  to start, after executing the beforeunload handler. This is equivalent to the\n  browser UI starting to show a spinner or other visual indicator for\n  navigation and is invoked before the DidStartNavigation method for the\n  navigation.\n* `DOMContentLoaded` - invoked per RenderFrameHost, when the document itself\n  has completed loading, but before subresources may have completed loading.\n* `DidFinishLoad` - invoked per RenderFrameHost, when the document and all of\n  its subresources have finished loading.\n* `DidStopLoading` - invoked once per WebContents, when the top-level document,\n  all of its subresources, all subframes, and their subresources have completed\n  loading. This is equivalent to the browser UI stop showing a spinner or other\n  visual indicator for navigation and loading.\n* `DidFailLoad` - invoked per RenderFrameHost, when the document load failed,\n  for example due to network connection termination before reading all of the\n  response data.\n\n\n## NavigationThrottles\n\nNavigationThrottles allow observing, deferring, blocking, and canceling a given\nnavigation. They should not generally be used for modifying a navigation (e.g.,\nsimulating a redirect), as discussed in\n[Navigation Concepts](navigation_concepts.md#rules-for-canceling-navigations).\nThey are typically registered in\n`NavigationThrottleRunner::RegisterNavigationThrottles` or\n`ContentBrowserClient::CreateThrottlesForNavigation`.\n\nThe most common NavigationThrottles events are `WillStartRequest`,\n`WillRedirectRequest`, and `WillProcessResponse`, which allow intercepting a\nnavigation before sending the network request, during any redirects, and after\nreceiving the response. These events are only invoked on navigations that\nrequire a URLLoader (see NavigationRequest::NeedsUrlLoader).\nA NavigationThrottle that wishes to intercept a non-URLLoader navigation\n(same-document navigations, about:blank, etc.) should register itself in\n`NavigationThrottleRunner::RegisterNavigationThrottlesForCommitWithoutUrlLoader`,\nand will get a single `WillCommitWithoutUrlLoader` event instead of the full\nset of events centered on network requests. Page-activation navigations, such\nas activating a prerendered page or restoring a page from the back-forward\ncache, skip NavigationThrottles entirely.\n\n[WebContentsObserver]: https://source.chromium.org/chromium/chromium/src/+/main:content/public/browser/web_contents_observer.h\n"
  },
  {
    "path": "architecture/module-layering",
    "title": "Modern Chromium Module Layering Architecture (v134+)",
    "content": "# Modern Chromium Module Layering Architecture (v134+)\r\n\r\nThe architectural evolution of Chromium v134+ represents a sophisticated transformation from its early multi-process design into a comprehensive, service-oriented ecosystem. This modern layering approach emphasizes security, performance, maintainability, and cross-platform excellence while supporting cutting-edge web technologies and enterprise requirements.\r\n\r\n---\r\n\r\n## 1. Architectural Evolution & Design Philosophy\r\n\r\n### Historical Context and Modern Transformation\r\nChromium's architecture has evolved from a simple Browser/Renderer separation into a sophisticated multi-layered ecosystem. Early versions focused primarily on process isolation for security, while v134+ introduces comprehensive service architecture, enhanced security boundaries, and AI-powered capabilities.\r\n\r\n### Core Design Principles (v134+)\r\n- **Zero-Trust Security**: Every component operates under least-privilege with continuous verification\r\n- **Service-Oriented Architecture**: Modular services with capability-based communication via Mojo IPC\r\n- **Platform Agnostic Design**: Consistent behavior across Windows, macOS, Linux, Android, iOS, and Chrome OS\r\n- **Performance Excellence**: Sub-100ms navigation, 120+ FPS rendering with VRR display optimization\r\n- **Extensibility & Maintainability**: Clean abstractions enabling custom browser development and feature addition\r\n\r\n---\r\n\r\n## 2. Modern Layered Architecture Overview\r\n\r\n### Layer 1: Platform Abstraction Foundation\r\n```\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                     Platform Layer (base/)                  │\r\n├─────────────────────────────────────────────────────────────┤\r\n│ • Cross-platform abstractions (Windows, macOS, Linux)      │\r\n│ • Memory management with RAII and smart pointers           │\r\n│ • Threading primitives and task scheduling                 │\r\n│ • Cryptographic services and quantum-resistant algorithms  │\r\n│ • Hardware abstraction (CPU features, GPU capabilities)    │\r\n└─────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### Layer 2: Core Infrastructure Services\r\n```\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                  Infrastructure Services                    │\r\n├─────────────────────────────────────────────────────────────┤\r\n│ • Mojo IPC with capability-based security                  │\r\n│ • Service Manager with dependency injection                │\r\n│ • Process lifecycle management and health monitoring       │\r\n│ • Memory optimization and garbage collection               │\r\n│ • Performance instrumentation and telemetry                │\r\n└─────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### Layer 3: Core Web Platform (content/)\r\n```\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                 Web Platform Core (content/)                │\r\n├─────────────────────────────────────────────────────────────┤\r\n│ • Blink rendering engine with advanced DOM/CSS processing  │\r\n│ • V8 JavaScript engine with WebAssembly support           │\r\n│ • Site isolation with enhanced security boundaries         │\r\n│ • Navigation and frame management with security policies   │\r\n│ • Web APIs implementation (WebGPU, WebNN, Storage, etc.)   │\r\n└─────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### Layer 4: Specialized Services Ecosystem\r\n```\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                   Service Ecosystem                         │\r\n├─────────────────────────────────────────────────────────────┤\r\n│ • Network Service (HTTP/3, QUIC, DNS-over-HTTPS)          │\r\n│ • Storage Service (encrypted databases, OPFS)              │\r\n│ • Audio Service (spatial audio, hardware acceleration)     │\r\n│ • GPU Service (Vulkan, ray tracing, ML acceleration)       │\r\n│ • ML Service (TensorFlow Lite, privacy-preserving AI)      │\r\n│ • Device Service (WebHID, WebUSB, permission management)   │\r\n└─────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### Layer 5: Component Architecture\r\n```\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                  Component Layer                            │\r\n├─────────────────────────────────────────────────────────────┤\r\n│ • Reusable feature modules with clean interfaces           │\r\n│ • Privacy Sandbox components (Topics, FLEDGE, Attribution) │\r\n│ • Security components (sandboxing, CFI, exploit mitigation)│\r\n│ • Performance components (Core Web Vitals optimization)    │\r\n│ • Accessibility and internationalization components        │\r\n└─────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### Layer 6: Application Layer (chrome/)\r\n```\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                   Application Layer                         │\r\n├─────────────────────────────────────────────────────────────┤\r\n│ • Browser UI with modern design systems                    │\r\n│ • Extensions API v3 with enhanced security                 │\r\n│ • Enterprise features (policy management, SSO)             │\r\n│ • Developer tools and debugging interfaces                 │\r\n│ • Custom browser modifications and enterprise integration  │\r\n└─────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n---\r\n\r\n## 3. Service-Oriented Architecture Deep Dive\r\n\r\n### Modern Service Design Patterns\r\nChromium v134+ implements a sophisticated service architecture that goes far beyond traditional process separation:\r\n\r\n#### Service Manager and Dependency Injection\r\n```cpp\r\n// Modern service registration with capability-based security\r\nclass CustomBrowserService : public service_manager::Service {\r\n public:\r\n  explicit CustomBrowserService(service_manager::mojom::ServiceRequest request)\r\n      : service_binding_(this, std::move(request)) {\r\n    registry_.AddInterface(base::BindRepeating(\r\n        &CustomBrowserService::CreateSecureInterface,\r\n        base::Unretained(this)));\r\n  }\r\n\r\n private:\r\n  void CreateSecureInterface(\r\n      custom::mojom::SecureInterfaceRequest request) {\r\n    secure_interface_bindings_.AddBinding(\r\n        std::make_unique<SecureInterfaceImpl>(), std::move(request));\r\n  }\r\n\r\n  service_manager::ServiceBinding service_binding_;\r\n  service_manager::BinderRegistry registry_;\r\n  mojo::BindingSet<custom::mojom::SecureInterface> secure_interface_bindings_;\r\n};\r\n```\r\n\r\n#### Enhanced Service Communication\r\n```cpp\r\n// Capability-based service communication with security validation\r\nclass ServiceConnector {\r\n public:\r\n  template<typename Interface>\r\n  void ConnectToService(mojo::InterfaceRequest<Interface> request) {\r\n    if (!ValidateServiceCapability<Interface>()) {\r\n      SECURITY_LOG(ERROR) << \"Service capability validation failed\";\r\n      return;\r\n    }\r\n    \r\n    content::GetServiceManagerConnection()\r\n        ->GetConnector()\r\n        ->BindInterface(Interface::Name_, std::move(request));\r\n  }\r\n\r\n private:\r\n  template<typename Interface>\r\n  bool ValidateServiceCapability() {\r\n    return security_policy_.HasCapability(Interface::Name_);\r\n  }\r\n\r\n  SecurityPolicy security_policy_;\r\n};\r\n```\r\n\r\n### Service Isolation and Security Boundaries\r\nEach service operates in its own security context with strict capability enforcement:\r\n\r\n- **Network Service**: Isolated network processing with encrypted DNS and HTTP/3 support\r\n- **Storage Service**: Encrypted data persistence with privacy-preserving access controls\r\n- **GPU Service**: Hardware-accelerated rendering with Vulkan backend and security isolation\r\n- **Audio Service**: Low-latency audio processing with hardware acceleration and spatial audio\r\n- **ML Service**: On-device machine learning with privacy-preserving inference capabilities\r\n\r\n---\r\n\r\n## 4. Modern Process Architecture & Security Model\r\n\r\n### Enhanced Multi-Process Design\r\n```\r\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\r\n│  Browser Process│    │ Renderer Process│    │   GPU Process   │\r\n│                 │    │                 │    │                 │\r\n│ • UI Management │◄──►│ • Site Isolation│◄──►│ • Viz Compositor│\r\n│ • Service Coord │    │ • Blink Rendering│    │ • Vulkan Backend│\r\n│ • Policy Mgmt   │    │ • V8 JavaScript │    │ • ML Acceleration│\r\n│ • Security Enf  │    │ • WebAssembly   │    │ • Ray Tracing   │\r\n└─────────────────┘    └─────────────────┘    └─────────────────┘\r\n         │                       │                       │\r\n         ▼                       ▼                       ▼\r\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\r\n│ Network Service │    │ Storage Service │    │  Audio Service  │\r\n│                 │    │                 │    │                 │\r\n│ • HTTP/3 & QUIC │    │ • Encrypted DB  │    │ • Spatial Audio │\r\n│ • DNS-over-HTTPS│    │ • OPFS Support  │    │ • HW Acceleration│\r\n│ • Privacy Proxy │    │ • Cache Mgmt    │    │ • Real-time FX  │\r\n└─────────────────┘    └─────────────────┘    └─────────────────┘\r\n```\r\n\r\n### Advanced Security Architecture\r\n```cpp\r\n// Site isolation with enhanced security boundaries\r\nclass SiteIsolationPolicy {\r\n public:\r\n  struct SecurityBoundary {\r\n    // Enhanced origin isolation with sub-resource integrity\r\n    url::Origin primary_origin;\r\n    std::vector<url::Origin> related_origins;\r\n    SecurityContext security_context;\r\n    PermissionPolicy permission_policy;\r\n    \r\n    // Hardware-assisted security features\r\n    bool cfi_enabled = true;\r\n    bool memory_tagging_enabled = true;\r\n    bool pointer_authentication_enabled = true;\r\n  };\r\n\r\n  bool ShouldIsolateOrigin(const url::Origin& origin) const {\r\n    // Advanced heuristics for origin isolation\r\n    return IsHighRiskOrigin(origin) || \r\n           HasSpecialPermissions(origin) ||\r\n           RequiresEnhancedSecurity(origin);\r\n  }\r\n\r\n private:\r\n  bool IsHighRiskOrigin(const url::Origin& origin) const {\r\n    return high_risk_origins_.contains(origin) ||\r\n           IsKnownMaliciousOrigin(origin);\r\n  }\r\n\r\n  std::unordered_set<url::Origin> high_risk_origins_;\r\n  ThreatIntelligence threat_intelligence_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 5. Component Architecture & Modularity\r\n\r\n### Modern Component Design Patterns\r\nComponents in v134+ follow strict architectural principles for maintainability and extensibility:\r\n\r\n#### Component Interface Definition\r\n```cpp\r\n// Modern component with capability-based interfaces\r\nclass PrivacySandboxComponent : public Component {\r\n public:\r\n  // Component lifecycle with enhanced initialization\r\n  bool Initialize(const ComponentConfig& config) override {\r\n    if (!ValidateConfiguration(config)) {\r\n      return false;\r\n    }\r\n    \r\n    // Initialize Topics API with differential privacy\r\n    topics_api_ = std::make_unique<TopicsAPI>(\r\n        config.privacy_budget, config.noise_parameters);\r\n    \r\n    // Initialize FLEDGE with enhanced security\r\n    fledge_api_ = std::make_unique<FLEDGEAPI>(\r\n        config.security_policy, config.auction_parameters);\r\n    \r\n    return RegisterMojoInterfaces();\r\n  }\r\n\r\n  // Capability-based interface exposure\r\n  void BindTopicsInterface(\r\n      mojo::PendingReceiver<privacy_sandbox::mojom::TopicsAPI> receiver) {\r\n    if (!HasTopicsCapability()) {\r\n      SECURITY_LOG(WARNING) << \"Topics capability not granted\";\r\n      return;\r\n    }\r\n    topics_api_receivers_.Add(topics_api_.get(), std::move(receiver));\r\n  }\r\n\r\n private:\r\n  bool ValidateConfiguration(const ComponentConfig& config) {\r\n    return config.IsValid() && config.HasRequiredCapabilities();\r\n  }\r\n\r\n  bool HasTopicsCapability() const {\r\n    return capability_manager_.HasCapability(\"privacy_sandbox.topics\");\r\n  }\r\n\r\n  std::unique_ptr<TopicsAPI> topics_api_;\r\n  std::unique_ptr<FLEDGEAPI> fledge_api_;\r\n  mojo::ReceiverSet<privacy_sandbox::mojom::TopicsAPI> topics_api_receivers_;\r\n  CapabilityManager capability_manager_;\r\n};\r\n```\r\n\r\n### Component Registration and Discovery\r\n```cpp\r\n// Modern component registry with dependency injection\r\nclass ComponentRegistry {\r\n public:\r\n  template<typename ComponentType>\r\n  void RegisterComponent(std::unique_ptr<ComponentType> component) {\r\n    static_assert(std::is_base_of_v<Component, ComponentType>);\r\n    \r\n    const std::string component_name = ComponentType::GetName();\r\n    \r\n    // Validate component dependencies\r\n    if (!ValidateDependencies<ComponentType>()) {\r\n      LOG(ERROR) << \"Component dependencies not satisfied: \" << component_name;\r\n      return;\r\n    }\r\n    \r\n    // Register component with capability constraints\r\n    components_[component_name] = std::move(component);\r\n    capability_manager_.RegisterComponentCapabilities<ComponentType>();\r\n  }\r\n\r\n  template<typename ComponentType>\r\n  ComponentType* GetComponent() {\r\n    const std::string component_name = ComponentType::GetName();\r\n    auto it = components_.find(component_name);\r\n    \r\n    if (it == components_.end()) {\r\n      return nullptr;\r\n    }\r\n    \r\n    return static_cast<ComponentType*>(it->second.get());\r\n  }\r\n\r\n private:\r\n  template<typename ComponentType>\r\n  bool ValidateDependencies() {\r\n    for (const auto& dependency : ComponentType::GetDependencies()) {\r\n      if (components_.find(dependency) == components_.end()) {\r\n        return false;\r\n      }\r\n    }\r\n    return true;\r\n  }\r\n\r\n  std::unordered_map<std::string, std::unique_ptr<Component>> components_;\r\n  CapabilityManager capability_manager_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 6. Performance & Optimization Architecture\r\n\r\n### Advanced Performance Monitoring\r\n```cpp\r\n// Real-time performance monitoring with Core Web Vitals\r\nclass PerformanceMonitor {\r\n public:\r\n  struct PerformanceMetrics {\r\n    // Core Web Vitals with enhanced precision\r\n    base::TimeDelta largest_contentful_paint;\r\n    base::TimeDelta interaction_to_next_paint;\r\n    double cumulative_layout_shift;\r\n    \r\n    // Advanced metrics for v134+\r\n    base::TimeDelta time_to_interactive;\r\n    base::TimeDelta first_input_delay;\r\n    double throughput_score;\r\n    \r\n    // Hardware-specific metrics\r\n    double gpu_utilization;\r\n    double memory_pressure;\r\n    double thermal_state;\r\n  };\r\n\r\n  void RecordNavigationMetrics(const GURL& url, \r\n                              const PerformanceMetrics& metrics) {\r\n    // Record with privacy-preserving aggregation\r\n    performance_database_.RecordMetrics(\r\n        GetOriginHash(url), metrics, GetPrivacyBudget());\r\n    \r\n    // Trigger optimization recommendations\r\n    if (ShouldOptimize(metrics)) {\r\n      optimization_engine_.TriggerOptimization(url, metrics);\r\n    }\r\n  }\r\n\r\n private:\r\n  bool ShouldOptimize(const PerformanceMetrics& metrics) {\r\n    return metrics.largest_contentful_paint > kLCPThreshold ||\r\n           metrics.cumulative_layout_shift > kCLSThreshold ||\r\n           metrics.interaction_to_next_paint > kINPThreshold;\r\n  }\r\n\r\n  PerformanceDatabase performance_database_;\r\n  OptimizationEngine optimization_engine_;\r\n  PrivacyBudgetManager privacy_budget_manager_;\r\n};\r\n```\r\n\r\n### Memory Management Architecture\r\n```cpp\r\n// Advanced memory management with predictive optimization\r\nclass MemoryManager {\r\n public:\r\n  enum class MemoryPressureLevel {\r\n    kNone,\r\n    kModerate,\r\n    kCritical\r\n  };\r\n\r\n  struct MemoryAllocationStrategy {\r\n    // Adaptive allocation based on usage patterns\r\n    size_t initial_capacity;\r\n    double growth_factor;\r\n    size_t max_capacity;\r\n    \r\n    // Machine learning-based prediction\r\n    bool enable_predictive_allocation = true;\r\n    bool enable_compression = true;\r\n    bool enable_memory_mapping = true;\r\n  };\r\n\r\n  void OptimizeMemoryUsage(MemoryPressureLevel pressure_level) {\r\n    switch (pressure_level) {\r\n      case MemoryPressureLevel::kModerate:\r\n        TriggerGarbageCollection();\r\n        CompressInactiveFrames();\r\n        break;\r\n        \r\n      case MemoryPressureLevel::kCritical:\r\n        DiscardBackgroundTabs();\r\n        FreeNonEssentialCaches();\r\n        TriggerEmergencyCompaction();\r\n        break;\r\n        \r\n      default:\r\n        PerformPredictiveOptimization();\r\n        break;\r\n    }\r\n  }\r\n\r\n private:\r\n  void TriggerGarbageCollection() {\r\n    v8_isolate_->RequestGarbageCollectionForTesting(\r\n        v8::Isolate::kFullGarbageCollection);\r\n  }\r\n\r\n  void CompressInactiveFrames() {\r\n    for (auto& frame : inactive_frames_) {\r\n      frame->CompressMemoryFootprint();\r\n    }\r\n  }\r\n\r\n  v8::Isolate* v8_isolate_;\r\n  std::vector<std::unique_ptr<Frame>> inactive_frames_;\r\n  PredictiveOptimizer memory_predictor_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 7. Custom Browser Development Integration\r\n\r\n### Modern Extension Points\r\nChromium v134+ provides sophisticated extension points for custom browser development:\r\n\r\n#### Custom Service Integration\r\n```cpp\r\n// Custom service with enterprise-grade capabilities\r\nclass CustomEnterpriseService : public service_manager::Service {\r\n public:\r\n  struct EnterpriseConfiguration {\r\n    // Policy management\r\n    PolicyConfiguration policy_config;\r\n    \r\n    // Single Sign-On integration\r\n    SSOConfiguration sso_config;\r\n    \r\n    // Compliance and auditing\r\n    ComplianceConfiguration compliance_config;\r\n    \r\n    // Custom security policies\r\n    SecurityPolicyConfiguration security_config;\r\n  };\r\n\r\n  void InitializeEnterpriseFeatures(const EnterpriseConfiguration& config) {\r\n    // Initialize policy management\r\n    policy_manager_ = std::make_unique<PolicyManager>(config.policy_config);\r\n    \r\n    // Setup SSO integration\r\n    sso_provider_ = SSOProviderFactory::Create(config.sso_config);\r\n    \r\n    // Configure compliance monitoring\r\n    compliance_monitor_ = std::make_unique<ComplianceMonitor>(\r\n        config.compliance_config);\r\n    \r\n    // Apply custom security policies\r\n    ApplySecurityPolicies(config.security_config);\r\n  }\r\n\r\n private:\r\n  void ApplySecurityPolicies(const SecurityPolicyConfiguration& config) {\r\n    security_enforcer_.ApplyPolicies(config);\r\n    \r\n    // Register for security events\r\n    security_enforcer_.RegisterEventHandler(\r\n        base::BindRepeating(&CustomEnterpriseService::OnSecurityEvent,\r\n                           base::Unretained(this)));\r\n  }\r\n\r\n  std::unique_ptr<PolicyManager> policy_manager_;\r\n  std::unique_ptr<SSOProvider> sso_provider_;\r\n  std::unique_ptr<ComplianceMonitor> compliance_monitor_;\r\n  SecurityEnforcer security_enforcer_;\r\n};\r\n```\r\n\r\n#### Custom UI Integration\r\n```cpp\r\n// Modern custom UI component with accessibility and theming\r\nclass CustomBrowserUI : public views::View {\r\n public:\r\n  CustomBrowserUI() {\r\n    SetLayoutManager(std::make_unique<views::BoxLayout>(\r\n        views::BoxLayout::Orientation::kVertical));\r\n    \r\n    // Custom toolbar with accessibility support\r\n    custom_toolbar_ = AddChildView(std::make_unique<CustomToolbar>());\r\n    custom_toolbar_->SetAccessibleName(u\"Custom Browser Toolbar\");\r\n    \r\n    // Enhanced content area with security indicators\r\n    content_area_ = AddChildView(std::make_unique<SecureContentArea>());\r\n    \r\n    // Status bar with privacy and performance metrics\r\n    status_bar_ = AddChildView(std::make_unique<EnhancedStatusBar>());\r\n  }\r\n\r\n  // Theme integration with system preferences\r\n  void OnThemeChanged() override {\r\n    views::View::OnThemeChanged();\r\n    \r\n    const ui::ColorProvider* color_provider = GetColorProvider();\r\n    SetBackground(views::CreateSolidBackground(\r\n        color_provider->GetColor(ui::kColorWindowBackground)));\r\n    \r\n    // Update custom components with new theme\r\n    custom_toolbar_->UpdateTheme(color_provider);\r\n    status_bar_->UpdateTheme(color_provider);\r\n  }\r\n\r\n private:\r\n  CustomToolbar* custom_toolbar_ = nullptr;\r\n  SecureContentArea* content_area_ = nullptr;\r\n  EnhancedStatusBar* status_bar_ = nullptr;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 8. Modern Build System & Development Workflow\r\n\r\n### Advanced Build Configuration\r\n```python\r\n# Modern GN build configuration for custom browser\r\n# BUILD.gn\r\n\r\nimport(\"//build/config/features.gni\")\r\nimport(\"//chrome/common/features.gni\")\r\n\r\n# Custom browser configuration\r\ndeclare_args() {\r\n  # Enable custom enterprise features\r\n  enable_custom_enterprise_features = true\r\n  \r\n  # Enhanced security features\r\n  enable_enhanced_security = true\r\n  \r\n  # Performance optimizations\r\n  enable_performance_optimizations = true\r\n  \r\n  # AI/ML integration\r\n  enable_ml_features = true\r\n}\r\n\r\n# Custom browser executable\r\nexecutable(\"custom_browser\") {\r\n  sources = [\r\n    \"custom_browser_main.cc\",\r\n    \"custom_browser_main.h\",\r\n  ]\r\n\r\n  deps = [\r\n    \"//chrome:chrome_initial\",\r\n    \"//custom/browser:browser_lib\",\r\n    \"//custom/common:common_lib\",\r\n  ]\r\n\r\n  if (enable_custom_enterprise_features) {\r\n    deps += [ \"//custom/enterprise:enterprise_lib\" ]\r\n    defines += [ \"ENABLE_CUSTOM_ENTERPRISE_FEATURES\" ]\r\n  }\r\n\r\n  if (enable_enhanced_security) {\r\n    deps += [ \"//custom/security:security_lib\" ]\r\n    defines += [ \"ENABLE_ENHANCED_SECURITY\" ]\r\n  }\r\n\r\n  if (enable_ml_features) {\r\n    deps += [\r\n      \"//components/ml:ml_service\",\r\n      \"//third_party/tensorflow_lite\",\r\n    ]\r\n    defines += [ \"ENABLE_ML_FEATURES\" ]\r\n  }\r\n}\r\n\r\n# Custom service library\r\ncomponent(\"custom_service_lib\") {\r\n  sources = [\r\n    \"custom_service.cc\",\r\n    \"custom_service.h\",\r\n    \"custom_service_impl.cc\",\r\n    \"custom_service_impl.h\",\r\n  ]\r\n\r\n  deps = [\r\n    \"//base\",\r\n    \"//mojo/public/cpp/bindings\",\r\n    \"//services/service_manager/public/cpp\",\r\n  ]\r\n\r\n  public_deps = [\r\n    \"//custom/public/mojom\",\r\n  ]\r\n}\r\n```\r\n\r\n### Modern Development Tools Integration\r\n```bash\r\n#!/bin/bash\r\n# Modern development workflow script\r\n\r\n# Environment setup with advanced tooling\r\nsetup_development_environment() {\r\n    echo \"Setting up Chromium v134+ development environment...\"\r\n    \r\n    # Install modern build tools\r\n    python3 -m pip install --upgrade build-tools\r\n    \r\n    # Configure advanced debugging\r\n    gn gen out/Debug --args='\r\n        is_debug=true\r\n        symbol_level=2\r\n        enable_iterator_debugging=true\r\n        use_goma=true\r\n        enable_nacl=false\r\n        enable_custom_features=true\r\n    '\r\n    \r\n    # Setup performance profiling\r\n    gn gen out/Profile --args='\r\n        is_debug=false\r\n        symbol_level=1\r\n        enable_profiling=true\r\n        use_thin_lto=true\r\n        enable_custom_features=true\r\n    '\r\n    \r\n    # Configure security-hardened build\r\n    gn gen out/Security --args='\r\n        is_debug=false\r\n        is_cfi=true\r\n        use_cfi_icall=true\r\n        enable_control_flow_integrity=true\r\n        enable_custom_security=true\r\n    '\r\n}\r\n\r\n# Advanced testing with comprehensive coverage\r\nrun_comprehensive_tests() {\r\n    echo \"Running comprehensive test suite...\"\r\n    \r\n    # Unit tests with enhanced coverage\r\n    ninja -C out/Debug custom_browser_unittests\r\n    ./out/Debug/custom_browser_unittests --gtest_output=xml:test_results.xml\r\n    \r\n    # Integration tests\r\n    ninja -C out/Debug browser_tests\r\n    ./out/Debug/browser_tests --test-launcher-filter-file=custom_tests.filter\r\n    \r\n    # Security tests\r\n    ninja -C out/Security security_tests\r\n    ./out/Security/security_tests\r\n    \r\n    # Performance benchmarks\r\n    ninja -C out/Profile performance_tests\r\n    ./out/Profile/performance_tests --benchmark_format=json\r\n}\r\n```\r\n\r\n---\r\n\r\n## 9. Enterprise Integration & Deployment\r\n\r\n### Modern Enterprise Features\r\n```cpp\r\n// Enterprise policy management with modern configuration\r\nclass EnterprisePolicyManager {\r\n public:\r\n  struct PolicyConfiguration {\r\n    // Security policies\r\n    SecurityPolicySet security_policies;\r\n    \r\n    // Network policies with advanced controls\r\n    NetworkPolicySet network_policies;\r\n    \r\n    // Content filtering with ML-based classification\r\n    ContentFilteringPolicy content_filtering;\r\n    \r\n    // Privacy and compliance policies\r\n    PrivacyPolicySet privacy_policies;\r\n    \r\n    // Custom extension policies\r\n    ExtensionPolicySet extension_policies;\r\n  };\r\n\r\n  void ApplyEnterpriseConfiguration(const PolicyConfiguration& config) {\r\n    // Apply security policies with validation\r\n    for (const auto& policy : config.security_policies) {\r\n      if (ValidateSecurityPolicy(policy)) {\r\n        security_enforcer_.ApplyPolicy(policy);\r\n      }\r\n    }\r\n    \r\n    // Configure network policies\r\n    network_policy_enforcer_.ApplyPolicies(config.network_policies);\r\n    \r\n    // Setup content filtering with ML classification\r\n    content_filter_.Initialize(config.content_filtering);\r\n    \r\n    // Apply privacy policies with GDPR compliance\r\n    privacy_manager_.ApplyPolicies(config.privacy_policies);\r\n  }\r\n\r\n private:\r\n  SecurityPolicyEnforcer security_enforcer_;\r\n  NetworkPolicyEnforcer network_policy_enforcer_;\r\n  MLContentFilter content_filter_;\r\n  PrivacyPolicyManager privacy_manager_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 10. Future Architecture Considerations\r\n\r\n### Emerging Technologies Integration\r\n- **Quantum-Resistant Cryptography**: Preparing for post-quantum security\r\n- **Advanced AI Integration**: On-device language models and intelligent automation\r\n- **Extended Reality (XR)**: WebXR enhancements and spatial computing\r\n- **Edge Computing**: Distributed rendering and computation\r\n- **Blockchain Integration**: Decentralized identity and secure transactions\r\n\r\n### Architectural Evolution Roadmap\r\n- **Microkernel Architecture**: Further service isolation and modularity\r\n- **WebAssembly System Interface**: Enhanced WASI support for system integration\r\n- **Advanced ML Optimization**: Hardware-accelerated inference and training\r\n- **Privacy-Preserving Technologies**: Homomorphic encryption and secure computation\r\n- **Cross-Platform Optimization**: Universal binary format and adaptive UIs\r\n\r\n---\r\n\r\n## Summary\r\n\r\nModern Chromium v134+ represents the pinnacle of browser architecture, featuring sophisticated service-oriented design, enhanced security boundaries, performance optimization, and extensibility for custom browser development. This layered architecture provides:\r\n\r\n1. **Robust Foundation**: Platform-agnostic base with advanced abstractions\r\n2. **Service Excellence**: Modular services with capability-based security\r\n3. **Performance Leadership**: Sub-100ms navigation and 120+ FPS rendering\r\n4. **Security Innovation**: Zero-trust architecture with hardware-assisted protection\r\n5. **Developer Experience**: Comprehensive tools and extension points\r\n6. **Enterprise Ready**: Policy management, compliance, and integration capabilities\r\n\r\nThe architecture continues to evolve, embracing emerging technologies while maintaining backward compatibility and providing excellent developer experience for custom browser development.\r\n\r\n**Related Documentation**:\r\n- [Process Model](../architecture/process-model.md) - Multi-process architecture details\r\n- [IPC Internals](../architecture/ipc-internals.md) - Mojo communication patterns\r\n- [Security Architecture](../architecture/security/sandbox-architecture.md) - Security boundaries and sandboxing\r\n- [Performance Optimization](../modules/performance.md) - Performance best practices\r\n- [Custom Development Guide](../getting-started/custom-development.md) - Building custom features\r\n\r\n---\r\n\r\n*Last Updated: August 2025 | Chromium v134+ | Advanced Module Layering Architecture*\r\n"
  },
  {
    "path": "architecture/life_of_a_frame",
    "title": "Life of a frame",
    "content": "# Life of a frame\n\n[Original google doc](https://docs.google.com/document/d/1y6ZlGc2uH4ZBIrpEVM_wmfHgyvrcQG3LPRzG32FuVkQ/edit?usp=sharing)\n\n[TOC]\n\n![flow diagram](images/Life_of_frame.png)\n\n## Steps\n\n### `[1]` BeginFrame\nBeginImplFrame starts a compositor frame (with each vsync) that will wait up until a deadline for a BeginMainFrame plus activation to complete before it times out and draws any asynchronous animation and scroll/pinch updates.\n\n### `[2]` Compositor updates\nAll updates on Impl thread such as scrolls and zoom animations.\n\n### `[3]` BeginMainFrame\nIf there is main thread damage, BeginMainFrame will be sent and if there is no main thread damage steps [3], [4] and [5] will be skipped (SchedulerStateMachine::ShouldSendBeginMainFrame()).\n\n### `[4]` MainThreadUpdates\nAll updates on main thread such as HandleInputEvents, Animate, StyleUpdate and LayoutUpdate.\n\n### `[5]` Commit\nCommits updates back to Impl thread for activation. Specifically, the main thread copies its own version of layertree onto the pending tree (pending_tree_) on the impl thread. The main thread is blocked during the copying process.\n\n### `[6]` Wait for raster\nRasterization may occur asynchronously on separate threads, or even another process (OOP-R, or out-of-process rasterization). If rasterization cannot be finished before the deadline for activation, the pipeline will not perform the copying from pending tree to the active tree in step [7], and will instead proceed to step [8] with whatever is already in the active tree.\n\n### `[7]` Activation\nActivation is the process of pushing layer trees and properties from the pending tree to the active tree (active_tree_). Note that Impl thread can also directly manipulate the activate tree to reflect updates the impl thread makes in step [2].\n\n### `[8]` Wait for deadline\nThe deadline for compositor frame submission is provided by the GPU process (Scheduler::BeginImplFrameWithDeadline()).\n\n### `[9]` SubmitCompositorFrame\nAll activated updates will be submitted to the GPU process as a compositor frame that is produced based on the content of the active tree. If any of the steps [2], [3], [4], [5], [6] takes so long that the update that step is responsible for cannot be delivered to the active tree by deadline, the pipeline will proceed to submit with the existing active tree without said update. This is a possible source of a dropped frame.\n\n### `[10]` AggregateSurfaces\nAfter each client submitted CompositorFrame (or signalled that it DidNotProduceFrame) the Display Compositor can proceed with the draw. Note that if the DisplayScheduler hits a deadline it will still draw if any client has submitted a new CompositorFrame, even if it's still waiting on a response for other clients. Before the actual draw could happen SurfaceAggregator will recursively walk over compositor frames and replace SurfaceQuads (quads produced by SurfaceLayer) with contents of the embedded compositor frame. This step produces single CompositorFrame in the end that can be drawn by the Display Compositor.\n\n### `[11]` Draw Frame\nDuring draw Display Compositor will go over quads and render passes in the aggregated compositor frame and produce draw commands. For SkiaRenderer it's recording of Deferred Display Lists (DDL).\n\n### `[12]` RequestSwap\nAfter commands were recorded they will be submitted to the GPU thread to replay along with SwapBuffers request to show the result on screen.\n\n### `[13]` Wait until ready to draw\nWhen the draw was submitted to GPU Main Thread some of the resources may be not ready yet. Chrome uses SyncTokens to ensure this type of synchronization. GPU Task submitted at step [12] won't be scheduled until all associated SyncTokens will be signaled.\n\n### `[14]` Queueing delay\nGPU Main Thread does all the GPU work and by the time display compositor is ready to draw it might still be busy doing other tasks (e.g raster for next frame). gpu::Scheduler uses cooperative multi-tasking and can't preempt the current task unless it yields, so the task submitted by the display compositor might have to wait until the current task (and potentially some other high priority tasks) are finished.\n\n### `[15]` GPU draw\nFinally tasks that DisplayCompositor posted to GPU Main thread executed and we replay draw commands recorded during Draw Frame [11]. For SkiaRenderer Skia will be replaying DDLs and issue commands to the GPU. This step is when we finally submit the job to the GPU (not GPU thread on CPU).\n\n### `[16]` Swap\nThe GPU work has been submitted and we signal that we want to present the result (Submits commands to request displaying framebuffer and/or overlays after drawing new content into them). Depending on the platform this step can be blocking or not and take a substantial amount of time (eg. if we have too many queued swaps sometimes this will just block until the next vblank).\n\n### `[17]` Presentation\nThis is when the GPU has finished all the work and the display controller started scanning out the results. The pixels are finally visible on the screen. Depending on the platform, before this could happen the system compositor might need to do its work. Unfortunately it's not possible to get the exact timestamp on every platform and Chrome makes best efforts to estimate it. This estimation can be something like the swap time (on mac where we don't have any better information), the completion time for work on the GPU aligned to the next vblank as a good estimate, or a signal from the OS with the exact time the content was displayed.\n\n## PipelineReporter trace events\n\nMultiple stages of the pipeline are highlighted for each frame in the traces titled **PipelineReporter**. Stages that are tracked in PipelineReporter events are:\n- BeginImplFrameToSendBeginMainFrame\n- SendBeginMainFrameToCommit\n- Commit\n- EndCommitToActivation\n- Activation\n- EndActivateToSubmitCompositorFrame\n- SubmitCompositorFrameToPresentationCompositorFrame\n\nImage below shows how each segment of the Pipeline is tracked in PipelineReporter trace events.\n\n![segmented flow diagram](images/Life_of_frame_segmented.png)\n\nIn the traces these pipeline reporters would look like\n\n![PipelineReporter trace event](images/PipelineReporter.png)\n\nIn which the length of each segment demonstrates the latency of that stage.\nAlso SubmitCompositorFrameToPresentationCompositorFrame would have its own, which are:\n- SubmitToReceiveCompositorFrame: The time of communicating the submit (step `[9]`)\n- ReceiveCompositorFrameToStartDraw: The time it takes for steps `[10]` to `[14]`\n- StartDrawToSwapStart: The time that “Draw” takes in GPU Main (step `[15]`)\n- Swap: The time that “Swap” takes in GPU Main (step `[16]`)\n- SwapEndToPresentationCompositorFrame: The remaining time until presentation (step `[17]`)\n\n## Overlapping pipeline reporter events\n\nOne advantage of having multiple processes handling the frame is that multiple frames can be worked on simultaneously. This might create some overlapping pipeline reporter events. In this section we would explore a few examples of these cases.\n\n### Example 1:\n\n![PipelineReporter trace event example_1](images/PipelineReporter_example_1.png)\n\nIn this example the PipelineReporter(PR) on the bottom started earlier and while it was in the *EndActivateToSubmitCompositorFrame*, vsync for the next frame is reached. In this stage, the compositor thread is already done with activation and is waiting for the deadline to submit the compositor frame, so it can start working on the second frame (top PR).\nWhen we reach the deadline for submission of frame#1, the frame#2 is in the stage of *SendBeginMainFrameToCommit* (on the main thread). Frame#1 will be submitted while frame#2 will continue on its stages and will be submitted in the next vsync.\n\n### Example 2:\n\n![PipelineReporter trace event example_2](images/PipelineReporter_example_2.png)\n\nIn this example the two PipelineReporters start at the same time but would not end at the same time. If we look into the arguments of each we would notice that they both correspond to the same sequence number (this would be expected for PRs that start at the same time corresponding to the same vsync).\nIn this case, the PR on top has been in middle of *SendBeginMainFrameToCommit* while the second PR moves to *EndActivateToSubmitCompositorFrame* and then *SubmitCompositorFrameToPresentationCompositorFrame*. That means that while the main thread work was taking place we reached the deadline and submitted the compositor updates that were ready (bottom PR), and the PR on top would do the same with main thread updates on the next vsync.\n\n### Example 3:\n\n![PipelineReporter trace event example_3](images/PipelineReporter_example_3.png)\n\nIn this example the two PRs started at different times but are ending at the same time. This would often happen when a PR takes longer than one vsync and its last stage (e.g. *SubmitCompositorFrameToPresentationCompositorFrame*) would be synced with the next PR (top PR on the image).\nIn such cases the combined update from two PRs would be submitted to the GPU process and be presented together.\n"
  },
  {
    "path": "architecture/ipc-internals",
    "title": "Chromium IPC Internals (Mojo Architecture v134+)",
    "content": "# Chromium IPC Internals (Mojo Architecture v134+)\r\n\r\nChromium's Inter-Process Communication has evolved significantly with the introduction and maturation of **Mojo IPC**, which provides a modern, type-safe, and secure foundation for communication between processes in Chromium's multi-process architecture. This document covers the modern Mojo-based IPC system used in v134+ and its advanced features.\r\n\r\n---\r\n\r\n## Modern IPC Architecture Overview (v134+)\r\n\r\nChromium has transitioned from legacy IPC to **Mojo**, a sophisticated message-passing framework that provides enhanced security, type safety, and performance for cross-process communication.\r\n\r\n### Key Evolution from Legacy IPC:\r\n1. **Mojo Replaces Legacy IPC**: \r\n   - Type-safe interface definitions using Mojom IDL\r\n   - Capability-based security model\r\n   - Advanced message routing and filtering\r\n\r\n2. **Modern Communication Mechanisms**:\r\n   - **Message Pipes**: Bidirectional communication channels\r\n   - **Shared Buffers**: Efficient large data transfer\r\n   - **Data Pipes**: High-throughput streaming\r\n   - **Interface Pipes**: Type-safe service communication\r\n\r\n3. **Platform Abstraction**:\r\n   - Unified API across Windows, Linux, macOS, Android\r\n   - Platform-specific optimizations handled transparently\r\n   - Enhanced security through capability delegation\r\n\r\n---\r\n\r\n## Modern Mojo IPC Components (v134+)\r\n\r\n### 1. **Mojo Core**\r\n- **Low-level primitives**: Message pipes, shared buffers, data pipes\r\n- **Platform abstraction**: Unified interface across all supported platforms\r\n- **Handle management**: Automatic resource cleanup and security enforcement\r\n- **Performance optimization**: Zero-copy transfers and efficient routing\r\n\r\n### 2. **Mojom Interface Definition Language (IDL)**\r\n- **Type-safe interfaces**: Strongly-typed service definitions\r\n- **Code generation**: Automatic client/server stub generation\r\n- **Versioning support**: Interface evolution without breaking compatibility\r\n- **Documentation integration**: Self-documenting service APIs\r\n\r\n### 3. **Service Manager (v134+)**\r\n- **Service discovery**: Automatic service location and connection\r\n- **Capability delegation**: Fine-grained permission system\r\n- **Lifecycle management**: Service startup, shutdown, and recovery\r\n- **Security policy**: Capability-based access control\r\n\r\n### 4. **Interface Brokers**\r\n- **Cross-process binding**: Secure interface establishment\r\n- **Permission validation**: Capability-based access checks\r\n- **Connection management**: Automatic cleanup and error handling\r\n- **Load balancing**: Intelligent service distribution\r\n\r\n### 5. **Mojo Bindings (C++, JavaScript)**\r\n- **Language bindings**: Native integration with C++ and JavaScript\r\n- **Async/Await support**: Modern asynchronous programming patterns\r\n- **Error handling**: Comprehensive error propagation and recovery\r\n- **Performance optimizations**: Efficient serialization and deserialization\r\n\r\n---\r\n\r\n## Modern Message Patterns & Communication (v134+)\r\n\r\n### Service Interface Pattern\r\n```cpp\r\n// Example Mojom interface definition\r\nmodule example.mojom;\r\n\r\ninterface DatabaseService {\r\n  // Async method with callback\r\n  GetUser(int32 user_id) => (User? user);\r\n  \r\n  // Fire-and-forget method  \r\n  LogEvent(string event_name);\r\n  \r\n  // Streaming interface\r\n  WatchUsers() => (array<User> users);\r\n};\r\n```\r\n\r\n### Message Types & Features:\r\n1. **Request/Response Pattern**:\r\n   - Type-safe method calls with strongly-typed responses\r\n   - Automatic timeout handling and error propagation\r\n   - Support for complex data types and nested structures\r\n\r\n2. **Fire-and-Forget Messages**:\r\n   - One-way communication for performance-critical operations\r\n   - No response expected, optimized for throughput\r\n   - Automatic message ordering and delivery guarantees\r\n\r\n3. **Streaming Interfaces**:\r\n   - High-throughput data streaming using data pipes\r\n   - Backpressure handling and flow control\r\n   - Efficient large data transfer without memory copying\r\n\r\n4. **Associated Interfaces**:\r\n   - Ordered message delivery within interface groups\r\n   - Maintains message ordering across related interfaces\r\n   - Critical for coordinated operations like rendering\r\n\r\n### Modern Message Priorities (v134+):\r\n- **URGENT**: Critical system messages (input, vsync)\r\n- **HIGH**: User-visible operations (navigation, rendering)\r\n- **NORMAL**: Standard application logic\r\n- **LOW**: Background tasks and maintenance\r\n- **BEST_EFFORT**: Non-critical telemetry and logging\r\n\r\n---\r\n\r\n## Platform-Specific Implementations (v134+)\r\n\r\n### Windows (Enhanced for v134+):\r\n- **Message Pipes over Named Pipes**:\r\n  - Improved security with restricted access controls\r\n  - Enhanced performance through IOCP integration\r\n  - Support for large message batching\r\n\r\n- **Shared Memory Integration**:\r\n  - Direct memory mapping for large data transfers\r\n  - Copy-on-write semantics for efficient sharing\r\n  - Automatic cleanup on process termination\r\n\r\n- **Security Enhancements**:\r\n  - App Container integration for sandboxed processes\r\n  - Enhanced token validation and capability delegation\r\n  - Protection against handle duplication attacks\r\n\r\n### Linux/POSIX (Enhanced for v134+):\r\n- **Message Pipes over UNIX Domain Sockets**:\r\n  - Abstract namespace sockets for improved security\r\n  - SCM_RIGHTS for secure file descriptor passing\r\n  - Peer credential validation using SO_PEERCRED\r\n\r\n- **Shared Memory via memfd**:\r\n  - Modern memfd_create() for anonymous shared memory\r\n  - Sealing support for immutable shared data\r\n  - Integration with namespace isolation\r\n\r\n- **Security Features**:\r\n  - Seccomp-BPF filtering for system call restriction\r\n  - User namespace integration for privilege isolation\r\n  - Landlock support for filesystem access control\r\n\r\n### macOS (Enhanced for v134+):\r\n- **Message Pipes over Mach Ports**:\r\n  - Native Mach port integration for optimal performance\r\n  - Automatic port right management and cleanup\r\n  - Support for complex data structures via Mach messages\r\n\r\n- **Shared Memory via Mach**:\r\n  - Mach virtual memory objects for efficient sharing\r\n  - Copy-on-write semantics with automatic optimization\r\n  - Integration with macOS memory pressure system\r\n\r\n- **Security Integration**:\r\n  - App Sandbox compatibility with restricted entitlements\r\n  - System Integrity Protection (SIP) compliance\r\n  - Hardened Runtime support for code signing validation\r\n\r\n---\r\n\r\n## Advanced Mojo Features (v134+)\r\n\r\n### 1. **Capability-Based Security**\r\n- **Interface Filtering**: Process-specific access control for service interfaces\r\n- **Capability Delegation**: Secure forwarding of permissions between processes\r\n- **Dynamic Permissions**: Runtime capability adjustment based on context\r\n- **Audit Logging**: Comprehensive tracking of capability usage and violations\r\n\r\n### 2. **High-Performance Optimizations**\r\n- **Zero-Copy Transfers**: Direct memory sharing for large payloads using shared buffers\r\n- **Message Coalescing**: Batching related messages for improved throughput\r\n- **Priority Scheduling**: Critical path optimization for time-sensitive operations\r\n- **Connection Pooling**: Efficient reuse of communication channels\r\n\r\n### 3. **Advanced Data Transfer**\r\n- **Shared Buffers**: \r\n  ```cpp\r\n  // Example: Large data transfer via shared buffer\r\n  mojo::ScopedSharedBufferHandle buffer = \r\n      mojo::SharedBufferHandle::Create(size);\r\n  interface_ptr->ProcessLargeData(std::move(buffer));\r\n  ```\r\n\r\n- **Data Pipes**:\r\n  ```cpp\r\n  // Example: Streaming data with flow control\r\n  mojo::ScopedDataPipeProducerHandle producer;\r\n  mojo::ScopedDataPipeConsumerHandle consumer;\r\n  mojo::CreateDataPipe(nullptr, &producer, &consumer);\r\n  interface_ptr->StreamData(std::move(consumer));\r\n  ```\r\n\r\n### 4. **Modern Error Handling**\r\n- **Connection Error Callbacks**: Automatic cleanup on process termination\r\n- **Message Validation**: Runtime validation of all incoming messages\r\n- **Timeout Management**: Configurable timeouts for request/response patterns\r\n- **Recovery Mechanisms**: Automatic service restart and state restoration\r\n\r\n### 5. **Development & Debugging Tools**\r\n- **Mojo Tracing**: Detailed IPC performance analysis\r\n- **Interface Inspector**: Runtime service discovery and monitoring\r\n- **Message Logging**: Comprehensive audit trail for security analysis\r\n- **Performance Profiling**: Real-time IPC performance metrics\r\n\r\n---\r\n\r\n## Modern Service Architecture Examples (v134+)\r\n\r\n### Network Service Interface:\r\n```cpp\r\n// Modern Mojom interface for network operations\r\nmodule network.mojom;\r\n\r\ninterface NetworkService {\r\n  // URL loading with streaming response\r\n  CreateURLLoader(URLLoaderFactory& factory);\r\n  \r\n  // DNS resolution with caching\r\n  ResolveHost(string hostname) => (array<IPAddress> addresses);\r\n  \r\n  // Certificate validation\r\n  ValidateCertificate(Certificate cert) => (CertificateStatus status);\r\n};\r\n```\r\n\r\n### GPU Service Communication:\r\n```cpp\r\n// GPU process communication for hardware acceleration\r\nmodule gpu.mojom;\r\n\r\ninterface GpuService {\r\n  // Context creation for rendering\r\n  CreateGpuMemoryBuffer(GpuMemoryBufferType type) => \r\n      (GpuMemoryBufferHandle? handle);\r\n  \r\n  // Command buffer for GPU operations\r\n  CreateCommandBuffer(CommandBufferSpec spec) => \r\n      (CommandBuffer? buffer);\r\n  \r\n  // Video decode acceleration\r\n  CreateVideoDecoder(VideoDecoderConfig config) => \r\n      (VideoDecoder? decoder);\r\n};\r\n```\r\n\r\n### Modern Message Flow:\r\n```text\r\nBrowser Process                 Renderer Process\r\n      │                              │\r\n      ├─[Service Discovery]───────────┤\r\n      │                              │\r\n      ├─[Interface Binding]───────────┤\r\n      │                              │\r\n      ├─[Method Call + Callback]──────┤\r\n      │                              │\r\n      ├─[Shared Buffer Transfer]──────┤\r\n      │                              │\r\n      └─[Connection Cleanup]──────────┘\r\n```\r\n\r\n---\r\n\r\n## Debugging & Performance Analysis (v134+)\r\n\r\n### Mojo Debugging Tools:\r\n- **chrome://tracing/**: Advanced IPC timeline analysis with Mojo-specific categories\r\n- **chrome://mojo-internals/**: Real-time interface monitoring and statistics\r\n- **chrome://process-internals/**: Process-specific IPC performance metrics\r\n- **Mojo Shell Inspector**: Service discovery and connection visualization\r\n\r\n### Command Line Debugging:\r\n```bash\r\n# Enable detailed Mojo logging\r\n--enable-logging=stderr --vmodule=\"*mojo*=2\"\r\n\r\n# Trace specific IPC categories\r\n--trace-startup --trace-config=\"mojo,ipc,mojom\"\r\n\r\n# Enable service manager tracing\r\n--enable-service-manager-tracing\r\n\r\n# Debug interface binding issues\r\n--mojo-core-library-path=debug_path\r\n```\r\n\r\n### Performance Optimization:\r\n```cpp\r\n// Example: Optimized large data transfer\r\nclass OptimizedService : public mojom::DataService {\r\n  void TransferLargeData(\r\n      mojo::ScopedSharedBufferHandle buffer,\r\n      TransferLargeDataCallback callback) override {\r\n    \r\n    // Zero-copy processing of shared buffer\r\n    auto mapping = buffer->Map(buffer_size);\r\n    ProcessDataInPlace(mapping.get());\r\n    \r\n    std::move(callback).Run(ProcessingResult::SUCCESS);\r\n  }\r\n};\r\n```\r\n\r\n### Security Analysis:\r\n- **Capability Audit**: Track capability delegation and usage\r\n- **Message Validation**: Runtime validation of all IPC messages\r\n- **Interface Access Control**: Monitor unauthorized service access attempts\r\n- **Connection Security**: Validate all cross-process connections\r\n\r\n---\r\n\r\n## Migration from Legacy IPC (Historical Context)\r\n\r\n### Legacy vs Modern Comparison:\r\n| Feature | Legacy IPC | Modern Mojo IPC |\r\n|---------|------------|-----------------|\r\n| **Type Safety** | Manual serialization | Generated type-safe bindings |\r\n| **Security** | Basic validation | Capability-based access control |\r\n| **Performance** | Copy-heavy operations | Zero-copy optimizations |\r\n| **Platform Support** | Platform-specific code | Unified cross-platform API |\r\n| **Error Handling** | Manual error propagation | Automatic error handling |\r\n| **Debugging** | Limited tooling | Comprehensive debugging suite |\r\n\r\n### Migration Status (v134+):\r\n- **Renderer ↔ Browser**: Fully migrated to Mojo\r\n- **GPU Process**: Complete Mojo integration with Viz compositor\r\n- **Network Service**: Native Mojo implementation\r\n- **Audio/Video Services**: Modern Mojo-based architecture\r\n- **Utility Processes**: Mojo-first design for all new services\r\n\r\n---\r\n\r\n## References & Further Reading\r\n\r\n### Official Documentation:\r\n1. [Mojo Documentation](https://chromium.googlesource.com/chromium/src/+/main/mojo/README.md)\r\n2. [Service Manager Guide](https://chromium.googlesource.com/chromium/src/+/main/services/README.md)\r\n3. [Mojom IDL Specification](https://chromium.googlesource.com/chromium/src/+/main/mojo/public/tools/bindings/README.md)\r\n\r\n### Design Documents:\r\n4. [Mojo Design Principles](https://docs.google.com/document/d/1n7qYjQ5iy8xAkQVe_EJrYd-5_vJwsOomsH9hbw_-WVE)\r\n5. [Service-Oriented Architecture](https://docs.google.com/document/d/15I7sQyQo6zsqXVNAlVd520tdGaS8FCicfHrPacHd_lk)\r\n6. [Security Model Evolution](https://docs.google.com/document/d/1Lj0sKqHg-3VK3n5zCv8gf2FLR1l5QhX1DCt-rqOaKVs)\r\n\r\n### Performance Analysis:\r\n7. [IPC Performance Optimization](https://docs.google.com/document/d/1x5zGP0l5gH1y4B1U5I6vKgvKO2vQ4V7zqF8T0F9a5zQ)\r\n8. [Mojo Benchmarking](https://chromium.googlesource.com/chromium/src/+/main/mojo/core/test/)\r\n\r\n---\r\n\r\n## Summary\r\n\r\nChromium's modern Mojo IPC system represents a significant evolution from legacy IPC mechanisms, providing:\r\n\r\n- **Enhanced Security**: Capability-based access control and comprehensive validation\r\n- **Improved Performance**: Zero-copy transfers and intelligent message routing  \r\n- **Type Safety**: Strong typing through IDL-generated bindings\r\n- **Platform Consistency**: Unified API across all supported platforms\r\n- **Developer Experience**: Comprehensive debugging tools and documentation\r\n- **Future-Proof Architecture**: Extensible design for emerging web platform features\r\n\r\nThe transition to Mojo has enabled Chromium's sophisticated multi-process architecture while maintaining security, performance, and maintainability at scale. For developers working with Chromium, understanding Mojo IPC is essential for effective cross-process communication and service development."
  },
  {
    "path": "architecture/ios_sandbox_forcefield",
    "title": "ForceField: An iOS Sandbox Primitive",
    "content": "# ForceField: An iOS Sandbox Primitive\n\n_**Status:** Filed as FB9007081_ \\\n_**Author:** rsesek@, palmer@_ \\\n_**Created:** 2021-02-04_ \\\n_**Updated:** 2021-02-16_\n\n## Description\n\nThis is a request for a new iOS feature (here called **ForceField**), which\nwould provide app developers a primitive to process-isolate and sandbox\nmemory-unsafe code in a way that is safe for manipulating untrustworthy and\npotentially malicious data.\n\n## Objective\n\nThe goal of ForceField is to improve the safety and security of users, by\nreducing the privilege level of memory-unsafe code that processes untrustworthy\ndata from the Internet.\n\nMany complex applications have components that are written in memory-unsafe\nlanguages like C/C++. While iOS does offer Swift as a (mostly) memory-safe\nlanguage, often these components are shared across platforms or are third-party\ndependencies. Today, if an iOS application uses these components to process data\nfrom the Internet, they make themselves vulnerable to memory unsafety bugs.\n\nA common solution to mitigate these vulnerabilities is to perform the operations\non untrustworthy data in a separate process that runs under a tight sandbox,\nfollowing the [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege).\nCurrently iOS does not offer apps a mechanism to compose their components into\nhigh- and low-privilege execution environments. However, iOS itself uses\n[process isolation and sandboxing](https://googleprojectzero.blogspot.com/2021/01/a-look-at-imessage-in-ios-14.html)\nfor privilege reduction in similar situations. ForceField would give developers\na primitive to do the same, which would help protect the people who use their\napps.\n\n## The Ideal Solution\n\nA perfect implementation of ForceField would allow app developers to create a\nnew component that is packaged in their application bundle. iOS would launch\nthis component as a new, sandboxed process running under a security principal\ndistinct from the bundle’s primary process. ForceField processes would:\n\n*   Not have access to the containing app’s data storage\n*   Not have access to privileged shared system resources (Keychain, clipboard,\n    persistent storage locations)\n*   Not have access to system services that access user data, such as Location\n    Services, Photos, HomeKit, HealthKit, AutoFill, etc.\n*   By default, not have access to draw to the screen\n*   By default, not have network access\n\nThus, ForceField would provide a compute-only process, which the main app\nprocess would communicate with over an IPC mechanism. By default, the only\nresources FourceField could access would be the ones explicitly brokered in; and\nthe only way to extract data from the ForceField process would be for it to\nlikewise send results over IPC to the primary app. ForceField would enable\nrunning memory-unsafe code in such a way that it would be much safer to process\nuntrustworthy and potentially malicious data.\n\nFurthermore, ForceField could be enhanced by allowing the developer to opt-in to\nspecific privileged capabilities, for example network access. This would be\nuseful for initiating NSURLSession connections, allowing the ForceField\ncomponent to directly process untrustworthy network data without needing to\nround-trip it through the primary app component. Removing the round-trip would\nbe more performant and reduce the risk of mis-handling the data in the trusted,\nprimary app component.\n\n## Leverage Existing Technologies\n\niOS’s existing technologies provide all the necessary pieces to create ForceField:\n\n### App Extensions\n\niOS provides a mechanism for apps to run context-limited code in a distinct\nprocess through [App Extensions](https://developer.apple.com/app-extensions/). A\nnew type of “Compute” App Extension could be created to implement ForceField.\nThe app could engage one of its Compute Extensions at any point during its\nlifecycle, whenever it is necessary to process data in a sandbox. An app should\nbe able to launch a Compute App Extension when its foreground, but also\npotentially when processing data downloaded during\n[background refresh](https://developer.apple.com/documentation/uikit/app_and_environment/scenes/preparing_your_ui_to_run_in_the_background/updating_your_app_with_background_app_refresh?).\n\nThe app should also have the ability to forcefully terminate a Compute\nExtension, in response to e.g. user cancellation or memory pressure signals. And\nthe app should be able to register a termination handler for the Compute\nExtension, so it can determine if the process exited cleanly, crashed, was\nkilled by the app, or was killed by the operating system. The operating system\ncould kill all Compute App Extension processes a few seconds after the\nforeground app moves to the background. And the operating system could place\nlimits on total resource consumption (CPU and memory) of the Compute App\nExtension. The Compute App Extension should be tightly sandboxed, per the\ndescription of ForceField above.\n\n### XPC\n\niOS already exposes\n[NSXPCConnection](https://developer.apple.com/documentation/foundation/nsxpcconnection?language=objc)\nand\n[NSXPCInterface](https://developer.apple.com/documentation/foundation/nsxpcinterface?language=objc)\nin the iPhoneOS SDK, and it is used to implement e.g.\n[File Provider](https://developer.apple.com/documentation/fileprovider/nsfileproviderservicesource/2915876-makelistenerendpointandreturnerr?language=objc)\napp extensions. The NSXPC API is also already [used by developers on macOS](https://developer.apple.com/library/archive/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/CreatingXPCServices.html)\nto create application-specific IPC protocols between components. iOS developers\ncould use the NSXPC API to define the IPC protocol between the primary app and\nits ForceField components.\n\n### Entitlements\n\nNew iOS entitlements could be created to enable ForceField components to opt\ninto additional, privileged capabilities on top of the tightly-sandboxed\nbaseline. For example, an entitlement could be created to enable a ForceField\ncomponent to access the network. Another entitlement could enable rendering into\na CALayer brokered in over IPC, to enable remote rendering of UI by the\nForceField process into the primary application process.\n"
  },
  {
    "path": "architecture/gpu_synchronization",
    "title": "GPU Synchronization in Chrome",
    "content": "# GPU Synchronization in Chrome\n\nChrome supports multiple mechanisms for sequencing GPU drawing operations, this\ndocument provides a brief overview. The main focus is a high-level explanation\nof when synchronization is needed and which mechanism is appropriate.\n\n[TOC]\n\n## Glossary\n\n**GL Sync Object**: Generic GL-level synchronization object that can be in a\n\"unsignaled\" or \"signaled\" state. The only current implementation of this is a\nGL fence.\n\n**GL Fence**: A GL sync object that is inserted into the GL command stream. It\nstarts out unsignaled and becomes signaled when the GPU reaches this point in the\ncommand stream, implying that all previous commands have completed.\n\n**Client Wait**: Block the client thread until a sync object becomes signaled,\nor until a timeout occurs.\n\n**Server Wait**: Tells the GPU to defer executing commands issued after a fence\nuntil the fence signals. The client thread continues executing immediately and\ncan continue submitting GL commands.\n\n**CHROMIUM fence sync**: A command buffer specific GL fence that sequences\noperations among command buffer GL contexts without requiring driver-level\nexecution of previous commands.\n\n**Native GL Fence**: A GL Fence backed by a platform-specific cross-process\nsynchronization mechanism.\n\n**GPU Fence Handle**: An IPC-transportable object (typically a file descriptor)\nthat can be used to duplicate a native GL fence into a different process's\ncontext.\n\n**GPU Fence**: A Chrome abstraction that owns a GPU fence handle representing a\nnative GL fence, usable for cross-process synchronization.\n\n## Use case overview\n\nThe core scenario is synchronizing read and write access to a shared resource,\nfor example drawing an image into an offscreen texture and compositing the\nresult into a final image. The drawing operations need to be completed before\nreading to ensure correct output. A typical effect of wrong synchronization is\nthat the output contains blank or incomplete results instead of the expected\nrendered sub-images, causing flickering or tearing.\n\n\"Completed\" in this case means that the end result of using a resource as input\nwill be equivalent to waiting for everything to finish rendering, but it does\nnot necessarily mean that the GPU has fully finished all drawing operations at\nthat time.\n\n## Single GL context: no synchronization needed\n\nIf all access to the shared resource happens in the same GL context, there is no\nneed for explicit synchronization. GL guarantees that commands are logically\nprocessed in the order they are submitted. This is true both for local GL\ncontexts (GL calls via ui/gl/ interfaces) and for a single command buffer GL\ncontext.\n\n## Multiple driver-level GL contexts in the same share group: use GLFence\n\nA process can create multiple GL contexts that are part of the same share group.\nThese contexts can be created in different threads within this process.\n\nIn this case, GL fences must be used for sequencing, for example:\n\n1. Context A: draw image, create GLFence\n1. Context B: server wait or client wait for GLFence, read image\n\n[gl::GLFence](/ui/gl/gl_fence.h) and its subclasses provide wrappers for\nGL/EGL fence handling methods such as `eglFenceSyncKHR` and `eglWaitSyncKHR`.\nThese fence objects can be used cross-thread as long as both thread's GL\ncontexts are part of the same share group.\n\nFor more details, please refer to the underlying extension documentation, for example:\n\n* https://www.khronos.org/opengl/wiki/Synchronization\n* https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_fence_sync.txt\n* https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_wait_sync.txt\n\n## Implementation-dependent: same-thread driver-level GL contexts\n\nMany GL driver implementations are based on a per-thread command queue,\nwith the effect that commands are processed in order even if they were issued\nfrom different contexts on that thread without explicit synchronization.\n\nThis behavior is not part of the GL standard, and some driver implementations\nuse a per-context command queue where this assumption is not true.\n\nSee [issue 510232](http://crbug.com/510243#c23) for an example of a problematic\nsequence:\n\n```\n// In one thread:\nMakeCurrent(A);\nRender1();\nMakeCurrent(B);\nRender2();\nCreateSync(X);\n\n// And in another thread:\nMakeCurrent(C);\nWaitSync(X);\nRender3();\nMakeCurrent(D);\nRender4();\n```\n\nThe only serialization guarantee is that Render2 will complete before Render3,\nbut Render4 could theoretically complete before Render1.\n\nChrome assumes that the render steps happen in order Render1, Render2, Render3,\nand Render4, and requires this behavior to ensure security. If the driver doesn't\nensure this sequencing, Chrome has to emulate it using virtual contexts. (Or by\nusing explicit synchronization, but it doesn't do that today.) See also the\n\"CHROMIUM fence sync\" section below.\n\n## Command buffer GL clients: use CHROMIUM sync tokens\n\nChrome's command buffer IPC interface uses multiple layers. There are multiple\nactive IPC channels (typically one per process, i.e. one per Renderer and one\nfor Browser). Each IPC channel has multiple scheduling groups (also called\nstreams), and each stream can contain multiple command buffers, which in turn\ncontain a sequence of GL commands.\n\nCommand buffers in the same client-side share group must be in the same stream.\nCommand scheduling granuarity is at the stream level, and a client can choose to\ncreate and use multiple streams with different stream priorities. Stream IDs are\narbitrary integers assigned by the client at creation time, see for example the\n[viz::ContextProviderCommandBuffer](/services/viz/public/cpp/gpu/context_provider_command_buffer.h)\nconstructor.\n\nThe CHROMIUM sync token is intended to order operations among command buffer GL\ninstructions. It inserts an internal fence sync command in the stream, flushing\nit appropriately (see below), and generating a sync token from it which is a\ncross-context transportable reference to the underlying fence sync. A\nWaitSyncTokenCHROMIUM call does **not** ensure that the underlying GL commands\nhave been executed at the GPU driver level, this mechanism is not suitable for\nsynchronizing command buffer GL operations with a local driver-level GL context.\n\nSee the\n[CHROMIUM_sync_point](/gpu/GLES2/extensions/CHROMIUM/CHROMIUM_sync_point.txt)\ndocumentation for details.\n\nCommands issued within a single command buffer don't need to be synchronized\nexplicitly, they will be executed in the same order that they were issued.\n\nMultiple command buffers within the same stream can use an ordering barrier to\nsequence their commands. Sync tokens are not necessary. Example:\n\n```c++\n// Command buffers gl1 and gl2 are in the same stream.\nRender1(gl1);\ngl1->OrderingBarrierCHROMIUM()\nRender2(gl2);  // will happen after Render1.\n```\n\nCommand buffers that are in different streams need to use sync tokens. If both\nare using the same IPC channel (i.e. same client process), an unverified sync\ntoken is sufficient, and commands do not need to be flushed to the server:\n\n```c++\n// stream A\nRender1(glA);\nglA->GenUnverifiedSyncTokenCHROMIUM(out_sync_token);\n\n// stream B\nglB->WaitSyncTokenCHROMIUM(sync_token);\nRender2(glB);  // will happen after Render1.\n```\n\nCommand buffers that are using different IPC channels must use verified sync\ntokens. Verification is a check that the underlying fence sync was flushed to\nthe server. Cross-process synchronization always uses verified sync tokens.\n`GenSyncTokenCHROMIUM` will force a shallow flush as a side effect if necessary.\nExample:\n\n```c++\n// IPC channel in process X\nRender1(glX);\nglX->GenSyncTokenCHROMIUM(out_sync_token);\n\n// IPC channel in process Y\nglY->WaitSyncTokenCHROMIUM(sync_token);\nRender2(glY);  // will happen after Render1.\n```\n\nAlternatively, unverified sync tokens can be converted to verified ones in bulk\nby calling `VerifySyncTokensCHROMIUM`. This will wait for a flush to complete as\nnecessary. Use this to avoid multiple sequential flushes:\n\n```c++\ngl->GenUnverifiedSyncTokenCHROMIUM(out_sync_tokens[0]);\ngl->GenUnverifiedSyncTokenCHROMIUM(out_sync_tokens[1]);\ngl->VerifySyncTokensCHROMIUM(out_sync_tokens, 2);\n```\n\n### Implementation notes\n\nCorrectness of the CHROMIUM fence sync mechanism depends on the assumption that\ncommands issued from the command buffer service side happen in the order they\nwere issued in that thread. This is handled in different ways:\n\n* Issue a glFlush on switching contexts on platforms where glFlush is sufficient\n  to ensure ordering, i.e. MacOS. (This approach would not be well suited to\n  tiling GPUs as used on many mobile GPUs where glFlush is an expensive\n  operation, it may force content load/store between tile memory and main\n  memory.) See for example\n  [gl::GLContextCGL::MakeCurrent](/ui/gl/gl_context_cgl.cc):\n```c++\n  // It's likely we're going to switch OpenGL contexts at this point.\n  // Before doing so, if there is a current context, flush it. There\n  // are many implicit assumptions of flush ordering between contexts\n  // at higher levels, and if a flush isn't performed, OpenGL commands\n  // may be issued in unexpected orders, causing flickering and other\n  // artifacts.\n```\n\n* Force context virtualization so that all commands are issued into a single\n  driver-level GL context. This is used on Qualcomm/Adreno chipsets, see [issue\n  691102](http://crbug.com/691102).\n\n* Assume per-thread command queues without explicit synchronization. GLX\n  effectively ensures this. On Windows, ANGLE uses a single D3D device\n  underneath all contexts which ensures strong ordering.\n\nGPU control tasks are processed out of band and are only partially ordered in\nrespect to GL commands. A gpu_control task always happens before any following\nGL commands issued on the same IPC channel. It usually executes before any\npreceding unflushed GL commands, but this is not guaranteed. A\n`ShallowFlushCHROMIUM` ensures that any following gpu_control tasks will execute\nafter the flushed GL commands.\n\nIn this example, DoTask will execute after GLCommandA and before GLCommandD, but\nthere is no ordering guarantee relative to CommandB and CommandC:\n\n```c++\n  // gles2_implementation.cc\n\n  helper_->GLCommandA();\n  ShallowFlushCHROMIUM();\n\n  helper_->GLCommandB();\n  helper_->GLCommandC();\n  gpu_control_->DoTask();\n\n  helper_->GLCommandD();\n\n  // Execution order is one of:\n  //   A | DoTask B C | D\n  //   A | B DoTask C | D\n  //   A | B C DoTask | D\n```\n\nThe shallow flush adds the pending GL commands to the service's task queue, and\nthis task queue is also used by incoming gpu control tasks and processed in\norder. The `ShallowFlushCHROMIUM` command returns as soon as the tasks are\nqueued and does not wait for them to be processed.\n\n## Cross-process transport: GpuFence and GpuFenceHandle\n\nSome platforms such as Android (most devices N and above) and ChromeOS support\nsynchronizing a native GL context with a command buffer GL context through a\nGpuFence.\n\nUse the static `gl::GLFence::IsGpuFenceSupported()` method to check at runtime if\nthe current platform has support for the GpuFence mechanism including\nGpuFenceHandle transport.\n\nThe GpuFence mechanism supports two use cases:\n\n* Create a GLFence object in a local context, convert it to a client-side\nGpuFence, duplicate it into a command buffer service-side gpu fence, and\nissue a server wait on the command buffer service side. That service-side\nwait will be unblocked when the *client-side* GpuFence signals.\n\n* Create a new command buffer service-side gpu fence, request a GpuFenceHandle\nfrom it, use this handle to create a native GL fence object in the local\ncontext, then issue a server wait on the local GL fence object. This local\nserver wait will be unblocked when the *service-side* gpu fence signals.\n\nThe [CHROMIUM_gpu_fence\nextension](/gpu/GLES2/extensions/CHROMIUM/CHROMIUM_gpu_fence.txt) documents\nthe GLES API as used through the command buffer interface. This section contains\nadditional information about the integration with local GL contexts that is\nneeded to work with these objects.\n\n### Driver-level wrappers\n\nIn general, you should use the static `gl::GLFence::CreateForGpuFence()` and\n`gl::GLFence::CreateFromGpuFence()` factory methods to create a\nplatform-specific local fence object instead of using an implementation class\ndirectly.\n\nFor Android and ChromeOS, the\n[gl::GLFenceAndroidNativeFenceSync](/ui/gl/gl_fence_android_native_fence_sync.h)\nimplementation wraps the\n[EGL_ANDROID_native_fence_sync](https://www.khronos.org/registry/EGL/extensions/ANDROID/EGL_ANDROID_native_fence_sync.txt)\nextension that allows creating a special EGLFence object from which a file\ndescriptor can be extracted, and then creating a duplicate fence object from\nthat file descriptor that is synchronized with the original fence.\n\n### GpuFence and GpuFenceHandle\n\nA [gfx::GpuFence](/ui/gfx/gpu_fence.h) object owns a GPU fence handle\nrepresenting a native GL fence. The `AsClientGpuFence` method casts it to a\nClientGpuFence type for use with the [CHROMIUM_gpu_fence\nextension](/gpu/GLES2/extensions/CHROMIUM/CHROMIUM_gpu_fence.txt)'s\n`CreateClientGpuFenceCHROMIUM` call.\n\nA [gfx::GpuFenceHandle](/ui/gfx/gpu_fence_handle.h) is an IPC-transportable\nwrapper for a file descriptor or other underlying primitive object, and is used\nto duplicate a native GL fence into another process. It has value semantics and\ncan be copied multiple times, and then consumed exactly one time. Consumers take\nownership of the underlying resource. Current GpuFenceHandle consumers are:\n\n* The `gfx::GpuFence(gpu_fence_handle)` constructor takes ownership of the\n  handle's resources without constructing a local fence.\n\n* The IPC subsystem closes resources after sending. The typical idiom is to call\n  `gfx::CloneHandleForIPC(handle)` on a GpuFenceHandle retrieved from a\n  scope-lifetime object to create a copied handle that will be owned by the IPC\n  subsystem.\n\n### Sample Code\n\nA usage example for two-process synchronization is to sequence access to a\nglobally shared drawable such as an AHardwareBuffer on Android, where the\nwriter uses a local GL context and the reader is a command buffer context in\nthe GPU process. The writer process draws into an AHardwareBuffer-backed\nSharedImage in the local GL context, then creates a gpu fence to mark the end of\ndrawing operations:\n\n```c++\n    // This example assumes that GpuFence is supported. If not, the application\n    // should fall back to a different transport or synchronization method.\n    DCHECK(gl::GLFence::IsGpuFenceSupported())\n\n    // ... write to the shared drawable in local context, then create\n    // a local fence.\n    std::unique_ptr<gl::GLFence> local_fence = gl::GLFence::CreateForGpuFence();\n\n    // Convert to a GpuFence.\n    std::unique_ptr<gfx::GpuFence> gpu_fence = local_fence->GetGpuFence();\n    // It's ok for local_fence to be destroyed now, the GpuFence remains valid.\n\n    // Create a matching gpu fence on the command buffer context, issue\n    // server wait, and destroy it.\n    GLuint id = gl->CreateClientGpuFenceCHROMIUM(gpu_fence.AsClientGpuFence());\n    // It's ok for gpu_fence to be destroyed now.\n    gl->WaitGpuFenceCHROMIUM(id);\n    gl->DestroyGpuFenceCHROMIUM(id);\n\n    // ... read from the shared drawable via command buffer. These reads\n    // will happen after the local_fence has signalled. The local\n    // fence and gpu_fence dn't need to remain alive for this.\n```\n\nIf a process wants to consume a drawable that was produced through a command\nbuffer context in the GPU process, the sequence is as follows:\n\n```c++\n    // Set up callback that's waiting for the drawable to be ready.\n    void callback(std::unique_ptr<gfx::GpuFence> gpu_fence) {\n        // Create a local context GL fence from the GpuFence.\n        std::unique_ptr<gl::GLFence> local_fence =\n            gl::GLFence::CreateFromGpuFence(*gpu_fence);\n        local_fence->ServerWait();\n        // ... read from the shared drawable in the local context.\n    }\n\n    // ... write to the shared drawable via command buffer, then\n    // create a gpu fence:\n    GLuint id = gl->CreateGpuFenceCHROMIUM();\n    context_support->GetGpuFenceHandle(id, base::BindOnce(callback));\n    gl->DestroyGpuFenceCHROMIUM(id);\n```\n\nIt is legal to create the GpuFence on a separate command buffer context instead\nof on the command buffer channel that did the drawing operations, but in that\ncase `gl->WaitSyncTokenCHROMIUM()` or equivalent must be used to sequence the\noperations between the distinct command buffer contexts as usual.\n"
  },
  {
    "path": "architecture/frame_trees",
    "title": "Demystifying FrameTree Concepts",
    "content": "# Demystifying FrameTree Concepts\n\n## What are Frame Trees?\n\nThere are two representations of FrameTrees used in rendering Web Pages.\n- Blink's [FrameTrees](../third_party/blink/renderer/core/page/frame_tree.h)\n- Content's [FrameTrees](../content/browser/renderer_host/frame_tree.h)\n\nThese concepts are very similar, however on the content side a placeholder\n[FrameTreeNode](../content/browser/renderer_host/frame_tree_node.h) can\nbe placed in the FrameTree to hold another frame tree. This `FrameTreeNode`'s\ncurrent RenderFrameHost will have a valid\n`RenderFrameHostImpl::inner_tree_main_frame_tree_node_id` frame tree node\nID.\n\nThe renderer side (Blink) will have no notion of this placeholder in the\nframe tree and its frame tree appears as it would for the web exposed\n[window.frames](https://developer.mozilla.org/en-US/docs/Web/API/Window/frames)\n\n## Why do we nest frame trees?\n\nCertain features that nest documents require a stronger boundary than what would\nbe achievable with iframes. We want to prevent the exposure of information\nacross this boundary. This may be for privacy reasons where we enforce\ncommunication restrictions between documents on the web (e.g. fenced frames).\nThe boundary could also be between content on the web and parts of the user\nagent implemented with web technologies (e.g. chrome's PDF viewer, webview tag).\n\n## What are Outermost Main Frames?\n\nBuilding on the concept above that a `FrameTree` can have an embedded\n`FrameTree` (and many nesting levels of them), there is the concept of\nthe `OutermostMainFrame`. The OutermostMainFrame is the main frame (root)\nof a FrameTree that is not embedded in other FrameTrees.\n[See footnote 1.](#footnote_1)\n\nSo that does mean there can be __multiple main frames__ in a displayed\ntab to the user. For features like `fencedframes` the inner `FrameTree`\nhas a main frame but it will not be an `OutermostMainFrame`.\n\nTo determine whether something is a main frame `RenderFrameHost::GetParent`\nis typically used. Likewise there is a `RenderFrameHost::GetParentOrOuterDocument` to determine if something is an `OutermostMainFrame`.\n\n```\nExample Frame Tree:\n    A\n     B (iframe)\n     C (fenced frame - placeholder frame) [See footnote 2.]\n      C* (main frame in fenced frame).\n\n    C* GetParent returns null.\n    C* GetParentOrOuterDocument returns A.\n    C GetParent & GetParentOrOuterDocument returns A.\n    B GetParent & GetParentOrOuterDocument returns A.\n    A GetParent & GetParentOrOuterDocument returns nullptr.\n```\n\n## Can I have multiple outermost main frames?\n\nPrerender and back/forward cache are features where there can be\nother outermost main frame present in a `WebContents`.\n\n## What are Pages?\n\nPages can be an overloaded term so we will clarify what we mean by the\nclass concepts:\n- Blink's [Page](../third_party/blink/renderer/core/page/page.h)\n- Content's [Page](../content/public/browser/page.h)\n\nThe two usages are very similar, they effectively are an object representing\nthe state of a `FrameTree`. Since frames can be hosted in different renderers\n(for isolation) there may be a number of Blink `Page` objects, one for each\nrenderer that participates in the rendering of a single `Page` in content.\n\n## What is the Primary Page?\n\nThere is only ever one Primary Page for a given `WebContents`. The primary\npage is defined by the fact that the main frame is the `OutermostMainFrame`\nand being actively displayed in the tab.\n\nThe primary page can change over time (see\n`WebContentsObserver::PrimaryPageChanged`). The primary page can change when\nnavigating, a `Page` is restored from the `BackForwardCache` or from the\nprendering pages.\n\n## Relationships between core classes in content/\n\nA WebContents represents a tab. A WebContents owns a FrameTree, the \"primary\nframe tree,\" for what is being displayed in the tab. A WebContents may\nindirectly own additional FrameTrees for features such as prerendering.\n\nA FrameTree consists of FrameTreeNodes. A FrameTreeNode contains a\nRenderFrameHost. FrameTreeNodes reflect the frame structure in the renderer.\nRenderFrameHosts represent documents loaded in a frame (roughly,\n[see footnote 3](#footnote_3)). As a frame navigates its RenderFrameHost may\nchange, but its FrameTreeNode stays the same.\n\nIn the case of nested frame trees, the RenderFrameHost corresponding to the\nhosting document owns the inner FrameTree (possibly through an intermediate\nobject, as is the case for content::FencedFrame).\n\n## \"MPArch\"\n\n\"MPArch,\" short for Multiple Page Architecture, refers to the name of the\nproject that introduced the capability of having multiple FrameTrees in a\nsingle WebContents.\n\nYou may also see comments which describe features relying on multiple FrameTrees\nin terms of MPArch (e.g. \"ignore navigations from MPArch pages\"). These are in\nreference to \"non-primary\" frame trees as described above.\n\nSee the original [design doc](https://docs.google.com/document/d/1NginQ8k0w3znuwTiJ5qjYmBKgZDekvEPC22q0I4swxQ/edit?usp=sharing)\nfor further info.\n\n## Footnotes\n\n<a name=\"footnote_1\"></a>1: GuestViews (embedding of a WebContents inside another WebContents) are\nconsidered embedded FrameTrees as well. However for consideration of\nOutermostMainFrames (ie. GetParentOrOuterDocument, Primary page) they do not\nescape the WebContents boundary because of the logical embedding boundary.\n\n<a name=\"footnote_2\"></a>2: The placeholder RenderFrameHost is generally not exposed outside\nof the content boundary. Iteration APIs such as ForEachRenderFrameHost\ndo not visit this node.\n\n<a name=\"footnote_3\"></a>3: RenderFrameHost is not 1:1 with a document in the renderer.\nSee [RenderDocument](/docs/render_document.md).\n"
  },
  {
    "path": "architecture/browser-components",
    "title": "Browser Components (Modern Architecture v134+)",
    "content": "# Browser Components (Modern Architecture v134+)\r\n\r\nThis article explores the major C++ components that comprise Chromium's **browser process** in v134+, showcasing the sophisticated service-oriented architecture that drives UI, navigation, security, and cross-process coordination. Understanding these components is essential for grasping how modern Chromium boots up, manages tabs, and orchestrates its multi-process ecosystem.\r\n\r\n---\r\n\r\n## 1. Modern High-Level Architecture (v134+)\r\n\r\n> **Service-Oriented Multi-Process Design**  \r\n> The browser process serves as the central coordinator, managing UI, security policies, and service orchestration while delegating specialized work to dedicated service processes.\r\n\r\n```text\r\n┌─────────────────┐    Mojo IPC    ┌──────────────────┐\r\n│ Browser Process │ ◄─────────────► │ Renderer Process │\r\n│                 │                │                  │\r\n│ ┌─────────────┐ │                │ ┌──────────────┐ │\r\n│ │ UI Manager  │ │                │ │ Blink Engine │ │\r\n│ │ Service Mgr │ │                │ │ V8 Engine    │ │\r\n│ │ Security    │ │                │ │ DOM/Layout   │ │\r\n│ └─────────────┘ │                │ └──────────────┘ │\r\n└─────────────────┘                └──────────────────┘\r\n         │ Mojo                               │\r\n         ▼                                    ▼\r\n┌─────────────────┐                ┌──────────────────┐\r\n│ Service         │                │ GPU Process      │\r\n│ Ecosystem       │                │                  │\r\n│                 │                │ ┌──────────────┐ │\r\n│ • Network       │                │ │ Viz Display  │ │\r\n│ • Audio         │                │ │ Compositor   │ │\r\n│ • Storage       │                │ │ OOP-R        │ │\r\n│ • ML/AI         │                │ └──────────────┘ │\r\n│ • Device        │                └──────────────────┘\r\n└─────────────────┘\r\n```\r\n\r\n**Modern Component Locations**: `src/chrome/`, `src/content/`, `src/services/`, `src/components/`, `src/ui/`\r\n\r\n---\r\n\r\n## 2. Modern Application Startup (v134+ Enhanced)\r\n\r\n### BrowserMain (Enhanced)\r\n- **Entry Point**: `chrome/app/chrome_main_delegate.cc`\r\n- **Advanced Initialization**:\r\n  - **Crashpad Integration**: Modern crash reporting with detailed telemetry\r\n  - **Feature Flag Management**: Runtime feature control with A/B testing support\r\n  - **Service Manager Bootstrap**: Early service discovery and connection\r\n  - **Security Policy Setup**: Site isolation and sandbox configuration\r\n\r\n### ProfileManager (Modern Multi-Profile)\r\n- **Enhanced Profile Loading**: `chrome/browser/profiles/profile_manager.cc`\r\n- **Modern Features**:\r\n  - **Multi-Profile Support**: Independent profile isolation with shared services\r\n  - **Profile Metrics**: Advanced usage analytics and performance tracking\r\n  - **Guest Mode**: Ephemeral browsing with complete isolation\r\n  - **Incognito Enhancement**: Improved privacy with service isolation\r\n\r\n### BrowserProcessImpl (Service Coordinator)\r\n- **Global Service Management**: `chrome/browser/browser_process_impl.cc`\r\n- **Core Services (v134+)**:\r\n  - **PrefService**: Advanced settings with cloud sync\r\n  - **DownloadService**: Modern download management with resumption\r\n  - **HistoryService**: Enhanced history with machine learning insights\r\n  - **SafeBrowsingService**: Real-time threat detection and response\r\n  - **NotificationService**: Cross-platform notification management\r\n\r\n### UI Loop (Platform-Optimized)\r\n- **BrowserWindow Management**: Native window integration per platform\r\n- **Event Loop Integration**: High-performance message pump with priority scheduling\r\n- **Input Handling**: Advanced touch, pen, and gesture support\r\n- **Accessibility**: Modern accessibility tree with cross-process coordination\r\n\r\n---\r\n\r\n## 3. Advanced Tab & Window Management (v134+)\r\n\r\n### TabStripModel (Enhanced)\r\n- **Location**: `chrome/browser/ui/tabs/tab_strip_model.cc`\r\n- **Modern Features**:\r\n  - **Tab Groups**: Visual and functional tab organization\r\n  - **Tab Search**: Intelligent tab discovery and management\r\n  - **Tab Freezing**: Memory optimization with intelligent tab lifecycle\r\n  - **Tab Restore**: Enhanced session restoration with state preservation\r\n\r\n### Browser (Multi-Window Coordinator)\r\n- **Location**: `chrome/browser/ui/browser.cc`\r\n- **Enhanced Capabilities**:\r\n  - **Command Routing**: Advanced action delegation with extension support\r\n  - **Window State Management**: Multi-monitor support with DPI awareness\r\n  - **Keyboard Shortcuts**: Configurable shortcuts with accessibility support\r\n  - **Context Menus**: Dynamic context-aware menu generation\r\n\r\n### WebContents (Content Host)\r\n- **Modern WebContents**: Enhanced content lifecycle management\r\n- **Key Features**:\r\n  - **Site Isolation**: Per-origin process boundaries\r\n  - **Navigation Prediction**: Preloading and speculative navigation\r\n  - **Resource Management**: Intelligent memory and CPU allocation\r\n  - **Security Boundaries**: Enhanced cross-origin protection\r\n\r\n---\r\n\r\n## 4. Modern Navigation & Session Management (v134+)\r\n\r\n### NavigationController (Enhanced)\r\n- **Advanced History Management**: Machine learning-powered navigation prediction\r\n- **Features**:\r\n  - **Back-Forward Cache**: Instant navigation with preserved state\r\n  - **Navigation Timing**: Performance metrics and optimization\r\n  - **Session Restoration**: Robust crash recovery with state preservation\r\n  - **Cross-Process Navigation**: Seamless site isolation boundaries\r\n\r\n### NavigationRequest (Modern Navigation)\r\n- **Location**: `content/browser/loader/navigation_request.cc`\r\n- **Enhanced Flow**:\r\n  - **Navigation Prediction**: Preemptive DNS resolution and connection setup\r\n  - **Security Checks**: Comprehensive security policy validation\r\n  - **Performance Optimization**: Resource prioritization and loading strategies\r\n  - **Error Handling**: Graceful degradation and recovery mechanisms\r\n\r\n### SessionService (Advanced Persistence)\r\n- **Modern Session Management**: Cloud sync integration with local fallback\r\n- **Features**:\r\n  - **Tab Groups Persistence**: Restore tab organization and grouping\r\n  - **Window Layout Restore**: Multi-monitor configuration preservation\r\n  - **Crash Recovery**: Intelligent state restoration with data validation\r\n  - **Privacy-Aware Storage**: Encrypted session data with user consent\r\n\r\n---\r\n\r\n## 5. Service-Oriented Networking & Resource Loading (v134+)\r\n\r\n### Network Service (Standalone Process)\r\n- **Location**: `services/network/`\r\n- **Modern Architecture**:\r\n  - **Process Isolation**: Dedicated network process for enhanced security\r\n  - **HTTP/3 Support**: QUIC protocol implementation with performance optimization\r\n  - **Connection Pooling**: Intelligent connection reuse and load balancing\r\n  - **Certificate Transparency**: Enhanced security with CT log validation\r\n\r\n### URLLoaderFactory (Mojo-Based)\r\n- **Type-Safe Interfaces**: Mojom-defined network request APIs\r\n- **Features**:\r\n  - **Request Prioritization**: Critical resource path optimization\r\n  - **CORS Enforcement**: Strict cross-origin security policy\r\n  - **Content Security Policy**: Advanced CSP validation and reporting\r\n  - **Performance Budgets**: Resource loading limits and throttling\r\n\r\n### ResourceScheduler (Intelligent Management)\r\n- **Advanced Scheduling**: Machine learning-powered resource prioritization\r\n- **Modern Features**:\r\n  - **Core Web Vitals**: Optimization for LCP, FID, and CLS metrics\r\n  - **Background Tab Throttling**: Aggressive resource management for inactive tabs\r\n  - **Network Quality Adaptation**: Dynamic quality adjustment based on connection\r\n  - **Predictive Loading**: Speculative resource loading based on user behavior\r\n\r\n---\r\n\r\n## 6. Modern Storage & State Management (v134+)\r\n\r\n### Profile (Enhanced User Data)\r\n- **Location**: `chrome/browser/profiles/profile.cc`\r\n- **Modern Features**:\r\n  - **Storage Partitioning**: Advanced isolation with per-origin boundaries\r\n  - **Quota Management**: Intelligent storage allocation with user control\r\n  - **Cloud Sync Integration**: Seamless cross-device data synchronization\r\n  - **Privacy Controls**: Granular user control over data storage and sharing\r\n\r\n### Storage Service (Dedicated Process)\r\n- **Process Isolation**: Dedicated storage process for enhanced security\r\n- **Modern Storage APIs**:\r\n  - **Origin Private File System**: Secure file system access for web apps\r\n  - **Persistent Storage**: Enhanced quota management with user prompts\r\n  - **IndexedDB v3**: Performance improvements with better transaction handling\r\n  - **Cache API**: Advanced caching with intelligent eviction policies\r\n\r\n### CookieManager (Mojo-Based)\r\n- **Location**: `services/network/cookie_manager/`\r\n- **Enhanced Features**:\r\n  - **SameSite Enforcement**: Strict SameSite cookie policy enforcement\r\n  - **Secure Cookies**: Enhanced security with automatic HTTPS upgrades\r\n  - **Privacy Sandbox**: Cookie alternatives with Topics API integration\r\n  - **Cross-Site Tracking Prevention**: Advanced tracking protection mechanisms\r\n\r\n---\r\n\r\n## 7. Modern UI Layer Architecture (v134+)\r\n\r\n### BrowserView/BrowserFrame (Cross-Platform)\r\n- **Enhanced UI Framework**: Modern views system with accessibility support\r\n- **Platform Integration**:\r\n  - **Windows**: WinUI 3 integration with modern styling\r\n  - **macOS**: SwiftUI bridge with native appearance\r\n  - **Linux**: GTK4 integration with Wayland support\r\n  - **Chrome OS**: Enhanced integration with system UI\r\n\r\n### Omnibox (Intelligent Address Bar)\r\n- **Location**: `chrome/browser/ui/omnibox/`\r\n- **Modern Features**:\r\n  - **Machine Learning Suggestions**: AI-powered search and navigation suggestions\r\n  - **Voice Input**: Advanced speech recognition with privacy protection\r\n  - **Visual Search**: Image-based search capabilities\r\n  - **Enhanced Autocomplete**: Context-aware suggestions with user learning\r\n\r\n### Modern Toolbar & UI Elements\r\n- **Material Design 3**: Latest design system implementation\r\n- **Features**:\r\n  - **Dynamic Theming**: Automatic color adaptation and user customization\r\n  - **Responsive Design**: Adaptive UI for different screen sizes and orientations\r\n  - **Gesture Support**: Advanced touch and pen input handling\r\n  - **Accessibility**: Comprehensive screen reader and keyboard navigation support\r\n\r\n---\r\n\r\n## 8. Enhanced Extensions & Modern Plugin Architecture (v134+)\r\n\r\n### Extension System (Manifest V3)\r\n- **Location**: `extensions/`\r\n- **Modern Security Model**:\r\n  - **Service Workers**: Background script replacement with enhanced lifecycle\r\n  - **Host Permissions**: Granular permission model with user control\r\n  - **Content Security Policy**: Strict CSP enforcement for extension security\r\n  - **Cross-Origin Isolation**: Enhanced security boundaries for extension content\r\n\r\n### Modern Web Platform APIs\r\n- **WebAssembly Integration**: Enhanced WASM support with SIMD and threading\r\n- **Features**:\r\n  - **Origin Private File System**: Secure file access for web applications\r\n  - **Web Locks**: Cross-tab coordination and resource management\r\n  - **Background Sync**: Reliable background task execution\r\n  - **Push Notifications**: Enhanced notification system with user control\r\n\r\n---\r\n\r\n## 9. Advanced Security & Sandboxing (v134+)\r\n\r\n### Site Isolation (Enhanced)\r\n- **Origin Agent Clusters**: Fine-grained process allocation for related origins\r\n- **Modern Features**:\r\n  - **Cross-Origin Isolation**: Enhanced protection against Spectre-style attacks\r\n  - **COOP/COEP**: Cross-Origin Opener/Embedder Policy enforcement\r\n  - **Trusted Types**: XSS prevention through API design\r\n  - **Feature Policy**: Granular control over powerful web platform features\r\n\r\n### Permission Model (User-Centric)\r\n- **Enhanced Permission UI**: `chrome/browser/permissions/`\r\n- **Modern Features**:\r\n  - **Permission Delegation**: Iframe permission inheritance\r\n  - **Temporary Permissions**: Time-based permission grants\r\n  - **Privacy Indicators**: Clear visual feedback for active permissions\r\n  - **Bulk Permission Management**: Easy review and modification of granted permissions\r\n\r\n### Safe Browsing (AI-Enhanced)\r\n- **Location**: `components/safe_browsing/`\r\n- **Advanced Protection**:\r\n  - **Real-Time Protection**: Cloud-based threat detection with local fallback\r\n  - **Machine Learning**: AI-powered phishing and malware detection\r\n  - **Enhanced Downloads**: Comprehensive file scanning and validation\r\n  - **Password Protection**: Breach detection and secure password management\r\n\r\n---\r\n\r\n## 10. Modern Diagnostics & Performance Management (v134+)\r\n\r\n### PerformanceManager (System-Wide)\r\n- **Location**: `chrome/browser/performance_manager/`\r\n- **Advanced Monitoring**:\r\n  - **Resource Attribution**: Precise tracking of CPU, memory, and network usage\r\n  - **Tab Lifecycle Management**: Intelligent freezing and discarding policies\r\n  - **Performance Budgets**: Real-time performance constraint enforcement\r\n  - **Core Web Vitals**: Automatic measurement and optimization recommendations\r\n\r\n### TaskManager (Enhanced Visibility)\r\n- **Modern Process Monitoring**: Real-time resource usage with detailed attribution\r\n- **Features**:\r\n  - **Service Process Tracking**: Visibility into all service processes\r\n  - **GPU Process Monitoring**: Graphics performance and memory usage\r\n  - **Network Activity**: Real-time network usage by process and site\r\n  - **Extension Impact**: Per-extension resource usage analysis\r\n\r\n### Modern Tracing & Analytics\r\n- **Chrome DevTools Protocol**: Enhanced debugging capabilities\r\n- **Features**:\r\n  - **Performance Timeline**: Detailed frame-by-frame analysis\r\n  - **Memory Profiling**: Advanced heap analysis with leak detection\r\n  - **Network Waterfall**: Comprehensive request timing and optimization\r\n  - **Core Web Vitals**: Real-time performance metric tracking\r\n\r\n---\r\n\r\n## 11. Emerging Technologies & Future Directions (v134+)\r\n\r\n### AI/ML Integration\r\n- **On-Device ML**: Privacy-preserving machine learning with TensorFlow Lite\r\n- **Features**:\r\n  - **Smart Suggestions**: AI-powered browsing assistance\r\n  - **Content Understanding**: Enhanced accessibility and summarization\r\n  - **Threat Detection**: Local malware and phishing detection\r\n  - **Performance Optimization**: Predictive resource management\r\n\r\n### Privacy Sandbox\r\n- **Location**: `components/privacy_sandbox/`\r\n- **Privacy-First Technologies**:\r\n  - **Topics API**: Interest-based advertising without cross-site tracking\r\n  - **FLEDGE**: Remarketing with privacy protection\r\n  - **Attribution Reporting**: Conversion measurement with differential privacy\r\n  - **Trust Tokens**: Anti-fraud without fingerprinting\r\n\r\n### WebGPU & Advanced Graphics\r\n- **Modern Graphics Pipeline**: Next-generation graphics API support\r\n- **Features**:\r\n  - **Compute Shaders**: General-purpose GPU computing for web applications\r\n  - **Advanced Rendering**: Modern graphics techniques with low-level access\r\n  - **AI Acceleration**: GPU-accelerated machine learning for web apps\r\n  - **Cross-Platform Consistency**: Unified graphics API across all platforms\r\n\r\n---\r\n\r\n## 12. Next Steps & Architecture Deep Dives\r\n\r\n### Essential Reading\r\n- **[Process Model](process-model.md)**: Detailed multi-process architecture and security boundaries\r\n- **[IPC Internals](ipc-internals.md)**: Modern Mojo communication patterns and service interfaces\r\n- **[Render Pipeline](render-pipeline.md)**: How browser and renderer coordinate for frame construction\r\n\r\n### Advanced Topics\r\n- **[Security Model](../security/security-model.md)**: Comprehensive security architecture and threat model\r\n- **[Networking Internals](../modules/networking-http.md)**: Deep dive into modern network stack\r\n- **[Storage Architecture](../modules/storage-cache.md)**: Advanced storage systems and privacy controls\r\n\r\n### Development Resources\r\n```bash\r\n# Explore browser component source code\r\ncd src/chrome/browser/        # Browser-specific components\r\ncd src/content/browser/       # Cross-platform browser foundation\r\ncd src/services/              # Modern service architecture\r\ncd src/components/            # Shared component library\r\n\r\n# Debug browser components\r\nchrome://components/          # Component status and versions\r\nchrome://process-internals/   # Process and service monitoring\r\nchrome://system/              # Comprehensive system information\r\n```\r\n\r\n---\r\n\r\n**End of Modern Browser Components Guide**\r\n\r\n### Key Evolution in v134+\r\n- **Service-Oriented Architecture**: Microservice design with Mojo IPC\r\n- **Enhanced Security**: Advanced site isolation and privacy protection\r\n- **Performance Optimization**: AI-powered resource management and Core Web Vitals focus\r\n- **Modern UI Framework**: Material Design 3 with cross-platform consistency\r\n- **Privacy-First Design**: Privacy Sandbox integration and user control\r\n\r\n**Notes for Developers:**\r\n- Browser components are increasingly service-oriented - understand Mojo interfaces\r\n- Security boundaries are fundamental to architecture - respect process isolation\r\n- Performance is measured through Core Web Vitals - optimize for user experience\r\n- Privacy is paramount - implement privacy-by-design principles\r\n- Accessibility is essential - ensure inclusive design from the ground up\r\n"
  },
  {
    "path": "architecture/security/sandbox-architecture",
    "title": "Sandbox Architecture in Chromium v134+",
    "content": "# Sandbox Architecture in Chromium v134+\r\n\r\nModern Chromium's sandbox architecture represents one of the most sophisticated security systems in contemporary software engineering. The v134+ sandbox provides comprehensive isolation through multiple defense layers, process-based security boundaries, and platform-specific mitigations that protect against both known and emerging threats.\r\n\r\n---\r\n\r\n## 1. Modern Sandbox Architecture Overview (v134+)\r\n\r\n### Core Security Principles\r\n\r\n1. **Defense in Depth**: Multiple independent security layers that fail safely\r\n2. **Principle of Least Privilege**: Processes receive minimal necessary permissions\r\n3. **Process Isolation**: Strong boundaries between browser components\r\n4. **Capability-Based Security**: Explicit permission grants for specific operations\r\n5. **Zero-Trust Architecture**: Assume all untrusted code is potentially malicious\r\n\r\n### Multi-Layered Security Model\r\n\r\n```text\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                    Browser Process (Privileged)             │\r\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────┐ │\r\n│  │   UI Process    │  │ Network Service │  │ GPU Process │ │\r\n│  └─────────────────┘  └─────────────────┘  └─────────────┘ │\r\n└─────────────────────────────┬───────────────────────────────┘\r\n                              │ Mojo IPC (Capability-Based)\r\n┌─────────────────────────────┴───────────────────────────────┐\r\n│                 Sandboxed Processes (Restricted)            │\r\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────┐ │\r\n│  │ Renderer Process│  │ Utility Process │  │ Plugin Host │ │\r\n│  │  (Site Isolated)│  │   (Sandboxed)   │  │ (Deprecated)│ │\r\n│  └─────────────────┘  └─────────────────┘  └─────────────┘ │\r\n└─────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### Modern Sandbox Components (v134+)\r\n\r\n- **Multi-Process Architecture**: Process-per-site isolation with security boundaries\r\n- **Mojo IPC System**: Type-safe, capability-based inter-process communication\r\n- **Site Isolation**: Per-origin process boundaries for enhanced security\r\n- **Control Flow Integrity (CFI)**: Hardware-assisted exploit mitigation\r\n- **Privacy Sandbox Integration**: Isolated execution contexts for privacy features\r\n- **Advanced Mitigations**: Platform-specific exploit prevention mechanisms\r\n\r\n---\r\n\r\n## 2. Cross-Platform Sandbox Implementation\r\n\r\n### Windows Sandbox (v134+)\r\n\r\n#### Core Windows Security Mechanisms\r\n\r\n```cpp\r\n// Modern Windows sandbox configuration\r\nnamespace sandbox {\r\n\r\nclass WindowsSandboxPolicy {\r\n public:\r\n  // Enhanced token restrictions for v134+\r\n  enum class TokenLevel {\r\n    kRestrictedToken = 0,           // Highly restricted, minimal SIDs\r\n    kLockDownToken,                 // Maximum restrictions, site isolation\r\n    kInteractiveToken,              // Limited UI interactions\r\n    kUnrestrictedToken              // Full user privileges (browser process)\r\n  };\r\n  \r\n  // Job object restrictions with modern mitigations\r\n  enum class JobLevel {\r\n    kLockdown = 0,                  // Maximum restrictions\r\n    kLimitedUser,                   // Standard renderer restrictions  \r\n    kInteractive,                   // UI process level\r\n    kUnprotected                    // No job restrictions\r\n  };\r\n  \r\n  // Integrity levels for mandatory access control\r\n  enum class IntegrityLevel {\r\n    kLow = 0,                       // Sandboxed processes\r\n    kMedium,                        // Standard user processes\r\n    kHigh,                          // Elevated processes\r\n    kSystem                         // System-level access\r\n  };\r\n\r\n  // Modern policy configuration\r\n  base::expected<void, SandboxError> ConfigurePolicy() {\r\n    // Set restrictive token with minimal privileges\r\n    if (auto result = SetTokenLevel(TokenLevel::kLockDownToken); !result.has_value()) {\r\n      return result;\r\n    }\r\n    \r\n    // Configure job object with enhanced restrictions\r\n    SetJobLevel(JobLevel::kLockdown);\r\n    \r\n    // Apply low integrity level for mandatory access control\r\n    SetIntegrityLevel(IntegrityLevel::kLow);\r\n    \r\n    // Enable modern Windows mitigations\r\n    EnableProcessMitigations();\r\n    \r\n    return base::ok();\r\n  }\r\n\r\n private:\r\n  void EnableProcessMitigations() {\r\n    // Control Flow Integrity (CFI)\r\n    EnableControlFlowGuard();\r\n    \r\n    // Return Flow Guard (RFG)\r\n    EnableReturnFlowGuard();\r\n    \r\n    // Arbitrary Code Guard (ACG)\r\n    EnableArbitraryCodeGuard();\r\n    \r\n    // Hardware Stack Protection (Intel CET)\r\n    EnableHardwareStackProtection();\r\n    \r\n    // Process creation restrictions\r\n    RestrictChildProcessCreation();\r\n  }\r\n};\r\n\r\n}  // namespace sandbox\r\n```\r\n\r\n#### Windows-Specific Mitigations\r\n\r\n**Advanced Exploit Mitigations**:\r\n- **Control Flow Integrity (CFI)**: Hardware-assisted ROP/JOP prevention\r\n- **Return Flow Guard (RFG)**: Return address validation\r\n- **Arbitrary Code Guard (ACG)**: Dynamic code prevention\r\n- **Hardware Enforcement**: Intel CET and ARM Pointer Authentication support\r\n\r\n**Process Isolation Enhancements**:\r\n- **Win32k Lockdown**: Kernel attack surface reduction\r\n- **Low-Box Tokens**: AppContainer isolation with capability restrictions\r\n- **Image Load Restrictions**: Preventing malicious DLL injection\r\n- **Font Loading Restrictions**: Reducing GDI attack surface\r\n\r\n### Linux Sandbox (v134+)\r\n\r\n#### Modern Linux Security Architecture\r\n\r\n```cpp\r\n// Linux sandbox implementation with advanced features\r\nnamespace sandbox {\r\n\r\nclass LinuxSandboxPolicy {\r\n public:\r\n  // Comprehensive Linux sandbox setup\r\n  base::expected<void, SandboxError> Initialize() {\r\n    // Set up namespace isolation\r\n    if (auto result = SetupNamespaces(); !result.has_value()) {\r\n      return result;\r\n    }\r\n    \r\n    // Configure seccomp-bpf filters\r\n    if (auto result = ApplySeccompFilters(); !result.has_value()) {\r\n      return result;\r\n    }\r\n    \r\n    // Apply LSM (SELinux/AppArmor) policies\r\n    if (auto result = ConfigureLSM(); !result.has_value()) {\r\n      return result;\r\n    }\r\n    \r\n    // Enable modern mitigations\r\n    EnableLinuxMitigations();\r\n    \r\n    return base::ok();\r\n  }\r\n\r\n private:\r\n  base::expected<void, SandboxError> SetupNamespaces() {\r\n    // PID namespace for process isolation\r\n    CreateNamespace(CLONE_NEWPID);\r\n    \r\n    // Network namespace for network isolation\r\n    CreateNamespace(CLONE_NEWNET);\r\n    \r\n    // Mount namespace for filesystem isolation\r\n    CreateNamespace(CLONE_NEWNS);\r\n    \r\n    // User namespace for privilege separation\r\n    CreateNamespace(CLONE_NEWUSER);\r\n    \r\n    // IPC namespace for System V IPC isolation\r\n    CreateNamespace(CLONE_NEWIPC);\r\n    \r\n    return base::ok();\r\n  }\r\n  \r\n  base::expected<void, SandboxError> ApplySeccompFilters() {\r\n    // Create comprehensive syscall filter\r\n    SyscallFilter filter;\r\n    \r\n    // Allow essential syscalls\r\n    filter.Allow(SYS_read);\r\n    filter.Allow(SYS_write);\r\n    filter.Allow(SYS_mmap);\r\n    filter.Allow(SYS_munmap);\r\n    \r\n    // Block dangerous syscalls\r\n    filter.Block(SYS_execve);\r\n    filter.Block(SYS_ptrace);\r\n    filter.Block(SYS_setuid);\r\n    filter.Block(SYS_setgid);\r\n    \r\n    // Apply conditional rules for IPC\r\n    filter.AllowIf(SYS_sendmsg, IsValidMojoIPC);\r\n    filter.AllowIf(SYS_recvmsg, IsValidMojoIPC);\r\n    \r\n    return filter.Apply();\r\n  }\r\n  \r\n  void EnableLinuxMitigations() {\r\n    // Stack canaries and FORTIFY_SOURCE\r\n    EnableStackProtection();\r\n    \r\n    // ASLR with enhanced entropy\r\n    EnableAddressSpaceRandomization();\r\n    \r\n    // Control Flow Integrity (if supported)\r\n    EnableControlFlowIntegrity();\r\n    \r\n    // Memory tagging (ARM64 MTE)\r\n    EnableMemoryTagging();\r\n  }\r\n};\r\n\r\n}  // namespace sandbox\r\n```\r\n\r\n#### Linux Security Features\r\n\r\n**Namespace Isolation**:\r\n- **PID Namespaces**: Process tree isolation\r\n- **Network Namespaces**: Network stack isolation  \r\n- **Mount Namespaces**: Filesystem view isolation\r\n- **User Namespaces**: UID/GID mapping and privilege isolation\r\n\r\n**Seccomp-BPF Filtering**:\r\n- **Fine-grained Syscall Control**: Allowlist-based syscall filtering\r\n- **Dynamic Policy Updates**: Runtime policy modifications for different phases\r\n- **Performance Optimization**: BPF JIT compilation for filter efficiency\r\n\r\n**Linux Security Modules (LSM)**:\r\n- **SELinux Integration**: Mandatory Access Control with type enforcement\r\n- **AppArmor Support**: Path-based access control\r\n- **Custom Policies**: Chromium-specific security policies\r\n\r\n### macOS Sandbox (v134+)\r\n\r\n#### Advanced macOS Security Integration\r\n\r\n```cpp\r\n// macOS sandbox with modern security features\r\nnamespace sandbox {\r\n\r\nclass MacOSSandboxPolicy {\r\n public:\r\n  base::expected<void, SandboxError> ConfigureMacOSSandbox() {\r\n    // Apply App Sandbox with minimal entitlements\r\n    if (auto result = ApplyAppSandbox(); !result.has_value()) {\r\n      return result;\r\n    }\r\n    \r\n    // Configure System Integrity Protection (SIP) awareness\r\n    ConfigureSIPCompliance();\r\n    \r\n    // Apply Hardened Runtime features\r\n    EnableHardenedRuntime();\r\n    \r\n    // Configure Gatekeeper compatibility\r\n    ConfigureGatekeeper();\r\n    \r\n    return base::ok();\r\n  }\r\n\r\n private:\r\n  base::expected<void, SandboxError> ApplyAppSandbox() {\r\n    // Minimal sandbox profile for renderer processes\r\n    const char* sandbox_profile = R\"(\r\n      (version 1)\r\n      (deny default)\r\n      \r\n      ; Allow basic system operations\r\n      (allow process-exec (literal \"/usr/lib/dyld\"))\r\n      (allow file-read* (literal \"/System/Library/Frameworks\"))\r\n      (allow file-read* (literal \"/usr/lib\"))\r\n      \r\n      ; Mojo IPC permissions\r\n      (allow mach-lookup (global-name \"org.chromium.Chromium.mojo.*\"))\r\n      (allow file-read* file-write* (regex #\"^/tmp/\\.org\\.chromium\\.Chromium\\.\"))\r\n      \r\n      ; Deny dangerous operations\r\n      (deny process-fork)\r\n      (deny process-exec)\r\n      (deny network-outbound)\r\n      (deny file-write* (regex #\"^/\"))\r\n    )\";\r\n    \r\n    return ApplySandboxProfile(sandbox_profile);\r\n  }\r\n  \r\n  void EnableHardenedRuntime() {\r\n    // Disable dangerous features\r\n    DisableExecutableMemory();\r\n    DisableDynamicCodeSigning();\r\n    DisableJITCompilation();\r\n    \r\n    // Enable security features\r\n    EnableLibraryValidation();\r\n    EnableSystemIntegrityProtection();\r\n  }\r\n};\r\n\r\n}  // namespace sandbox\r\n```\r\n\r\n#### macOS Security Features\r\n\r\n**App Sandbox**:\r\n- **Containerization**: Application-level isolation with minimal entitlements\r\n- **Resource Access Control**: File system and network access restrictions\r\n- **IPC Restrictions**: Limited inter-process communication capabilities\r\n\r\n**Hardened Runtime**:\r\n- **Code Signing Enforcement**: Strict validation of all loaded code\r\n- **JIT Restrictions**: Just-in-time compilation limitations\r\n- **Memory Protection**: Enhanced memory layout randomization\r\n\r\n**System Integration**:\r\n- **System Integrity Protection (SIP)**: System file and process protection\r\n- **Gatekeeper**: Code signing and notarization requirements\r\n- **XProtect**: Built-in malware detection integration\r\n\r\n---\r\n\r\n## 3. Site Isolation and Process Security\r\n\r\n### Site Isolation Architecture (v134+)\r\n\r\n```cpp\r\n// Modern site isolation with enhanced security\r\nnamespace content {\r\n\r\nclass SiteIsolationPolicy {\r\n public:\r\n  // Enhanced site isolation for v134+\r\n  enum class IsolationLevel {\r\n    kStrictSiteIsolation,           // Per-origin process isolation\r\n    kPartialSiteIsolation,          // High-value site isolation  \r\n    kProcessPerSiteInstance,        // Traditional site isolation\r\n    kSingleProcess                  // Debugging only (insecure)\r\n  };\r\n\r\n  // Configure site isolation based on security requirements\r\n  static void ConfigureIsolation(IsolationLevel level) {\r\n    switch (level) {\r\n      case IsolationLevel::kStrictSiteIsolation:\r\n        EnableStrictSiteIsolation();\r\n        EnableOriginAgentClusterIsolation();\r\n        EnableCrossOriginEmbedderPolicyIsolation();\r\n        break;\r\n        \r\n      case IsolationLevel::kPartialSiteIsolation:\r\n        EnableHighValueSiteIsolation();\r\n        EnablePasswordSiteIsolation();\r\n        break;\r\n        \r\n      default:\r\n        EnableDefaultSiteIsolation();\r\n    }\r\n  }\r\n\r\n private:\r\n  static void EnableStrictSiteIsolation() {\r\n    // Every origin gets its own process\r\n    SiteInstance::EnableStrictSiteIsolation();\r\n    \r\n    // Enhanced cross-origin read blocking (CORB)\r\n    EnableStrictCORB();\r\n    \r\n    // Out-of-process iframe isolation\r\n    EnableOOPIFIsolation();\r\n    \r\n    // Spectre mitigations\r\n    EnableSpectreV1Mitigations();\r\n    EnableSpectreV2Mitigations();\r\n  }\r\n};\r\n\r\n}  // namespace content\r\n```\r\n\r\n### Process Security Boundaries\r\n\r\n**Renderer Process Isolation**:\r\n- **Per-Site Process Allocation**: Separate processes for different origins\r\n- **Cross-Origin Read Blocking (CORB)**: Preventing cross-origin data leaks\r\n- **Out-of-Process iframes (OOPIF)**: Iframe isolation with separate processes\r\n- **Spectre Mitigations**: Side-channel attack prevention\r\n\r\n**Security Policy Enforcement**:\r\n- **Content Security Policy (CSP)**: Browser-enforced content restrictions\r\n- **Cross-Origin Embedder Policy (COEP)**: Embedding permission control\r\n- **Cross-Origin Opener Policy (COOP)**: Window reference isolation\r\n- **Same-Site Cookie Enforcement**: Cross-site request forgery prevention\r\n\r\n---\r\n\r\n## 4. Mojo IPC Security Model\r\n\r\n### Capability-Based IPC System\r\n\r\n```cpp\r\n// Modern Mojo IPC with enhanced security\r\nnamespace mojo {\r\n\r\n// Capability-based service interface\r\ninterface SecureService {\r\n  // Capability tokens for access control\r\n  struct ServiceCapability {\r\n    string capability_name;\r\n    array<uint8> capability_token;\r\n    uint64 expiration_time;\r\n    array<string> allowed_origins;\r\n  };\r\n\r\n  // Secure method invocation with capability checking\r\n  PerformSecureOperation(ServiceCapability capability, \r\n                        SecureOperationRequest request) \r\n      => (SecureOperationResponse response);\r\n  \r\n  // Capability delegation with restrictions\r\n  DelegateCapability(ServiceCapability parent_capability,\r\n                    CapabilityRestrictions restrictions)\r\n      => (ServiceCapability? delegated_capability);\r\n};\r\n\r\n// Enhanced IPC security validation\r\nclass MojoSecurityValidator {\r\n public:\r\n  // Validate capability before method invocation\r\n  base::expected<void, SecurityError> ValidateCapability(\r\n      const ServiceCapability& capability,\r\n      const url::Origin& requesting_origin) {\r\n    \r\n    // Check capability expiration\r\n    if (IsExpired(capability)) {\r\n      return base::unexpected(SecurityError::kCapabilityExpired);\r\n    }\r\n    \r\n    // Validate origin permissions\r\n    if (!IsOriginAllowed(capability, requesting_origin)) {\r\n      return base::unexpected(SecurityError::kOriginNotAllowed);\r\n    }\r\n    \r\n    // Verify capability token authenticity\r\n    if (!VerifyCapabilityToken(capability)) {\r\n      return base::unexpected(SecurityError::kInvalidToken);\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n\r\n private:\r\n  bool VerifyCapabilityToken(const ServiceCapability& capability) {\r\n    // Cryptographic verification of capability tokens\r\n    return crypto::VerifySignature(capability.capability_token, \r\n                                  capability_signing_key_);\r\n  }\r\n  \r\n  crypto::SigningKey capability_signing_key_;\r\n};\r\n\r\n}  // namespace mojo\r\n```\r\n\r\n### IPC Security Features\r\n\r\n**Capability-Based Access Control**:\r\n- **Explicit Permission Grants**: Services require specific capabilities\r\n- **Token-Based Authentication**: Cryptographically secure capability tokens\r\n- **Origin-Based Restrictions**: Fine-grained origin permission control\r\n- **Temporal Access Control**: Time-limited capability expiration\r\n\r\n**Message Validation and Filtering**:\r\n- **Type-Safe Serialization**: Automatic memory safety in IPC messages\r\n- **Message Size Limits**: Prevention of resource exhaustion attacks\r\n- **Rate Limiting**: Throttling to prevent IPC flooding\r\n- **Content Validation**: Schema-based message validation\r\n\r\n---\r\n\r\n## 5. Privacy Sandbox Security Integration\r\n\r\n### Isolated Execution Contexts\r\n\r\n```cpp\r\n// Privacy Sandbox isolation with enhanced security\r\nnamespace privacy_sandbox {\r\n\r\nclass PrivacySandboxIsolation {\r\n public:\r\n  // Isolated execution environment for privacy features\r\n  struct IsolationContext {\r\n    std::string context_id;\r\n    url::Origin top_level_origin;\r\n    std::vector<url::Origin> allowed_origins;\r\n    base::TimeDelta max_execution_time;\r\n    size_t memory_limit_bytes;\r\n    bool network_access_allowed;\r\n  };\r\n\r\n  // Create isolated context for privacy computation\r\n  base::expected<IsolationContext, PrivacyError> CreateIsolatedContext(\r\n      const url::Origin& requesting_origin,\r\n      PrivacySandboxFeature feature) {\r\n    \r\n    // Validate origin permissions for privacy feature\r\n    if (!IsOriginAllowedForFeature(requesting_origin, feature)) {\r\n      return base::unexpected(PrivacyError::kOriginNotAllowed);\r\n    }\r\n    \r\n    IsolationContext context{\r\n      .context_id = GenerateContextId(),\r\n      .top_level_origin = requesting_origin,\r\n      .allowed_origins = GetAllowedOriginsForFeature(feature),\r\n      .max_execution_time = GetExecutionTimeLimitForFeature(feature),\r\n      .memory_limit_bytes = GetMemoryLimitForFeature(feature),\r\n      .network_access_allowed = IsNetworkAccessAllowedForFeature(feature)\r\n    };\r\n    \r\n    return context;\r\n  }\r\n\r\n  // Execute privacy computation in isolated environment\r\n  void ExecutePrivacyComputation(\r\n      const IsolationContext& context,\r\n      const std::string& computation_code,\r\n      base::OnceCallback<void(PrivacyComputationResult)> callback) {\r\n    \r\n    // Create isolated execution environment\r\n    auto isolated_env = CreateIsolatedV8Environment(context);\r\n    \r\n    // Apply resource limits\r\n    isolated_env->SetMemoryLimit(context.memory_limit_bytes);\r\n    isolated_env->SetExecutionTimeLimit(context.max_execution_time);\r\n    \r\n    // Disable dangerous APIs\r\n    isolated_env->DisableFileSystemAccess();\r\n    isolated_env->DisableNetworkAccess(!context.network_access_allowed);\r\n    isolated_env->DisableCrossOriginAccess();\r\n    \r\n    // Execute computation with monitoring\r\n    isolated_env->ExecuteScript(computation_code, std::move(callback));\r\n  }\r\n};\r\n\r\n}  // namespace privacy_sandbox\r\n```\r\n\r\n### Privacy Feature Security\r\n\r\n**Topics API Security**:\r\n- **Interest Group Isolation**: Separate processes for interest computation\r\n- **Differential Privacy**: Mathematical privacy guarantees\r\n- **Cross-Site Tracking Prevention**: Strong origin isolation\r\n- **Temporal Privacy Controls**: Interest decay and rotation\r\n\r\n**FLEDGE/Protected Audience Security**:\r\n- **Trusted Execution Environment**: Isolated auction computation\r\n- **Bidding Script Isolation**: Separate contexts for ad auction logic\r\n- **Cross-Site Data Minimization**: Limited cross-site information flow\r\n- **Cryptographic Privacy**: Secure aggregation of auction results\r\n\r\n---\r\n\r\n## 6. Modern Exploit Mitigations\r\n\r\n### Hardware-Assisted Security Features\r\n\r\n```cpp\r\n// Advanced exploit mitigations for v134+\r\nnamespace security {\r\n\r\nclass ExploitMitigations {\r\n public:\r\n  // Enable comprehensive exploit mitigations\r\n  static void EnableAllMitigations() {\r\n    // Control Flow Integrity\r\n    EnableControlFlowIntegrity();\r\n    \r\n    // Stack protection\r\n    EnableStackProtection();\r\n    \r\n    // Memory safety features\r\n    EnableMemorySafetyFeatures();\r\n    \r\n    // Hardware-specific mitigations\r\n    EnableHardwareMitigations();\r\n  }\r\n\r\n private:\r\n  static void EnableControlFlowIntegrity() {\r\n    // Intel CET (Control-flow Enforcement Technology)\r\n    if (cpu_info_.has_cet_support()) {\r\n      EnableIntelCET();\r\n    }\r\n    \r\n    // ARM Pointer Authentication\r\n    if (cpu_info_.has_pointer_auth()) {\r\n      EnableArmPointerAuth();\r\n    }\r\n    \r\n    // Software CFI for unsupported hardware\r\n    EnableSoftwareCFI();\r\n  }\r\n  \r\n  static void EnableMemorySafetyFeatures() {\r\n    // Memory tagging (ARM64 MTE)\r\n    if (cpu_info_.has_memory_tagging()) {\r\n      EnableMemoryTagging();\r\n    }\r\n    \r\n    // Address sanitizer integration\r\n    #if defined(ADDRESS_SANITIZER)\r\n    EnableAddressSanitizerIntegration();\r\n    #endif\r\n    \r\n    // Hardware shadow stack\r\n    if (cpu_info_.has_shadow_stack()) {\r\n      EnableHardwareShadowStack();\r\n    }\r\n  }\r\n  \r\n  static void EnableHardwareMitigations() {\r\n    // Intel MPX (Memory Protection Extensions) - deprecated but relevant\r\n    // SMEP/SMAP kernel protections\r\n    // Branch Target Identification (BTI) on ARM\r\n    // Load/Store Multiple restrictions\r\n    ConfigureHardwareProtections();\r\n  }\r\n};\r\n\r\n}  // namespace security\r\n```\r\n\r\n### Modern Mitigation Techniques\r\n\r\n**Control Flow Protection**:\r\n- **Intel CET**: Hardware-enforced control flow integrity\r\n- **ARM Pointer Authentication**: Cryptographic return address protection\r\n- **Branch Target Identification**: Valid jump target enforcement\r\n- **Return Address Signing**: Cryptographic stack integrity\r\n\r\n**Memory Protection**:\r\n- **ARM Memory Tagging (MTE)**: Hardware-assisted use-after-free detection\r\n- **Intel MPK**: Memory protection keys for fine-grained access control\r\n- **SMEP/SMAP**: Supervisor mode execution/access prevention\r\n- **Enhanced ASLR**: High-entropy address space randomization\r\n\r\n**Speculative Execution Mitigations**:\r\n- **Spectre v1/v2 Protections**: Bounds check bypass and branch target injection\r\n- **Microarchitectural Data Sampling (MDS)**: L1TF, ZombieLoad, RIDL mitigations\r\n- **Store Buffer Bypass**: SWAPGS and other variant protections\r\n- **Load Value Injection (LVI)**: Intel microcode and compiler mitigations\r\n\r\n---\r\n\r\n## 7. Sandbox Policy Configuration\r\n\r\n### Dynamic Policy Management\r\n\r\n```cpp\r\n// Dynamic sandbox policy configuration for v134+\r\nnamespace sandbox {\r\n\r\nclass DynamicPolicyManager {\r\n public:\r\n  // Policy templates for different process types\r\n  enum class ProcessType {\r\n    kRenderer,              // Web content renderer\r\n    kUtility,              // Utility processes\r\n    kGpu,                  // GPU process\r\n    kNetwork,              // Network service\r\n    kAudio,                // Audio service\r\n    kStorage,              // Storage service\r\n    kPrintCompositor,      // Print compositor\r\n    kPrivacySandbox        // Privacy Sandbox worklet\r\n  };\r\n\r\n  // Configure policy based on process type and security requirements\r\n  static std::unique_ptr<SandboxPolicy> CreatePolicy(\r\n      ProcessType process_type,\r\n      const SecurityRequirements& requirements) {\r\n    \r\n    auto policy = std::make_unique<SandboxPolicy>();\r\n    \r\n    switch (process_type) {\r\n      case ProcessType::kRenderer:\r\n        ConfigureRendererPolicy(*policy, requirements);\r\n        break;\r\n        \r\n      case ProcessType::kPrivacySandbox:\r\n        ConfigurePrivacySandboxPolicy(*policy, requirements);\r\n        break;\r\n        \r\n      case ProcessType::kUtility:\r\n        ConfigureUtilityPolicy(*policy, requirements);\r\n        break;\r\n        \r\n      default:\r\n        ConfigureDefaultPolicy(*policy, requirements);\r\n    }\r\n    \r\n    // Apply platform-specific enhancements\r\n    ApplyPlatformSpecificMitigations(*policy);\r\n    \r\n    return policy;\r\n  }\r\n\r\n private:\r\n  static void ConfigureRendererPolicy(SandboxPolicy& policy,\r\n                                     const SecurityRequirements& requirements) {\r\n    // Maximum restrictions for web content\r\n    policy.SetTokenLevel(TokenLevel::kLockdown);\r\n    policy.SetJobLevel(JobLevel::kLockdown);\r\n    policy.SetIntegrityLevel(IntegrityLevel::kLow);\r\n    \r\n    // File system access rules\r\n    policy.AddRule(SubSystem::kFiles, FileRule::kReadOnly,\r\n                  L\"${temp}\\\\chromium_renderer_*\");\r\n    \r\n    // Network restrictions (no direct network access)\r\n    policy.BlockNetworkAccess();\r\n    \r\n    // IPC permissions (only Mojo)\r\n    policy.AllowMojoIPC();\r\n    policy.BlockLegacyIPC();\r\n    \r\n    // Enhanced mitigations\r\n    policy.EnableControlFlowIntegrity();\r\n    policy.EnableArbitraryCodeGuard();\r\n    policy.EnableReturnFlowGuard();\r\n  }\r\n  \r\n  static void ConfigurePrivacySandboxPolicy(SandboxPolicy& policy,\r\n                                           const SecurityRequirements& requirements) {\r\n    // Extra-restrictive policy for privacy computations\r\n    ConfigureRendererPolicy(policy, requirements);\r\n    \r\n    // Additional privacy-specific restrictions\r\n    policy.BlockFileSystemAccess();\r\n    policy.BlockClipboardAccess();\r\n    policy.BlockScreenCapture();\r\n    \r\n    // Temporal restrictions\r\n    policy.SetMaxExecutionTime(base::Seconds(30));\r\n    policy.SetMemoryLimit(base::Megabytes(100));\r\n    \r\n    // Cryptographic isolation\r\n    policy.EnablePrivacyIsolation();\r\n  }\r\n};\r\n\r\n}  // namespace sandbox\r\n```\r\n\r\n### Policy Rule System\r\n\r\n**Resource Access Control**:\r\n- **File System Rules**: Path-based access control with pattern matching\r\n- **Registry Rules**: Windows registry access restrictions\r\n- **Network Rules**: Protocol and destination-based network controls\r\n- **IPC Rules**: Inter-process communication permission management\r\n\r\n**Dynamic Policy Updates**:\r\n- **Runtime Policy Modification**: Safe policy updates for running processes\r\n- **Feature-Based Policies**: Conditional rules based on enabled features\r\n- **Origin-Specific Rules**: Per-origin customization for enhanced security\r\n- **Experiment Integration**: A/B testing of security policy variations\r\n\r\n---\r\n\r\n## 8. Monitoring and Introspection\r\n\r\n### Security Event Monitoring\r\n\r\n```cpp\r\n// Comprehensive security monitoring for sandbox violations\r\nnamespace security {\r\n\r\nclass SandboxMonitor {\r\n public:\r\n  // Security event types for monitoring\r\n  enum class SecurityEvent {\r\n    kPolicyViolation,           // Sandbox policy violation attempt\r\n    kEscapeAttempt,            // Sandbox escape attempt detected\r\n    kUnauthorizedAccess,       // Access to restricted resource\r\n    kSuspiciousActivity,       // Anomalous behavior detected\r\n    kExploitMitigation,        // Exploit mitigation triggered\r\n    kCapabilityViolation       // Mojo capability violation\r\n  };\r\n\r\n  // Register security event handler\r\n  void RegisterEventHandler(\r\n      SecurityEvent event_type,\r\n      base::RepeatingCallback<void(const SecurityEventData&)> handler) {\r\n    event_handlers_[event_type].push_back(std::move(handler));\r\n  }\r\n\r\n  // Report security events with detailed context\r\n  void ReportSecurityEvent(SecurityEvent event_type,\r\n                          const SecurityEventData& event_data) {\r\n    // Log security event\r\n    LogSecurityEvent(event_type, event_data);\r\n    \r\n    // Update security metrics\r\n    UpdateSecurityMetrics(event_type);\r\n    \r\n    // Notify registered handlers\r\n    NotifyEventHandlers(event_type, event_data);\r\n    \r\n    // Take automatic response actions\r\n    TakeSecurityResponse(event_type, event_data);\r\n  }\r\n\r\n private:\r\n  void TakeSecurityResponse(SecurityEvent event_type,\r\n                           const SecurityEventData& event_data) {\r\n    switch (event_type) {\r\n      case SecurityEvent::kEscapeAttempt:\r\n        // Immediate process termination\r\n        TerminateCompromisedProcess(event_data.process_id);\r\n        break;\r\n        \r\n      case SecurityEvent::kPolicyViolation:\r\n        // Enhanced monitoring for process\r\n        EnableEnhancedMonitoring(event_data.process_id);\r\n        break;\r\n        \r\n      case SecurityEvent::kExploitMitigation:\r\n        // Report to security team\r\n        ReportToSecurityTeam(event_data);\r\n        break;\r\n    }\r\n  }\r\n  \r\n  std::map<SecurityEvent, std::vector<base::RepeatingCallback<void(const SecurityEventData&)>>> \r\n      event_handlers_;\r\n};\r\n\r\n}  // namespace security\r\n```\r\n\r\n### Debugging and Analysis Tools\r\n\r\n**Chrome Internals Integration**:\r\n- **chrome://sandbox/**: Real-time sandbox status and policy information\r\n- **chrome://process-internals/**: Process isolation and security boundary analysis\r\n- **chrome://security-state/**: Comprehensive security feature status\r\n- **chrome://policy-internals/**: Dynamic policy rule inspection\r\n\r\n**Advanced Debugging Features**:\r\n- **Sandbox Violation Logging**: Detailed logging of policy violations\r\n- **Performance Impact Analysis**: Security overhead measurement\r\n- **Security Metrics Dashboard**: Real-time security health monitoring\r\n- **Exploit Detection Telemetry**: Automatic exploit attempt reporting\r\n\r\n---\r\n\r\n## 9. Performance and Security Trade-offs\r\n\r\n### Optimized Security Implementation\r\n\r\n```cpp\r\n// Performance-optimized security checks\r\nnamespace security {\r\n\r\nclass OptimizedSecurityChecker {\r\n public:\r\n  // Fast-path security validation for common operations\r\n  bool FastPathSecurityCheck(const SecurityOperation& operation) {\r\n    // Cache frequently-used security decisions\r\n    if (auto cached_result = security_cache_.Get(operation.GetCacheKey())) {\r\n      return *cached_result;\r\n    }\r\n    \r\n    // Optimized validation for common patterns\r\n    bool result = PerformOptimizedValidation(operation);\r\n    \r\n    // Cache result for future use\r\n    security_cache_.Put(operation.GetCacheKey(), result);\r\n    \r\n    return result;\r\n  }\r\n\r\n private:\r\n  // Optimized validation strategies\r\n  bool PerformOptimizedValidation(const SecurityOperation& operation) {\r\n    // Use bloom filters for negative lookups\r\n    if (blocked_operations_filter_.MightContain(operation.GetHash())) {\r\n      return PerformFullSecurityCheck(operation);\r\n    }\r\n    \r\n    // Fast approval for allowlisted operations\r\n    if (allowed_operations_set_.contains(operation.GetPattern())) {\r\n      return true;\r\n    }\r\n    \r\n    // Fall back to full security check\r\n    return PerformFullSecurityCheck(operation);\r\n  }\r\n  \r\n  base::LRUCache<std::string, bool> security_cache_;\r\n  base::BloomFilter<uint64_t> blocked_operations_filter_;\r\n  base::flat_set<std::string> allowed_operations_set_;\r\n};\r\n\r\n}  // namespace security\r\n```\r\n\r\n### Security Performance Metrics\r\n\r\n**Overhead Measurement**:\r\n- **IPC Latency Impact**: Mojo security validation overhead\r\n- **Process Creation Time**: Sandbox initialization performance\r\n- **Memory Overhead**: Security metadata and isolation costs\r\n- **CPU Usage**: Ongoing security check performance\r\n\r\n**Optimization Strategies**:\r\n- **Security Decision Caching**: Frequently-used validation results\r\n- **Batch Security Operations**: Grouped validation for efficiency\r\n- **Lazy Security Initialization**: On-demand security feature activation\r\n- **Hardware Acceleration**: Leveraging security-specific CPU features\r\n\r\n---\r\n\r\n## 10. Future Security Enhancements\r\n\r\n### Emerging Security Technologies\r\n\r\n**WebAssembly Isolation**:\r\n- **Memory-Safe Compilation**: Hardware-enforced memory safety\r\n- **Capability-Based WASM**: Fine-grained permission systems\r\n- **Cross-Language Security**: Unified security across WASM and JavaScript\r\n- **Hardware Attestation**: Cryptographic execution environment verification\r\n\r\n**Advanced Hardware Integration**:\r\n- **Confidential Computing**: Intel TDX, AMD SEV integration\r\n- **Hardware Security Modules (HSM)**: Cryptographic key protection\r\n- **Secure Enclaves**: ARM TrustZone and Intel SGX utilization\r\n- **Post-Quantum Cryptography**: Quantum-resistant security algorithms\r\n\r\n**Zero-Trust Architecture Evolution**:\r\n- **Continuous Verification**: Real-time trust assessment\r\n- **Behavioral Analysis**: ML-based anomaly detection\r\n- **Adaptive Security Policies**: Dynamic risk-based adjustments\r\n- **Microservice Security**: Fine-grained service-to-service authentication\r\n\r\n---\r\n\r\n## 11. Security Best Practices for Developers\r\n\r\n### Secure Development Guidelines\r\n\r\n```cpp\r\n// Security-first development patterns\r\nnamespace security_patterns {\r\n\r\n// RAII-based security context management\r\nclass SecurityContext {\r\n public:\r\n  SecurityContext(const SecurityPolicy& policy) \r\n      : policy_(policy), is_active_(true) {\r\n    // Acquire security context\r\n    if (!AcquireSecurityContext(policy_)) {\r\n      is_active_ = false;\r\n      LOG(FATAL) << \"Failed to acquire security context\";\r\n    }\r\n  }\r\n  \r\n  ~SecurityContext() {\r\n    if (is_active_) {\r\n      ReleaseSecurityContext();\r\n    }\r\n  }\r\n  \r\n  // Non-copyable, movable\r\n  SecurityContext(const SecurityContext&) = delete;\r\n  SecurityContext& operator=(const SecurityContext&) = delete;\r\n  SecurityContext(SecurityContext&&) = default;\r\n  SecurityContext& operator=(SecurityContext&&) = default;\r\n\r\n  // Secure operation execution\r\n  template<typename Operation>\r\n  base::expected<typename Operation::Result, SecurityError> \r\n  ExecuteSecurely(Operation&& operation) {\r\n    if (!is_active_) {\r\n      return base::unexpected(SecurityError::kInactiveContext);\r\n    }\r\n    \r\n    // Validate operation against security policy\r\n    if (auto validation = ValidateOperation(operation); !validation.has_value()) {\r\n      return base::unexpected(validation.error());\r\n    }\r\n    \r\n    // Execute with security monitoring\r\n    return ExecuteWithMonitoring(std::forward<Operation>(operation));\r\n  }\r\n\r\n private:\r\n  SecurityPolicy policy_;\r\n  bool is_active_;\r\n};\r\n\r\n// Capability-based operation wrapper\r\ntemplate<typename T>\r\nclass SecureWrapper {\r\n public:\r\n  explicit SecureWrapper(T&& value, const Capability& capability)\r\n      : value_(std::forward<T>(value)), capability_(capability) {}\r\n  \r\n  // Access requires capability validation\r\n  const T& Get() const {\r\n    ValidateCapability(capability_);\r\n    return value_;\r\n  }\r\n  \r\n  T& GetMutable() {\r\n    ValidateCapability(capability_);\r\n    ValidateWriteAccess(capability_);\r\n    return value_;\r\n  }\r\n\r\n private:\r\n  T value_;\r\n  Capability capability_;\r\n};\r\n\r\n}  // namespace security_patterns\r\n```\r\n\r\n### Code Review Security Checklist\r\n\r\n**Memory Safety**:\r\n- ✅ Use smart pointers and RAII patterns\r\n- ✅ Validate all input boundaries and buffer sizes\r\n- ✅ Employ AddressSanitizer and MemorySanitizer in testing\r\n- ✅ Avoid raw pointer arithmetic and unsafe casts\r\n\r\n**IPC Security**:\r\n- ✅ Use Mojo interfaces instead of legacy IPC mechanisms\r\n- ✅ Validate all IPC message parameters thoroughly\r\n- ✅ Implement capability-based access control\r\n- ✅ Apply rate limiting to prevent IPC flooding\r\n\r\n**Process Isolation**:\r\n- ✅ Respect site isolation boundaries\r\n- ✅ Minimize cross-process data sharing\r\n- ✅ Use appropriate sandbox policies for process types\r\n- ✅ Validate origin permissions for cross-process operations\r\n\r\n---\r\n\r\n## 12. References and Further Reading\r\n\r\n### Core Implementation Files\r\n- `sandbox/` - Cross-platform sandbox implementation\r\n- `content/browser/renderer_host/render_process_host_impl.cc` - Process isolation\r\n- `services/service_manager/` - Service isolation and security\r\n- `chrome/browser/chrome_content_browser_client.cc` - Security policy configuration\r\n\r\n### Architecture Documentation\r\n- [Process Model](../process-model.md) - Multi-process architecture and isolation\r\n- [IPC Internals](../ipc-internals.md) - Mojo IPC security mechanisms\r\n- [Site Isolation](../site-isolation.md) - Per-origin process boundaries\r\n\r\n### Security Documentation\r\n- [Security Model](../../security/security-model.md) - Overall security architecture\r\n- [Privacy Sandbox](../../privacy/privacy-sandbox.md) - Privacy feature isolation\r\n- [Exploit Mitigations](../../security/exploit-mitigations.md) - Advanced protection mechanisms\r\n\r\n### External Resources\r\n- **Chromium Security Architecture**: Official security documentation\r\n- **Platform Security Guides**: OS-specific security implementation details\r\n- **CVE Database**: Historical vulnerability analysis and mitigations\r\n- **Security Research Papers**: Academic research on browser security\r\n\r\n---\r\n\r\nThe sandbox architecture in Chromium v134+ represents the state-of-the-art in browser security, providing comprehensive protection through multiple defense layers, process isolation, and modern exploit mitigations. Understanding this architecture is essential for developing secure browser features and maintaining the highest levels of user protection in the modern web environment.\r\n"
  },
  {
    "path": "architecture/security/overview",
    "title": "Security Architecture",
    "content": "# Security Architecture\r\n\r\nWelcome to the Security Architecture section! This area provides detailed technical documentation about the security architecture and implementation within the Wanderlust custom Chromium browser.\r\n\r\n## What You'll Find Here\r\n\r\nThis section focuses on the architectural aspects of browser security:\r\n\r\n- **[Sandbox Architecture](sandbox-architecture.md)**: Detailed technical implementation of browser sandboxing and process isolation\r\n- **Security Boundaries**: How security boundaries are maintained across browser components\r\n- **Process Isolation**: Technical details of multi-process security architecture\r\n- **Privilege Separation**: Implementation of least-privilege principles\r\n\r\n## Security Architecture Overview\r\n\r\nThe security architecture of our custom Chromium browser is built on several key principles:\r\n\r\n### Multi-Process Security Model\r\n- **Process Isolation**: Each major component runs in its own process with limited privileges\r\n- **Sandbox Boundaries**: Strict sandboxing prevents processes from accessing unauthorized resources\r\n- **Inter-Process Communication**: Secure communication channels between isolated processes\r\n- **Privilege Escalation Prevention**: Architecture prevents unauthorized privilege increases\r\n\r\n### Sandboxing Implementation\r\n- **Renderer Sandboxing**: Web content runs in heavily sandboxed renderer processes\r\n- **System Resource Isolation**: Limited access to file system, network, and system APIs\r\n- **GPU Process Sandboxing**: Graphics operations isolated in separate sandboxed process\r\n- **Plugin Sandboxing**: Third-party plugins run with minimal system access\r\n\r\n## Technical Architecture Components\r\n\r\n### Process Security Model\r\nThe browser implements a multi-layered process security model:\r\n\r\n1. **Browser Process**: Privileged process managing other processes and system access\r\n2. **Renderer Processes**: Sandboxed processes for web content execution\r\n3. **GPU Process**: Isolated graphics and hardware acceleration\r\n4. **Network Process**: Separate process for network operations\r\n5. **Utility Processes**: Specialized processes for specific tasks\r\n\r\n### Security Boundaries\r\n- **Same-Origin Policy**: Enforced at the process level for strong isolation\r\n- **Site Isolation**: Each site runs in its own dedicated process\r\n- **Cross-Origin Restrictions**: Strict controls on cross-origin resource access\r\n- **API Access Controls**: Limited API access based on process type and privileges\r\n\r\n### Threat Mitigation\r\n- **Code Injection Prevention**: Architecture prevents malicious code injection\r\n- **Data Exfiltration Protection**: Sandboxing limits unauthorized data access\r\n- **System Compromise Mitigation**: Isolated processes limit impact of security breaches\r\n- **Privilege Escalation Blocks**: Multiple barriers prevent privilege escalation attacks\r\n\r\n## Implementation Details\r\n\r\n### Sandbox Technology\r\n- **Operating System Integration**: Leveraging OS-specific sandboxing mechanisms\r\n- **Capability-Based Security**: Fine-grained capability control for process operations\r\n- **Resource Limitations**: CPU, memory, and I/O restrictions for sandboxed processes\r\n- **System Call Filtering**: Restricted system calls for enhanced security\r\n\r\n### Communication Security\r\n- **IPC Security**: Secure inter-process communication mechanisms\r\n- **Message Validation**: Strict validation of all inter-process messages\r\n- **Authentication**: Process identity verification and authentication\r\n- **Encryption**: Encrypted communication channels where appropriate\r\n\r\n## Security Architecture Patterns\r\n\r\n### Defense in Depth\r\n- **Multiple Security Layers**: No single point of failure in security architecture\r\n- **Redundant Protections**: Multiple mechanisms protecting against the same threats\r\n- **Fail-Safe Defaults**: Secure default behaviors when security checks fail\r\n\r\n### Least Privilege\r\n- **Minimal Permissions**: Each process has only the minimum required permissions\r\n- **Dynamic Privilege Adjustment**: Privileges adjusted based on current needs\r\n- **Capability Dropping**: Unused capabilities are removed during process execution\r\n\r\n## Integration with Browser Architecture\r\n\r\nSecurity architecture integrates deeply with:\r\n- [Process Model](../process-model.md): How security boundaries align with process boundaries\r\n- [IPC Internals](../ipc-internals.md): Secure communication between processes\r\n- [Browser Components](../browser-components.md): Security considerations for each component\r\n- [General Security Model](../../security/overview.md): High-level security principles and policies\r\n\r\n## Development Considerations\r\n\r\nWhen working with security-sensitive code:\r\n\r\n1. **Security Review Required**: All changes affecting security boundaries need review\r\n2. **Threat Modeling**: Consider potential attacks and mitigation strategies\r\n3. **Testing Requirements**: Comprehensive security testing including penetration testing\r\n4. **Documentation Updates**: Keep security documentation current with implementation changes\r\n\r\n## Security Compliance\r\n\r\nOur security architecture adheres to:\r\n- **Industry Standards**: Following established security architecture best practices\r\n- **Regulatory Requirements**: Compliance with relevant security regulations\r\n- **Security Frameworks**: Implementation based on proven security frameworks\r\n- **Continuous Assessment**: Regular security architecture reviews and updates\r\n\r\n## Performance Considerations\r\n\r\nSecurity architecture balances security with performance:\r\n- **Efficient IPC**: Optimized secure communication between processes\r\n- **Resource Management**: Efficient use of system resources within security constraints\r\n- **Caching Strategies**: Secure caching that doesn't compromise security boundaries\r\n- **Startup Optimization**: Fast startup while maintaining security guarantees\r\n\r\n---\r\n\r\n*Start with the [Sandbox Architecture](sandbox-architecture.md) documentation to understand the foundational security isolation mechanisms that protect users and the system.*\r\n"
  },
  {
    "path": "architecture/design-patterns/state-pattern",
    "title": "State Pattern in Chromium v134+",
    "content": "# State Pattern in Chromium v134+\r\n\r\nThe State pattern enables objects to alter their behavior when internal state changes, appearing as if the object changed its class. Modern Chromium v134+ leverages sophisticated state machines for navigation, downloads, security contexts, service lifecycles, and complex UI interactions with type safety and performance optimization.\r\n\r\n---\r\n\r\n## 1. Modern State Pattern Applications (v134+)\r\n\r\n### Core Browser States\r\n- **Navigation State Management**: Multi-phase navigation with security boundaries\r\n- **Download Pipeline States**: Progressive download phases with resume capability  \r\n- **Service Lifecycle States**: Mojo service initialization, active, and cleanup phases\r\n- **Security Context States**: Site isolation, permission states, and capability management\r\n- **Renderer Process States**: Loading, active, backgrounded, and memory optimization\r\n- **Network Request States**: DNS resolution, connection establishment, data transfer phases\r\n\r\n### Advanced State Machines\r\n- **Privacy Sandbox States**: Consent flow, feature enablement, and configuration phases\r\n- **WebRTC Connection States**: ICE gathering, connection establishment, and media flow\r\n- **Cache Management States**: Storage allocation, eviction policies, and cleanup cycles\r\n- **Extension Lifecycle States**: Installation, activation, suspension, and removal phases\r\n\r\n---\r\n\r\n## 2. Modern C++ State Implementation Patterns\r\n\r\n### Type-Safe State Machines with std::variant\r\n\r\n```cpp\r\n// Modern state machine using C++20 features\r\nnamespace content {\r\n\r\n// State definitions with strong typing\r\nstruct NavigationPending {\r\n  GURL url;\r\n  base::TimeTicks start_time;\r\n  NavigationRequest::Priority priority = NavigationRequest::Priority::kNormal;\r\n};\r\n\r\nstruct NavigationInProgress {\r\n  std::unique_ptr<NavigationRequest> request;\r\n  base::TimeTicks navigation_start;\r\n  int64_t request_id;\r\n  bool is_same_process = false;\r\n};\r\n\r\nstruct NavigationComplete {\r\n  NavigationResult result;\r\n  base::TimeTicks completion_time;\r\n  std::optional<NavigationMetrics> metrics;\r\n};\r\n\r\nstruct NavigationFailed {\r\n  net::Error error_code;\r\n  std::string error_description;\r\n  base::TimeTicks failure_time;\r\n  bool is_recoverable = false;\r\n};\r\n\r\n// Type-safe state variant\r\nusing NavigationState = std::variant<\r\n    NavigationPending,\r\n    NavigationInProgress, \r\n    NavigationComplete,\r\n    NavigationFailed>;\r\n\r\nclass NavigationStateMachine {\r\n public:\r\n  // Modern state transition with error handling\r\n  base::expected<void, NavigationError> TransitionTo(NavigationState new_state) {\r\n    if (!IsValidTransition(current_state_, new_state)) {\r\n      return base::unexpected(NavigationError::kInvalidStateTransition);\r\n    }\r\n    \r\n    // Execute state exit actions\r\n    ExecuteExitActions(current_state_);\r\n    \r\n    // Update state atomically\r\n    current_state_ = std::move(new_state);\r\n    \r\n    // Execute state entry actions  \r\n    ExecuteEntryActions(current_state_);\r\n    \r\n    // Notify observers with new state\r\n    NotifyStateChanged();\r\n    \r\n    return base::ok();\r\n  }\r\n\r\n  // Pattern matching for state-specific behavior\r\n  template<typename Visitor>\r\n  auto Visit(Visitor&& visitor) const {\r\n    return std::visit(std::forward<Visitor>(visitor), current_state_);\r\n  }\r\n\r\n  // Type-safe state queries\r\n  bool IsPending() const { \r\n    return std::holds_alternative<NavigationPending>(current_state_); \r\n  }\r\n  \r\n  bool IsInProgress() const { \r\n    return std::holds_alternative<NavigationInProgress>(current_state_); \r\n  }\r\n\r\n private:\r\n  NavigationState current_state_{NavigationPending{}};\r\n  base::ObserverList<NavigationStateObserver> observers_;\r\n  \r\n  bool IsValidTransition(const NavigationState& from, const NavigationState& to) const {\r\n    // Implementation of state transition validation\r\n    return state_transition_table_.IsValidTransition(\r\n        GetStateType(from), GetStateType(to));\r\n  }\r\n};\r\n\r\n}  // namespace content\r\n```\r\n\r\n### Hierarchical State Machines for Complex Components\r\n\r\n```cpp\r\n// Hierarchical state management for download system\r\nnamespace download {\r\n\r\nclass DownloadStateMachine : public base::RefCountedThreadSafe<DownloadStateMachine> {\r\n public:\r\n  // Nested state hierarchy\r\n  enum class MainState {\r\n    kInitializing,\r\n    kActive,      // Has sub-states\r\n    kPaused,\r\n    kCompleted,\r\n    kFailed\r\n  };\r\n  \r\n  enum class ActiveSubState {\r\n    kConnecting,\r\n    kDownloading,\r\n    kVerifying,\r\n    kMovingToFinalLocation\r\n  };\r\n  \r\n  struct StateContext {\r\n    MainState main_state = MainState::kInitializing;\r\n    std::optional<ActiveSubState> sub_state;\r\n    base::FilePath target_path;\r\n    int64_t total_bytes = 0;\r\n    int64_t received_bytes = 0;\r\n    base::TimeTicks last_update;\r\n    DownloadDangerType danger_type = DownloadDangerType::kNotDangerous;\r\n  };\r\n\r\n  // Modern async state transitions\r\n  void TransitionToState(MainState new_main_state, \r\n                        std::optional<ActiveSubState> new_sub_state = std::nullopt) {\r\n    DCHECK_CURRENTLY_ON(content::BrowserThread::UI);\r\n    \r\n    // Validate transition on UI thread\r\n    if (!IsValidStateTransition(context_.main_state, new_main_state)) {\r\n      LOG(ERROR) << \"Invalid state transition attempted: \" \r\n                 << static_cast<int>(context_.main_state) \r\n                 << \" -> \" << static_cast<int>(new_main_state);\r\n      return;\r\n    }\r\n    \r\n    // Execute async state transition\r\n    base::ThreadPool::PostTaskAndReplyWithResult(\r\n        FROM_HERE,\r\n        {base::MayBlock(), base::TaskPriority::USER_VISIBLE},\r\n        base::BindOnce(&DownloadStateMachine::ExecuteStateTransition,\r\n                      base::Unretained(this), new_main_state, new_sub_state),\r\n        base::BindOnce(&DownloadStateMachine::OnStateTransitionComplete,\r\n                      weak_factory_.GetWeakPtr()));\r\n  }\r\n\r\n private:\r\n  StateContext context_;\r\n  base::WeakPtrFactory<DownloadStateMachine> weak_factory_{this};\r\n  \r\n  // Executed on background thread\r\n  bool ExecuteStateTransition(MainState new_main_state, \r\n                             std::optional<ActiveSubState> new_sub_state) {\r\n    // Perform heavy state transition work\r\n    return true;  // Success\r\n  }\r\n  \r\n  // Back on UI thread\r\n  void OnStateTransitionComplete(bool success) {\r\n    if (success) {\r\n      NotifyObserversOfStateChange();\r\n    }\r\n  }\r\n};\r\n\r\n}  // namespace download\r\n```\r\n\r\n---\r\n\r\n## 3. Mojo Service State Management\r\n\r\n### Service Lifecycle State Patterns\r\n\r\n```cpp\r\n// Modern Mojo service with comprehensive state management\r\nnamespace network {\r\n\r\nclass NetworkServiceImpl : public mojom::NetworkService {\r\n public:\r\n  enum class ServiceState {\r\n    kInitializing,\r\n    kConfiguring,\r\n    kActive,\r\n    kShuttingDown,\r\n    kTerminated\r\n  };\r\n\r\n  class StateManager {\r\n   public:\r\n    // Thread-safe state transitions with validation\r\n    bool TransitionTo(ServiceState new_state) {\r\n      base::AutoLock lock(state_lock_);\r\n      \r\n      if (!IsValidTransition(current_state_, new_state)) {\r\n        DLOG(WARNING) << \"Invalid service state transition: \"\r\n                      << StateToString(current_state_) \r\n                      << \" -> \" << StateToString(new_state);\r\n        return false;\r\n      }\r\n      \r\n      ServiceState old_state = current_state_;\r\n      current_state_ = new_state;\r\n      \r\n      // Execute state change callbacks without holding lock\r\n      {\r\n        base::AutoUnlock unlock(state_lock_);\r\n        ExecuteStateChangeCallbacks(old_state, new_state);\r\n      }\r\n      \r\n      return true;\r\n    }\r\n    \r\n    ServiceState GetCurrentState() const {\r\n      base::AutoLock lock(state_lock_);\r\n      return current_state_;\r\n    }\r\n    \r\n    // Wait for specific state with timeout\r\n    base::expected<void, base::TimeDelta> WaitForState(\r\n        ServiceState target_state, \r\n        base::TimeDelta timeout = base::Seconds(30)) {\r\n      \r\n      base::WaitableEvent state_reached;\r\n      base::OneShotTimer timeout_timer;\r\n      \r\n      auto callback = base::BindRepeating([&](ServiceState old_state, ServiceState new_state) {\r\n        if (new_state == target_state) {\r\n          state_reached.Signal();\r\n        }\r\n      });\r\n      \r\n      AddStateChangeCallback(callback);\r\n      \r\n      // Set up timeout\r\n      timeout_timer.Start(FROM_HERE, timeout, base::BindOnce([&]() {\r\n        state_reached.Signal();  // Signal timeout\r\n      }));\r\n      \r\n      state_reached.Wait();\r\n      \r\n      RemoveStateChangeCallback(callback);\r\n      \r\n      base::AutoLock lock(state_lock_);\r\n      if (current_state_ == target_state) {\r\n        return base::ok();\r\n      } else {\r\n        return base::unexpected(timeout);\r\n      }\r\n    }\r\n\r\n   private:\r\n    mutable base::Lock state_lock_;\r\n    ServiceState current_state_ = ServiceState::kInitializing;\r\n    std::vector<base::RepeatingCallback<void(ServiceState, ServiceState)>> callbacks_;\r\n    \r\n    bool IsValidTransition(ServiceState from, ServiceState to) const {\r\n      // Define valid state transition matrix\r\n      static const std::set<std::pair<ServiceState, ServiceState>> valid_transitions = {\r\n        {ServiceState::kInitializing, ServiceState::kConfiguring},\r\n        {ServiceState::kConfiguring, ServiceState::kActive},\r\n        {ServiceState::kActive, ServiceState::kShuttingDown},\r\n        {ServiceState::kShuttingDown, ServiceState::kTerminated},\r\n        // Allow emergency shutdown from any state\r\n        {ServiceState::kInitializing, ServiceState::kTerminated},\r\n        {ServiceState::kConfiguring, ServiceState::kTerminated},\r\n      };\r\n      \r\n      return valid_transitions.count({from, to}) > 0;\r\n    }\r\n  };\r\n\r\n private:\r\n  std::unique_ptr<StateManager> state_manager_;\r\n};\r\n\r\n}  // namespace network\r\n```\r\n\r\n### Async State Coordination with Mojo\r\n\r\n```cpp\r\n// Coordinated state management across processes\r\nnamespace content {\r\n\r\nclass RenderProcessHostStateCoordinator {\r\n public:\r\n  enum class ProcessState {\r\n    kLaunching,\r\n    kInitializing,\r\n    kReady,\r\n    kBackgrounded,\r\n    kSuspended,\r\n    kTerminating\r\n  };\r\n\r\n  // Coordinate state changes across process boundaries\r\n  void CoordinateStateTransition(ProcessState target_state) {\r\n    auto coordinator_request = CreateCoordinationRequest(target_state);\r\n    \r\n    // Coordinate with browser process\r\n    coordinator_request->CoordinateWithBrowser(base::BindOnce(\r\n        &RenderProcessHostStateCoordinator::OnBrowserCoordinationComplete,\r\n        weak_factory_.GetWeakPtr(), target_state));\r\n  }\r\n\r\n private:\r\n  void OnBrowserCoordinationComplete(ProcessState target_state, bool browser_ready) {\r\n    if (!browser_ready) {\r\n      LOG(ERROR) << \"Browser process coordination failed for state: \"\r\n                 << static_cast<int>(target_state);\r\n      return;\r\n    }\r\n    \r\n    // Coordinate with renderer process via Mojo\r\n    if (renderer_coordinator_.is_bound()) {\r\n      renderer_coordinator_->PrepareForStateTransition(\r\n          ConvertToMojoState(target_state),\r\n          base::BindOnce(&RenderProcessHostStateCoordinator::OnRendererReady,\r\n                        weak_factory_.GetWeakPtr(), target_state));\r\n    }\r\n  }\r\n  \r\n  void OnRendererReady(ProcessState target_state, bool renderer_ready) {\r\n    if (renderer_ready) {\r\n      ExecuteStateTransition(target_state);\r\n    } else {\r\n      HandleStateTransitionFailure(target_state);\r\n    }\r\n  }\r\n  \r\n  mojo::Remote<mojom::RendererStateCoordinator> renderer_coordinator_;\r\n  base::WeakPtrFactory<RenderProcessHostStateCoordinator> weak_factory_{this};\r\n};\r\n\r\n}  // namespace content\r\n```\r\n\r\n---\r\n\r\n## 4. Privacy Sandbox State Management\r\n\r\n### Consent and Configuration State Patterns\r\n\r\n```cpp\r\n// Privacy Sandbox state management with compliance tracking\r\nnamespace privacy_sandbox {\r\n\r\nclass PrivacySandboxStateManager {\r\n public:\r\n  enum class ConsentState {\r\n    kNotRequired,\r\n    kRequired,\r\n    kPending,\r\n    kGranted,\r\n    kDenied,\r\n    kWithdrawn\r\n  };\r\n  \r\n  enum class FeatureState {\r\n    kDisabled,\r\n    kEnabledWithConsent,\r\n    kEnabledWithoutConsent,\r\n    kTemporarilyDisabled,\r\n    kPermanentlyDisabled\r\n  };\r\n\r\n  struct PrivacySandboxContext {\r\n    ConsentState consent_state = ConsentState::kNotRequired;\r\n    std::map<PrivacySandboxFeature, FeatureState> feature_states;\r\n    base::Time consent_timestamp;\r\n    std::string consent_jurisdiction;\r\n    bool is_measurement_enabled = false;\r\n    bool is_topics_enabled = false;\r\n    bool is_fledge_enabled = false;\r\n  };\r\n\r\n  // Complex state validation with regulatory compliance\r\n  base::expected<void, PrivacySandboxError> UpdateConsentState(\r\n      ConsentState new_consent_state,\r\n      const ConsentMetadata& metadata) {\r\n    \r\n    if (!IsValidConsentTransition(context_.consent_state, new_consent_state)) {\r\n      return base::unexpected(PrivacySandboxError::kInvalidConsentTransition);\r\n    }\r\n    \r\n    // Validate regulatory compliance\r\n    if (auto compliance_check = ValidateRegulatoryCompliance(new_consent_state, metadata);\r\n        !compliance_check.has_value()) {\r\n      return compliance_check;\r\n    }\r\n    \r\n    // Update consent state atomically\r\n    context_.consent_state = new_consent_state;\r\n    context_.consent_timestamp = base::Time::Now();\r\n    context_.consent_jurisdiction = metadata.jurisdiction;\r\n    \r\n    // Propagate state changes to dependent features\r\n    UpdateDependentFeatureStates();\r\n    \r\n    // Log compliance event\r\n    RecordComplianceEvent(new_consent_state, metadata);\r\n    \r\n    return base::ok();\r\n  }\r\n\r\n private:\r\n  PrivacySandboxContext context_;\r\n  \r\n  void UpdateDependentFeatureStates() {\r\n    // Update Topics API state\r\n    if (context_.consent_state == ConsentState::kGranted) {\r\n      context_.feature_states[PrivacySandboxFeature::kTopics] = \r\n          FeatureState::kEnabledWithConsent;\r\n    } else if (context_.consent_state == ConsentState::kDenied) {\r\n      context_.feature_states[PrivacySandboxFeature::kTopics] = \r\n          FeatureState::kPermanentlyDisabled;\r\n    }\r\n    \r\n    // Update FLEDGE state based on consent and feature flags\r\n    UpdateFledgeState();\r\n    UpdateMeasurementState();\r\n  }\r\n};\r\n\r\n}  // namespace privacy_sandbox\r\n```\r\n\r\n---\r\n\r\n## 5. Performance-Optimized State Machines\r\n\r\n### Cache-Friendly State Management\r\n\r\n```cpp\r\n// High-performance state management for renderer processes\r\nnamespace blink {\r\n\r\nclass RenderFrameStateManager {\r\n public:\r\n  // Bit-packed state for cache efficiency\r\n  struct CompactFrameState {\r\n    uint32_t lifecycle_state : 4;      // FrameLifecycleState (16 values max)\r\n    uint32_t visibility_state : 2;     // VisibilityState (4 values max)\r\n    uint32_t loading_state : 3;        // LoadingState (8 values max)\r\n    uint32_t is_main_frame : 1;        // Boolean\r\n    uint32_t is_cross_origin : 1;      // Boolean\r\n    uint32_t has_committed_navigation : 1;  // Boolean\r\n    uint32_t is_frozen : 1;             // Boolean\r\n    uint32_t reserved : 19;             // Reserved for future use\r\n    \r\n    // Fast state queries using bit operations\r\n    bool IsActiveAndVisible() const {\r\n      return (lifecycle_state == static_cast<uint32_t>(FrameLifecycleState::kActive)) &&\r\n             (visibility_state == static_cast<uint32_t>(mojom::VisibilityState::kVisible));\r\n    }\r\n    \r\n    bool CanStartNavigation() const {\r\n      return (lifecycle_state <= static_cast<uint32_t>(FrameLifecycleState::kActive)) &&\r\n             !is_frozen;\r\n    }\r\n  };\r\n\r\n  // Lockless state updates using atomic operations\r\n  void UpdateState(const StateUpdate& update) {\r\n    CompactFrameState current_state;\r\n    CompactFrameState new_state;\r\n    \r\n    do {\r\n      current_state.value = state_.load(std::memory_order_acquire);\r\n      new_state = current_state;\r\n      \r\n      // Apply state update\r\n      ApplyUpdate(new_state, update);\r\n      \r\n    } while (!state_.compare_exchange_weak(\r\n        current_state.value, new_state.value, \r\n        std::memory_order_release, std::memory_order_relaxed));\r\n    \r\n    // Notify state change observers if necessary\r\n    if (HasStateChangeObservers() && StateChangeRequiresNotification(current_state, new_state)) {\r\n      NotifyStateChangeAsync(current_state, new_state);\r\n    }\r\n  }\r\n\r\n private:\r\n  // Atomic state storage for lockless access\r\n  std::atomic<uint32_t> state_{0};\r\n  \r\n  // Observer notification queue to avoid blocking state updates\r\n  base::circular_deque<StateChangeNotification> pending_notifications_;\r\n  base::SequenceBound<StateChangeNotifier> notifier_;\r\n};\r\n\r\n}  // namespace blink\r\n```\r\n\r\n### Memory-Efficient State History\r\n\r\n```cpp\r\n// Efficient state history tracking for debugging and rollback\r\nnamespace base {\r\n\r\ntemplate<typename StateType, size_t MaxHistorySize = 32>\r\nclass CompactStateHistory {\r\n public:\r\n  struct StateEntry {\r\n    StateType state;\r\n    base::TimeTicks timestamp;\r\n    uint32_t transition_id;\r\n    \r\n    // Optional debug information (only in debug builds)\r\n#if DCHECK_IS_ON()\r\n    std::string debug_info;\r\n    base::Location location;\r\n#endif\r\n  };\r\n\r\n  void RecordStateTransition(const StateType& new_state, \r\n                           const base::Location& location = FROM_HERE) {\r\n    uint32_t transition_id = next_transition_id_++;\r\n    \r\n    StateEntry entry{\r\n      .state = new_state,\r\n      .timestamp = base::TimeTicks::Now(),\r\n      .transition_id = transition_id\r\n    };\r\n    \r\n#if DCHECK_IS_ON()\r\n    entry.location = location;\r\n#endif\r\n    \r\n    // Circular buffer for memory efficiency\r\n    history_[history_index_] = std::move(entry);\r\n    history_index_ = (history_index_ + 1) % MaxHistorySize;\r\n    \r\n    if (size_ < MaxHistorySize) {\r\n      ++size_;\r\n    }\r\n  }\r\n  \r\n  // Efficient state history queries\r\n  std::optional<StateType> GetStateAt(base::TimeTicks timestamp) const {\r\n    for (size_t i = 0; i < size_; ++i) {\r\n      const auto& entry = history_[i];\r\n      if (entry.timestamp <= timestamp) {\r\n        return entry.state;\r\n      }\r\n    }\r\n    return std::nullopt;\r\n  }\r\n\r\n private:\r\n  std::array<StateEntry, MaxHistorySize> history_;\r\n  size_t history_index_ = 0;\r\n  size_t size_ = 0;\r\n  std::atomic<uint32_t> next_transition_id_{1};\r\n};\r\n\r\n}  // namespace base\r\n```\r\n\r\n---\r\n\r\n## 6. Testing State Machines in Chromium v134+\r\n\r\n### Comprehensive State Machine Testing\r\n\r\n```cpp\r\n// Modern testing patterns for state machines\r\nnamespace content {\r\n\r\nclass NavigationStateMachineTest : public testing::Test {\r\n public:\r\n  void SetUp() override {\r\n    state_machine_ = std::make_unique<NavigationStateMachine>();\r\n    \r\n    // Set up test observers\r\n    state_observer_ = std::make_unique<TestStateObserver>();\r\n    state_machine_->AddObserver(state_observer_.get());\r\n  }\r\n\r\n  // Test state transition validation\r\n  TEST_F(NavigationStateMachineTest, ValidatesStateTransitions) {\r\n    // Test valid transition\r\n    EXPECT_TRUE(state_machine_->TransitionTo(NavigationInProgress{}).has_value());\r\n    \r\n    // Test invalid transition (should fail)\r\n    auto result = state_machine_->TransitionTo(NavigationPending{});\r\n    EXPECT_FALSE(result.has_value());\r\n    EXPECT_EQ(result.error(), NavigationError::kInvalidStateTransition);\r\n  }\r\n\r\n  // Test state machine behavior under concurrent access\r\n  TEST_F(NavigationStateMachineTest, HandlesConcurrentStateChanges) {\r\n    constexpr int kNumThreads = 10;\r\n    constexpr int kTransitionsPerThread = 100;\r\n    \r\n    std::vector<std::thread> threads;\r\n    std::atomic<int> successful_transitions{0};\r\n    \r\n    for (int i = 0; i < kNumThreads; ++i) {\r\n      threads.emplace_back([&]() {\r\n        for (int j = 0; j < kTransitionsPerThread; ++j) {\r\n          if (state_machine_->TransitionTo(GenerateRandomValidState()).has_value()) {\r\n            successful_transitions.fetch_add(1);\r\n          }\r\n        }\r\n      });\r\n    }\r\n    \r\n    for (auto& thread : threads) {\r\n      thread.join();\r\n    }\r\n    \r\n    // Verify state machine remained consistent\r\n    EXPECT_GT(successful_transitions.load(), 0);\r\n    EXPECT_TRUE(state_machine_->IsInValidState());\r\n  }\r\n\r\n private:\r\n  std::unique_ptr<NavigationStateMachine> state_machine_;\r\n  std::unique_ptr<TestStateObserver> state_observer_;\r\n};\r\n\r\n// Property-based testing for state machines\r\nclass StatePropertyTest : public testing::TestWithParam<StateTransitionTestCase> {\r\n public:\r\n  // Generate comprehensive test cases\r\n  static std::vector<StateTransitionTestCase> GenerateTestCases() {\r\n    std::vector<StateTransitionTestCase> test_cases;\r\n    \r\n    // Generate all valid state transitions\r\n    for (auto from_state : AllPossibleStates()) {\r\n      for (auto to_state : AllPossibleStates()) {\r\n        test_cases.push_back({\r\n          .from_state = from_state,\r\n          .to_state = to_state,\r\n          .should_succeed = IsValidTransition(from_state, to_state),\r\n          .expected_side_effects = ComputeExpectedSideEffects(from_state, to_state)\r\n        });\r\n      }\r\n    }\r\n    \r\n    return test_cases;\r\n  }\r\n};\r\n\r\nINSTANTIATE_TEST_SUITE_P(\r\n    AllStateTransitions,\r\n    StatePropertyTest,\r\n    testing::ValuesIn(StatePropertyTest::GenerateTestCases())\r\n);\r\n\r\n}  // namespace content\r\n```\r\n\r\n### Mock State Coordination Testing\r\n\r\n```cpp\r\n// Testing complex state coordination scenarios\r\nnamespace content {\r\n\r\nclass MockRenderProcessStateCoordinator : public RenderProcessStateCoordinator {\r\n public:\r\n  MOCK_METHOD(void, CoordinateStateTransition, (ProcessState target_state), (override));\r\n  MOCK_METHOD(bool, WaitForStateTransition, (ProcessState state, base::TimeDelta timeout), (override));\r\n  \r\n  // Simulate state coordination failures\r\n  void SimulateCoordinationFailure(ProcessState failing_state) {\r\n    ON_CALL(*this, CoordinateStateTransition(failing_state))\r\n        .WillByDefault(testing::Invoke([this](ProcessState state) {\r\n          // Simulate failure after delay\r\n          base::SequencedTaskRunner::GetCurrentDefault()->PostDelayedTask(\r\n              FROM_HERE,\r\n              base::BindOnce(&MockRenderProcessStateCoordinator::NotifyCoordinationFailure,\r\n                            base::Unretained(this), state),\r\n              base::Milliseconds(100));\r\n        }));\r\n  }\r\n  \r\n private:\r\n  void NotifyCoordinationFailure(ProcessState state) {\r\n    // Notify observers of coordination failure\r\n  }\r\n};\r\n\r\n// Integration test for state coordination\r\nTEST_F(RenderProcessStateTest, HandlesCoordinationFailures) {\r\n  auto mock_coordinator = std::make_unique<MockRenderProcessStateCoordinator>();\r\n  \r\n  // Configure mock to fail background state transition\r\n  mock_coordinator->SimulateCoordinationFailure(ProcessState::kBackgrounded);\r\n  \r\n  // Attempt state transition\r\n  bool transition_result = process_host_->TransitionToBackgroundState();\r\n  \r\n  // Verify graceful failure handling\r\n  EXPECT_FALSE(transition_result);\r\n  EXPECT_EQ(process_host_->GetCurrentState(), ProcessState::kReady);\r\n  \r\n  // Verify error was logged\r\n  EXPECT_TRUE(HasLoggedError(\"State coordination failed\"));\r\n}\r\n\r\n}  // namespace content\r\n```\r\n\r\n---\r\n\r\n## 7. Real-World Examples from Chromium v134+\r\n\r\n### Download State Management\r\n\r\n**File**: `content/browser/download/download_item_impl.h`\r\n\r\nModern download state management with resumption support:\r\n\r\n```cpp\r\n// Simplified view of actual Chromium v134+ download states\r\nenum class DownloadInternalState {\r\n  kInitial,\r\n  kTarget,\r\n  kInterrupted,\r\n  kInProgress, \r\n  kComplete,\r\n  kCancelled\r\n};\r\n\r\n// Enhanced with security and privacy features\r\nclass DownloadItemImpl {\r\n private:\r\n  DownloadInternalState internal_state_ = DownloadInternalState::kInitial;\r\n  DownloadDangerType danger_type_ = DownloadDangerType::kNotDangerous;\r\n  bool is_mixed_content_ = false;\r\n  bool is_from_privacy_sandbox_ = false;\r\n};\r\n```\r\n\r\n### Service Worker State Management  \r\n\r\n**File**: `content/browser/service_worker/service_worker_version.h`\r\n\r\n```cpp\r\n// Service Worker lifecycle with enhanced security\r\nenum class ServiceWorkerVersion::Status {\r\n  kNew,\r\n  kDownloading,\r\n  kInstalling,\r\n  kInstalled,\r\n  kActivating,\r\n  kActivated,\r\n  kRedundant\r\n};\r\n\r\n// Modern state tracking with privacy considerations\r\nclass ServiceWorkerVersion {\r\n private:\r\n  Status status_ = Status::kNew;\r\n  bool is_privacy_sandbox_enabled_ = false;\r\n  std::optional<PrivacySandboxPolicy> privacy_policy_;\r\n};\r\n```\r\n\r\n### Navigation State with Site Isolation\r\n\r\n**File**: `content/browser/renderer_host/navigation_request.h`\r\n\r\n```cpp\r\n// Enhanced navigation states for site isolation\r\nenum class NavigationRequest::NavigationState {\r\n  kWillStartRequest,\r\n  kWillRedirectRequest, \r\n  kWillFailRequest,\r\n  kWillProcessResponse,\r\n  kReadyToCommit,\r\n  kWillCommitNavigation\r\n};\r\n\r\n// Modern security-aware navigation tracking\r\nclass NavigationRequest {\r\n private:\r\n  NavigationState state_ = NavigationState::kWillStartRequest;\r\n  bool is_cross_site_cross_rph_ = false;\r\n  SiteInstance* target_site_instance_ = nullptr;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 8. Advanced State Pattern Best Practices (v134+)\r\n\r\n### Modern C++ Best Practices\r\n\r\n1. **Use std::variant for Type Safety**\r\n   - Prefer `std::variant` over enum + union patterns\r\n   - Leverage `std::visit` for exhaustive state handling\r\n   - Use concepts to constrain state types\r\n\r\n2. **Implement RAII for State Resources**\r\n   - Acquire resources on state entry, release on exit\r\n   - Use smart pointers for automatic cleanup\r\n   - Implement exception-safe state transitions\r\n\r\n3. **Leverage base::expected for Error Handling**\r\n   - Return `base::expected<void, ErrorType>` from state transitions\r\n   - Provide detailed error information for debugging\r\n   - Support both synchronous and asynchronous error propagation\r\n\r\n### Performance Optimization\r\n\r\n1. **Memory Layout Optimization**\r\n   - Pack state data for cache efficiency\r\n   - Use atomic operations for lockless state updates\r\n   - Minimize memory allocations during transitions\r\n\r\n2. **Concurrent State Management**\r\n   - Use thread-safe state machines for shared state\r\n   - Implement lockless algorithms where possible\r\n   - Provide async state coordination mechanisms\r\n\r\n### Security Considerations\r\n\r\n1. **State Validation**\r\n   - Validate all state transitions against security policies\r\n   - Implement capability-based state access control\r\n   - Log security-relevant state changes for auditing\r\n\r\n2. **Privacy-Aware State Management**\r\n   - Consider privacy implications of state persistence\r\n   - Implement appropriate state cleanup on privacy events\r\n   - Support privacy sandbox state isolation\r\n\r\n---\r\n\r\n## 9. Modern Debugging and Monitoring\r\n\r\n### State Machine Introspection\r\n\r\n```cpp\r\n// Advanced debugging support for state machines\r\nclass StateDebugInfo {\r\n public:\r\n  struct StateSnapshot {\r\n    std::string current_state_name;\r\n    base::TimeTicks last_transition_time;\r\n    std::vector<std::string> recent_transitions;\r\n    std::map<std::string, base::Value> debug_properties;\r\n  };\r\n  \r\n  // Export state for chrome://internals pages\r\n  base::Value::Dict ExportToValue() const {\r\n    base::Value::Dict result;\r\n    result.Set(\"current_state\", GetCurrentStateName());\r\n    result.Set(\"transition_count\", static_cast<int>(transition_count_));\r\n    result.Set(\"uptime\", GetUptimeString());\r\n    result.Set(\"recent_transitions\", GetRecentTransitionsAsValue());\r\n    return result;\r\n  }\r\n};\r\n```\r\n\r\n### Performance Metrics Integration\r\n\r\n```cpp\r\n// Integration with Chromium's metrics system\r\nclass StateMetricsRecorder {\r\n public:\r\n  void RecordStateTransition(const std::string& state_machine_name,\r\n                           const std::string& from_state,\r\n                           const std::string& to_state,\r\n                           base::TimeDelta transition_duration) {\r\n    // Record UMA histogram\r\n    base::UmaHistogramTimes(\r\n        base::StrCat({\"StateTransition.\", state_machine_name, \".Duration\"}),\r\n        transition_duration);\r\n        \r\n    // Record state distribution\r\n    base::UmaHistogramEnumeration(\r\n        base::StrCat({\"StateTransition.\", state_machine_name, \".ToState\"}),\r\n        HashStateString(to_state));\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 10. Integration with Modern Chromium Architecture\r\n\r\n### Mojo Service Integration\r\n\r\nState machines in v134+ integrate seamlessly with Mojo services for cross-process state synchronization:\r\n\r\n```cpp\r\n// State coordination across process boundaries\r\ninterface StateCoordinator {\r\n  // Prepare for state transition\r\n  PrepareStateTransition(StateTransitionRequest request) \r\n      => (StateTransitionResponse response);\r\n  \r\n  // Commit state transition after coordination\r\n  CommitStateTransition(StateCommitRequest request);\r\n  \r\n  // Observer interface for state changes\r\n  AddStateObserver(pending_remote<StateObserver> observer);\r\n};\r\n```\r\n\r\n### Privacy Sandbox Integration\r\n\r\nState machines respect Privacy Sandbox boundaries and provide appropriate isolation:\r\n\r\n```cpp\r\n// Privacy-aware state management\r\nclass PrivacyAwareStateMachine {\r\n public:\r\n  // State transitions respect privacy context\r\n  bool CanTransitionTo(State new_state) const {\r\n    return IsValidTransition(current_state_, new_state) &&\r\n           privacy_policy_.AllowsStateTransition(current_state_, new_state);\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 11. References and Further Reading\r\n\r\n### Core Implementation Files\r\n- `base/state_machine/` - Base state machine utilities\r\n- `content/browser/navigation/navigation_request.h` - Navigation state management\r\n- `content/browser/download/download_item_impl.h` - Download state implementation  \r\n- `services/network/network_service.h` - Network service lifecycle states\r\n- `chrome/browser/privacy_sandbox/` - Privacy Sandbox state management\r\n\r\n### Architecture Documentation\r\n- [Process Model](../process-model.md) - Multi-process state coordination\r\n- [IPC Internals](../ipc-internals.md) - State synchronization via Mojo\r\n- [Security Model](../../security/security-model.md) - Security-aware state management\r\n\r\n### Modern C++ Patterns\r\n- [RAII Patterns](../design-patterns/raii-pattern.md) - Resource management in state machines\r\n- [Observer Pattern](../design-patterns/observer-pattern.md) - State change notification\r\n- [Factory Pattern](../design-patterns/factory-pattern.md) - State object creation\r\n\r\n### Testing and Debugging\r\n- [Testing Strategies](../../debugging/testing-strategies.md) - State machine testing approaches\r\n- [Chrome Internals](../../debugging/chrome-internals-urls.md) - State inspection tools\r\n\r\n---\r\n\r\nThe State pattern in modern Chromium v134+ demonstrates sophisticated software engineering with type safety, performance optimization, security awareness, and seamless integration with the browser's multi-process architecture. Understanding these patterns is essential for contributing to or extending Chromium's state management systems.\r\n"
  },
  {
    "path": "architecture/design-patterns/pre-post-contract",
    "title": "Contract Programming and Pre/Post Patterns in Modern Chromium (v134+)",
    "content": "# Contract Programming and Pre/Post Patterns in Modern Chromium (v134+)\r\n\r\nContract programming is a fundamental architectural principle in modern Chromium v134+, emphasizing **preconditions**, **postconditions**, and **invariants** to ensure reliable, secure, and maintainable software components. This approach has evolved significantly with modern C++20/23 features, Mojo services, and advanced error handling patterns like `base::expected`, providing formal contracts that improve code reliability, security, and performance.\r\n\r\n---\r\n\r\n## Modern Contract Programming in Chromium (v134+)\r\n\r\nContract programming in modern Chromium has evolved to leverage cutting-edge C++20/23 features and sophisticated error handling patterns. It defines formal, precise, and verifiable interfaces for software components through:\r\n\r\n- **Preconditions**: Conditions that must hold true before a function executes, validated using modern assertions and `base::expected`\r\n- **Postconditions**: Conditions guaranteed after successful function execution, enforced through RAII and result validation\r\n- **Invariants**: Properties that remain true throughout an object's lifetime, maintained via class design and security boundaries\r\n- **Error Contracts**: Explicit error handling using `base::expected<T, Error>` instead of exceptions\r\n- **Capability Contracts**: Security-oriented contracts that define what operations are permitted in different contexts\r\n\r\n### Modern Implementation Approaches (v134+)\r\n\r\n```cpp\r\n// Example: Modern contract programming with base::expected\r\nclass NetworkService {\r\n public:\r\n  // Precondition: url must be valid, context must be authenticated\r\n  // Postcondition: Returns success result or specific error\r\n  base::expected<std::unique_ptr<URLLoader>, NetworkError> \r\n  CreateURLLoader(const GURL& url, const SecurityContext& context) {\r\n    // Precondition validation\r\n    if (!url.is_valid()) {\r\n      return base::unexpected(NetworkError::kInvalidURL);\r\n    }\r\n    if (!context.IsAuthenticated()) {\r\n      return base::unexpected(NetworkError::kUnauthenticated);\r\n    }\r\n    \r\n    // Main operation with guaranteed postcondition\r\n    auto loader = CreateLoaderInternal(url, context);\r\n    DCHECK(loader);  // Postcondition: never returns null on success\r\n    return loader;\r\n  }\r\n};\r\n```\r\n\r\nAt its core, modern contract programming in Chromium relies on **compile-time validation**, **runtime assertions**, and **type-safe error handling**—ensuring that contract violations are caught early and handled gracefully.\r\n\r\n---\r\n\r\n## Contract Patterns in Modern Chromium (v134+)\r\n\r\nModern Chromium v134+ extensively implements contract programming through sophisticated architectural patterns that ensure security, performance, and reliability. This evolution goes far beyond simple assertions to include **service contracts**, **security validation**, and **cross-process guarantees**.\r\n\r\n### Service Lifecycle Contracts\r\n\r\nModern Chromium's service-oriented architecture relies heavily on contract programming for service initialization and coordination:\r\n\r\n```cpp\r\n// Modern service contract example\r\nclass DownloadService : public mojom::DownloadService {\r\n public:\r\n  // Contract: Service must be initialized before any operations\r\n  base::expected<void, ServiceError> Initialize(Profile* profile) {\r\n    // Preconditions\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    if (!profile || is_initialized_) {\r\n      return base::unexpected(ServiceError::kInvalidState);\r\n    }\r\n    \r\n    // Initialization with guaranteed postconditions\r\n    storage_partition_ = profile->GetStoragePartition();\r\n    network_context_ = storage_partition_->GetNetworkContext();\r\n    is_initialized_ = true;\r\n    \r\n    // Postcondition: Service is ready for operations\r\n    DCHECK(IsInitialized());\r\n    return base::ok();\r\n  }\r\n  \r\n  // Contract: StartDownload requires initialized service and valid parameters\r\n  void StartDownload(const GURL& url, \r\n                    mojo::PendingReceiver<mojom::DownloadController> receiver,\r\n                    StartDownloadCallback callback) override {\r\n    // Precondition validation\r\n    if (!IsInitialized()) {\r\n      std::move(callback).Run(DownloadResult::kServiceNotReady);\r\n      return;\r\n    }\r\n    \r\n    if (!url.is_valid() || !url.SchemeIsHTTPOrHTTPS()) {\r\n      std::move(callback).Run(DownloadResult::kInvalidURL);\r\n      return;\r\n    }\r\n    \r\n    // Execute with postcondition guarantee\r\n    StartDownloadInternal(url, std::move(receiver), std::move(callback));\r\n  }\r\n  \r\n private:\r\n  bool IsInitialized() const { return is_initialized_ && storage_partition_; }\r\n  \r\n  bool is_initialized_ = false;\r\n  StoragePartition* storage_partition_ = nullptr;\r\n  network::NetworkContext* network_context_ = nullptr;\r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n};\r\n```\r\n\r\n### Security Contract Validation\r\n\r\nModern Chromium implements rigorous security contracts that validate permissions and capabilities:\r\n\r\n```cpp\r\n// Security contract pattern for site isolation\r\nclass RenderFrameHostImpl {\r\n public:\r\n  // Contract: CreateChild requires valid security context and site isolation\r\n  base::expected<std::unique_ptr<RenderFrameHost>, SecurityError>\r\n  CreateChildFrame(const std::string& name, \r\n                   const blink::FramePolicy& frame_policy) {\r\n    // Security preconditions\r\n    if (!GetSiteInstance()->IsValid()) {\r\n      return base::unexpected(SecurityError::kInvalidSiteInstance);\r\n    }\r\n    \r\n    if (!CanCreateChildFrame(frame_policy)) {\r\n      return base::unexpected(SecurityError::kPermissionDenied);\r\n    }\r\n    \r\n    // Create with security guarantees\r\n    auto child_frame = CreateChildFrameInternal(name, frame_policy);\r\n    \r\n    // Postcondition: Child frame inherits security properties\r\n    DCHECK_EQ(child_frame->GetSiteInstance()->GetSiteURL(),\r\n              GetSiteInstance()->GetSiteURL());\r\n    \r\n    return child_frame;\r\n  }\r\n};\r\n```\r\n\r\nThis ensures that each component operates within well-defined security boundaries and adheres to site isolation requirements.\r\n\r\n---\r\n\r\n## Benefits of Modern Contract Programming (v134+)\r\n\r\nModern contract programming in Chromium v134+ provides significant advantages over traditional error handling and validation approaches:\r\n\r\n### 1. **Enhanced Security and Safety**\r\n- **Compile-time Validation**: Contracts catch errors during compilation using C++20 concepts\r\n- **Runtime Security**: Automatic validation of security contexts and permissions\r\n- **Memory Safety**: RAII-based contracts prevent resource leaks and use-after-free bugs\r\n- **Site Isolation**: Contracts enforce process boundaries and origin restrictions\r\n\r\n### 2. **Superior Error Handling**\r\n- **Explicit Error Contracts**: `base::expected<T, Error>` makes error paths visible and testable\r\n- **Graceful Degradation**: Well-defined failure modes instead of crashes or undefined behavior\r\n- **Error Propagation**: Structured error handling across service boundaries\r\n- **Debugging Support**: Clear contract violations with detailed error information\r\n\r\n### 3. **Performance and Reliability**\r\n- **Early Validation**: Preconditions prevent expensive operations on invalid inputs\r\n- **Predictable Behavior**: Postconditions guarantee consistent outcomes\r\n- **Optimized Code Paths**: Compiler optimizations based on contract assumptions\r\n- **Reduced Testing Overhead**: Contracts serve as executable specifications\r\n\r\n### 4. **Developer Experience**\r\n- **Self-Documenting Code**: Contracts make API expectations explicit\r\n- **IDE Integration**: Better code completion and error detection\r\n- **Refactoring Safety**: Contract violations are caught during code changes\r\n- **Onboarding Efficiency**: New developers understand system guarantees quickly\r\n\r\n### 5. **Cross-Process Reliability**\r\n- **Service Contracts**: Mojo interfaces with built-in validation and capability checking\r\n- **IPC Safety**: Type-safe communication with automatic serialization validation\r\n- **Process Isolation**: Contracts enforce security boundaries between processes\r\n- **Capability-Based Security**: Fine-grained permission contracts for system resources\r\n\r\n---\r\n\r\n## Modern C++ Contract Patterns (v134+)\r\n\r\nChromium v134+ leverages cutting-edge C++20/23 features to implement sophisticated contract patterns that provide compile-time safety, runtime validation, and elegant error handling.\r\n\r\n### 1. **Concept-Based Contracts**\r\n\r\nModern C++ concepts enable compile-time contract validation:\r\n\r\n```cpp\r\n// Concept defining contract requirements\r\ntemplate<typename T>\r\nconcept ValidNetworkRequest = requires(T request) {\r\n  { request.GetURL() } -> std::convertible_to<GURL>;\r\n  { request.IsValid() } -> std::convertible_to<bool>;\r\n  { request.GetSecurityLevel() } -> std::convertible_to<SecurityLevel>;\r\n};\r\n\r\n// Function with concept-based contract\r\ntemplate<ValidNetworkRequest RequestType>\r\nbase::expected<std::unique_ptr<URLLoader>, NetworkError>\r\nCreateSecureLoader(const RequestType& request) {\r\n  // Concept guarantees these methods exist and return correct types\r\n  static_assert(ValidNetworkRequest<RequestType>);\r\n  \r\n  if (!request.IsValid()) {\r\n    return base::unexpected(NetworkError::kInvalidRequest);\r\n  }\r\n  \r\n  return CreateLoaderForRequest(request);\r\n}\r\n```\r\n\r\n### 2. **RAII-Based Resource Contracts**\r\n\r\nModern RAII patterns ensure resource cleanup and state consistency:\r\n\r\n```cpp\r\n// RAII contract for GPU context management\r\nclass ScopedGPUContext {\r\n public:\r\n  // Contract: Constructor establishes valid GPU context\r\n  explicit ScopedGPUContext(viz::Display* display) \r\n      : display_(display), context_lost_(false) {\r\n    DCHECK(display_);\r\n    \r\n    // Precondition: Display must be valid\r\n    if (!display_->IsValid()) {\r\n      context_lost_ = true;\r\n      return;\r\n    }\r\n    \r\n    // Establish GPU context with guaranteed cleanup\r\n    context_ = display_->CreateContext();\r\n    DCHECK(context_ || context_lost_);\r\n  }\r\n  \r\n  // Contract: Destructor guarantees resource cleanup\r\n  ~ScopedGPUContext() {\r\n    if (context_ && !context_lost_) {\r\n      context_->Destroy();\r\n    }\r\n    // Postcondition: All resources cleaned up\r\n  }\r\n  \r\n  // Contract: IsValid indicates usable context\r\n  bool IsValid() const { \r\n    return context_ && !context_lost_; \r\n  }\r\n  \r\n  // Contract: GetContext only valid when IsValid() == true\r\n  viz::GLContext* GetContext() const {\r\n    DCHECK(IsValid());  // Precondition enforcement\r\n    return context_.get();\r\n  }\r\n  \r\n private:\r\n  viz::Display* display_;\r\n  std::unique_ptr<viz::GLContext> context_;\r\n  bool context_lost_;\r\n};\r\n```\r\n\r\n### 3. **Async Contract Patterns**\r\n\r\nModern async programming with contract guarantees:\r\n\r\n```cpp\r\n// Async contract pattern with base::expected\r\nclass AsyncDownloadManager {\r\n public:\r\n  // Contract: Async operation with guaranteed completion callback\r\n  void StartDownloadAsync(\r\n      const GURL& url,\r\n      base::OnceCallback<void(base::expected<DownloadId, DownloadError>)> callback) {\r\n    \r\n    // Precondition validation\r\n    if (!url.is_valid()) {\r\n      std::move(callback).Run(base::unexpected(DownloadError::kInvalidURL));\r\n      return;\r\n    }\r\n    \r\n    // Contract: Callback will be called exactly once\r\n    auto wrapped_callback = base::BindOnce(\r\n        [](base::OnceCallback<void(base::expected<DownloadId, DownloadError>)> cb,\r\n           base::expected<DownloadId, DownloadError> result) {\r\n          // Postcondition: Result is always valid (success or well-defined error)\r\n          DCHECK(result.has_value() || IsValidDownloadError(result.error()));\r\n          std::move(cb).Run(std::move(result));\r\n        }, \r\n        std::move(callback));\r\n    \r\n    // Start async operation\r\n    StartDownloadInternal(url, std::move(wrapped_callback));\r\n  }\r\n  \r\n private:\r\n  static bool IsValidDownloadError(DownloadError error) {\r\n    return error != DownloadError::kUnknown;\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## Mojo Service Contracts (v134+)\r\n\r\nChromium's Mojo IPC system implements sophisticated contract patterns for cross-process communication, ensuring type safety, capability validation, and security enforcement.\r\n\r\n### 1. **Interface Contracts**\r\n\r\nMojo interfaces define explicit contracts for cross-process communication:\r\n\r\n```cpp\r\n// Mojo interface with built-in contract validation\r\ninterface DownloadService {\r\n  // Contract: StartDownload validates parameters and returns status\r\n  StartDownload(url.mojom.Url download_url, \r\n               pending_receiver<DownloadController> controller)\r\n      => (DownloadResult result);\r\n  \r\n  // Contract: Observer must be valid and will receive notifications\r\n  AddObserver(pending_remote<DownloadObserver> observer);\r\n  \r\n  // Contract: Returns current downloads matching filter criteria\r\n  GetDownloads(DownloadFilter filter) \r\n      => (array<DownloadInfo> downloads);\r\n};\r\n\r\n// Implementation with contract enforcement\r\nclass DownloadServiceImpl : public mojom::DownloadService {\r\n public:\r\n  void StartDownload(\r\n      const GURL& url,\r\n      mojo::PendingReceiver<mojom::DownloadController> controller,\r\n      StartDownloadCallback callback) override {\r\n    \r\n    // Mojo contract: Validate receiver before proceeding\r\n    if (!controller.is_valid()) {\r\n      std::move(callback).Run(DownloadResult::kInvalidReceiver);\r\n      return;\r\n    }\r\n    \r\n    // Security contract: Validate URL permissions\r\n    if (!security_policy_->CanDownload(url, GetCurrentOrigin())) {\r\n      std::move(callback).Run(DownloadResult::kPermissionDenied);\r\n      return;\r\n    }\r\n    \r\n    // Business logic with guaranteed callback\r\n    auto download_id = CreateDownload(url);\r\n    BindController(std::move(controller), download_id);\r\n    std::move(callback).Run(DownloadResult::kSuccess);\r\n  }\r\n};\r\n```\r\n\r\n### 2. **Capability-Based Security Contracts**\r\n\r\nModern Mojo services implement capability-based security through contract validation:\r\n\r\n```cpp\r\n// Security contract for file system access\r\nclass FileSystemAccessService : public mojom::FileSystemAccessService {\r\n public:\r\n  void RequestFileAccess(\r\n      const base::FilePath& path,\r\n      FileAccessMode mode,\r\n      mojo::PendingRemote<mojom::FileAccessObserver> observer,\r\n      RequestFileAccessCallback callback) override {\r\n    \r\n    // Security contract: Validate capability tokens\r\n    auto capability_result = ValidateFileCapability(path, mode);\r\n    if (!capability_result.has_value()) {\r\n      std::move(callback).Run(\r\n          FileAccessResult::FromError(capability_result.error()));\r\n      return;\r\n    }\r\n    \r\n    // Privacy contract: Check user permissions\r\n    auto permission_result = CheckUserPermission(path, mode);\r\n    if (!permission_result.has_value()) {\r\n      std::move(callback).Run(\r\n          FileAccessResult::FromError(permission_result.error()));\r\n      return;\r\n    }\r\n    \r\n    // Contract fulfilled: Grant access with monitoring\r\n    auto access_token = GrantFileAccess(path, mode, std::move(observer));\r\n    std::move(callback).Run(FileAccessResult::FromToken(access_token));\r\n  }\r\n  \r\n private:\r\n  base::expected<void, SecurityError> ValidateFileCapability(\r\n      const base::FilePath& path, FileAccessMode mode) {\r\n    // Implement capability validation logic\r\n    if (!IsPathAllowed(path)) {\r\n      return base::unexpected(SecurityError::kPathNotAllowed);\r\n    }\r\n    if (!IsModeAllowed(mode)) {\r\n      return base::unexpected(SecurityError::kModeNotAllowed);\r\n    }\r\n    return base::ok();\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## Modern Contract Implementation in Practice (v134+)\r\n\r\nModern Chromium v134+ implements contract patterns throughout its architecture, from browser initialization to service lifecycle management and security enforcement.\r\n\r\n### 1. **Enhanced Browser Initialization Contracts**\r\n\r\nThe modern browser startup sequence implements sophisticated contract patterns:\r\n\r\n```cpp\r\n// Modern BrowserMainParts with contract validation\r\nclass ChromeBrowserMainParts : public content::BrowserMainParts {\r\n public:\r\n  // Contract: PreCreateMainMessageLoop establishes prerequisites\r\n  base::expected<void, StartupError> PreCreateMainMessageLoop() override {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    // Precondition: System must meet minimum requirements\r\n    auto system_check = ValidateSystemRequirements();\r\n    if (!system_check.has_value()) {\r\n      return base::unexpected(system_check.error());\r\n    }\r\n    \r\n    // Initialize with guaranteed postconditions\r\n    auto init_result = InitializePlatformSupport();\r\n    if (!init_result.has_value()) {\r\n      return base::unexpected(init_result.error());\r\n    }\r\n    \r\n    // Postcondition: Platform support is ready\r\n    DCHECK(IsPlatformSupportInitialized());\r\n    return base::ok();\r\n  }\r\n  \r\n  // Contract: PostCreateMainMessageLoop configures services\r\n  void PostCreateMainMessageLoop() override {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    DCHECK(IsPlatformSupportInitialized());  // Precondition from previous stage\r\n    \r\n    // Service initialization with dependency contracts\r\n    InitializeServiceManagerWithContracts();\r\n    \r\n    // Postcondition: All essential services are available\r\n    DCHECK(AreEssentialServicesReady());\r\n  }\r\n  \r\n private:\r\n  void InitializeServiceManagerWithContracts() {\r\n    // Each service has initialization contracts\r\n    auto audio_result = InitializeAudioService();\r\n    DCHECK(audio_result.has_value()) << \"Audio service failed: \" \r\n                                     << static_cast<int>(audio_result.error());\r\n    \r\n    auto network_result = InitializeNetworkService();\r\n    DCHECK(network_result.has_value()) << \"Network service failed: \"\r\n                                       << static_cast<int>(network_result.error());\r\n  }\r\n  \r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n};\r\n```\r\n\r\n### 2. **Service Dependency Contracts**\r\n\r\nModern services implement explicit dependency contracts:\r\n\r\n```cpp\r\n// Service with dependency contract validation\r\nclass MediaService : public mojom::MediaService {\r\n public:\r\n  // Contract: Initialize with required dependencies\r\n  static base::expected<std::unique_ptr<MediaService>, ServiceError>\r\n  Create(ServiceDependencies deps) {\r\n    // Validate all required dependencies\r\n    if (!deps.audio_manager || !deps.gpu_service || !deps.storage_partition) {\r\n      return base::unexpected(ServiceError::kMissingDependencies);\r\n    }\r\n    \r\n    auto service = base::WrapUnique(new MediaService(std::move(deps)));\r\n    \r\n    // Postcondition: Service is ready for media operations\r\n    DCHECK(service->IsReadyForMediaOperations());\r\n    return service;\r\n  }\r\n  \r\n  // Contract: CreateMediaSession requires valid context\r\n  void CreateMediaSession(\r\n      mojo::PendingReceiver<mojom::MediaSession> receiver,\r\n      const MediaSessionConfig& config,\r\n      CreateMediaSessionCallback callback) override {\r\n    \r\n    // Precondition validation\r\n    if (!receiver.is_valid() || !IsValidConfig(config)) {\r\n      std::move(callback).Run(MediaSessionResult::kInvalidParameters);\r\n      return;\r\n    }\r\n    \r\n    // Security contract: Validate permissions\r\n    if (!HasMediaPermission(config.origin)) {\r\n      std::move(callback).Run(MediaSessionResult::kPermissionDenied);\r\n      return;\r\n    }\r\n    \r\n    // Create session with postcondition guarantee\r\n    auto session = CreateSessionInternal(config);\r\n    BindSession(std::move(receiver), std::move(session));\r\n    std::move(callback).Run(MediaSessionResult::kSuccess);\r\n  }\r\n  \r\n private:\r\n  explicit MediaService(ServiceDependencies deps) : deps_(std::move(deps)) {}\r\n  \r\n  bool IsReadyForMediaOperations() const {\r\n    return deps_.audio_manager && deps_.gpu_service && deps_.storage_partition;\r\n  }\r\n  \r\n  ServiceDependencies deps_;\r\n};\r\n```\r\n\r\n### 3. **Performance Contract Monitoring**\r\n\r\nModern Chromium implements performance contracts with real-time monitoring:\r\n\r\n```cpp\r\n// Performance contract with Core Web Vitals validation\r\nclass RenderFrameMetricsCollector {\r\n public:\r\n  // Contract: ReportMetrics validates performance thresholds\r\n  void ReportCoreWebVitals(const CoreWebVitalsData& data) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    // Performance contract validation\r\n    auto validation_result = ValidatePerformanceContract(data);\r\n    if (!validation_result.has_value()) {\r\n      ReportPerformanceViolation(validation_result.error());\r\n      return;\r\n    }\r\n    \r\n    // Update metrics with contract compliance\r\n    UpdateMetricsWithValidation(data);\r\n    \r\n    // Postcondition: Metrics are consistent and within bounds\r\n    DCHECK(AreMetricsConsistent());\r\n  }\r\n  \r\n private:\r\n  base::expected<void, PerformanceError> ValidatePerformanceContract(\r\n      const CoreWebVitalsData& data) {\r\n    // Contract: LCP should be reasonable (< 10 seconds)\r\n    if (data.largest_contentful_paint > base::Seconds(10)) {\r\n      return base::unexpected(PerformanceError::kUnreasonableLCP);\r\n    }\r\n    \r\n    // Contract: FID should be valid (>= 0)\r\n    if (data.first_input_delay < base::TimeDelta()) {\r\n      return base::unexpected(PerformanceError::kInvalidFID);\r\n    }\r\n    \r\n    // Contract: CLS should be in valid range [0.0, 5.0]\r\n    if (data.cumulative_layout_shift < 0.0 || \r\n        data.cumulative_layout_shift > 5.0) {\r\n      return base::unexpected(PerformanceError::kInvalidCLS);\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n  \r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n};\r\n```\r\n\r\n---\r\n\r\n## Security Contract Patterns (v134+)\r\n\r\nModern Chromium implements sophisticated security contract patterns that enforce isolation, validate permissions, and prevent security vulnerabilities through compile-time and runtime guarantees.\r\n\r\n### 1. **Site Isolation Security Contracts**\r\n\r\nSite isolation relies on strict contracts to maintain security boundaries:\r\n\r\n```cpp\r\n// Site isolation contract enforcement\r\nclass SiteInstanceImpl : public SiteInstance {\r\n public:\r\n  // Contract: CreateRelatedSiteInstance maintains security invariants\r\n  static base::expected<scoped_refptr<SiteInstance>, SecurityError>\r\n  CreateRelatedSiteInstance(BrowserContext* context, \r\n                           const GURL& url,\r\n                           const SiteInstance* initiator) {\r\n    // Security preconditions\r\n    if (!context || !initiator) {\r\n      return base::unexpected(SecurityError::kInvalidContext);\r\n    }\r\n    \r\n    if (!url.is_valid() || url.is_empty()) {\r\n      return base::unexpected(SecurityError::kInvalidURL);\r\n    }\r\n    \r\n    // Site isolation contract: Validate cross-origin constraints\r\n    auto site_url = GetSiteForURL(url);\r\n    if (ShouldIsolateSite(context, site_url)) {\r\n      auto validation_result = ValidateSiteIsolationContract(\r\n          initiator->GetSiteURL(), site_url);\r\n      if (!validation_result.has_value()) {\r\n        return base::unexpected(validation_result.error());\r\n      }\r\n    }\r\n    \r\n    // Create with security guarantees\r\n    auto site_instance = base::WrapRefCounted(\r\n        new SiteInstanceImpl(context, site_url));\r\n    \r\n    // Postcondition: Site instance maintains isolation boundaries\r\n    DCHECK(site_instance->IsIsolatedFromOtherSites());\r\n    return site_instance;\r\n  }\r\n  \r\n private:\r\n  static base::expected<void, SecurityError> ValidateSiteIsolationContract(\r\n      const GURL& initiator_site, const GURL& target_site) {\r\n    // Contract: Cross-origin navigations require proper validation\r\n    if (url::Origin::Create(initiator_site) != url::Origin::Create(target_site)) {\r\n      if (!CanNavigateCrossOrigin(initiator_site, target_site)) {\r\n        return base::unexpected(SecurityError::kCrossOriginViolation);\r\n      }\r\n    }\r\n    return base::ok();\r\n  }\r\n};\r\n```\r\n\r\n### 2. **Permission System Contracts**\r\n\r\nModern permission contracts ensure secure capability delegation:\r\n\r\n```cpp\r\n// Permission contract with capability validation\r\nclass PermissionManagerImpl {\r\n public:\r\n  // Contract: RequestPermission validates security context and user intent\r\n  void RequestPermission(\r\n      blink::mojom::PermissionName permission,\r\n      RenderFrameHost* render_frame_host,\r\n      const GURL& requesting_origin,\r\n      bool user_gesture,\r\n      base::OnceCallback<void(blink::mojom::PermissionStatus)> callback) {\r\n    \r\n    // Security contract preconditions\r\n    auto validation_result = ValidatePermissionRequest(\r\n        permission, render_frame_host, requesting_origin, user_gesture);\r\n    if (!validation_result.has_value()) {\r\n      std::move(callback).Run(blink::mojom::PermissionStatus::DENIED);\r\n      return;\r\n    }\r\n    \r\n    // Privacy contract: Check Privacy Sandbox compliance\r\n    if (IsPrivacySandboxPermission(permission)) {\r\n      auto privacy_result = ValidatePrivacySandboxContract(\r\n          requesting_origin, permission);\r\n      if (!privacy_result.has_value()) {\r\n        std::move(callback).Run(blink::mojom::PermissionStatus::DENIED);\r\n        return;\r\n      }\r\n    }\r\n    \r\n    // Execute permission flow with guaranteed callback\r\n    ProcessPermissionRequestWithContract(\r\n        permission, render_frame_host, requesting_origin, std::move(callback));\r\n  }\r\n  \r\n private:\r\n  base::expected<void, SecurityError> ValidatePermissionRequest(\r\n      blink::mojom::PermissionName permission,\r\n      RenderFrameHost* render_frame_host,\r\n      const GURL& requesting_origin,\r\n      bool user_gesture) {\r\n    \r\n    // Contract: Frame must be valid and live\r\n    if (!render_frame_host || !render_frame_host->IsRenderFrameLive()) {\r\n      return base::unexpected(SecurityError::kInvalidFrame);\r\n    }\r\n    \r\n    // Contract: Powerful features require user gesture\r\n    if (IsPowerfulFeature(permission) && !user_gesture) {\r\n      return base::unexpected(SecurityError::kNoUserGesture);\r\n    }\r\n    \r\n    // Contract: Origin must match frame's committed origin\r\n    auto frame_origin = render_frame_host->GetLastCommittedOrigin();\r\n    if (frame_origin != url::Origin::Create(requesting_origin)) {\r\n      return base::unexpected(SecurityError::kOriginMismatch);\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n  \r\n  base::expected<void, PrivacyError> ValidatePrivacySandboxContract(\r\n      const GURL& origin, blink::mojom::PermissionName permission) {\r\n    // Privacy contract: Validate Topics API usage\r\n    if (permission == blink::mojom::PermissionName::TOPICS_API) {\r\n      if (!IsTopicsAPIAllowed(origin)) {\r\n        return base::unexpected(PrivacyError::kTopicsNotAllowed);\r\n      }\r\n    }\r\n    \r\n    // Privacy contract: Validate FLEDGE usage\r\n    if (permission == blink::mojom::PermissionName::FLEDGE_API) {\r\n      if (!IsFledgeAPIAllowed(origin)) {\r\n        return base::unexpected(PrivacyError::kFledgeNotAllowed);\r\n      }\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n};\r\n```\r\n\r\n### 3. **Mojo Security Contracts**\r\n\r\nCross-process security contracts ensure safe IPC communication:\r\n\r\n```cpp\r\n// Mojo security contract for sensitive operations\r\nclass CryptographyService : public mojom::CryptographyService {\r\n public:\r\n  // Contract: EncryptData validates security context and data integrity\r\n  void EncryptData(\r\n      const std::vector<uint8_t>& plaintext,\r\n      const std::string& key_id,\r\n      mojo::PendingRemote<mojom::CryptographyObserver> observer,\r\n      EncryptDataCallback callback) override {\r\n    \r\n    // Security contract: Validate caller privileges\r\n    auto security_result = ValidateCallerSecurityContract();\r\n    if (!security_result.has_value()) {\r\n      std::move(callback).Run(CryptographyResult::FromError(security_result.error()));\r\n      return;\r\n    }\r\n    \r\n    // Data contract: Validate input parameters\r\n    if (plaintext.empty() || key_id.empty()) {\r\n      std::move(callback).Run(CryptographyResult::FromError(\r\n          CryptographyError::kInvalidParameters));\r\n      return;\r\n    }\r\n    \r\n    // Key management contract: Validate key access\r\n    auto key_result = ValidateKeyAccessContract(key_id);\r\n    if (!key_result.has_value()) {\r\n      std::move(callback).Run(CryptographyResult::FromError(key_result.error()));\r\n      return;\r\n    }\r\n    \r\n    // Execute with security monitoring\r\n    PerformEncryptionWithMonitoring(\r\n        plaintext, key_id, std::move(observer), std::move(callback));\r\n  }\r\n  \r\n private:\r\n  base::expected<void, SecurityError> ValidateCallerSecurityContract() {\r\n    // Contract: Caller must have cryptography capability\r\n    auto* current_context = mojo::GetMessageContext();\r\n    if (!current_context || !current_context->HasCapability(\"cryptography\")) {\r\n      return base::unexpected(SecurityError::kInsufficientCapabilities);\r\n    }\r\n    \r\n    // Contract: Process must be properly sandboxed\r\n    if (!IsCallerProperlySandboxed()) {\r\n      return base::unexpected(SecurityError::kSandboxViolation);\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n  \r\n  base::expected<void, CryptographyError> ValidateKeyAccessContract(\r\n      const std::string& key_id) {\r\n    // Contract: Key must exist and be accessible\r\n    if (!key_store_->KeyExists(key_id)) {\r\n      return base::unexpected(CryptographyError::kKeyNotFound);\r\n    }\r\n    \r\n    // Contract: Caller must have permission for this key\r\n    if (!key_store_->CanAccessKey(key_id, GetCallerIdentity())) {\r\n      return base::unexpected(CryptographyError::kKeyAccessDenied);\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## Performance Contract Patterns (v134+)\r\n\r\nModern Chromium implements performance contracts that ensure optimal user experience through measurable guarantees and real-time monitoring.\r\n\r\n### 1. **Core Web Vitals Performance Contracts**\r\n\r\nPerformance contracts ensure adherence to Core Web Vitals standards:\r\n\r\n```cpp\r\n// Core Web Vitals contract enforcement\r\nclass PagePerformanceManager {\r\n public:\r\n  // Contract: TrackPageLoad ensures performance metrics collection\r\n  void TrackPageLoad(content::WebContents* web_contents,\r\n                    const GURL& url,\r\n                    base::OnceCallback<void(PerformanceReport)> callback) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    // Performance contract preconditions\r\n    if (!web_contents || !url.is_valid()) {\r\n      std::move(callback).Run(PerformanceReport::CreateError(\r\n          PerformanceError::kInvalidParameters));\r\n      return;\r\n    }\r\n    \r\n    // Contract: Start performance monitoring with guaranteed reporting\r\n    auto tracker = CreatePerformanceTracker(web_contents, url);\r\n    tracker->SetPerformanceContract(CreateCoreWebVitalsContract());\r\n    \r\n    // Contract: Callback will be called with complete metrics\r\n    auto wrapped_callback = base::BindOnce(\r\n        &PagePerformanceManager::ValidateAndReportPerformance,\r\n        weak_factory_.GetWeakPtr(), std::move(callback));\r\n    \r\n    tracker->StartTracking(std::move(wrapped_callback));\r\n  }\r\n  \r\n private:\r\n  PerformanceContract CreateCoreWebVitalsContract() {\r\n    PerformanceContract contract;\r\n    \r\n    // LCP contract: Largest Contentful Paint < 2.5s for good rating\r\n    contract.AddMetricContract(\r\n        MetricType::kLargestContentfulPaint,\r\n        MetricThreshold{\r\n          .good_threshold = base::Milliseconds(2500),\r\n          .needs_improvement_threshold = base::Milliseconds(4000),\r\n          .violation_action = ViolationAction::kReportAndOptimize\r\n        });\r\n    \r\n    // FID contract: First Input Delay < 100ms for good rating\r\n    contract.AddMetricContract(\r\n        MetricType::kFirstInputDelay,\r\n        MetricThreshold{\r\n          .good_threshold = base::Milliseconds(100),\r\n          .needs_improvement_threshold = base::Milliseconds(300),\r\n          .violation_action = ViolationAction::kReportAndOptimize\r\n        });\r\n    \r\n    // CLS contract: Cumulative Layout Shift < 0.1 for good rating\r\n    contract.AddMetricContract(\r\n        MetricType::kCumulativeLayoutShift,\r\n        MetricThreshold{\r\n          .good_threshold = 0.1,\r\n          .needs_improvement_threshold = 0.25,\r\n          .violation_action = ViolationAction::kReportAndOptimize\r\n        });\r\n    \r\n    return contract;\r\n  }\r\n  \r\n  void ValidateAndReportPerformance(\r\n      base::OnceCallback<void(PerformanceReport)> callback,\r\n      const PerformanceMetrics& metrics) {\r\n    \r\n    // Contract validation: All required metrics must be present\r\n    auto validation_result = ValidateMetricsCompleteness(metrics);\r\n    if (!validation_result.has_value()) {\r\n      std::move(callback).Run(PerformanceReport::CreateError(\r\n          validation_result.error()));\r\n      return;\r\n    }\r\n    \r\n    // Performance contract evaluation\r\n    auto report = EvaluatePerformanceContracts(metrics);\r\n    \r\n    // Contract: Report violations for optimization\r\n    if (report.HasViolations()) {\r\n      TriggerPerformanceOptimization(report.GetViolations());\r\n    }\r\n    \r\n    std::move(callback).Run(std::move(report));\r\n  }\r\n  \r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n  base::WeakPtrFactory<PagePerformanceManager> weak_factory_{this};\r\n};\r\n```\r\n\r\n### 2. **Memory Usage Contracts**\r\n\r\nMemory management contracts prevent resource exhaustion:\r\n\r\n```cpp\r\n// Memory usage contract with automatic enforcement\r\nclass MemoryManager {\r\n public:\r\n  // Contract: AllocateMemory enforces memory limits and tracking\r\n  base::expected<std::unique_ptr<MemoryBlock>, MemoryError>\r\n  AllocateMemory(size_t size, MemoryPriority priority) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    // Memory contract preconditions\r\n    if (size == 0 || size > kMaxAllocationSize) {\r\n      return base::unexpected(MemoryError::kInvalidSize);\r\n    }\r\n    \r\n    // Contract: Check memory availability before allocation\r\n    auto availability_check = CheckMemoryAvailability(size, priority);\r\n    if (!availability_check.has_value()) {\r\n      return base::unexpected(availability_check.error());\r\n    }\r\n    \r\n    // Contract: Track allocation for lifecycle management\r\n    auto memory_block = AllocateWithTracking(size, priority);\r\n    if (!memory_block) {\r\n      return base::unexpected(MemoryError::kAllocationFailed);\r\n    }\r\n    \r\n    // Postcondition: Memory is tracked and within limits\r\n    DCHECK(IsMemoryWithinLimits());\r\n    return memory_block;\r\n  }\r\n  \r\n  // Contract: Memory pressure triggers managed cleanup\r\n  void HandleMemoryPressure(MemoryPressureLevel level) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    switch (level) {\r\n      case MemoryPressureLevel::kModerate:\r\n        // Contract: Free low-priority allocations\r\n        FreeLowPriorityMemory();\r\n        break;\r\n        \r\n      case MemoryPressureLevel::kCritical:\r\n        // Contract: Aggressive cleanup to prevent OOM\r\n        FreeNonEssentialMemory();\r\n        TriggerGarbageCollection();\r\n        break;\r\n    }\r\n    \r\n    // Postcondition: Memory usage reduced appropriately\r\n    DCHECK(IsMemoryUsageAppropriate(level));\r\n  }\r\n  \r\n private:\r\n  base::expected<void, MemoryError> CheckMemoryAvailability(\r\n      size_t size, MemoryPriority priority) {\r\n    \r\n    // Contract: High-priority allocations have reserved capacity\r\n    size_t available_memory = GetAvailableMemory();\r\n    if (priority == MemoryPriority::kHigh) {\r\n      if (size > available_memory + GetReservedMemory()) {\r\n        return base::unexpected(MemoryError::kInsufficientMemory);\r\n      }\r\n    } else {\r\n      if (size > available_memory) {\r\n        return base::unexpected(MemoryError::kInsufficientMemory);\r\n      }\r\n    }\r\n    \r\n    // Contract: Total memory usage stays within system limits\r\n    if (GetTotalMemoryUsage() + size > GetMemoryLimit()) {\r\n      return base::unexpected(MemoryError::kMemoryLimitExceeded);\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n  \r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n};\r\n```\r\n\r\n### 3. **Thread Safety Contracts**\r\n\r\nThread safety contracts ensure correct concurrent behavior:\r\n\r\n```cpp\r\n// Thread safety contract with sequence validation\r\nclass ThreadSafeCache {\r\n public:\r\n  // Contract: Get operation is thread-safe and returns valid data\r\n  base::expected<CacheEntry, CacheError> Get(const std::string& key) {\r\n    base::AutoLock lock(cache_lock_);\r\n    \r\n    // Thread safety contract: Validate key before access\r\n    if (key.empty()) {\r\n      return base::unexpected(CacheError::kInvalidKey);\r\n    }\r\n    \r\n    // Contract: Return valid entry or explicit miss\r\n    auto it = cache_entries_.find(key);\r\n    if (it == cache_entries_.end()) {\r\n      return base::unexpected(CacheError::kEntryNotFound);\r\n    }\r\n    \r\n    // Contract: Validate entry freshness\r\n    if (IsEntryExpired(it->second)) {\r\n      cache_entries_.erase(it);\r\n      return base::unexpected(CacheError::kEntryExpired);\r\n    }\r\n    \r\n    // Postcondition: Return valid, fresh cache entry\r\n    return it->second;\r\n  }\r\n  \r\n  // Contract: Set operation maintains cache invariants\r\n  void Set(const std::string& key, CacheEntry entry) {\r\n    base::AutoLock lock(cache_lock_);\r\n    \r\n    // Contract: Enforce cache size limits\r\n    if (cache_entries_.size() >= kMaxCacheSize) {\r\n      EvictLeastRecentlyUsed();\r\n    }\r\n    \r\n    // Contract: Entry is valid and properly timestamped\r\n    entry.access_time = base::TimeTicks::Now();\r\n    cache_entries_[key] = std::move(entry);\r\n    \r\n    // Postcondition: Cache remains within size limits\r\n    DCHECK_LE(cache_entries_.size(), kMaxCacheSize);\r\n  }\r\n  \r\n private:\r\n  mutable base::Lock cache_lock_;\r\n  std::unordered_map<std::string, CacheEntry> cache_entries_ GUARDED_BY(cache_lock_);\r\n  static constexpr size_t kMaxCacheSize = 1000;\r\n};\r\n```\r\n\r\n---\r\n\r\n## Testing Contract Patterns (v134+)\r\n\r\nModern Chromium employs sophisticated testing strategies that validate contract adherence, ensuring that preconditions, postconditions, and error handling work correctly across all scenarios.\r\n\r\n### 1. **Google Mock Contract Validation**\r\n\r\nContract testing with Google Mock ensures proper validation and error handling:\r\n\r\n```cpp\r\n// Mock class for testing contract validation\r\nclass MockNetworkService : public NetworkService {\r\n public:\r\n  MOCK_METHOD(base::expected<std::unique_ptr<URLLoader>, NetworkError>,\r\n              CreateURLLoader,\r\n              (const GURL& url, const SecurityContext& context),\r\n              (override));\r\n  \r\n  MOCK_METHOD(void, \r\n              ValidateSecurityContext,\r\n              (const SecurityContext& context),\r\n              (const));\r\n};\r\n\r\n// Test fixture for contract validation\r\nclass NetworkServiceContractTest : public testing::Test {\r\n protected:\r\n  void SetUp() override {\r\n    mock_network_service_ = std::make_unique<MockNetworkService>();\r\n    security_context_ = CreateValidSecurityContext();\r\n  }\r\n  \r\n  std::unique_ptr<MockNetworkService> mock_network_service_;\r\n  SecurityContext security_context_;\r\n};\r\n\r\n// Test contract precondition violations\r\nTEST_F(NetworkServiceContractTest, CreateURLLoaderRejectsInvalidURL) {\r\n  const GURL invalid_url(\"\");  // Empty URL violates precondition\r\n  \r\n  // Expect contract violation to be handled gracefully\r\n  EXPECT_CALL(*mock_network_service_, CreateURLLoader(invalid_url, security_context_))\r\n      .WillOnce(testing::Return(base::unexpected(NetworkError::kInvalidURL)));\r\n  \r\n  auto result = mock_network_service_->CreateURLLoader(invalid_url, security_context_);\r\n  \r\n  // Verify contract enforcement\r\n  ASSERT_FALSE(result.has_value());\r\n  EXPECT_EQ(result.error(), NetworkError::kInvalidURL);\r\n}\r\n\r\n// Test contract postcondition guarantees\r\nTEST_F(NetworkServiceContractTest, CreateURLLoaderGuaranteesValidResult) {\r\n  const GURL valid_url(\"https://example.com\");\r\n  auto expected_loader = std::make_unique<MockURLLoader>();\r\n  auto* expected_loader_ptr = expected_loader.get();\r\n  \r\n  // Mock successful creation with postcondition guarantee\r\n  EXPECT_CALL(*mock_network_service_, CreateURLLoader(valid_url, security_context_))\r\n      .WillOnce(testing::Return(testing::ByMove(std::move(expected_loader))));\r\n  \r\n  auto result = mock_network_service_->CreateURLLoader(valid_url, security_context_);\r\n  \r\n  // Verify postcondition: successful result contains valid loader\r\n  ASSERT_TRUE(result.has_value());\r\n  EXPECT_EQ(result.value().get(), expected_loader_ptr);\r\n  EXPECT_NE(result.value().get(), nullptr);  // Postcondition: never null on success\r\n}\r\n\r\n// Test error contract propagation\r\nTEST_F(NetworkServiceContractTest, ErrorContractsPropagateCorrectly) {\r\n  const GURL valid_url(\"https://example.com\");\r\n  SecurityContext invalid_context;  // Unauthenticated context\r\n  \r\n  // Test each possible error condition\r\n  std::vector<std::pair<SecurityContext, NetworkError>> error_cases = {\r\n    {invalid_context, NetworkError::kUnauthenticated},\r\n    {CreateExpiredSecurityContext(), NetworkError::kExpiredCredentials},\r\n    {CreateRevokedSecurityContext(), NetworkError::kAccessRevoked}\r\n  };\r\n  \r\n  for (const auto& [context, expected_error] : error_cases) {\r\n    EXPECT_CALL(*mock_network_service_, CreateURLLoader(valid_url, context))\r\n        .WillOnce(testing::Return(base::unexpected(expected_error)));\r\n    \r\n    auto result = mock_network_service_->CreateURLLoader(valid_url, context);\r\n    \r\n    ASSERT_FALSE(result.has_value()) << \"Expected error for context type\";\r\n    EXPECT_EQ(result.error(), expected_error) << \"Error contract mismatch\";\r\n  }\r\n}\r\n```\r\n\r\n### 2. **Contract Violation Testing**\r\n\r\nSystematic testing of contract violations ensures robust error handling:\r\n\r\n```cpp\r\n// Test fixture for contract violation scenarios\r\nclass ContractViolationTest : public testing::Test {\r\n protected:\r\n  void SetUp() override {\r\n    download_service_ = std::make_unique<DownloadServiceImpl>(\r\n        CreateTestProfile());\r\n  }\r\n  \r\n  // Helper method to create invalid test scenarios\r\n  template<typename ContractViolation>\r\n  void TestContractViolation(ContractViolation violation,\r\n                            const std::string& expected_error_message) {\r\n    // Enable contract violation reporting for testing\r\n    base::test::ScopedFeatureList feature_list;\r\n    feature_list.InitAndEnableFeature(features::kStrictContractValidation);\r\n    \r\n    // Set up violation detection\r\n    bool violation_detected = false;\r\n    std::string violation_message;\r\n    \r\n    SetContractViolationHandler([&](const std::string& message) {\r\n      violation_detected = true;\r\n      violation_message = message;\r\n    });\r\n    \r\n    // Execute the violation scenario\r\n    violation();\r\n    \r\n    // Verify contract violation was detected\r\n    EXPECT_TRUE(violation_detected) << \"Contract violation not detected\";\r\n    EXPECT_THAT(violation_message, testing::HasSubstr(expected_error_message));\r\n  }\r\n  \r\n  std::unique_ptr<DownloadServiceImpl> download_service_;\r\n};\r\n\r\nTEST_F(ContractViolationTest, DetectsInvalidStateTransition) {\r\n  TestContractViolation(\r\n    [this]() {\r\n      // Violate contract: Call StartDownload on uninitialized service\r\n      download_service_->StartDownload(\r\n          GURL(\"https://example.com/file.pdf\"), \r\n          mojo::NullReceiver(), \r\n          base::DoNothing());\r\n    },\r\n    \"Service not initialized\"\r\n  );\r\n}\r\n\r\nTEST_F(ContractViolationTest, DetectsResourceLeaks) {\r\n  TestContractViolation(\r\n    [this]() {\r\n      // Violate contract: Create resource without proper cleanup\r\n      auto resource = download_service_->CreateTempResource();\r\n      // Intentionally don't clean up to trigger contract violation\r\n    },\r\n    \"Resource leak detected\"\r\n  );\r\n}\r\n```\r\n\r\n### 3. **Integration Contract Testing**\r\n\r\nEnd-to-end testing of contract interactions across components:\r\n\r\n```cpp\r\n// Integration test for cross-service contract validation\r\nclass ServiceContractIntegrationTest : public testing::Test {\r\n protected:\r\n  void SetUp() override {\r\n    // Set up service chain with contract validation enabled\r\n    network_service_ = CreateNetworkServiceWithContracts();\r\n    download_service_ = CreateDownloadServiceWithContracts(network_service_.get());\r\n    storage_service_ = CreateStorageServiceWithContracts();\r\n    \r\n    // Enable contract monitoring across services\r\n    EnableCrossServiceContractMonitoring();\r\n  }\r\n  \r\n  void TearDown() override {\r\n    // Verify all contracts were maintained during test\r\n    VerifyNoContractViolations();\r\n  }\r\n  \r\n  std::unique_ptr<NetworkService> network_service_;\r\n  std::unique_ptr<DownloadService> download_service_;\r\n  std::unique_ptr<StorageService> storage_service_;\r\n};\r\n\r\nTEST_F(ServiceContractIntegrationTest, DownloadFlowMaintainsAllContracts) {\r\n  const GURL download_url(\"https://example.com/large-file.zip\");\r\n  \r\n  // Contract: Download request should succeed with valid parameters\r\n  base::RunLoop download_started_loop;\r\n  download_service_->StartDownload(\r\n      download_url,\r\n      CreateMockDownloadReceiver(),\r\n      base::BindLambdaForTesting([&](DownloadResult result) {\r\n        EXPECT_EQ(result, DownloadResult::kSuccess);\r\n        download_started_loop.Quit();\r\n      }));\r\n  \r\n  download_started_loop.Run();\r\n  \r\n  // Contract: Network service should receive valid requests\r\n  EXPECT_TRUE(network_service_->ReceivedValidRequest(download_url));\r\n  \r\n  // Contract: Storage service should be prepared for data\r\n  EXPECT_TRUE(storage_service_->HasAvailableSpace());\r\n  \r\n  // Simulate download progress with contract validation\r\n  SimulateDownloadProgressWithContracts();\r\n  \r\n  // Contract: All services should maintain their invariants\r\n  EXPECT_TRUE(network_service_->MaintainsInvariants());\r\n  EXPECT_TRUE(download_service_->MaintainsInvariants());\r\n  EXPECT_TRUE(storage_service_->MaintainsInvariants());\r\n}\r\n```\r\n\r\n---\r\n\r\n## Real-World v134+ Examples\r\n\r\nModern Chromium's contract programming patterns are implemented throughout the codebase, providing concrete examples of sophisticated contract enforcement.\r\n\r\n### 1. **RenderFrameHost Contract Implementation**\r\n\r\nReal-world security contract in frame management:\r\n\r\n```cpp\r\n// Simplified from content/browser/renderer_host/render_frame_host_impl.cc\r\nclass RenderFrameHostImpl : public RenderFrameHost {\r\n public:\r\n  // Contract: CreateChildFrame maintains site isolation invariants\r\n  RenderFrameHost* CreateChildFrame(\r\n      int new_routing_id,\r\n      mojo::PendingAssociatedRemote<mojom::Frame> frame_remote,\r\n      blink::mojom::PolicyContainerBindParamsPtr policy_container_bind_params,\r\n      blink::mojom::TreeScopeType scope,\r\n      const std::string& frame_name,\r\n      const std::string& frame_unique_name,\r\n      bool is_created_by_script,\r\n      const blink::LocalFrameToken& frame_token,\r\n      const base::UnguessableToken& devtools_frame_token,\r\n      const blink::FramePolicy& frame_policy) {\r\n    \r\n    // Security contract: Validate frame creation parameters\r\n    if (!ValidateFrameCreationContract(new_routing_id, frame_token, frame_policy)) {\r\n      RecordContractViolation(\"CreateChildFrame: Invalid parameters\");\r\n      return nullptr;\r\n    }\r\n    \r\n    // Site isolation contract: Ensure proper origin separation\r\n    auto site_instance_result = GetSiteInstanceForChildFrame(frame_policy);\r\n    if (!site_instance_result.has_value()) {\r\n      RecordContractViolation(\"CreateChildFrame: Site isolation violation\");\r\n      return nullptr;\r\n    }\r\n    \r\n    // Create child with contract guarantees\r\n    auto child_frame = CreateChildFrameInternal(\r\n        new_routing_id, std::move(frame_remote), scope, frame_name,\r\n        frame_unique_name, is_created_by_script, frame_token,\r\n        devtools_frame_token, frame_policy, site_instance_result.value());\r\n    \r\n    // Postcondition: Child frame inherits security properties\r\n    DCHECK(child_frame->GetSiteInstance()->IsRelatedSiteInstance(GetSiteInstance()));\r\n    \r\n    return child_frame;\r\n  }\r\n  \r\n private:\r\n  bool ValidateFrameCreationContract(int routing_id,\r\n                                   const blink::LocalFrameToken& frame_token,\r\n                                   const blink::FramePolicy& frame_policy) {\r\n    // Contract: Routing ID must be valid and unique\r\n    if (routing_id == MSG_ROUTING_NONE || \r\n        RenderFrameHostImpl::FromID(GetProcess()->GetID(), routing_id)) {\r\n      return false;\r\n    }\r\n    \r\n    // Contract: Frame token must be valid and unique\r\n    if (frame_token.is_empty() || \r\n        RenderFrameHostImpl::FromFrameToken(GetProcess()->GetID(), frame_token)) {\r\n      return false;\r\n    }\r\n    \r\n    // Contract: Frame policy must be valid for current context\r\n    return ValidateFramePolicy(frame_policy);\r\n  }\r\n};\r\n```\r\n\r\n### 2. **URLLoaderFactory Security Contracts**\r\n\r\nReal-world network security contract implementation:\r\n\r\n```cpp\r\n// Simplified from services/network/url_loader_factory.cc\r\nclass URLLoaderFactoryImpl : public mojom::URLLoaderFactory {\r\n public:\r\n  void CreateLoaderAndStart(\r\n      mojo::PendingReceiver<mojom::URLLoader> receiver,\r\n      int32_t request_id,\r\n      uint32_t options,\r\n      const ResourceRequest& url_request,\r\n      mojo::PendingRemote<mojom::URLLoaderClient> client,\r\n      const net::MutableNetworkTrafficAnnotationTag& traffic_annotation) override {\r\n    \r\n    // Security contract: Validate request against factory parameters\r\n    auto validation_result = ValidateRequestContract(url_request, options);\r\n    if (!validation_result.has_value()) {\r\n      RecordSecurityViolation(\"URLLoaderFactory\", validation_result.error());\r\n      client->OnComplete(network::URLLoaderCompletionStatus(\r\n          validation_result.error()));\r\n      return;\r\n    }\r\n    \r\n    // CORS contract: Validate cross-origin requests\r\n    if (IsCrossOriginRequest(url_request)) {\r\n      auto cors_result = ValidateCORSContract(url_request);\r\n      if (!cors_result.has_value()) {\r\n        client->OnComplete(network::URLLoaderCompletionStatus(\r\n            net::ERR_BLOCKED_BY_CLIENT));\r\n        return;\r\n      }\r\n    }\r\n    \r\n    // Create loader with security guarantees\r\n    auto loader = std::make_unique<URLLoaderImpl>(\r\n        url_request, std::move(client), traffic_annotation, factory_params_);\r\n    \r\n    auto* raw_loader = loader.get();\r\n    mojo::MakeSelfOwnedReceiver(std::move(loader), std::move(receiver));\r\n    \r\n    // Postcondition: Loader is properly configured and secured\r\n    DCHECK(raw_loader->IsProperlyConfigured());\r\n  }\r\n  \r\n private:\r\n  base::expected<void, net::Error> ValidateRequestContract(\r\n      const ResourceRequest& request, uint32_t options) {\r\n    \r\n    // Contract: URL must be valid and allowed\r\n    if (!request.url.is_valid()) {\r\n      return base::unexpected(net::ERR_INVALID_URL);\r\n    }\r\n    \r\n    // Contract: Respect factory's origin restrictions\r\n    if (factory_params_->request_initiator_origin_lock.has_value()) {\r\n      auto expected_origin = factory_params_->request_initiator_origin_lock.value();\r\n      if (request.request_initiator != expected_origin) {\r\n        return base::unexpected(net::ERR_BLOCKED_BY_CLIENT);\r\n      }\r\n    }\r\n    \r\n    // Contract: Validate resource type permissions\r\n    if (!CanRequestResourceType(request.resource_type)) {\r\n      return base::unexpected(net::ERR_BLOCKED_BY_CLIENT);\r\n    }\r\n    \r\n    return base::ok();\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## Modern Contract Programming Best Practices (v134+)\r\n\r\nBased on Chromium's extensive use of contract programming, here are the essential best practices for implementing robust contracts:\r\n\r\n### 1. **Contract Design Principles**\r\n\r\n- **Explicit Error Contracts**: Use `base::expected<T, Error>` to make error conditions explicit and testable\r\n- **Fail-Fast Validation**: Validate preconditions early and fail fast with clear error messages\r\n- **Invariant Maintenance**: Design classes that maintain their invariants automatically through RAII\r\n- **Security by Default**: Implement security contracts that deny by default and require explicit permission\r\n- **Performance Awareness**: Design contracts that can be optimized away in release builds when appropriate\r\n\r\n### 2. **Implementation Guidelines**\r\n\r\n- **Use Modern C++ Features**: Leverage concepts, `base::expected`, and RAII for robust contract implementation\r\n- **Sequence Checking**: Use `SEQUENCE_CHECKER` to enforce single-threaded access contracts\r\n- **Memory Safety**: Implement contracts that prevent use-after-free and memory leaks\r\n- **Cross-Process Contracts**: Use Mojo interfaces to enforce contracts across process boundaries\r\n- **Testing Coverage**: Write comprehensive tests that validate contract adherence and violation handling\r\n\r\n### 3. **Error Handling Best Practices**\r\n\r\n- **Structured Error Types**: Define specific error types that provide actionable information\r\n- **Error Propagation**: Use `base::expected` to propagate errors up the call stack cleanly\r\n- **Graceful Degradation**: Design contracts that allow graceful degradation when possible\r\n- **Security Response**: Implement contracts that respond appropriately to security violations\r\n- **Performance Monitoring**: Monitor contract violations for performance optimization opportunities\r\n\r\n---\r\n\r\n## Why Contract Programming is Essential in Modern Chromium (v134+)\r\n\r\nModern contract programming in Chromium v134+ goes far beyond traditional assertions, providing a comprehensive framework for building reliable, secure, and performant software:\r\n\r\n### **Foundation for Security**\r\n- **Zero-Trust Architecture**: Contracts enforce security boundaries at every interface\r\n- **Site Isolation Guarantees**: Formal contracts prevent cross-origin security violations\r\n- **Permission Validation**: Explicit contracts govern capability delegation and resource access\r\n- **Privacy Protection**: Contracts ensure Privacy Sandbox APIs operate within defined boundaries\r\n\r\n### **Performance and Reliability**\r\n- **Early Error Detection**: Contract violations are caught at development time, not in production\r\n- **Optimized Code Paths**: Compiler optimizations based on contract assumptions improve performance\r\n- **Predictable Behavior**: Postconditions guarantee consistent system state across all scenarios\r\n- **Resource Management**: RAII-based contracts prevent memory leaks and resource exhaustion\r\n\r\n### **Developer Productivity**\r\n- **Self-Documenting APIs**: Contracts make interface expectations explicit and verifiable\r\n- **Refactoring Safety**: Contract violations are detected automatically during code changes\r\n- **Testing Efficiency**: Contracts serve as executable specifications, reducing testing overhead\r\n- **Cross-Team Collaboration**: Clear contracts enable safe integration across team boundaries\r\n\r\n### **Modern C++ Advantages**\r\n- **Type Safety**: `base::expected<T, Error>` provides compile-time error handling validation\r\n- **Async Safety**: Contracts ensure proper callback execution and resource cleanup in async operations\r\n- **Thread Safety**: Sequence checkers and lock annotations prevent concurrency bugs\r\n- **Service Integration**: Mojo contracts enable safe cross-process communication with capability validation\r\n\r\n---\r\n\r\n## Conclusion: Contract Programming as Chromium's Architectural Foundation\r\n\r\nContract programming has evolved from a simple pre/post pattern into a sophisticated architectural foundation that underpins modern Chromium v134+. Through the integration of cutting-edge C++20/23 features, advanced error handling with `base::expected`, and comprehensive security validation, contract programming enables Chromium to maintain its position as the world's most secure and performant browser engine.\r\n\r\n### **Key Achievements**\r\n\r\n**Security Excellence**: Contract programming provides the foundation for Chromium's advanced security model, including site isolation, Privacy Sandbox compliance, and capability-based resource access. Every security boundary is enforced through explicit contracts that are validated at compile-time and runtime.\r\n\r\n**Performance Leadership**: By establishing clear performance contracts with Core Web Vitals integration, memory usage limits, and thread safety guarantees, Chromium maintains its performance leadership while handling billions of web requests daily.\r\n\r\n**Developer Experience**: Modern contract patterns with `base::expected`, concepts, and RAII enable developers to write safer, more maintainable code while providing clear interfaces that are self-documenting and automatically testable.\r\n\r\n### **The Future of Contract Programming in Chromium**\r\n\r\nAs Chromium continues to evolve, contract programming will become even more central to its architecture:\r\n\r\n- **Enhanced Compile-Time Validation**: Future C++ standards will enable even more sophisticated compile-time contract validation\r\n- **Cross-Process Security**: Expanding Mojo contract patterns will provide stronger guarantees for service isolation\r\n- **Performance Optimization**: Advanced contract-based optimizations will further improve Core Web Vitals and user experience\r\n- **Privacy Innovation**: Contract programming will enable new privacy-preserving technologies while maintaining security guarantees\r\n\r\n### **Best Practices for Implementation**\r\n\r\n1. **Design Contracts First**: Define preconditions, postconditions, and error contracts before implementation\r\n2. **Use Modern C++ Features**: Leverage `base::expected`, concepts, and RAII for robust contract enforcement\r\n3. **Test Contract Violations**: Comprehensive testing of contract violations ensures robust error handling\r\n4. **Monitor in Production**: Real-time contract monitoring enables proactive performance and security optimization\r\n5. **Document Contract Guarantees**: Clear documentation of contract expectations enables safe code evolution\r\n\r\n**Contract programming in modern Chromium demonstrates how formal software engineering principles can be applied at massive scale to create software that is simultaneously secure, performant, and maintainable. By defining clear expectations, validating assumptions, and handling errors gracefully, contract programming enables Chromium to continue pushing the boundaries of what's possible in browser technology while maintaining the reliability and security that billions of users depend on daily.**\r\n\r\n---\r\n\r\n**Related Documentation:**\r\n- [Modern C++ Patterns in Chromium](../cpp-patterns.md)\r\n- [Mojo IPC Architecture](../../ipc-internals.md)\r\n- [Security Model and Site Isolation](../../../security/security-model.md)\r\n- [Performance Optimization Patterns](../../performance-patterns.md)"
  },
  {
    "path": "architecture/design-patterns/overview",
    "title": "Design Patterns in Chromium Architecture",
    "content": "# Design Patterns in Chromium Architecture\r\n\r\nWelcome to the Design Patterns section! This area documents the key design patterns and architectural patterns used throughout the Wanderlust custom Chromium browser implementation.\r\n\r\n## What You'll Find Here\r\n\r\nThis section covers the fundamental design patterns that shape our Chromium architecture:\r\n\r\n- **[Delegate Pattern](delegate-pattern.md)**: Delegation and callback mechanisms for flexible component interactions\r\n- **[Factory Pattern](factory-pattern.md)**: Object creation patterns for managing complex component instantiation\r\n- **[Observer Pattern](observer-pattern.md)**: Event notification and subscription systems\r\n- **[Pre/Post Contract Pattern](pre-post-contract.md)**: Contracts for method preconditions and postconditions\r\n- **[State Pattern](state-pattern.md)**: State management and state machine implementations\r\n\r\n## Why Design Patterns Matter\r\n\r\nIn a complex codebase like Chromium, design patterns provide:\r\n\r\n### Code Organization\r\n- **Consistent Structure**: Predictable code organization across modules\r\n- **Separation of Concerns**: Clear boundaries between different responsibilities\r\n- **Modularity**: Loosely coupled components that can evolve independently\r\n\r\n### Maintainability\r\n- **Common Vocabulary**: Shared understanding of architectural concepts\r\n- **Proven Solutions**: Time-tested approaches to common problems\r\n- **Refactoring Safety**: Patterns that support safe code evolution\r\n\r\n### Team Collaboration\r\n- **Design Communication**: Clear ways to express architectural intentions\r\n- **Code Reviews**: Common patterns make code easier to review and understand\r\n- **Knowledge Transfer**: Patterns help new team members understand the codebase\r\n\r\n## Pattern Categories\r\n\r\n### Behavioral Patterns\r\n- **Observer Pattern**: Managing event notifications and subscriptions\r\n- **State Pattern**: Handling complex state transitions and behaviors\r\n- **Delegate Pattern**: Flexible callback and delegation mechanisms\r\n\r\n### Creational Patterns\r\n- **Factory Pattern**: Controlled object creation and initialization\r\n- **Pre/Post Contracts**: Ensuring proper object construction and usage\r\n\r\n### Architectural Patterns\r\n- **Component Separation**: How different browser components interact\r\n- **Process Boundaries**: Patterns for inter-process communication\r\n- **Security Boundaries**: Patterns that maintain security isolation\r\n\r\n## Implementation Context\r\n\r\nThese patterns are implemented within the broader Chromium architecture:\r\n\r\n### Process Model Integration\r\n- Patterns work across process boundaries in multi-process architecture\r\n- Security considerations for pattern implementations\r\n- Performance implications of pattern choices\r\n\r\n### Module Integration\r\n- How patterns facilitate communication between browser modules\r\n- Pattern usage in networking, rendering, and JavaScript execution\r\n- Cross-module pattern consistency\r\n\r\n## Pattern Usage Guidelines\r\n\r\nWhen implementing or modifying code that uses these patterns:\r\n\r\n1. **Understand the Intent**: Know why the pattern was chosen for specific use cases\r\n2. **Follow Conventions**: Maintain consistency with existing pattern implementations\r\n3. **Consider Performance**: Understand the performance implications of pattern choices\r\n4. **Respect Boundaries**: Ensure patterns don't violate security or process boundaries\r\n\r\n## Learning Path\r\n\r\nFor developers new to these patterns:\r\n\r\n1. **Start with Observer**: Most commonly encountered in browser event systems\r\n2. **Study Delegate**: Critical for understanding callback mechanisms\r\n3. **Explore Factory**: Important for component creation and initialization\r\n4. **Advanced Patterns**: Pre/Post contracts and State patterns for complex scenarios\r\n\r\n## Integration with Architecture\r\n\r\nThese design patterns integrate with:\r\n- [Architecture Overview](../overview.md): How patterns fit into overall system design\r\n- [Process Model](../process-model.md): Patterns in multi-process architecture\r\n- [IPC Internals](../ipc-internals.md): Communication patterns between processes\r\n- [Security Architecture](../security/overview.md): Security-aware pattern implementations\r\n\r\n## Practical Applications\r\n\r\nEach pattern documentation includes:\r\n- **Real-world Examples**: Actual usage in the Chromium codebase\r\n- **Implementation Details**: Code examples and best practices\r\n- **Common Pitfalls**: What to avoid when using each pattern\r\n- **Performance Considerations**: Impact on browser performance\r\n\r\n---\r\n\r\n*Begin with the [Observer Pattern](observer-pattern.md) to understand the most fundamental pattern in browser event handling, or explore the [Delegate Pattern](delegate-pattern.md) for callback mechanisms.*\r\n"
  },
  {
    "path": "architecture/design-patterns/observer-pattern",
    "title": "Observer Pattern in Modern Chromium (v134+)",
    "content": "# Observer Pattern in Modern Chromium (v134+)\r\n\r\nThe Observer pattern is fundamental to Chromium's event-driven architecture, enabling loose coupling between components that need to react to state changes. In modern Chromium v134+, the observer pattern has evolved to support thread-safe notifications, weak pointer management, and integration with Mojo services for cross-process observation.\r\n\r\n---\r\n\r\n## 1. Modern Observer Pattern Evolution (v134+)\r\n\r\n### Core Principles\r\n- **Decoupled Communication**: Subjects notify observers without tight coupling\r\n- **Event-Driven Architecture**: Real-time notifications for state changes\r\n- **Memory Safety**: Automatic cleanup using weak pointers and RAII\r\n- **Thread Safety**: Safe cross-thread notification mechanisms\r\n- **Service Integration**: Seamless integration with Mojo IPC for cross-process events\r\n\r\n### Contemporary Applications\r\n- **Cross-Process Events**: Mojo-based observer patterns for multi-process coordination\r\n- **UI State Management**: Modern reactive UI updates with efficient batching\r\n- **Service Lifecycle**: Service state change notifications with capability management\r\n- **Performance Monitoring**: Real-time metrics collection and reporting\r\n- **Security Events**: Privacy and security state change notifications\r\n\r\n---\r\n\r\n## 2. Modern Observer Implementations (v134+)\r\n\r\n### 2.1. Thread-Safe Observer Pattern\r\n\r\nModern thread-safe observer implementation with weak pointer management:\r\n\r\n```cpp\r\n// Modern observer interface with weak pointer support\r\ntemplate<typename ObserverType>\r\nclass ThreadSafeObserverList {\r\n public:\r\n  ThreadSafeObserverList() = default;\r\n  ~ThreadSafeObserverList() = default;\r\n\r\n  // Non-copyable, movable\r\n  ThreadSafeObserverList(const ThreadSafeObserverList&) = delete;\r\n  ThreadSafeObserverList& operator=(const ThreadSafeObserverList&) = delete;\r\n  ThreadSafeObserverList(ThreadSafeObserverList&&) = default;\r\n  ThreadSafeObserverList& operator=(ThreadSafeObserverList&&) = default;\r\n\r\n  void AddObserver(ObserverType* observer) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    CHECK(observer);\r\n    observers_.push_back(observer->AsWeakPtr());\r\n  }\r\n\r\n  void RemoveObserver(ObserverType* observer) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    CHECK(observer);\r\n    \r\n    auto it = std::find_if(observers_.begin(), observers_.end(),\r\n                          [observer](const auto& weak_observer) {\r\n                            return weak_observer.get() == observer;\r\n                          });\r\n    if (it != observers_.end()) {\r\n      observers_.erase(it);\r\n    }\r\n  }\r\n\r\n  // Notify all observers with automatic cleanup of expired weak pointers\r\n  template<typename Method, typename... Args>\r\n  void NotifyObservers(Method method, Args&&... args) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    // Remove expired weak pointers while notifying\r\n    auto it = observers_.begin();\r\n    while (it != observers_.end()) {\r\n      if (auto observer = it->get()) {\r\n        (observer->*method)(std::forward<Args>(args)...);\r\n        ++it;\r\n      } else {\r\n        it = observers_.erase(it);\r\n      }\r\n    }\r\n  }\r\n\r\n  // Async notification for cross-thread scenarios\r\n  template<typename Method, typename... Args>\r\n  void NotifyObserversAsync(const base::Location& location,\r\n                           base::SequencedTaskRunner* task_runner,\r\n                           Method method,\r\n                           Args&&... args) {\r\n    // Capture weak pointers for async execution\r\n    std::vector<base::WeakPtr<ObserverType>> observer_copies;\r\n    {\r\n      DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n      observer_copies = observers_;\r\n    }\r\n    \r\n    task_runner->PostTask(\r\n        location,\r\n        base::BindOnce(&ThreadSafeObserverList::NotifyObserversOnSequence<Method, Args...>,\r\n                      std::move(observer_copies), method, std::forward<Args>(args)...));\r\n  }\r\n\r\n  bool HasObservers() const {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    // Clean up expired pointers when checking\r\n    auto& mutable_observers = const_cast<std::vector<base::WeakPtr<ObserverType>>&>(observers_);\r\n    mutable_observers.erase(\r\n        std::remove_if(mutable_observers.begin(), mutable_observers.end(),\r\n                      [](const auto& weak_ptr) { return !weak_ptr; }),\r\n        mutable_observers.end());\r\n    return !observers_.empty();\r\n  }\r\n\r\n private:\r\n  template<typename Method, typename... Args>\r\n  static void NotifyObserversOnSequence(\r\n      std::vector<base::WeakPtr<ObserverType>> observers,\r\n      Method method,\r\n      Args... args) {\r\n    for (auto& weak_observer : observers) {\r\n      if (auto observer = weak_observer.get()) {\r\n        (observer->*method)(args...);\r\n      }\r\n    }\r\n  }\r\n\r\n  std::vector<base::WeakPtr<ObserverType>> observers_;\r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n};\r\n```\r\n\r\n### 2.2. Mojo Observer Pattern\r\n\r\nCross-process observer pattern using Mojo for service communication:\r\n\r\n```cpp\r\n// Mojo-based observer for cross-process notifications\r\nnamespace download {\r\n\r\n// Observer interface for download events\r\nclass DownloadObserver : public base::SupportsWeakPtr<DownloadObserver> {\r\n public:\r\n  virtual ~DownloadObserver() = default;\r\n  \r\n  virtual void OnDownloadCreated(const std::string& download_id,\r\n                                const DownloadMetadata& metadata) {}\r\n  virtual void OnDownloadUpdated(const std::string& download_id,\r\n                                const DownloadProgress& progress) {}\r\n  virtual void OnDownloadCompleted(const std::string& download_id,\r\n                                  const base::FilePath& file_path) {}\r\n  virtual void OnDownloadFailed(const std::string& download_id,\r\n                               DownloadError error) {}\r\n};\r\n\r\n// Modern download service with observer support\r\nclass DownloadServiceImpl : public mojom::DownloadService {\r\n public:\r\n  explicit DownloadServiceImpl(Profile* profile)\r\n      : profile_(profile) {}\r\n  \r\n  ~DownloadServiceImpl() override = default;\r\n\r\n  // Mojo service methods\r\n  void StartDownload(const GURL& url,\r\n                    const std::string& referrer,\r\n                    StartDownloadCallback callback) override {\r\n    auto download_id = base::Uuid::GenerateRandomV4().AsLowercaseString();\r\n    \r\n    // Create download item\r\n    auto download_item = std::make_unique<DownloadItem>(\r\n        download_id, url, profile_->GetPath());\r\n    \r\n    // Notify local observers\r\n    DownloadMetadata metadata;\r\n    metadata.url = url;\r\n    metadata.file_name = ExtractFileNameFromUrl(url);\r\n    metadata.total_bytes = -1;  // Unknown initially\r\n    \r\n    observers_.NotifyObservers(&DownloadObserver::OnDownloadCreated,\r\n                              download_id, metadata);\r\n    \r\n    // Notify remote observers via Mojo\r\n    for (auto& remote_observer : remote_observers_) {\r\n      if (remote_observer.is_connected()) {\r\n        remote_observer->OnDownloadCreated(download_id, metadata.Clone());\r\n      }\r\n    }\r\n    \r\n    // Start the download asynchronously\r\n    StartDownloadInternal(std::move(download_item), std::move(callback));\r\n  }\r\n\r\n  void AddObserver(mojo::PendingRemote<mojom::DownloadObserver> observer) override {\r\n    mojo::Remote<mojom::DownloadObserver> remote_observer(std::move(observer));\r\n    \r\n    // Set up disconnect handler for cleanup\r\n    remote_observer.set_disconnect_handler(\r\n        base::BindOnce(&DownloadServiceImpl::OnRemoteObserverDisconnected,\r\n                      weak_factory_.GetWeakPtr(),\r\n                      remote_observer.get()));\r\n    \r\n    remote_observers_.push_back(std::move(remote_observer));\r\n  }\r\n\r\n  // Local observer management\r\n  void AddLocalObserver(DownloadObserver* observer) {\r\n    observers_.AddObserver(observer);\r\n  }\r\n\r\n  void RemoveLocalObserver(DownloadObserver* observer) {\r\n    observers_.RemoveObserver(observer);\r\n  }\r\n\r\n private:\r\n  void StartDownloadInternal(std::unique_ptr<DownloadItem> item,\r\n                           StartDownloadCallback callback) {\r\n    const std::string download_id = item->GetId();\r\n    \r\n    // Set up progress tracking\r\n    item->SetProgressCallback(\r\n        base::BindRepeating(&DownloadServiceImpl::OnDownloadProgress,\r\n                           weak_factory_.GetWeakPtr(), download_id));\r\n    \r\n    item->SetCompletionCallback(\r\n        base::BindOnce(&DownloadServiceImpl::OnDownloadCompleted,\r\n                      weak_factory_.GetWeakPtr(), download_id));\r\n    \r\n    // Store and start download\r\n    active_downloads_[download_id] = std::move(item);\r\n    active_downloads_[download_id]->Start();\r\n    \r\n    std::move(callback).Run(DownloadResult::kSuccess);\r\n  }\r\n\r\n  void OnDownloadProgress(const std::string& download_id,\r\n                         int64_t bytes_downloaded,\r\n                         int64_t total_bytes) {\r\n    DownloadProgress progress;\r\n    progress.bytes_downloaded = bytes_downloaded;\r\n    progress.total_bytes = total_bytes;\r\n    progress.percentage = total_bytes > 0 ? \r\n        static_cast<float>(bytes_downloaded) / total_bytes * 100.0f : 0.0f;\r\n    \r\n    // Notify local observers\r\n    observers_.NotifyObservers(&DownloadObserver::OnDownloadUpdated,\r\n                              download_id, progress);\r\n    \r\n    // Notify remote observers\r\n    for (auto& remote_observer : remote_observers_) {\r\n      if (remote_observer.is_connected()) {\r\n        remote_observer->OnDownloadUpdated(download_id, progress.Clone());\r\n      }\r\n    }\r\n  }\r\n\r\n  void OnDownloadCompleted(const std::string& download_id,\r\n                          const base::FilePath& file_path) {\r\n    // Notify local observers\r\n    observers_.NotifyObservers(&DownloadObserver::OnDownloadCompleted,\r\n                              download_id, file_path);\r\n    \r\n    // Notify remote observers\r\n    for (auto& remote_observer : remote_observers_) {\r\n      if (remote_observer.is_connected()) {\r\n        remote_observer->OnDownloadCompleted(download_id, file_path);\r\n      }\r\n    }\r\n    \r\n    // Clean up completed download\r\n    active_downloads_.erase(download_id);\r\n  }\r\n\r\n  void OnRemoteObserverDisconnected(mojom::DownloadObserver* observer) {\r\n    auto it = std::find_if(remote_observers_.begin(), remote_observers_.end(),\r\n                          [observer](const auto& remote) {\r\n                            return remote.get() == observer;\r\n                          });\r\n    if (it != remote_observers_.end()) {\r\n      remote_observers_.erase(it);\r\n    }\r\n  }\r\n\r\n  Profile* profile_;\r\n  ThreadSafeObserverList<DownloadObserver> observers_;\r\n  std::vector<mojo::Remote<mojom::DownloadObserver>> remote_observers_;\r\n  std::unordered_map<std::string, std::unique_ptr<DownloadItem>> active_downloads_;\r\n  \r\n  base::WeakPtrFactory<DownloadServiceImpl> weak_factory_{this};\r\n};\r\n\r\n} // namespace download\r\n```\r\n\r\n### 2.3. Reactive Observer Pattern\r\n\r\nModern reactive observer pattern with efficient batching and filtering:\r\n\r\n```cpp\r\n// Reactive observer pattern with batching and filtering\r\ntemplate<typename EventType>\r\nclass ReactiveObserver : public base::SupportsWeakPtr<ReactiveObserver<EventType>> {\r\n public:\r\n  virtual ~ReactiveObserver() = default;\r\n  \r\n  // Override this method to handle events\r\n  virtual void OnEvent(const EventType& event) = 0;\r\n  \r\n  // Optional: Override for batch processing\r\n  virtual void OnEventBatch(const std::vector<EventType>& events) {\r\n    for (const auto& event : events) {\r\n      OnEvent(event);\r\n    }\r\n  }\r\n  \r\n  // Optional: Override to filter events\r\n  virtual bool ShouldReceiveEvent(const EventType& event) const {\r\n    return true;\r\n  }\r\n};\r\n\r\ntemplate<typename EventType>\r\nclass ReactiveSubject {\r\n public:\r\n  ReactiveSubject() : weak_factory_(this) {}\r\n  \r\n  void AddObserver(ReactiveObserver<EventType>* observer) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    observers_.AddObserver(observer);\r\n  }\r\n  \r\n  void RemoveObserver(ReactiveObserver<EventType>* observer) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    observers_.RemoveObserver(observer);\r\n  }\r\n  \r\n  // Emit single event with optional batching\r\n  void EmitEvent(EventType event) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    if (batch_events_) {\r\n      pending_events_.push_back(std::move(event));\r\n      ScheduleBatchFlush();\r\n    } else {\r\n      NotifyObserversOfEvent(event);\r\n    }\r\n  }\r\n  \r\n  // Emit multiple events efficiently\r\n  void EmitEvents(std::vector<EventType> events) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    if (batch_events_) {\r\n      pending_events_.insert(pending_events_.end(),\r\n                            std::make_move_iterator(events.begin()),\r\n                            std::make_move_iterator(events.end()));\r\n      ScheduleBatchFlush();\r\n    } else {\r\n      for (const auto& event : events) {\r\n        NotifyObserversOfEvent(event);\r\n      }\r\n    }\r\n  }\r\n  \r\n  // Configure batching behavior\r\n  void SetBatchingEnabled(bool enabled) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    batch_events_ = enabled;\r\n    \r\n    if (!enabled && !pending_events_.empty()) {\r\n      FlushPendingEvents();\r\n    }\r\n  }\r\n  \r\n  void SetBatchDelay(base::TimeDelta delay) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    batch_delay_ = delay;\r\n  }\r\n  \r\n private:\r\n  void NotifyObserversOfEvent(const EventType& event) {\r\n    std::vector<base::WeakPtr<ReactiveObserver<EventType>>> filtered_observers;\r\n    \r\n    // Apply filtering\r\n    auto it = observers_.begin();\r\n    while (it != observers_.end()) {\r\n      if (auto observer = it->get()) {\r\n        if (observer->ShouldReceiveEvent(event)) {\r\n          filtered_observers.push_back(*it);\r\n        }\r\n        ++it;\r\n      } else {\r\n        it = observers_.erase(it);\r\n      }\r\n    }\r\n    \r\n    // Notify filtered observers\r\n    for (auto& weak_observer : filtered_observers) {\r\n      if (auto observer = weak_observer.get()) {\r\n        observer->OnEvent(event);\r\n      }\r\n    }\r\n  }\r\n  \r\n  void ScheduleBatchFlush() {\r\n    if (batch_flush_pending_) {\r\n      return;\r\n    }\r\n    \r\n    batch_flush_pending_ = true;\r\n    base::SequencedTaskRunner::GetCurrentDefault()->PostDelayedTask(\r\n        FROM_HERE,\r\n        base::BindOnce(&ReactiveSubject::FlushPendingEvents,\r\n                      weak_factory_.GetWeakPtr()),\r\n        batch_delay_);\r\n  }\r\n  \r\n  void FlushPendingEvents() {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    batch_flush_pending_ = false;\r\n    \r\n    if (pending_events_.empty()) {\r\n      return;\r\n    }\r\n    \r\n    std::vector<EventType> events_to_flush;\r\n    events_to_flush.swap(pending_events_);\r\n    \r\n    // Group events by observer for efficient batch processing\r\n    std::unordered_map<ReactiveObserver<EventType>*, std::vector<EventType>> \r\n        observer_events;\r\n    \r\n    for (const auto& event : events_to_flush) {\r\n      auto it = observers_.begin();\r\n      while (it != observers_.end()) {\r\n        if (auto observer = it->get()) {\r\n          if (observer->ShouldReceiveEvent(event)) {\r\n            observer_events[observer].push_back(event);\r\n          }\r\n          ++it;\r\n        } else {\r\n          it = observers_.erase(it);\r\n        }\r\n      }\r\n    }\r\n    \r\n    // Notify observers with batched events\r\n    for (auto& [observer, events] : observer_events) {\r\n      if (events.size() == 1) {\r\n        observer->OnEvent(events[0]);\r\n      } else if (!events.empty()) {\r\n        observer->OnEventBatch(events);\r\n      }\r\n    }\r\n  }\r\n  \r\n  std::vector<base::WeakPtr<ReactiveObserver<EventType>>> observers_;\r\n  std::vector<EventType> pending_events_;\r\n  bool batch_events_ = false;\r\n  bool batch_flush_pending_ = false;\r\n  base::TimeDelta batch_delay_ = base::Milliseconds(16);  // ~60 FPS\r\n  \r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n  base::WeakPtrFactory<ReactiveSubject<EventType>> weak_factory_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 3. Key Observer Implementations in v134+\r\n\r\n### 3.1. TabStripModelObserver\r\n\r\nModern tab management with enhanced lifecycle tracking:\r\n\r\n```cpp\r\n// Modern tab strip observer with comprehensive event handling\r\nclass TabStripModelObserver : public base::SupportsWeakPtr<TabStripModelObserver> {\r\n public:\r\n  virtual ~TabStripModelObserver() = default;\r\n\r\n  // Tab lifecycle events\r\n  virtual void OnTabStripModelChanged(\r\n      TabStripModel* tab_strip_model,\r\n      const TabStripModelChange& change,\r\n      const TabStripSelectionChange& selection) {}\r\n\r\n  // Enhanced tab events for v134+\r\n  virtual void OnTabWillBeAdded(TabStripModel* model, \r\n                               content::WebContents* contents,\r\n                               int index) {}\r\n  \r\n  virtual void OnTabAdded(TabStripModel* model,\r\n                         content::WebContents* contents,\r\n                         int index,\r\n                         bool foreground) {}\r\n  \r\n  virtual void OnTabWillBeRemoved(TabStripModel* model,\r\n                                 content::WebContents* contents,\r\n                                 int index,\r\n                                 TabStripModelChange::RemoveReason reason) {}\r\n  \r\n  virtual void OnTabRemoved(TabStripModel* model,\r\n                           content::WebContents* contents,\r\n                           int index,\r\n                           TabStripModelChange::RemoveReason reason) {}\r\n\r\n  // Modern navigation and security events\r\n  virtual void OnTabSecurityStateChanged(TabStripModel* model,\r\n                                        content::WebContents* contents,\r\n                                        int index,\r\n                                        SecurityLevel security_level) {}\r\n  \r\n  virtual void OnTabPrivacySandboxStateChanged(TabStripModel* model,\r\n                                              content::WebContents* contents,\r\n                                              int index,\r\n                                              bool privacy_sandbox_enabled) {}\r\n\r\n  // Performance and resource events\r\n  virtual void OnTabResourceUsageChanged(TabStripModel* model,\r\n                                        content::WebContents* contents,\r\n                                        int index,\r\n                                        const ResourceUsageInfo& usage) {}\r\n  \r\n  virtual void OnTabFrozenStateChanged(TabStripModel* model,\r\n                                      content::WebContents* contents,\r\n                                      int index,\r\n                                      bool is_frozen) {}\r\n};\r\n\r\n// Modern TabStripModel implementation\r\nclass TabStripModel {\r\n public:\r\n  void AddObserver(TabStripModelObserver* observer) {\r\n    observers_.AddObserver(observer);\r\n  }\r\n  \r\n  void RemoveObserver(TabStripModelObserver* observer) {\r\n    observers_.RemoveObserver(observer);\r\n  }\r\n  \r\n  // Modern tab addition with comprehensive notification\r\n  void AddWebContents(std::unique_ptr<content::WebContents> contents,\r\n                     int index,\r\n                     ui::PageTransition transition,\r\n                     int add_types) {\r\n    DCHECK(contents);\r\n    \r\n    // Notify observers before addition\r\n    observers_.NotifyObservers(&TabStripModelObserver::OnTabWillBeAdded,\r\n                              this, contents.get(), index);\r\n    \r\n    // Add tab to model\r\n    auto* raw_contents = contents.get();\r\n    web_contents_.insert(web_contents_.begin() + index, std::move(contents));\r\n    \r\n    // Set up content observers for enhanced events\r\n    SetupContentObservers(raw_contents, index);\r\n    \r\n    // Notify observers after addition\r\n    bool foreground = (add_types & ADD_ACTIVE) != 0;\r\n    observers_.NotifyObservers(&TabStripModelObserver::OnTabAdded,\r\n                              this, raw_contents, index, foreground);\r\n    \r\n    // Update selection if necessary\r\n    if (foreground) {\r\n      SetActiveIndex(index);\r\n    }\r\n  }\r\n  \r\n  // Enhanced removal with detailed reason tracking\r\n  std::unique_ptr<content::WebContents> DetachWebContentsAt(\r\n      int index,\r\n      TabStripModelChange::RemoveReason reason) {\r\n    DCHECK(IsValidIndex(index));\r\n    \r\n    auto* contents = web_contents_[index].get();\r\n    \r\n    // Notify observers before removal\r\n    observers_.NotifyObservers(&TabStripModelObserver::OnTabWillBeRemoved,\r\n                              this, contents, index, reason);\r\n    \r\n    // Clean up content observers\r\n    CleanupContentObservers(contents);\r\n    \r\n    // Remove from model\r\n    auto detached_contents = std::move(web_contents_[index]);\r\n    web_contents_.erase(web_contents_.begin() + index);\r\n    \r\n    // Notify observers after removal\r\n    observers_.NotifyObservers(&TabStripModelObserver::OnTabRemoved,\r\n                              this, detached_contents.get(), index, reason);\r\n    \r\n    // Update active index if necessary\r\n    if (active_index_ >= index && active_index_ > 0) {\r\n      SetActiveIndex(active_index_ - 1);\r\n    }\r\n    \r\n    return detached_contents;\r\n  }\r\n  \r\n private:\r\n  void SetupContentObservers(content::WebContents* contents, int index) {\r\n    // Set up security state observer\r\n    auto security_observer = std::make_unique<SecurityStateObserver>(\r\n        base::BindRepeating(&TabStripModel::OnSecurityStateChanged,\r\n                           weak_factory_.GetWeakPtr(), contents, index));\r\n    content_observers_[contents] = std::move(security_observer);\r\n  }\r\n  \r\n  void OnSecurityStateChanged(content::WebContents* contents,\r\n                             int index,\r\n                             SecurityLevel level) {\r\n    observers_.NotifyObservers(&TabStripModelObserver::OnTabSecurityStateChanged,\r\n                              this, contents, index, level);\r\n  }\r\n  \r\n  ThreadSafeObserverList<TabStripModelObserver> observers_;\r\n  std::vector<std::unique_ptr<content::WebContents>> web_contents_;\r\n  std::unordered_map<content::WebContents*, \r\n                     std::unique_ptr<SecurityStateObserver>> content_observers_;\r\n  int active_index_ = -1;\r\n  \r\n  base::WeakPtrFactory<TabStripModel> weak_factory_{this};\r\n};\r\n```\r\n\r\n### 3.2. PermissionObserver\r\n\r\nModern permission system observer with Privacy Sandbox integration:\r\n\r\n```cpp\r\n// Observer for permission and privacy state changes\r\nclass PermissionObserver : public base::SupportsWeakPtr<PermissionObserver> {\r\n public:\r\n  virtual ~PermissionObserver() = default;\r\n  \r\n  // Traditional permission events\r\n  virtual void OnPermissionChanged(const url::Origin& origin,\r\n                                  ContentSettingsType type,\r\n                                  ContentSetting setting) {}\r\n  \r\n  // Privacy Sandbox events (v134+)\r\n  virtual void OnTopicsPermissionChanged(const url::Origin& origin,\r\n                                        bool allowed) {}\r\n  \r\n  virtual void OnFledgePermissionChanged(const url::Origin& origin,\r\n                                        bool allowed) {}\r\n  \r\n  virtual void OnAttributionReportingPermissionChanged(const url::Origin& origin,\r\n                                                      bool allowed) {}\r\n  \r\n  // Trust Token events\r\n  virtual void OnTrustTokenPermissionChanged(const url::Origin& origin,\r\n                                           bool allowed) {}\r\n  \r\n  // Device permission events\r\n  virtual void OnDevicePermissionChanged(const url::Origin& origin,\r\n                                        blink::mojom::PermissionName permission,\r\n                                        blink::mojom::PermissionStatus status) {}\r\n};\r\n\r\n// Modern permission manager with comprehensive notification\r\nclass PermissionManagerImpl {\r\n public:\r\n  void AddObserver(PermissionObserver* observer) {\r\n    observers_.AddObserver(observer);\r\n  }\r\n  \r\n  void RemoveObserver(PermissionObserver* observer) {\r\n    observers_.RemoveObserver(observer);\r\n  }\r\n  \r\n  // Set permission with automatic notification\r\n  void SetPermission(const url::Origin& origin,\r\n                    ContentSettingsType type,\r\n                    ContentSetting setting) {\r\n    // Update internal state\r\n    permission_store_[{origin, type}] = setting;\r\n    \r\n    // Notify observers\r\n    observers_.NotifyObservers(&PermissionObserver::OnPermissionChanged,\r\n                              origin, type, setting);\r\n    \r\n    // Handle Privacy Sandbox specific permissions\r\n    if (type == ContentSettingsType::PRIVACY_SANDBOX_TOPICS_API) {\r\n      bool allowed = (setting == CONTENT_SETTING_ALLOW);\r\n      observers_.NotifyObservers(&PermissionObserver::OnTopicsPermissionChanged,\r\n                                origin, allowed);\r\n    }\r\n    \r\n    // Update related permissions and notify\r\n    UpdateRelatedPermissions(origin, type, setting);\r\n  }\r\n  \r\n  // Async permission request with observer notification\r\n  void RequestPermissionAsync(\r\n      const url::Origin& origin,\r\n      ContentSettingsType type,\r\n      base::OnceCallback<void(ContentSetting)> callback) {\r\n    \r\n    // Check if permission is already granted\r\n    auto current_setting = GetPermission(origin, type);\r\n    if (current_setting != CONTENT_SETTING_ASK) {\r\n      std::move(callback).Run(current_setting);\r\n      return;\r\n    }\r\n    \r\n    // Show permission prompt asynchronously\r\n    permission_prompt_->ShowPrompt(\r\n        origin, type,\r\n        base::BindOnce(&PermissionManagerImpl::OnPermissionPromptComplete,\r\n                      weak_factory_.GetWeakPtr(), origin, type, \r\n                      std::move(callback)));\r\n  }\r\n  \r\n private:\r\n  void OnPermissionPromptComplete(const url::Origin& origin,\r\n                                 ContentSettingsType type,\r\n                                 base::OnceCallback<void(ContentSetting)> callback,\r\n                                 ContentSetting result) {\r\n    // Update permission state\r\n    SetPermission(origin, type, result);\r\n    \r\n    // Complete the request\r\n    std::move(callback).Run(result);\r\n  }\r\n  \r\n  void UpdateRelatedPermissions(const url::Origin& origin,\r\n                               ContentSettingsType type,\r\n                               ContentSetting setting) {\r\n    // Handle cascading permission changes\r\n    if (type == ContentSettingsType::NOTIFICATIONS && \r\n        setting == CONTENT_SETTING_BLOCK) {\r\n      // Also block push messaging\r\n      SetPermission(origin, ContentSettingsType::PUSH_MESSAGING, \r\n                   CONTENT_SETTING_BLOCK);\r\n    }\r\n    \r\n    // Privacy Sandbox cascading\r\n    if (type == ContentSettingsType::COOKIES && \r\n        setting == CONTENT_SETTING_BLOCK) {\r\n      // Disable Privacy Sandbox APIs when cookies are blocked\r\n      SetPermission(origin, ContentSettingsType::PRIVACY_SANDBOX_TOPICS_API,\r\n                   CONTENT_SETTING_BLOCK);\r\n      SetPermission(origin, ContentSettingsType::PRIVACY_SANDBOX_FLEDGE_API,\r\n                   CONTENT_SETTING_BLOCK);\r\n    }\r\n  }\r\n  \r\n  ContentSetting GetPermission(const url::Origin& origin,\r\n                              ContentSettingsType type) {\r\n    auto key = std::make_pair(origin, type);\r\n    auto it = permission_store_.find(key);\r\n    return it != permission_store_.end() ? it->second : CONTENT_SETTING_ASK;\r\n  }\r\n  \r\n  ThreadSafeObserverList<PermissionObserver> observers_;\r\n  std::map<std::pair<url::Origin, ContentSettingsType>, ContentSetting> permission_store_;\r\n  std::unique_ptr<PermissionPrompt> permission_prompt_;\r\n  \r\n  base::WeakPtrFactory<PermissionManagerImpl> weak_factory_{this};\r\n};\r\n```\r\n\r\n### 3.3. Performance Observer\r\n\r\nModern performance monitoring with Core Web Vitals integration:\r\n\r\n```cpp\r\n// Performance event types for v134+\r\nstruct PerformanceEvent {\r\n  enum class Type {\r\n    kPageLoad,\r\n    kResourceLoad,\r\n    kCoreWebVitals,\r\n    kMemoryUsage,\r\n    kCPUUsage,\r\n    kNetworkLatency,\r\n    kRenderingMetrics\r\n  };\r\n  \r\n  Type type;\r\n  base::TimeTicks timestamp;\r\n  url::Origin origin;\r\n  std::unordered_map<std::string, double> metrics;\r\n  std::unordered_map<std::string, std::string> metadata;\r\n};\r\n\r\n// Performance observer with filtering and aggregation\r\nclass PerformanceObserver : public ReactiveObserver<PerformanceEvent> {\r\n public:\r\n  virtual ~PerformanceObserver() = default;\r\n  \r\n  // Implement filtering for specific performance events\r\n  bool ShouldReceiveEvent(const PerformanceEvent& event) const override {\r\n    // Filter by event type\r\n    if (!interested_types_.empty() && \r\n        interested_types_.find(event.type) == interested_types_.end()) {\r\n      return false;\r\n    }\r\n    \r\n    // Filter by origin\r\n    if (!interested_origins_.empty() &&\r\n        interested_origins_.find(event.origin) == interested_origins_.end()) {\r\n      return false;\r\n    }\r\n    \r\n    return true;\r\n  }\r\n  \r\n  // Override for batch processing of performance events\r\n  void OnEventBatch(const std::vector<PerformanceEvent>& events) override {\r\n    ProcessPerformanceBatch(events);\r\n  }\r\n  \r\n  // Configuration methods\r\n  void SetInterestedTypes(std::set<PerformanceEvent::Type> types) {\r\n    interested_types_ = std::move(types);\r\n  }\r\n  \r\n  void SetInterestedOrigins(std::set<url::Origin> origins) {\r\n    interested_origins_ = std::move(origins);\r\n  }\r\n  \r\n protected:\r\n  virtual void ProcessPerformanceBatch(const std::vector<PerformanceEvent>& events) {\r\n    for (const auto& event : events) {\r\n      OnEvent(event);\r\n    }\r\n  }\r\n  \r\n private:\r\n  std::set<PerformanceEvent::Type> interested_types_;\r\n  std::set<url::Origin> interested_origins_;\r\n};\r\n\r\n// Modern performance monitor with Core Web Vitals\r\nclass PerformanceMonitor {\r\n public:\r\n  PerformanceMonitor() {\r\n    // Enable batching for efficient performance data collection\r\n    subject_.SetBatchingEnabled(true);\r\n    subject_.SetBatchDelay(base::Milliseconds(100));  // 10 Hz reporting\r\n  }\r\n  \r\n  void AddObserver(PerformanceObserver* observer) {\r\n    subject_.AddObserver(observer);\r\n  }\r\n  \r\n  void RemoveObserver(PerformanceObserver* observer) {\r\n    subject_.RemoveObserver(observer);\r\n  }\r\n  \r\n  // Report Core Web Vitals\r\n  void ReportCoreWebVitals(const url::Origin& origin,\r\n                          double lcp,    // Largest Contentful Paint\r\n                          double fid,    // First Input Delay\r\n                          double cls) {  // Cumulative Layout Shift\r\n    PerformanceEvent event;\r\n    event.type = PerformanceEvent::Type::kCoreWebVitals;\r\n    event.timestamp = base::TimeTicks::Now();\r\n    event.origin = origin;\r\n    event.metrics[\"lcp\"] = lcp;\r\n    event.metrics[\"fid\"] = fid;\r\n    event.metrics[\"cls\"] = cls;\r\n    \r\n    // Calculate Web Vitals score\r\n    double score = CalculateWebVitalsScore(lcp, fid, cls);\r\n    event.metrics[\"score\"] = score;\r\n    \r\n    subject_.EmitEvent(std::move(event));\r\n  }\r\n  \r\n  // Report resource loading performance\r\n  void ReportResourceLoad(const url::Origin& origin,\r\n                         const GURL& resource_url,\r\n                         base::TimeDelta load_time,\r\n                         size_t resource_size) {\r\n    PerformanceEvent event;\r\n    event.type = PerformanceEvent::Type::kResourceLoad;\r\n    event.timestamp = base::TimeTicks::Now();\r\n    event.origin = origin;\r\n    event.metrics[\"load_time_ms\"] = load_time.InMillisecondsF();\r\n    event.metrics[\"size_bytes\"] = static_cast<double>(resource_size);\r\n    event.metrics[\"throughput_mbps\"] = \r\n        (resource_size * 8.0) / (load_time.InSecondsF() * 1024 * 1024);\r\n    event.metadata[\"resource_url\"] = resource_url.spec();\r\n    \r\n    subject_.EmitEvent(std::move(event));\r\n  }\r\n  \r\n  // Report memory usage with detailed breakdown\r\n  void ReportMemoryUsage(const url::Origin& origin,\r\n                        const MemoryUsageInfo& usage_info) {\r\n    PerformanceEvent event;\r\n    event.type = PerformanceEvent::Type::kMemoryUsage;\r\n    event.timestamp = base::TimeTicks::Now();\r\n    event.origin = origin;\r\n    event.metrics[\"heap_used_mb\"] = usage_info.heap_used / (1024.0 * 1024.0);\r\n    event.metrics[\"heap_total_mb\"] = usage_info.heap_total / (1024.0 * 1024.0);\r\n    event.metrics[\"external_mb\"] = usage_info.external / (1024.0 * 1024.0);\r\n    event.metrics[\"dom_nodes\"] = static_cast<double>(usage_info.dom_nodes);\r\n    \r\n    subject_.EmitEvent(std::move(event));\r\n  }\r\n  \r\n private:\r\n  double CalculateWebVitalsScore(double lcp, double fid, double cls) {\r\n    // Implement Google's Core Web Vitals scoring algorithm\r\n    double lcp_score = lcp <= 2500 ? 1.0 : (lcp <= 4000 ? 0.5 : 0.0);\r\n    double fid_score = fid <= 100 ? 1.0 : (fid <= 300 ? 0.5 : 0.0);\r\n    double cls_score = cls <= 0.1 ? 1.0 : (cls <= 0.25 ? 0.5 : 0.0);\r\n    \r\n    return (lcp_score + fid_score + cls_score) / 3.0;\r\n  }\r\n  \r\n  ReactiveSubject<PerformanceEvent> subject_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 4. Advanced Observer Patterns (v134+)\r\n\r\n### 4.1. Hierarchical Observer Pattern\r\n\r\nObserver pattern with parent-child relationships and event bubbling:\r\n\r\n```cpp\r\n// Hierarchical observer for tree-structured events\r\ntemplate<typename EventType>\r\nclass HierarchicalObserver : public base::SupportsWeakPtr<HierarchicalObserver<EventType>> {\r\n public:\r\n  virtual ~HierarchicalObserver() = default;\r\n  \r\n  // Handle events at this level\r\n  virtual bool OnEvent(const EventType& event, bool from_child = false) {\r\n    return false;  // Return true to stop event propagation\r\n  }\r\n  \r\n  // Handle events bubbling up from children\r\n  virtual bool OnChildEvent(const EventType& event, \r\n                           HierarchicalObserver* child) {\r\n    return OnEvent(event, true);\r\n  }\r\n  \r\n  // Set parent for event bubbling\r\n  void SetParent(HierarchicalObserver* parent) {\r\n    parent_ = parent ? parent->AsWeakPtr() : base::WeakPtr<HierarchicalObserver>();\r\n  }\r\n  \r\n  // Add child observer\r\n  void AddChild(HierarchicalObserver* child) {\r\n    if (child) {\r\n      children_.push_back(child->AsWeakPtr());\r\n      child->SetParent(this);\r\n    }\r\n  }\r\n  \r\n  // Remove child observer\r\n  void RemoveChild(HierarchicalObserver* child) {\r\n    auto it = std::find_if(children_.begin(), children_.end(),\r\n                          [child](const auto& weak_child) {\r\n                            return weak_child.get() == child;\r\n                          });\r\n    if (it != children_.end()) {\r\n      children_.erase(it);\r\n      if (child) {\r\n        child->SetParent(nullptr);\r\n      }\r\n    }\r\n  }\r\n  \r\n protected:\r\n  // Emit event with automatic bubbling\r\n  bool EmitEvent(const EventType& event) {\r\n    // Handle locally first\r\n    if (OnEvent(event)) {\r\n      return true;  // Event handled, stop propagation\r\n    }\r\n    \r\n    // Bubble up to parent\r\n    if (auto parent = parent_.get()) {\r\n      return parent->OnChildEvent(event, this);\r\n    }\r\n    \r\n    return false;\r\n  }\r\n  \r\n  // Broadcast event down to children\r\n  void BroadcastToChildren(const EventType& event) {\r\n    auto it = children_.begin();\r\n    while (it != children_.end()) {\r\n      if (auto child = it->get()) {\r\n        if (child->OnEvent(event)) {\r\n          break;  // Stop broadcast if child handles event\r\n        }\r\n        ++it;\r\n      } else {\r\n        it = children_.erase(it);\r\n      }\r\n    }\r\n  }\r\n  \r\n private:\r\n  base::WeakPtr<HierarchicalObserver> parent_;\r\n  std::vector<base::WeakPtr<HierarchicalObserver>> children_;\r\n};\r\n```\r\n\r\n### 4.2. Filtered Observer Pattern\r\n\r\nObserver pattern with sophisticated filtering and transformation:\r\n\r\n```cpp\r\n// Event filter interface\r\ntemplate<typename EventType>\r\nclass EventFilter {\r\n public:\r\n  virtual ~EventFilter() = default;\r\n  virtual bool ShouldProcess(const EventType& event) const = 0;\r\n  virtual EventType Transform(EventType event) const { return event; }\r\n};\r\n\r\n// Compound filter for complex filtering logic\r\ntemplate<typename EventType>\r\nclass CompoundEventFilter : public EventFilter<EventType> {\r\n public:\r\n  enum class Logic { kAnd, kOr };\r\n  \r\n  CompoundEventFilter(Logic logic) : logic_(logic) {}\r\n  \r\n  void AddFilter(std::unique_ptr<EventFilter<EventType>> filter) {\r\n    filters_.push_back(std::move(filter));\r\n  }\r\n  \r\n  bool ShouldProcess(const EventType& event) const override {\r\n    if (filters_.empty()) {\r\n      return true;\r\n    }\r\n    \r\n    if (logic_ == Logic::kAnd) {\r\n      return std::all_of(filters_.begin(), filters_.end(),\r\n                        [&event](const auto& filter) {\r\n                          return filter->ShouldProcess(event);\r\n                        });\r\n    } else {\r\n      return std::any_of(filters_.begin(), filters_.end(),\r\n                        [&event](const auto& filter) {\r\n                          return filter->ShouldProcess(event);\r\n                        });\r\n    }\r\n  }\r\n  \r\n  EventType Transform(EventType event) const override {\r\n    for (const auto& filter : filters_) {\r\n      event = filter->Transform(std::move(event));\r\n    }\r\n    return event;\r\n  }\r\n  \r\n private:\r\n  Logic logic_;\r\n  std::vector<std::unique_ptr<EventFilter<EventType>>> filters_;\r\n};\r\n\r\n// Filtered observer implementation\r\ntemplate<typename EventType>\r\nclass FilteredObserver : public base::SupportsWeakPtr<FilteredObserver<EventType>> {\r\n public:\r\n  virtual ~FilteredObserver() = default;\r\n  \r\n  // Pure virtual method for handling filtered events\r\n  virtual void OnFilteredEvent(const EventType& event) = 0;\r\n  \r\n  // Set event filter\r\n  void SetFilter(std::unique_ptr<EventFilter<EventType>> filter) {\r\n    filter_ = std::move(filter);\r\n  }\r\n  \r\n  // Internal event processing with filtering\r\n  bool ProcessEvent(const EventType& event) {\r\n    if (!filter_ || filter_->ShouldProcess(event)) {\r\n      EventType processed_event = filter_ ? filter_->Transform(event) : event;\r\n      OnFilteredEvent(processed_event);\r\n      return true;\r\n    }\r\n    return false;\r\n  }\r\n  \r\n private:\r\n  std::unique_ptr<EventFilter<EventType>> filter_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 5. Testing Observer Patterns (v134+)\r\n\r\n### 5.1. Mock Observer Implementation\r\n\r\nComprehensive mock observer for testing:\r\n\r\n```cpp\r\n// Mock observer with expectation validation\r\ntemplate<typename EventType>\r\nclass MockObserver : public ReactiveObserver<EventType> {\r\n public:\r\n  MockObserver() = default;\r\n  ~MockObserver() override = default;\r\n  \r\n  // Mock method that can be configured\r\n  MOCK_METHOD(void, OnEvent, (const EventType& event), (override));\r\n  MOCK_METHOD(void, OnEventBatch, (const std::vector<EventType>& events), (override));\r\n  MOCK_METHOD(bool, ShouldReceiveEvent, (const EventType& event), (const override));\r\n  \r\n  // Helper methods for test setup\r\n  void ExpectEvent(const EventType& expected_event) {\r\n    EXPECT_CALL(*this, OnEvent(testing::Eq(expected_event)))\r\n        .Times(1);\r\n  }\r\n  \r\n  void ExpectEventCount(int count) {\r\n    EXPECT_CALL(*this, OnEvent(testing::_))\r\n        .Times(count);\r\n  }\r\n  \r\n  void ExpectEventSequence(const std::vector<EventType>& sequence) {\r\n    testing::InSequence seq;\r\n    for (const auto& event : sequence) {\r\n      EXPECT_CALL(*this, OnEvent(testing::Eq(event)))\r\n          .Times(1);\r\n    }\r\n  }\r\n  \r\n  void ExpectNoEvents() {\r\n    EXPECT_CALL(*this, OnEvent(testing::_))\r\n        .Times(0);\r\n  }\r\n  \r\n  // Set up filtering behavior for tests\r\n  void SetupFiltering(std::function<bool(const EventType&)> filter_func) {\r\n    ON_CALL(*this, ShouldReceiveEvent(testing::_))\r\n        .WillByDefault(testing::Invoke(filter_func));\r\n  }\r\n  \r\n  // Record events for verification\r\n  void EnableEventRecording() {\r\n    ON_CALL(*this, OnEvent(testing::_))\r\n        .WillByDefault(testing::Invoke([this](const EventType& event) {\r\n          recorded_events_.push_back(event);\r\n        }));\r\n  }\r\n  \r\n  const std::vector<EventType>& GetRecordedEvents() const {\r\n    return recorded_events_;\r\n  }\r\n  \r\n  void ClearRecordedEvents() {\r\n    recorded_events_.clear();\r\n  }\r\n  \r\n private:\r\n  std::vector<EventType> recorded_events_;\r\n};\r\n\r\n// Test helper for observer pattern testing\r\ntemplate<typename SubjectType, typename EventType>\r\nclass ObserverTestHelper {\r\n public:\r\n  ObserverTestHelper() : mock_observer_(std::make_unique<MockObserver<EventType>>()) {}\r\n  \r\n  void SetUp() {\r\n    subject_.AddObserver(mock_observer_.get());\r\n  }\r\n  \r\n  void TearDown() {\r\n    subject_.RemoveObserver(mock_observer_.get());\r\n  }\r\n  \r\n  MockObserver<EventType>* mock_observer() { return mock_observer_.get(); }\r\n  SubjectType* subject() { return &subject_; }\r\n  \r\n  // Verify observer was called within timeout\r\n  void VerifyObserverCalledWithin(base::TimeDelta timeout) {\r\n    base::RunLoop run_loop;\r\n    base::OneShotTimer timer;\r\n    \r\n    EXPECT_CALL(*mock_observer_, OnEvent(testing::_))\r\n        .WillOnce(testing::InvokeWithoutArgs([&run_loop]() {\r\n          run_loop.Quit();\r\n        }));\r\n    \r\n    timer.Start(FROM_HERE, timeout, \r\n                base::BindOnce([](base::RunLoop* loop) { loop->Quit(); }, \r\n                              &run_loop));\r\n    \r\n    run_loop.Run();\r\n  }\r\n  \r\n private:\r\n  SubjectType subject_;\r\n  std::unique_ptr<MockObserver<EventType>> mock_observer_;\r\n};\r\n```\r\n\r\n### 5.2. Test Examples\r\n\r\nComprehensive test examples for observer patterns:\r\n\r\n```cpp\r\n// Test fixture for download observer testing\r\nclass DownloadObserverTest : public testing::Test {\r\n protected:\r\n  void SetUp() override {\r\n    download_service_ = std::make_unique<DownloadServiceImpl>(\r\n        TestProfile::CreateProfile());\r\n    mock_observer_ = std::make_unique<MockDownloadObserver>();\r\n    download_service_->AddLocalObserver(mock_observer_.get());\r\n  }\r\n  \r\n  void TearDown() override {\r\n    download_service_->RemoveLocalObserver(mock_observer_.get());\r\n  }\r\n  \r\n  std::unique_ptr<DownloadServiceImpl> download_service_;\r\n  std::unique_ptr<MockDownloadObserver> mock_observer_;\r\n};\r\n\r\nTEST_F(DownloadObserverTest, NotifiesOnDownloadCreated) {\r\n  const GURL test_url(\"https://example.com/file.pdf\");\r\n  const std::string expected_id = \"test-download-id\";\r\n  \r\n  // Set up expectation\r\n  EXPECT_CALL(*mock_observer_, OnDownloadCreated(expected_id, testing::_))\r\n      .Times(1);\r\n  \r\n  // Trigger download creation\r\n  download_service_->StartDownload(test_url, \"\", base::DoNothing());\r\n  \r\n  // Verify expectation\r\n  testing::Mock::VerifyAndClearExpectations(mock_observer_.get());\r\n}\r\n\r\nTEST_F(DownloadObserverTest, BatchNotificationForMultipleDownloads) {\r\n  std::vector<GURL> urls = {\r\n    GURL(\"https://example.com/file1.pdf\"),\r\n    GURL(\"https://example.com/file2.pdf\"),\r\n    GURL(\"https://example.com/file3.pdf\")\r\n  };\r\n  \r\n  // Expect three creation events\r\n  EXPECT_CALL(*mock_observer_, OnDownloadCreated(testing::_, testing::_))\r\n      .Times(3);\r\n  \r\n  // Start multiple downloads\r\n  for (const auto& url : urls) {\r\n    download_service_->StartDownload(url, \"\", base::DoNothing());\r\n  }\r\n  \r\n  // Allow event processing\r\n  base::RunLoop().RunUntilIdle();\r\n  \r\n  testing::Mock::VerifyAndClearExpectations(mock_observer_.get());\r\n}\r\n\r\n// Test for filtered observer\r\nTEST_F(DownloadObserverTest, FilteredObserverReceivesOnlyRelevantEvents) {\r\n  auto filtered_observer = std::make_unique<MockFilteredDownloadObserver>();\r\n  \r\n  // Set up filter to only receive PDF downloads\r\n  filtered_observer->SetFilter(\r\n      std::make_unique<FileTypeFilter>(\"application/pdf\"));\r\n  \r\n  download_service_->AddLocalObserver(filtered_observer.get());\r\n  \r\n  // Expect only PDF download to be notified\r\n  EXPECT_CALL(*filtered_observer, OnFilteredEvent(testing::_))\r\n      .Times(1);\r\n  \r\n  // Start downloads of different types\r\n  download_service_->StartDownload(GURL(\"https://example.com/image.jpg\"), \"\", base::DoNothing());\r\n  download_service_->StartDownload(GURL(\"https://example.com/document.pdf\"), \"\", base::DoNothing());\r\n  download_service_->StartDownload(GURL(\"https://example.com/video.mp4\"), \"\", base::DoNothing());\r\n  \r\n  base::RunLoop().RunUntilIdle();\r\n  \r\n  download_service_->RemoveLocalObserver(filtered_observer.get());\r\n}\r\n\r\n// Test for async observer notification\r\nTEST_F(DownloadObserverTest, AsyncNotificationWorksCorrectly) {\r\n  base::test::TaskEnvironment task_environment{\r\n      base::test::TaskEnvironment::TimeSource::MOCK_TIME};\r\n  \r\n  auto async_observer = std::make_unique<MockAsyncDownloadObserver>();\r\n  download_service_->AddLocalObserver(async_observer.get());\r\n  \r\n  // Set up expectation for async notification\r\n  base::RunLoop run_loop;\r\n  EXPECT_CALL(*async_observer, OnDownloadCreated(testing::_, testing::_))\r\n      .WillOnce(testing::InvokeWithoutArgs([&run_loop]() {\r\n        run_loop.Quit();\r\n      }));\r\n  \r\n  // Start download\r\n  download_service_->StartDownload(GURL(\"https://example.com/file.pdf\"), \"\", base::DoNothing());\r\n  \r\n  // Fast-forward time and run pending tasks\r\n  task_environment.FastForwardBy(base::Milliseconds(100));\r\n  run_loop.Run();\r\n  \r\n  download_service_->RemoveLocalObserver(async_observer.get());\r\n}\r\n```\r\n\r\n---\r\n\r\n## 6. Observer Pattern Best Practices (v134+)\r\n\r\n### 6.1. Memory Management\r\n\r\n```cpp\r\n// RAII observer registration helper\r\ntemplate<typename SubjectType, typename ObserverType>\r\nclass ScopedObserverRegistration {\r\n public:\r\n  ScopedObserverRegistration(SubjectType* subject, ObserverType* observer)\r\n      : subject_(subject), observer_(observer) {\r\n    DCHECK(subject_);\r\n    DCHECK(observer_);\r\n    subject_->AddObserver(observer_);\r\n  }\r\n  \r\n  ~ScopedObserverRegistration() {\r\n    if (subject_ && observer_) {\r\n      subject_->RemoveObserver(observer_);\r\n    }\r\n  }\r\n  \r\n  // Non-copyable, movable\r\n  ScopedObserverRegistration(const ScopedObserverRegistration&) = delete;\r\n  ScopedObserverRegistration& operator=(const ScopedObserverRegistration&) = delete;\r\n  \r\n  ScopedObserverRegistration(ScopedObserverRegistration&& other) noexcept\r\n      : subject_(std::exchange(other.subject_, nullptr)),\r\n        observer_(std::exchange(other.observer_, nullptr)) {}\r\n  \r\n  ScopedObserverRegistration& operator=(ScopedObserverRegistration&& other) noexcept {\r\n    if (this != &other) {\r\n      if (subject_ && observer_) {\r\n        subject_->RemoveObserver(observer_);\r\n      }\r\n      subject_ = std::exchange(other.subject_, nullptr);\r\n      observer_ = std::exchange(other.observer_, nullptr);\r\n    }\r\n    return *this;\r\n  }\r\n  \r\n  void Reset() {\r\n    if (subject_ && observer_) {\r\n      subject_->RemoveObserver(observer_);\r\n      subject_ = nullptr;\r\n      observer_ = nullptr;\r\n    }\r\n  }\r\n  \r\n private:\r\n  SubjectType* subject_;\r\n  ObserverType* observer_;\r\n};\r\n```\r\n\r\n### 6.2. Performance Optimization\r\n\r\n```cpp\r\n// High-performance observer list with minimal allocations\r\ntemplate<typename ObserverType>\r\nclass OptimizedObserverList {\r\n public:\r\n  OptimizedObserverList() {\r\n    observers_.reserve(kInitialCapacity);\r\n  }\r\n  \r\n  void AddObserver(ObserverType* observer) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    CHECK(observer);\r\n    \r\n    // Check if already exists to avoid duplicates\r\n    if (std::find(observers_.begin(), observers_.end(), observer) == observers_.end()) {\r\n      observers_.push_back(observer);\r\n    }\r\n  }\r\n  \r\n  void RemoveObserver(ObserverType* observer) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    CHECK(observer);\r\n    \r\n    auto it = std::find(observers_.begin(), observers_.end(), observer);\r\n    if (it != observers_.end()) {\r\n      // Use swap-and-pop for O(1) removal\r\n      std::swap(*it, observers_.back());\r\n      observers_.pop_back();\r\n    }\r\n  }\r\n  \r\n  // High-performance notification with minimal overhead\r\n  template<typename Method, typename... Args>\r\n  void NotifyObservers(Method method, Args&&... args) {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    \r\n    // Use simple iteration for maximum performance\r\n    for (auto* observer : observers_) {\r\n      if (observer) {\r\n        (observer->*method)(args...);\r\n      }\r\n    }\r\n  }\r\n  \r\n  size_t size() const {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    return observers_.size();\r\n  }\r\n  \r\n  bool empty() const {\r\n    DCHECK_CALLED_ON_VALID_SEQUENCE(sequence_checker_);\r\n    return observers_.empty();\r\n  }\r\n  \r\n private:\r\n  static constexpr size_t kInitialCapacity = 4;\r\n  \r\n  std::vector<ObserverType*> observers_;\r\n  SEQUENCE_CHECKER(sequence_checker_);\r\n};\r\n```\r\n\r\n---\r\n\r\n## 7. Real-World Examples from Chromium v134+\r\n\r\n### 7.1. WebContents Observer\r\n\r\nModern web contents observer with enhanced security and privacy events:\r\n\r\n```cpp\r\n// Simplified WebContentsObserver for v134+\r\nclass WebContentsObserver : public base::SupportsWeakPtr<WebContentsObserver> {\r\n public:\r\n  explicit WebContentsObserver(content::WebContents* web_contents)\r\n      : web_contents_(web_contents) {\r\n    if (web_contents_) {\r\n      web_contents_->AddObserver(this);\r\n    }\r\n  }\r\n  \r\n  virtual ~WebContentsObserver() {\r\n    if (web_contents_) {\r\n      web_contents_->RemoveObserver(this);\r\n    }\r\n  }\r\n  \r\n  // Navigation events\r\n  virtual void DidStartNavigation(content::NavigationHandle* navigation) {}\r\n  virtual void DidFinishNavigation(content::NavigationHandle* navigation) {}\r\n  \r\n  // Security events (v134+)\r\n  virtual void DidChangeSecurityState(SecurityLevel security_level) {}\r\n  virtual void OnPermissionStatusChanged(blink::mojom::PermissionName permission,\r\n                                        blink::mojom::PermissionStatus status) {}\r\n  \r\n  // Privacy Sandbox events (v134+)\r\n  virtual void OnTopicsAPIUsed(const std::vector<int>& topic_ids) {}\r\n  virtual void OnFledgeAuctionCompleted(const GURL& seller_origin,\r\n                                       bool success) {}\r\n  \r\n  // Performance events (v134+)\r\n  virtual void OnCoreWebVitalsUpdated(double lcp, double fid, double cls) {}\r\n  virtual void OnMemoryUsageUpdated(const MemoryUsageInfo& usage) {}\r\n  \r\n protected:\r\n  content::WebContents* web_contents() const { return web_contents_; }\r\n  \r\n private:\r\n  content::WebContents* web_contents_;\r\n};\r\n```\r\n\r\n### 7.2. Preference Observer\r\n\r\nModern preference system observer with hierarchical settings:\r\n\r\n```cpp\r\n// Preference observer with fine-grained change detection\r\nclass PrefObserver : public base::SupportsWeakPtr<PrefObserver> {\r\n public:\r\n  virtual ~PrefObserver() = default;\r\n  \r\n  // Called when a preference value changes\r\n  virtual void OnPreferenceChanged(const std::string& pref_name,\r\n                                  const base::Value& old_value,\r\n                                  const base::Value& new_value) {}\r\n  \r\n  // Called for Privacy Sandbox preference changes\r\n  virtual void OnPrivacySandboxPrefChanged(const std::string& pref_name,\r\n                                          bool enabled) {}\r\n  \r\n  // Called for security-related preference changes\r\n  virtual void OnSecurityPrefChanged(const std::string& pref_name,\r\n                                    const base::Value& value) {}\r\n};\r\n\r\nclass ModernPrefService {\r\n public:\r\n  void AddObserver(PrefObserver* observer) {\r\n    observers_.AddObserver(observer);\r\n  }\r\n  \r\n  void RemoveObserver(PrefObserver* observer) {\r\n    observers_.RemoveObserver(observer);\r\n  }\r\n  \r\n  void SetUserPref(const std::string& pref_name, base::Value value) {\r\n    auto old_value = GetValue(pref_name);\r\n    \r\n    // Update preference\r\n    user_prefs_[pref_name] = value.Clone();\r\n    \r\n    // Notify observers\r\n    observers_.NotifyObservers(&PrefObserver::OnPreferenceChanged,\r\n                              pref_name, old_value, value);\r\n    \r\n    // Handle special preference categories\r\n    if (IsPrivacySandboxPref(pref_name)) {\r\n      bool enabled = value.is_bool() ? value.GetBool() : false;\r\n      observers_.NotifyObservers(&PrefObserver::OnPrivacySandboxPrefChanged,\r\n                                pref_name, enabled);\r\n    }\r\n    \r\n    if (IsSecurityPref(pref_name)) {\r\n      observers_.NotifyObservers(&PrefObserver::OnSecurityPrefChanged,\r\n                                pref_name, value);\r\n    }\r\n  }\r\n  \r\n private:\r\n  bool IsPrivacySandboxPref(const std::string& pref_name) const {\r\n    return base::StartsWith(pref_name, \"privacy_sandbox.\") ||\r\n           pref_name == \"profile.cookie_controls_mode\";\r\n  }\r\n  \r\n  bool IsSecurityPref(const std::string& pref_name) const {\r\n    return base::StartsWith(pref_name, \"security.\") ||\r\n           pref_name == \"safebrowsing.enabled\";\r\n  }\r\n  \r\n  base::Value GetValue(const std::string& pref_name) const {\r\n    auto it = user_prefs_.find(pref_name);\r\n    return it != user_prefs_.end() ? it->second.Clone() : base::Value();\r\n  }\r\n  \r\n  ThreadSafeObserverList<PrefObserver> observers_;\r\n  std::unordered_map<std::string, base::Value> user_prefs_;\r\n};\r\n```\r\n\r\n---\r\n\r\n## 8. Observer Pattern Summary (v134+)\r\n\r\n### Key Advantages\r\n- **Decoupled Architecture**: Subjects and observers remain loosely coupled\r\n- **Real-Time Updates**: Immediate notification of state changes\r\n- **Scalability**: Multiple observers can monitor a single subject\r\n- **Testability**: Easy mocking and verification of event handling\r\n- **Flexibility**: Dynamic subscription and unsubscription\r\n\r\n### Modern Enhancements (v134+)\r\n- **Memory Safety**: Automatic cleanup using weak pointers and RAII\r\n- **Thread Safety**: Safe cross-thread notification mechanisms\r\n- **Performance**: Optimized notification with batching and filtering\r\n- **Service Integration**: Seamless Mojo IPC for cross-process observation\r\n- **Error Handling**: Robust error propagation and recovery\r\n\r\n### Best Practices\r\n1. **Use weak pointers** to prevent memory leaks and dangling references\r\n2. **Implement proper cleanup** in destructors and scope guards\r\n3. **Consider batching** for high-frequency events to improve performance\r\n4. **Apply filtering** to reduce unnecessary notifications\r\n5. **Test thoroughly** with mock observers and comprehensive scenarios\r\n6. **Document event contracts** clearly for maintainable code\r\n7. **Handle edge cases** like observer removal during notification\r\n\r\nThe Observer pattern continues to be essential in modern Chromium architecture, providing the foundation for event-driven communication across the complex browser ecosystem while maintaining performance, security, and maintainability standards.\r\n"
  },
  {
    "path": "architecture/design-patterns/factory-pattern",
    "title": "Factory Pattern in Modern Chromium (v134+)",
    "content": "# Factory Pattern in Modern Chromium (v134+)\r\n\r\nThe Factory pattern is fundamental to Chromium's architecture, providing flexible object creation that abstracts implementation details and enables platform-specific, feature-gated, and service-oriented instantiation. In modern Chromium v134+, factory patterns have evolved to support Mojo services, modern C++ practices, and sophisticated dependency injection.\r\n\r\n---\r\n\r\n## 1. Modern Factory Pattern Evolution (v134+)\r\n\r\n### Core Principles\r\n- **Abstraction**: Hide complex instantiation logic behind clean interfaces\r\n- **Flexibility**: Support multiple implementations based on runtime conditions\r\n- **Testability**: Enable easy mocking and dependency injection for tests\r\n- **Service Integration**: Seamless integration with Mojo service architecture\r\n- **Memory Safety**: Modern C++ RAII and smart pointer practices\r\n\r\n### Contemporary Applications\r\n- **Service Factories**: Creating Mojo service implementations\r\n- **Platform Abstraction**: OS-specific component instantiation\r\n- **Feature Flags**: Conditional feature implementation based on experiments\r\n- **Renderer Components**: Process-safe object creation with IPC awareness\r\n- **Privacy Sandbox**: Secure component instantiation with capability restrictions\r\n\r\n---\r\n\r\n## 2. Factory Pattern Types in Chromium v134+\r\n\r\n### 2.1. Static Factory Methods\r\n\r\nModern static factories with enhanced type safety and error handling:\r\n\r\n```cpp\r\n// Modern factory with base::expected for error handling\r\nclass NetworkContextFactory {\r\n public:\r\n  static base::expected<std::unique_ptr<NetworkContext>, NetworkError>\r\n  CreateForProfile(Profile* profile, \r\n                   const NetworkContextParams& params) {\r\n    if (!profile || !profile->IsValidForNetworking()) {\r\n      return base::unexpected(NetworkError::kInvalidProfile);\r\n    }\r\n    \r\n    auto context = std::make_unique<NetworkContextImpl>(profile);\r\n    if (auto result = context->Initialize(params); !result.has_value()) {\r\n      return base::unexpected(result.error());\r\n    }\r\n    \r\n    return context;\r\n  }\r\n  \r\n  // Factory method with modern C++20 concepts\r\n  template<typename ContextType>\r\n    requires std::derived_from<ContextType, NetworkContext>\r\n  static std::unique_ptr<ContextType> CreateCustomContext(\r\n      const CustomNetworkParams& params) {\r\n    return std::make_unique<ContextType>(params);\r\n  }\r\n};\r\n```\r\n\r\n### 2.2. Mojo Service Factories\r\n\r\nModern service factories integrated with Mojo IPC and capability-based security:\r\n\r\n```cpp\r\n// Service factory with Mojo integration\r\nclass DownloadServiceFactory {\r\n public:\r\n  static void Create(\r\n      Profile* profile,\r\n      mojo::PendingReceiver<download::mojom::DownloadService> receiver) {\r\n    // Validate profile and permissions\r\n    if (!profile || !profile->HasDownloadPermission()) {\r\n      receiver.ResetWithReason(1, \"Insufficient permissions\");\r\n      return;\r\n    }\r\n    \r\n    // Create service with proper context\r\n    auto service = std::make_unique<DownloadServiceImpl>(\r\n        profile->GetPath(),\r\n        profile->GetPrefs(),\r\n        profile->GetDownloadManager());\r\n    \r\n    // Bind with automatic cleanup on disconnect\r\n    mojo::MakeSelfOwnedReceiver(std::move(service), std::move(receiver));\r\n  }\r\n  \r\n  // Factory for testing with dependency injection\r\n  static std::unique_ptr<download::mojom::DownloadService> CreateForTesting(\r\n      std::unique_ptr<DownloadManager> manager,\r\n      std::unique_ptr<PrefService> prefs) {\r\n    return std::make_unique<MockDownloadService>(\r\n        std::move(manager), std::move(prefs));\r\n  }\r\n};\r\n```\r\n\r\n### 2.3. Abstract Factory Pattern\r\n\r\nModern abstract factories for platform-specific and feature-specific implementations:\r\n\r\n```cpp\r\n// Abstract factory for UI components with modern C++\r\nclass UIComponentFactory {\r\n public:\r\n  virtual ~UIComponentFactory() = default;\r\n  \r\n  // Pure virtual factory methods\r\n  virtual std::unique_ptr<TabStripModel> CreateTabStripModel(\r\n      Browser* browser) = 0;\r\n  virtual std::unique_ptr<LocationBar> CreateLocationBar(\r\n      Browser* browser) = 0;\r\n  virtual std::unique_ptr<BookmarkBar> CreateBookmarkBar(\r\n      Browser* browser) = 0;\r\n  \r\n  // Static factory to get platform-specific implementation\r\n  static std::unique_ptr<UIComponentFactory> Create();\r\n};\r\n\r\n// Platform-specific implementation\r\nclass WindowsUIComponentFactory : public UIComponentFactory {\r\n public:\r\n  std::unique_ptr<TabStripModel> CreateTabStripModel(\r\n      Browser* browser) override {\r\n    return std::make_unique<WindowsTabStripModel>(browser);\r\n  }\r\n  \r\n  std::unique_ptr<LocationBar> CreateLocationBar(\r\n      Browser* browser) override {\r\n    return std::make_unique<WindowsLocationBar>(browser);\r\n  }\r\n  \r\n  std::unique_ptr<BookmarkBar> CreateBookmarkBar(\r\n      Browser* browser) override {\r\n    return std::make_unique<WindowsBookmarkBar>(browser);\r\n  }\r\n};\r\n```\r\n\r\n### 2.4. Builder-Style Factories\r\n\r\nModern fluent interfaces for complex object construction:\r\n\r\n```cpp\r\n// Builder-style factory for complex configuration\r\nclass RenderFrameHostFactory {\r\n public:\r\n  class Builder {\r\n   public:\r\n    Builder& SetSiteInstance(scoped_refptr<SiteInstance> site_instance) {\r\n      site_instance_ = std::move(site_instance);\r\n      return *this;\r\n    }\r\n    \r\n    Builder& SetRenderViewHost(RenderViewHost* render_view_host) {\r\n      render_view_host_ = render_view_host;\r\n      return *this;\r\n    }\r\n    \r\n    Builder& SetFrameTreeNode(FrameTreeNode* frame_tree_node) {\r\n      frame_tree_node_ = frame_tree_node;\r\n      return *this;\r\n    }\r\n    \r\n    Builder& EnableFeatures(const std::vector<std::string>& features) {\r\n      enabled_features_.insert(enabled_features_.end(), \r\n                              features.begin(), features.end());\r\n      return *this;\r\n    }\r\n    \r\n    // Build with validation and error handling\r\n    base::expected<std::unique_ptr<RenderFrameHost>, FrameError> Build() {\r\n      if (!site_instance_ || !render_view_host_ || !frame_tree_node_) {\r\n        return base::unexpected(FrameError::kMissingRequiredComponents);\r\n      }\r\n      \r\n      auto frame_host = std::make_unique<RenderFrameHostImpl>(\r\n          site_instance_, render_view_host_, frame_tree_node_);\r\n      \r\n      // Apply feature configuration\r\n      for (const auto& feature : enabled_features_) {\r\n        frame_host->EnableFeature(feature);\r\n      }\r\n      \r\n      return frame_host;\r\n    }\r\n    \r\n   private:\r\n    scoped_refptr<SiteInstance> site_instance_;\r\n    RenderViewHost* render_view_host_ = nullptr;\r\n    FrameTreeNode* frame_tree_node_ = nullptr;\r\n    std::vector<std::string> enabled_features_;\r\n  };\r\n  \r\n  static Builder CreateBuilder() { return Builder{}; }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 3. Key Factory Implementations in v134+\r\n\r\n### 3.1. BrowserContextKeyedServiceFactory\r\n\r\nModern service factory with enhanced lifecycle management:\r\n\r\n```cpp\r\nclass DownloadCoreServiceFactory : public BrowserContextKeyedServiceFactory {\r\n public:\r\n  static DownloadCoreService* GetForBrowserContext(BrowserContext* context) {\r\n    return static_cast<DownloadCoreService*>(\r\n        GetInstance()->GetServiceForBrowserContext(context, true));\r\n  }\r\n  \r\n  static DownloadCoreServiceFactory* GetInstance() {\r\n    static base::NoDestructor<DownloadCoreServiceFactory> instance;\r\n    return instance.get();\r\n  }\r\n  \r\n private:\r\n  friend base::NoDestructor<DownloadCoreServiceFactory>;\r\n  \r\n  DownloadCoreServiceFactory()\r\n      : BrowserContextKeyedServiceFactory(\r\n            \"DownloadCoreService\",\r\n            BrowserContextDependencyManager::GetInstance()) {\r\n    // Declare dependencies\r\n    DependsOn(SimpleDownloadManagerCoordinatorFactory::GetInstance());\r\n    DependsOn(HistoryServiceFactory::GetInstance());\r\n  }\r\n  \r\n  KeyedService* BuildServiceInstanceFor(\r\n      content::BrowserContext* context) const override {\r\n    if (!base::FeatureList::IsEnabled(features::kDownloadService)) {\r\n      return nullptr;\r\n    }\r\n    \r\n    auto* profile = Profile::FromBrowserContext(context);\r\n    return new DownloadCoreServiceImpl(\r\n        profile, \r\n        std::make_unique<DownloadDriverImpl>(),\r\n        std::make_unique<TaskSchedulerImpl>());\r\n  }\r\n  \r\n  bool ServiceIsCreatedWithBrowserContext() const override { return true; }\r\n  bool ServiceIsNULLWhileTesting() const override { return false; }\r\n};\r\n```\r\n\r\n### 3.2. URLLoaderFactory Pattern\r\n\r\nModern network factory with enhanced security and capability delegation:\r\n\r\n```cpp\r\nclass NetworkServiceURLLoaderFactory {\r\n public:\r\n  // Create factory with proper security context\r\n  static mojo::PendingRemote<network::mojom::URLLoaderFactory>\r\n  CreateForFrame(RenderFrameHost* frame_host,\r\n                 const url::Origin& request_initiator) {\r\n    if (!frame_host || !frame_host->IsRenderFrameLive()) {\r\n      return mojo::PendingRemote<network::mojom::URLLoaderFactory>();\r\n    }\r\n    \r\n    // Create with appropriate security restrictions\r\n    auto factory_params = network::mojom::URLLoaderFactoryParams::New();\r\n    factory_params->process_id = frame_host->GetProcess()->GetID();\r\n    factory_params->request_initiator_origin_lock = request_initiator;\r\n    factory_params->is_corb_enabled = true;\r\n    factory_params->is_trusted = false;\r\n    \r\n    // Apply Content Security Policy\r\n    if (auto* policy = frame_host->GetContentSecurityPolicy()) {\r\n      factory_params->client_security_state = \r\n          policy->CreateClientSecurityState();\r\n    }\r\n    \r\n    mojo::PendingRemote<network::mojom::URLLoaderFactory> factory_remote;\r\n    frame_host->GetStoragePartition()\r\n        ->GetNetworkContext()\r\n        ->CreateURLLoaderFactory(\r\n            factory_remote.InitWithNewPipeAndPassReceiver(),\r\n            std::move(factory_params));\r\n    \r\n    return factory_remote;\r\n  }\r\n  \r\n  // Factory for service workers with restricted capabilities\r\n  static mojo::PendingRemote<network::mojom::URLLoaderFactory>\r\n  CreateForServiceWorker(\r\n      ServiceWorkerVersion* version,\r\n      const blink::StorageKey& storage_key) {\r\n    \r\n    auto factory_params = network::mojom::URLLoaderFactoryParams::New();\r\n    factory_params->process_id = version->embedded_worker()->process_id();\r\n    factory_params->request_initiator_origin_lock = storage_key.origin();\r\n    factory_params->is_trusted = false;\r\n    factory_params->automatically_assign_isolation_info = true;\r\n    \r\n    // Restrict to same-origin and specific schemes\r\n    factory_params->unsafe_non_webby_requestor = false;\r\n    \r\n    mojo::PendingRemote<network::mojom::URLLoaderFactory> factory_remote;\r\n    version->GetStoragePartition()\r\n        ->GetNetworkContext()\r\n        ->CreateURLLoaderFactory(\r\n            factory_remote.InitWithNewPipeAndPassReceiver(),\r\n            std::move(factory_params));\r\n    \r\n    return factory_remote;\r\n  }\r\n};\r\n```\r\n\r\n### 3.3. GPU Service Factory\r\n\r\nModern graphics factory with advanced GPU capabilities:\r\n\r\n```cpp\r\nclass GpuServiceFactory {\r\n public:\r\n  static std::unique_ptr<viz::GpuServiceImpl> Create(\r\n      const gpu::GpuPreferences& gpu_preferences,\r\n      base::WeakPtr<viz::ImageTransportSurfaceDelegate> delegate) {\r\n    \r\n    // Initialize GPU feature info\r\n    auto gpu_feature_info = gpu::GetGpuFeatureInfo(\r\n        gpu::GpuDriverBugWorkarounds(), gpu_preferences);\r\n    \r\n    // Create with WebGPU support if available\r\n    auto gpu_service = std::make_unique<viz::GpuServiceImpl>(\r\n        gpu_feature_info,\r\n        gpu_preferences,\r\n        std::move(delegate),\r\n        /*enable_webgpu=*/base::FeatureList::IsEnabled(features::kWebGPU));\r\n    \r\n    // Configure advanced features\r\n    if (gpu_feature_info.IsWebGPUSupported()) {\r\n      gpu_service->EnableWebGPUService();\r\n    }\r\n    \r\n    if (base::FeatureList::IsEnabled(features::kVaapiVideoDecoding)) {\r\n      gpu_service->EnableHardwareVideoDecoding();\r\n    }\r\n    \r\n    return gpu_service;\r\n  }\r\n  \r\n  // Factory for testing with mock GPU capabilities\r\n  static std::unique_ptr<viz::GpuServiceImpl> CreateForTesting(\r\n      std::unique_ptr<gpu::GpuMemoryBufferFactory> memory_buffer_factory,\r\n      std::unique_ptr<gpu::ImageFactory> image_factory) {\r\n    \r\n    auto gpu_service = std::make_unique<MockGpuServiceImpl>();\r\n    gpu_service->SetMemoryBufferFactory(std::move(memory_buffer_factory));\r\n    gpu_service->SetImageFactory(std::move(image_factory));\r\n    \r\n    return gpu_service;\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 4. Modern C++ Factory Techniques (v134+)\r\n\r\n### 4.1. Template-Based Factories\r\n\r\nUsing concepts and template metaprogramming for type-safe factories:\r\n\r\n```cpp\r\n// Concept for factory-creatable types\r\ntemplate<typename T>\r\nconcept FactoryCreatable = requires(T t) {\r\n  typename T::InitParams;\r\n  { T::Create(std::declval<typename T::InitParams>()) } \r\n    -> std::convertible_to<std::unique_ptr<T>>;\r\n};\r\n\r\n// Generic factory with compile-time validation\r\ntemplate<FactoryCreatable T>\r\nclass GenericFactory {\r\n public:\r\n  template<typename... Args>\r\n  static base::expected<std::unique_ptr<T>, FactoryError> \r\n  Create(Args&&... args) {\r\n    try {\r\n      auto params = typename T::InitParams{std::forward<Args>(args)...};\r\n      if (auto result = ValidateParams(params); !result.has_value()) {\r\n        return base::unexpected(result.error());\r\n      }\r\n      \r\n      return T::Create(std::move(params));\r\n    } catch (const std::exception& e) {\r\n      return base::unexpected(FactoryError::kCreationFailed);\r\n    }\r\n  }\r\n  \r\n private:\r\n  static base::expected<void, FactoryError> ValidateParams(\r\n      const typename T::InitParams& params) {\r\n    // Generic validation logic\r\n    if constexpr (requires { params.Validate(); }) {\r\n      return params.Validate() ? base::expected<void, FactoryError>{}\r\n                               : base::unexpected(FactoryError::kInvalidParams);\r\n    }\r\n    return {};\r\n  }\r\n};\r\n```\r\n\r\n### 4.2. Registry-Based Factories\r\n\r\nModern factory registration with type safety:\r\n\r\n```cpp\r\n// Factory registry for extensible component creation\r\ntemplate<typename BaseType>\r\nclass FactoryRegistry {\r\n public:\r\n  using FactoryFunction = std::function<std::unique_ptr<BaseType>()>;\r\n  using ConditionalFactory = std::function<std::unique_ptr<BaseType>(\r\n      const FactoryContext&)>;\r\n  \r\n  // Register factory with type information\r\n  template<typename DerivedType>\r\n    requires std::derived_from<DerivedType, BaseType>\r\n  static void RegisterFactory(const std::string& name,\r\n                              ConditionalFactory factory) {\r\n    GetRegistry()[name] = FactoryEntry{\r\n      .factory = std::move(factory),\r\n      .type_info = typeid(DerivedType),\r\n      .creation_flags = DerivedType::GetCreationFlags()\r\n    };\r\n  }\r\n  \r\n  // Create with automatic selection based on context\r\n  static base::expected<std::unique_ptr<BaseType>, FactoryError>\r\n  Create(const std::string& name, const FactoryContext& context) {\r\n    auto& registry = GetRegistry();\r\n    auto it = registry.find(name);\r\n    \r\n    if (it == registry.end()) {\r\n      return base::unexpected(FactoryError::kUnknownType);\r\n    }\r\n    \r\n    const auto& entry = it->second;\r\n    if (!IsCompatibleWithContext(entry, context)) {\r\n      return base::unexpected(FactoryError::kIncompatibleContext);\r\n    }\r\n    \r\n    try {\r\n      auto instance = entry.factory(context);\r\n      if (!instance) {\r\n        return base::unexpected(FactoryError::kCreationFailed);\r\n      }\r\n      \r\n      return instance;\r\n    } catch (const std::exception& e) {\r\n      LOG(ERROR) << \"Factory creation failed: \" << e.what();\r\n      return base::unexpected(FactoryError::kCreationFailed);\r\n    }\r\n  }\r\n  \r\n private:\r\n  struct FactoryEntry {\r\n    ConditionalFactory factory;\r\n    std::type_info type_info;\r\n    uint32_t creation_flags;\r\n  };\r\n  \r\n  static std::unordered_map<std::string, FactoryEntry>& GetRegistry() {\r\n    static base::NoDestructor<std::unordered_map<std::string, FactoryEntry>> \r\n        registry;\r\n    return *registry;\r\n  }\r\n  \r\n  static bool IsCompatibleWithContext(const FactoryEntry& entry,\r\n                                      const FactoryContext& context) {\r\n    return (entry.creation_flags & context.GetRequiredFlags()) == \r\n           context.GetRequiredFlags();\r\n  }\r\n};\r\n```\r\n\r\n### 4.3. Async Factory Pattern\r\n\r\nModern async factories with coroutine support:\r\n\r\n```cpp\r\n// Async factory for heavy initialization\r\nclass AsyncResourceFactory {\r\n public:\r\n  // Coroutine-based async factory\r\n  static base::expected<std::unique_ptr<Resource>, ResourceError>\r\n  CreateAsync(const ResourceParams& params) {\r\n    // Validate parameters synchronously\r\n    if (auto validation = ValidateParams(params); !validation.has_value()) {\r\n      return base::unexpected(validation.error());\r\n    }\r\n    \r\n    // Create resource with async initialization\r\n    auto resource = std::make_unique<ResourceImpl>(params);\r\n    \r\n    // Schedule async initialization\r\n    auto init_result = co_await resource->InitializeAsync();\r\n    if (!init_result.has_value()) {\r\n      return base::unexpected(init_result.error());\r\n    }\r\n    \r\n    return resource;\r\n  }\r\n  \r\n  // Traditional callback-based async factory\r\n  static void CreateAsync(\r\n      const ResourceParams& params,\r\n      base::OnceCallback<void(base::expected<std::unique_ptr<Resource>, \r\n                                           ResourceError>)> callback) {\r\n    \r\n    base::ThreadPool::PostTaskAndReplyWithResult(\r\n        FROM_HERE,\r\n        {base::TaskPriority::USER_BLOCKING, base::MayBlock()},\r\n        base::BindOnce(&DoAsyncCreation, params),\r\n        std::move(callback));\r\n  }\r\n  \r\n private:\r\n  static base::expected<std::unique_ptr<Resource>, ResourceError>\r\n  DoAsyncCreation(const ResourceParams& params) {\r\n    // Heavy initialization work on background thread\r\n    auto resource = std::make_unique<ResourceImpl>(params);\r\n    \r\n    if (auto result = resource->LoadData(); !result.has_value()) {\r\n      return base::unexpected(result.error());\r\n    }\r\n    \r\n    if (auto result = resource->InitializeConnections(); !result.has_value()) {\r\n      return base::unexpected(result.error());\r\n    }\r\n    \r\n    return resource;\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 5. Testing Factory Patterns (v134+)\r\n\r\n### 5.1. Test Factory Implementation\r\n\r\nModern test factories with comprehensive mocking:\r\n\r\n```cpp\r\nclass TestDownloadServiceFactory {\r\n public:\r\n  // Create mock service for unit tests\r\n  static std::unique_ptr<MockDownloadService> CreateMockService() {\r\n    auto mock_service = std::make_unique<MockDownloadService>();\r\n    \r\n    // Set up default expectations\r\n    EXPECT_CALL(*mock_service, StartDownload(testing::_))\r\n        .WillRepeatedly(testing::Return(DownloadResult::kSuccess));\r\n    \r\n    EXPECT_CALL(*mock_service, GetDownloadState(testing::_))\r\n        .WillRepeatedly(testing::Return(DownloadState::kComplete));\r\n    \r\n    return mock_service;\r\n  }\r\n  \r\n  // Create fake service for integration tests\r\n  static std::unique_ptr<FakeDownloadService> CreateFakeService(\r\n      const FakeServiceConfig& config) {\r\n    auto fake_service = std::make_unique<FakeDownloadService>();\r\n    \r\n    // Configure behavior based on test needs\r\n    fake_service->SetSimulatedBandwidth(config.bandwidth);\r\n    fake_service->SetFailureRate(config.failure_rate);\r\n    fake_service->SetLatencyRange(config.min_latency, config.max_latency);\r\n    \r\n    return fake_service;\r\n  }\r\n  \r\n  // Factory for parameterized tests\r\n  template<typename ServiceType>\r\n    requires std::derived_from<ServiceType, DownloadService>\r\n  static std::unique_ptr<ServiceType> CreateParameterizedService(\r\n      const TestParameters& params) {\r\n    auto service = std::make_unique<ServiceType>();\r\n    \r\n    // Apply test-specific configuration\r\n    for (const auto& [key, value] : params.GetConfigMap()) {\r\n      service->SetConfiguration(key, value);\r\n    }\r\n    \r\n    return service;\r\n  }\r\n};\r\n```\r\n\r\n### 5.2. Google Mock Integration\r\n\r\nModern factory testing with enhanced mock validation:\r\n\r\n```cpp\r\n// Test fixture with factory-created mocks\r\nclass DownloadServiceTest : public testing::Test {\r\n protected:\r\n  void SetUp() override {\r\n    // Create mock dependencies\r\n    mock_download_manager_ = std::make_unique<MockDownloadManager>();\r\n    mock_file_system_ = std::make_unique<MockFileSystem>();\r\n    mock_network_service_ = std::make_unique<MockNetworkService>();\r\n    \r\n    // Configure factory to use mocks\r\n    DownloadServiceFactory::SetDownloadManagerForTesting(\r\n        mock_download_manager_.get());\r\n    DownloadServiceFactory::SetFileSystemForTesting(\r\n        mock_file_system_.get());\r\n    DownloadServiceFactory::SetNetworkServiceForTesting(\r\n        mock_network_service_.get());\r\n    \r\n    // Create service under test\r\n    service_ = DownloadServiceFactory::CreateForTesting(\r\n        TestProfile::CreateProfile());\r\n  }\r\n  \r\n  void TearDown() override {\r\n    // Clean up factory state\r\n    DownloadServiceFactory::ResetForTesting();\r\n  }\r\n  \r\n  // Test helper methods\r\n  void ExpectSuccessfulDownload() {\r\n    EXPECT_CALL(*mock_download_manager_, StartDownload(testing::_))\r\n        .WillOnce(testing::Return(DownloadId{123}));\r\n    \r\n    EXPECT_CALL(*mock_file_system_, CreateTempFile(testing::_))\r\n        .WillOnce(testing::Return(base::FilePath(\"/tmp/download_123\")));\r\n    \r\n    EXPECT_CALL(*mock_network_service_, CreateURLLoader(testing::_))\r\n        .WillOnce(testing::Return(std::make_unique<MockURLLoader>()));\r\n  }\r\n  \r\n private:\r\n  std::unique_ptr<MockDownloadManager> mock_download_manager_;\r\n  std::unique_ptr<MockFileSystem> mock_file_system_;\r\n  std::unique_ptr<MockNetworkService> mock_network_service_;\r\n  std::unique_ptr<DownloadService> service_;\r\n};\r\n\r\nTEST_F(DownloadServiceTest, FactoryCreatesValidService) {\r\n  ASSERT_TRUE(service_);\r\n  EXPECT_TRUE(service_->IsInitialized());\r\n  EXPECT_FALSE(service_->HasActiveDownloads());\r\n}\r\n```\r\n\r\n---\r\n\r\n## 6. Advanced Factory Patterns (v134+)\r\n\r\n### 6.1. Dependency Injection Factory\r\n\r\nModern dependency injection with automatic resolution:\r\n\r\n```cpp\r\n// Dependency injection container\r\nclass DIContainer {\r\n public:\r\n  template<typename Interface, typename Implementation>\r\n    requires std::derived_from<Implementation, Interface>\r\n  void RegisterSingleton() {\r\n    auto factory = []() -> std::unique_ptr<Interface> {\r\n      return std::make_unique<Implementation>();\r\n    };\r\n    \r\n    singletons_[typeid(Interface)] = SingletonEntry{\r\n      .factory = std::move(factory),\r\n      .instance = nullptr\r\n    };\r\n  }\r\n  \r\n  template<typename Interface, typename Implementation>\r\n    requires std::derived_from<Implementation, Interface>\r\n  void RegisterTransient() {\r\n    auto factory = []() -> std::unique_ptr<Interface> {\r\n      return std::make_unique<Implementation>();\r\n    };\r\n    \r\n    transients_[typeid(Interface)] = std::move(factory);\r\n  }\r\n  \r\n  template<typename T>\r\n  std::unique_ptr<T> Resolve() {\r\n    const std::type_info& type = typeid(T);\r\n    \r\n    // Check for singleton\r\n    if (auto it = singletons_.find(type); it != singletons_.end()) {\r\n      auto& entry = it->second;\r\n      if (!entry.instance) {\r\n        entry.instance = entry.factory();\r\n      }\r\n      // Return a copy for singletons (or use weak_ptr pattern)\r\n      return std::unique_ptr<T>(static_cast<T*>(entry.instance.get()));\r\n    }\r\n    \r\n    // Check for transient\r\n    if (auto it = transients_.find(type); it != transients_.end()) {\r\n      return std::unique_ptr<T>(static_cast<T*>(it->second().release()));\r\n    }\r\n    \r\n    return nullptr;\r\n  }\r\n  \r\n private:\r\n  struct SingletonEntry {\r\n    std::function<std::unique_ptr<void>()> factory;\r\n    std::unique_ptr<void> instance;\r\n  };\r\n  \r\n  std::unordered_map<std::type_index, SingletonEntry> singletons_;\r\n  std::unordered_map<std::type_index, \r\n                     std::function<std::unique_ptr<void>()>> transients_;\r\n};\r\n```\r\n\r\n### 6.2. Plugin Factory System\r\n\r\nModern plugin architecture with dynamic loading:\r\n\r\n```cpp\r\n// Plugin factory for extensible components\r\nclass PluginFactory {\r\n public:\r\n  struct PluginInfo {\r\n    std::string name;\r\n    std::string version;\r\n    std::vector<std::string> dependencies;\r\n    std::function<std::unique_ptr<Plugin>()> creator;\r\n  };\r\n  \r\n  static void RegisterPlugin(const PluginInfo& info) {\r\n    auto& registry = GetRegistry();\r\n    \r\n    // Validate dependencies\r\n    for (const auto& dep : info.dependencies) {\r\n      if (registry.find(dep) == registry.end()) {\r\n        LOG(WARNING) << \"Plugin \" << info.name \r\n                     << \" depends on unregistered plugin: \" << dep;\r\n      }\r\n    }\r\n    \r\n    registry[info.name] = info;\r\n  }\r\n  \r\n  static base::expected<std::unique_ptr<Plugin>, PluginError>\r\n  CreatePlugin(const std::string& name) {\r\n    auto& registry = GetRegistry();\r\n    auto it = registry.find(name);\r\n    \r\n    if (it == registry.end()) {\r\n      return base::unexpected(PluginError::kPluginNotFound);\r\n    }\r\n    \r\n    const auto& info = it->second;\r\n    \r\n    // Check and load dependencies\r\n    for (const auto& dep : info.dependencies) {\r\n      if (auto result = EnsurePluginLoaded(dep); !result.has_value()) {\r\n        return base::unexpected(result.error());\r\n      }\r\n    }\r\n    \r\n    try {\r\n      auto plugin = info.creator();\r\n      if (!plugin) {\r\n        return base::unexpected(PluginError::kCreationFailed);\r\n      }\r\n      \r\n      // Initialize plugin\r\n      if (auto result = plugin->Initialize(); !result.has_value()) {\r\n        return base::unexpected(PluginError::kInitializationFailed);\r\n      }\r\n      \r\n      return plugin;\r\n    } catch (const std::exception& e) {\r\n      LOG(ERROR) << \"Plugin creation failed: \" << e.what();\r\n      return base::unexpected(PluginError::kCreationFailed);\r\n    }\r\n  }\r\n  \r\n private:\r\n  static std::unordered_map<std::string, PluginInfo>& GetRegistry() {\r\n    static base::NoDestructor<std::unordered_map<std::string, PluginInfo>> \r\n        registry;\r\n    return *registry;\r\n  }\r\n  \r\n  static std::unordered_set<std::string>& GetLoadedPlugins() {\r\n    static base::NoDestructor<std::unordered_set<std::string>> loaded;\r\n    return *loaded;\r\n  }\r\n  \r\n  static base::expected<void, PluginError> EnsurePluginLoaded(\r\n      const std::string& name) {\r\n    auto& loaded = GetLoadedPlugins();\r\n    if (loaded.find(name) != loaded.end()) {\r\n      return {};  // Already loaded\r\n    }\r\n    \r\n    auto plugin_result = CreatePlugin(name);\r\n    if (!plugin_result.has_value()) {\r\n      return base::unexpected(plugin_result.error());\r\n    }\r\n    \r\n    loaded.insert(name);\r\n    return {};\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 7. Factory Pattern Best Practices (v134+)\r\n\r\n### 7.1. Error Handling and Validation\r\n\r\n```cpp\r\n// Modern error handling in factories\r\nclass SecureComponentFactory {\r\n public:\r\n  static base::expected<std::unique_ptr<SecureComponent>, SecurityError>\r\n  CreateSecureComponent(const SecurityContext& context) {\r\n    // Pre-creation validation\r\n    if (auto validation = ValidateSecurityContext(context); \r\n        !validation.has_value()) {\r\n      return base::unexpected(validation.error());\r\n    }\r\n    \r\n    // Capability check\r\n    if (!context.HasRequiredCapabilities()) {\r\n      return base::unexpected(SecurityError::kInsufficientCapabilities);\r\n    }\r\n    \r\n    // Create with security restrictions\r\n    auto component = std::make_unique<SecureComponentImpl>(context);\r\n    \r\n    // Post-creation verification\r\n    if (auto verification = VerifyComponent(*component); \r\n        !verification.has_value()) {\r\n      return base::unexpected(verification.error());\r\n    }\r\n    \r\n    // Set up security monitoring\r\n    component->EnableSecurityMonitoring();\r\n    \r\n    return component;\r\n  }\r\n  \r\n private:\r\n  static base::expected<void, SecurityError> ValidateSecurityContext(\r\n      const SecurityContext& context) {\r\n    if (!context.IsValid()) {\r\n      return base::unexpected(SecurityError::kInvalidContext);\r\n    }\r\n    \r\n    if (context.GetTrustLevel() < SecurityLevel::kMinimumRequired) {\r\n      return base::unexpected(SecurityError::kInsufficientTrustLevel);\r\n    }\r\n    \r\n    return {};\r\n  }\r\n  \r\n  static base::expected<void, SecurityError> VerifyComponent(\r\n      const SecureComponent& component) {\r\n    if (!component.IsProperlySandboxed()) {\r\n      return base::unexpected(SecurityError::kSandboxingFailed);\r\n    }\r\n    \r\n    if (!component.HasValidSignature()) {\r\n      return base::unexpected(SecurityError::kInvalidSignature);\r\n    }\r\n    \r\n    return {};\r\n  }\r\n};\r\n```\r\n\r\n### 7.2. Performance Optimization\r\n\r\n```cpp\r\n// High-performance factory with object pooling\r\ntemplate<typename T>\r\nclass PooledFactory {\r\n public:\r\n  // Get object from pool or create new one\r\n  static std::unique_ptr<T> Acquire() {\r\n    auto& pool = GetPool();\r\n    \r\n    if (!pool.empty()) {\r\n      auto obj = std::move(pool.back());\r\n      pool.pop_back();\r\n      \r\n      // Reset object state\r\n      obj->Reset();\r\n      return obj;\r\n    }\r\n    \r\n    // Create new object if pool is empty\r\n    return std::make_unique<T>();\r\n  }\r\n  \r\n  // Return object to pool for reuse\r\n  static void Release(std::unique_ptr<T> obj) {\r\n    if (!obj) return;\r\n    \r\n    auto& pool = GetPool();\r\n    \r\n    // Limit pool size to prevent memory bloat\r\n    constexpr size_t kMaxPoolSize = 64;\r\n    if (pool.size() < kMaxPoolSize) {\r\n      pool.push_back(std::move(obj));\r\n    }\r\n    // Object is automatically destroyed if pool is full\r\n  }\r\n  \r\n  // Pre-warm pool with objects\r\n  static void PrewarmPool(size_t count) {\r\n    auto& pool = GetPool();\r\n    pool.reserve(count);\r\n    \r\n    for (size_t i = 0; i < count; ++i) {\r\n      pool.push_back(std::make_unique<T>());\r\n    }\r\n  }\r\n  \r\n private:\r\n  static std::vector<std::unique_ptr<T>>& GetPool() {\r\n    static base::NoDestructor<std::vector<std::unique_ptr<T>>> pool;\r\n    return *pool;\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 8. Real-World Examples from Chromium v134+\r\n\r\n### 8.1. Content Settings Factory\r\n\r\n```cpp\r\n// Simplified version of HostContentSettingsMapFactory\r\nclass HostContentSettingsMapFactory : public RefcountedBrowserContextKeyedServiceFactory {\r\n public:\r\n  static HostContentSettingsMap* GetForProfile(Profile* profile) {\r\n    return static_cast<HostContentSettingsMap*>(\r\n        GetInstance()->GetServiceForBrowserContext(profile, true).get());\r\n  }\r\n  \r\n  static HostContentSettingsMapFactory* GetInstance() {\r\n    static base::NoDestructor<HostContentSettingsMapFactory> instance;\r\n    return instance.get();\r\n  }\r\n  \r\n private:\r\n  HostContentSettingsMapFactory()\r\n      : RefcountedBrowserContextKeyedServiceFactory(\r\n            \"HostContentSettingsMap\",\r\n            BrowserContextDependencyManager::GetInstance()) {\r\n    DependsOn(PrefServiceSyncableFromProfile::GetInstance());\r\n  }\r\n  \r\n  scoped_refptr<RefcountedKeyedService> BuildServiceInstanceFor(\r\n      content::BrowserContext* context) const override {\r\n    Profile* profile = Profile::FromBrowserContext(context);\r\n    \r\n    auto map = base::MakeRefCounted<HostContentSettingsMapImpl>(\r\n        profile->GetPrefs(),\r\n        profile->IsOffTheRecord(),\r\n        profile->IsGuestSession(),\r\n        profile->IsExtensionProfile());\r\n    \r\n    // Configure for modern privacy features\r\n    if (base::FeatureList::IsEnabled(features::kPrivacySandboxSettings)) {\r\n      map->EnablePrivacySandboxSupport();\r\n    }\r\n    \r\n    return map;\r\n  }\r\n};\r\n```\r\n\r\n### 8.2. Media Device Factory\r\n\r\n```cpp\r\n// Factory for media device access with permissions\r\nclass MediaDeviceFactory {\r\n public:\r\n  static void CreateMediaDeviceService(\r\n      RenderFrameHost* frame_host,\r\n      mojo::PendingReceiver<blink::mojom::MediaDevicesDispatcherHost> receiver) {\r\n    \r\n    if (!frame_host || !frame_host->IsRenderFrameLive()) {\r\n      receiver.ResetWithReason(1, \"Invalid frame\");\r\n      return;\r\n    }\r\n    \r\n    // Check permissions\r\n    auto* permission_controller = \r\n        frame_host->GetBrowserContext()->GetPermissionController();\r\n    \r\n    auto mic_status = permission_controller->GetPermissionStatus(\r\n        ContentSettingsType::MEDIASTREAM_MIC,\r\n        frame_host->GetLastCommittedOrigin(),\r\n        frame_host->GetLastCommittedOrigin());\r\n    \r\n    auto camera_status = permission_controller->GetPermissionStatus(\r\n        ContentSettingsType::MEDIASTREAM_CAMERA,\r\n        frame_host->GetLastCommittedOrigin(),\r\n        frame_host->GetLastCommittedOrigin());\r\n    \r\n    // Create service with appropriate capabilities\r\n    auto service = std::make_unique<MediaDevicesDispatcherHostImpl>(\r\n        frame_host->GetProcess()->GetID(),\r\n        frame_host->GetRoutingID(),\r\n        mic_status == blink::mojom::PermissionStatus::GRANTED,\r\n        camera_status == blink::mojom::PermissionStatus::GRANTED);\r\n    \r\n    mojo::MakeSelfOwnedReceiver(std::move(service), std::move(receiver));\r\n  }\r\n};\r\n```\r\n\r\n---\r\n\r\n## 9. Factory Pattern Summary (v134+)\r\n\r\n### Key Advantages\r\n- **Flexibility**: Easy to swap implementations based on runtime conditions\r\n- **Testability**: Simplified dependency injection and mocking\r\n- **Maintainability**: Centralized object creation logic\r\n- **Security**: Controlled instantiation with proper validation\r\n- **Performance**: Object pooling and lazy initialization support\r\n\r\n### Modern Considerations\r\n- **Memory Safety**: Use smart pointers and RAII principles\r\n- **Error Handling**: Leverage `base::expected` for graceful error propagation\r\n- **Async Support**: Modern async patterns with coroutines and callbacks\r\n- **Type Safety**: Template metaprogramming and concepts for compile-time validation\r\n- **Service Integration**: Seamless Mojo service creation and capability management\r\n\r\n### Best Practices\r\n1. **Always validate inputs** before object creation\r\n2. **Use `base::expected`** for error-aware factory methods\r\n3. **Implement proper cleanup** and resource management\r\n4. **Consider object pooling** for frequently created objects\r\n5. **Document factory contracts** and expected behavior\r\n6. **Test factory behavior** with comprehensive unit tests\r\n7. **Follow security guidelines** for capability-restricted components\r\n\r\nThe Factory pattern remains essential in modern Chromium architecture, enabling flexible, secure, and maintainable object creation across the complex browser ecosystem.\r\n"
  },
  {
    "path": "architecture/design-patterns/delegate-pattern",
    "title": "Delegate Pattern in Modern Chromium (v134+)",
    "content": "# Delegate Pattern in Modern Chromium (v134+)\r\n\r\nThe **Delegate Pattern** is a cornerstone architectural pattern in modern Chromium, enabling sophisticated component decoupling, dependency injection, and extensibility throughout the browser's complex service-oriented architecture. In v134+, this pattern has evolved to support advanced features like Mojo interfaces, capability-based security, and cross-process communication.\r\n\r\n---\r\n\r\n## Modern Delegate Pattern Overview (v134+)\r\n\r\nThe Delegate Pattern in Chromium v134+ represents a sophisticated evolution of traditional delegation, incorporating modern C++20/23 features, type safety, and integration with the browser's advanced service architecture.\r\n\r\n### Enhanced Key Features:\r\n- **Type-Safe Delegation**: Modern C++ templates and concepts for compile-time safety\r\n- **Mojo Integration**: Seamless integration with Chromium's IPC system\r\n- **Capability-Based Security**: Delegates respect process boundaries and security policies\r\n- **Service-Oriented Architecture**: Delegates work across Chromium's microservice ecosystem\r\n- **Async/Await Support**: Modern asynchronous programming patterns\r\n- **Memory Safety**: Smart pointer usage and RAII principles\r\n\r\n---\r\n\r\n## Modern Chromium Architecture Integration (v134+)\r\n\r\nChromium's delegate pattern has evolved to support the browser's sophisticated multi-process, service-oriented architecture:\r\n\r\n### Service Manager Integration\r\n```cpp\r\n// Modern service-aware delegate pattern\r\nclass ServiceAwareDelegate {\r\n public:\r\n  virtual ~ServiceAwareDelegate() = default;\r\n  \r\n  // Capability-based access to services\r\n  virtual void OnServiceConnected(\r\n      mojo::PendingReceiver<mojom::SomeService> receiver) {}\r\n  \r\n  // Handle service disconnection gracefully\r\n  virtual void OnServiceDisconnected() {}\r\n  \r\n  // Modern async patterns with callbacks\r\n  virtual void ProcessRequestAsync(\r\n      mojom::RequestPtr request,\r\n      base::OnceCallback<void(mojom::ResponsePtr)> callback) = 0;\r\n};\r\n```\r\n\r\n### Cross-Process Delegate Communication\r\nModern delegates can operate across process boundaries using Mojo interfaces:\r\n\r\n```cpp\r\n// Mojo-based delegate for cross-process communication\r\nclass CrossProcessDelegate : public mojom::ProcessDelegate {\r\n public:\r\n  // Mojo interface implementation\r\n  void HandleCrossProcessRequest(\r\n      mojom::CrossProcessRequestPtr request,\r\n      HandleCrossProcessRequestCallback callback) override {\r\n    \r\n    // Process request with modern async patterns\r\n    ProcessRequestInBackground(\r\n        std::move(request),\r\n        base::BindOnce(&CrossProcessDelegate::OnRequestProcessed,\r\n                       weak_ptr_factory_.GetWeakPtr(),\r\n                       std::move(callback)));\r\n  }\r\n\r\n private:\r\n  void OnRequestProcessed(\r\n      HandleCrossProcessRequestCallback callback,\r\n      mojom::CrossProcessResponsePtr response) {\r\n    std::move(callback).Run(std::move(response));\r\n  }\r\n\r\n  base::WeakPtrFactory<CrossProcessDelegate> weak_ptr_factory_{this};\r\n};\r\n```\r\n\r\n---\r\n\r\n## Advanced Download Manager Example (v134+)\r\n\r\nThe modern Download Manager showcases sophisticated delegate patterns with enhanced security, performance, and integration:\r\n\r\n### Enhanced Download Manager Delegate\r\n```cpp\r\n// Modern download manager delegate with comprehensive capabilities\r\nclass DownloadManagerDelegate {\r\n public:\r\n  virtual ~DownloadManagerDelegate() = default;\r\n\r\n  // Security-enhanced path selection with sandbox integration\r\n  virtual base::FilePath DetermineDownloadTarget(\r\n      const GURL& url,\r\n      const std::string& suggested_filename,\r\n      const std::string& mime_type,\r\n      const std::optional<std::string>& content_disposition,\r\n      DownloadSecurity::ThreatLevel threat_level) = 0;\r\n\r\n  // Modern async validation with comprehensive security checks\r\n  virtual void ValidateDownloadAsync(\r\n      const base::FilePath& path,\r\n      const DownloadMetadata& metadata,\r\n      base::OnceCallback<void(DownloadValidationResult)> callback) = 0;\r\n\r\n  // Enhanced progress tracking with performance metrics\r\n  virtual void OnDownloadProgress(\r\n      const DownloadProgressInfo& progress,\r\n      const PerformanceMetrics& metrics) {}\r\n\r\n  // Integration with modern privacy controls\r\n  virtual bool ShouldBlockDownload(\r\n      const DownloadSecurityInfo& security_info,\r\n      const PrivacySettings& privacy_settings) = 0;\r\n\r\n  // Support for modern download features\r\n  virtual void HandleParallelDownload(\r\n      const ParallelDownloadConfig& config) {}\r\n  \r\n  // Integration with cloud services and sync\r\n  virtual void OnDownloadSyncRequest(\r\n      const DownloadSyncInfo& sync_info) {}\r\n};\r\n```\r\n\r\n### Production Implementation\r\n```cpp\r\n// Production-ready delegate with modern security and performance\r\nclass ProductionDownloadDelegate : public DownloadManagerDelegate {\r\n public:\r\n  explicit ProductionDownloadDelegate(\r\n      Profile* profile,\r\n      scoped_refptr<SafeBrowsingService> safe_browsing_service,\r\n      std::unique_ptr<QuarantineService> quarantine_service)\r\n      : profile_(profile),\r\n        safe_browsing_service_(std::move(safe_browsing_service)),\r\n        quarantine_service_(std::move(quarantine_service)) {}\r\n\r\n  base::FilePath DetermineDownloadTarget(\r\n      const GURL& url,\r\n      const std::string& suggested_filename,\r\n      const std::string& mime_type,\r\n      const std::optional<std::string>& content_disposition,\r\n      DownloadSecurity::ThreatLevel threat_level) override {\r\n    \r\n    // Modern path selection with security validation\r\n    auto sanitized_filename = SanitizeFilename(suggested_filename);\r\n    auto download_path = profile_->GetDownloadPath();\r\n    \r\n    // Apply threat-level specific restrictions\r\n    if (threat_level >= DownloadSecurity::ThreatLevel::DANGEROUS) {\r\n      return quarantine_service_->GetQuarantinePath(sanitized_filename);\r\n    }\r\n    \r\n    return download_path.Append(sanitized_filename);\r\n  }\r\n\r\n  void ValidateDownloadAsync(\r\n      const base::FilePath& path,\r\n      const DownloadMetadata& metadata,\r\n      base::OnceCallback<void(DownloadValidationResult)> callback) override {\r\n    \r\n    // Modern async validation chain\r\n    safe_browsing_service_->CheckDownloadAsync(\r\n        path, metadata,\r\n        base::BindOnce(&ProductionDownloadDelegate::OnSafeBrowsingCheck,\r\n                       weak_ptr_factory_.GetWeakPtr(),\r\n                       path, std::move(callback)));\r\n  }\r\n\r\n private:\r\n  void OnSafeBrowsingCheck(\r\n      const base::FilePath& path,\r\n      base::OnceCallback<void(DownloadValidationResult)> callback,\r\n      SafeBrowsingResult result) {\r\n    \r\n    if (result.is_safe()) {\r\n      // Proceed with additional validation\r\n      quarantine_service_->ScanFileAsync(\r\n          path,\r\n          base::BindOnce(&ProductionDownloadDelegate::OnQuarantineScan,\r\n                         weak_ptr_factory_.GetWeakPtr(),\r\n                         std::move(callback)));\r\n    } else {\r\n      std::move(callback).Run(\r\n          DownloadValidationResult::CreateBlocked(result.threat_type()));\r\n    }\r\n  }\r\n\r\n  void OnQuarantineScan(\r\n      base::OnceCallback<void(DownloadValidationResult)> callback,\r\n      QuarantineResult result) {\r\n    std::move(callback).Run(\r\n        result.is_clean() \r\n            ? DownloadValidationResult::CreateAllowed()\r\n            : DownloadValidationResult::CreateQuarantined());\r\n  }\r\n\r\n  raw_ptr<Profile> profile_;\r\n  scoped_refptr<SafeBrowsingService> safe_browsing_service_;\r\n  std::unique_ptr<QuarantineService> quarantine_service_;\r\n  base::WeakPtrFactory<ProductionDownloadDelegate> weak_ptr_factory_{this};\r\n};\r\n```\r\n\r\n### Modern UML Architecture\r\n```text\r\n┌─────────────────────┐    delegates to    ┌──────────────────────────┐\r\n│   DownloadManager   │ ──────────────────► │  DownloadManagerDelegate │\r\n│                     │                     │                          │\r\n│ + StartDownload()   │                     │ + DetermineDownloadTarget│\r\n│ + PauseDownload()   │                     │ + ValidateDownloadAsync  │\r\n│ + CancelDownload()  │                     │ + OnDownloadProgress     │\r\n│ + SetDelegate()     │                     │ + ShouldBlockDownload    │\r\n└─────────────────────┘                     └──────────────────────────┘\r\n         │                                              △\r\n         │                                              │\r\n         ▼                                              │\r\n┌─────────────────────┐                     ┌──────────────────────────┐\r\n│   Mojo IPC Layer    │                     │ ProductionDownloadDelegate│\r\n│                     │                     │                          │\r\n│ + SendProgress()    │                     │ + Integration with:      │\r\n│ + HandleErrors()    │                     │   - SafeBrowsingService  │\r\n│ + CrossProcess()    │                     │   - QuarantineService    │\r\n└─────────────────────┘                     │   - Privacy Controls     │\r\n                                            │   - Cloud Sync           │\r\n                                            └──────────────────────────┘\r\n```\r\n\r\n---\r\n\r\n## Modern Testing Patterns (v134+)\r\n\r\nAdvanced testing delegates leverage modern C++ features and Chromium's testing infrastructure:\r\n\r\n### Enhanced Test Delegate\r\n```cpp\r\n// Modern test delegate with comprehensive mocking capabilities\r\nclass MockDownloadManagerDelegate : public DownloadManagerDelegate {\r\n public:\r\n  MOCK_METHOD(base::FilePath, DetermineDownloadTarget,\r\n              (const GURL& url,\r\n               const std::string& suggested_filename,\r\n               const std::string& mime_type,\r\n               const std::optional<std::string>& content_disposition,\r\n               DownloadSecurity::ThreatLevel threat_level), (override));\r\n\r\n  MOCK_METHOD(void, ValidateDownloadAsync,\r\n              (const base::FilePath& path,\r\n               const DownloadMetadata& metadata,\r\n               base::OnceCallback<void(DownloadValidationResult)> callback),\r\n              (override));\r\n\r\n  MOCK_METHOD(bool, ShouldBlockDownload,\r\n              (const DownloadSecurityInfo& security_info,\r\n               const PrivacySettings& privacy_settings), (override));\r\n\r\n  // Helper methods for test setup\r\n  void SetupSuccessfulValidation() {\r\n    ON_CALL(*this, ValidateDownloadAsync)\r\n        .WillByDefault([](const base::FilePath&,\r\n                         const DownloadMetadata&,\r\n                         base::OnceCallback<void(DownloadValidationResult)> callback) {\r\n          std::move(callback).Run(DownloadValidationResult::CreateAllowed());\r\n        });\r\n  }\r\n\r\n  void SetupSecurityBlocking() {\r\n    ON_CALL(*this, ShouldBlockDownload)\r\n        .WillByDefault(Return(true));\r\n  }\r\n};\r\n```\r\n\r\n### Modern Test Implementation\r\n```cpp\r\n// Advanced test case with modern patterns\r\nclass DownloadManagerTest : public testing::Test {\r\n public:\r\n  void SetUp() override {\r\n    // Modern dependency injection with mock services\r\n    auto mock_delegate = std::make_unique<MockDownloadManagerDelegate>();\r\n    mock_delegate_ = mock_delegate.get();\r\n    \r\n    download_manager_ = std::make_unique<DownloadManager>(\r\n        &profile_, std::move(mock_delegate));\r\n  }\r\n\r\n  void TearDown() override {\r\n    // Modern RAII cleanup\r\n    download_manager_.reset();\r\n  }\r\n\r\n protected:\r\n  base::test::TaskEnvironment task_environment_;\r\n  TestingProfile profile_;\r\n  std::unique_ptr<DownloadManager> download_manager_;\r\n  raw_ptr<MockDownloadManagerDelegate> mock_delegate_;\r\n};\r\n\r\nTEST_F(DownloadManagerTest, HandlesSecureDownloadWithModernValidation) {\r\n  // Setup modern expectations\r\n  mock_delegate_->SetupSuccessfulValidation();\r\n  \r\n  EXPECT_CALL(*mock_delegate_, DetermineDownloadTarget)\r\n      .WillOnce(Return(base::FilePath(FILE_PATH_LITERAL(\"/safe/downloads/file.pdf\"))));\r\n  \r\n  EXPECT_CALL(*mock_delegate_, ShouldBlockDownload)\r\n      .WillOnce(Return(false));\r\n\r\n  // Execute with modern async patterns\r\n  base::RunLoop run_loop;\r\n  download_manager_->StartDownload(\r\n      GURL(\"https://example.com/secure-file.pdf\"),\r\n      \"secure-file.pdf\",\r\n      base::BindLambdaForTesting([&](DownloadResult result) {\r\n        EXPECT_EQ(result.status(), DownloadStatus::SUCCESS);\r\n        run_loop.Quit();\r\n      }));\r\n  \r\n  run_loop.Run();\r\n}\r\n```\r\n\r\n---\r\n\r\n## Advanced Delegate Patterns in v134+\r\n\r\n### Service-Oriented Delegates\r\nModern Chromium uses delegates extensively in its service architecture:\r\n\r\n```cpp\r\n// Network service delegate with modern capabilities\r\nclass NetworkServiceDelegate : public mojom::NetworkServiceClient {\r\n public:\r\n  // Modern SSL certificate handling\r\n  void OnCertificateRequested(\r\n      const std::optional<base::UnguessableToken>& window_id,\r\n      uint32_t process_id,\r\n      uint32_t routing_id,\r\n      const scoped_refptr<net::SSLCertRequestInfo>& cert_info,\r\n      mojo::PendingRemote<mojom::ClientCertificateResponder> responder) override;\r\n\r\n  // Advanced privacy controls\r\n  void OnCanSendReportingReports(\r\n      const std::vector<url::Origin>& origins,\r\n      base::OnceCallback<void(const std::vector<url::Origin>&)> callback) override;\r\n\r\n  // Modern cookie management with privacy sandbox\r\n  void OnCookiesRead(\r\n      const GURL& url,\r\n      const GURL& site_for_cookies,\r\n      const net::CookieAccessResultList& cookie_list,\r\n      OnCookiesReadCallback callback) override;\r\n};\r\n```\r\n\r\n### GPU Process Delegates\r\nAdvanced graphics delegates for the Viz compositor:\r\n\r\n```cpp\r\n// Modern GPU process delegate with Viz integration\r\nclass GpuProcessDelegate : public mojom::GpuHost {\r\n public:\r\n  // Advanced GPU memory management\r\n  void DidCreateContextSuccessfully() override;\r\n  \r\n  // Modern graphics diagnostics\r\n  void DidCreateOffscreenContext(const GURL& url) override;\r\n  \r\n  // Enhanced GPU process monitoring\r\n  void DidLoseContext(bool offscreen,\r\n                     gpu::error::ContextLostReason reason,\r\n                     const GURL& active_url) override;\r\n  \r\n  // Viz compositor integration\r\n  void SetChildSurface(gpu::SurfaceHandle parent,\r\n                      gpu::SurfaceHandle child) override;\r\n};\r\n```\r\n\r\n---\r\n\r\n## Modern Benefits & Best Practices (v134+)\r\n\r\n### Enhanced Architectural Benefits\r\n1. **Type Safety**: Modern C++ templates and concepts ensure compile-time correctness\r\n2. **Memory Safety**: RAII and smart pointers prevent memory leaks and use-after-free bugs\r\n3. **Async Programming**: Integration with Chromium's modern async patterns\r\n4. **Service Integration**: Seamless integration with Chromium's microservice architecture\r\n5. **Security**: Capability-based delegation respects process boundaries\r\n6. **Performance**: Zero-cost abstractions and efficient IPC integration\r\n\r\n### Modern Best Practices\r\n```cpp\r\n// Best practice: Modern delegate interface design\r\nclass ModernDelegate {\r\n public:\r\n  virtual ~ModernDelegate() = default;\r\n\r\n  // Use strong types instead of primitive parameters\r\n  virtual void ProcessRequest(\r\n      const TypedRequest& request,\r\n      base::OnceCallback<void(TypedResponse)> callback) = 0;\r\n\r\n  // Prefer span<> for array parameters\r\n  virtual void ProcessBatch(\r\n      base::span<const RequestItem> items) = 0;\r\n\r\n  // Use std::optional for optional parameters\r\n  virtual bool Configure(\r\n      const Config& config,\r\n      std::optional<SecurityLevel> security_level = std::nullopt) = 0;\r\n\r\n  // Modern error handling with base::expected\r\n  virtual base::expected<ProcessResult, ProcessError> TryProcess(\r\n      const Request& request) = 0;\r\n};\r\n```\r\n\r\n### Performance Considerations\r\n- **Minimal Virtual Function Calls**: Use templates where appropriate\r\n- **Efficient Memory Management**: Leverage move semantics and perfect forwarding\r\n- **Cache-Friendly Design**: Consider memory layout and access patterns\r\n- **IPC Optimization**: Minimize cross-process delegate calls\r\n\r\n---\r\n\r\n## Integration with Modern Chromium Features (v134+)\r\n\r\n### Privacy Sandbox Integration\r\n```cpp\r\nclass PrivacySandboxDelegate {\r\n public:\r\n  // Topics API integration\r\n  virtual void OnTopicsCalculated(\r\n      const std::vector<Topic>& topics,\r\n      const PrivacyBudget& budget) = 0;\r\n\r\n  // FLEDGE auction support\r\n  virtual void OnAuctionComplete(\r\n      const AuctionResult& result,\r\n      const std::vector<InterestGroup>& winning_groups) = 0;\r\n\r\n  // Attribution reporting\r\n  virtual void OnAttributionReportScheduled(\r\n      const AttributionReport& report) = 0;\r\n};\r\n```\r\n\r\n### WebGPU Integration\r\n```cpp\r\nclass WebGPUDelegate {\r\n public:\r\n  // Modern GPU resource management\r\n  virtual void OnGPUResourceCreated(\r\n      const WebGPUResource& resource,\r\n      const SecurityContext& context) = 0;\r\n\r\n  // Compute shader delegation\r\n  virtual void OnComputeShaderExecution(\r\n      const ComputeShaderInfo& shader_info,\r\n      const ExecutionMetrics& metrics) = 0;\r\n};\r\n```\r\n\r\n---\r\n\r\n## Migration Guide: Legacy to Modern Delegates\r\n\r\n### Legacy Pattern (Pre-v134)\r\n```cpp\r\n// Old-style delegate with raw pointers and synchronous methods\r\nclass LegacyDelegate {\r\n public:\r\n  virtual bool Process(const char* data, int size) = 0;\r\n  virtual void SetCallback(Callback* callback) = 0;  // Raw pointer!\r\n};\r\n```\r\n\r\n### Modern Pattern (v134+)\r\n```cpp\r\n// Modern delegate with type safety and async patterns\r\nclass ModernDelegate {\r\n public:\r\n  virtual ~ModernDelegate() = default;\r\n  \r\n  virtual void ProcessAsync(\r\n      base::span<const uint8_t> data,\r\n      base::OnceCallback<void(ProcessResult)> callback) = 0;\r\n      \r\n  virtual base::expected<void, ProcessError> ProcessSync(\r\n      base::span<const uint8_t> data) = 0;\r\n};\r\n```\r\n\r\n---\r\n\r\n## Conclusion\r\n\r\nThe Delegate Pattern in Chromium v134+ represents a sophisticated evolution of traditional design patterns, incorporating modern C++ features, type safety, and deep integration with the browser's advanced architecture. This pattern enables:\r\n\r\n- **Scalable Architecture**: Support for Chromium's massive, service-oriented codebase\r\n- **Security**: Respect for process boundaries and capability-based access\r\n- **Performance**: Efficient implementations with modern C++ optimizations\r\n- **Maintainability**: Clear separation of concerns and testable interfaces\r\n- **Extensibility**: Easy customization for different browser configurations\r\n\r\nBy mastering modern delegate patterns, developers can effectively contribute to and extend Chromium's sophisticated browser architecture while maintaining the high standards of security, performance, and reliability that modern web browsers demand.\r\n\r\n**Key Takeaways**:\r\n- Modern delegates integrate deeply with Chromium's service architecture\r\n- Type safety and memory safety are paramount in v134+ implementations\r\n- Async patterns are preferred for non-blocking operations\r\n- Security considerations are built into the delegate design\r\n- Testing is enhanced through modern mocking and dependency injection\r\n\r\n**Next Steps**:\r\n- Explore [Mojo IPC Integration](../ipc-internals.md) for cross-process delegate communication\r\n- Study [Service Architecture](../browser-components.md) for advanced delegate usage patterns\r\n- Review [Security Model](../../security/security-model.md) for security-aware delegate design\r\n"
  },
  {
    "path": "apis/servicification",
    "title": "Servicifying Chromium Features",
    "content": "# Servicifying Chromium Features\n\n[TOC]\n\n## Overview\n\nMuch to the dismay of Chromium developers, practicing linguists, and keyboard\noperators everywhere, the term **servicificificification** [sic] has been\negregiously smuggled into the Chromium parlance.\n\nLots of Chromium code is contained in reasonably well-isolated component\nlibraries with some occasionally fuzzy boundaries and often a surprising number\nof gnarly runtime interdependencies among a complex graph of components. Y\nimplements one of Z's delegate interfaces, while X implements one of Y's\ndelegate interfaces, and now it's possible for some ridiculous bug to creep in\nwhere W calls into Z at the wrong time and causes a crash in X. Yikes.\n\nServicification embodies the ongoing process of **servicifying** Chromium\nfeatures and subsystems, or refactoring these collections of library code into\nservices with well-defined public API boundaries and very strong runtime\nisolation via Mojo interfaces.\n\nThe primary goals are to improve maintainability and extensibility of the system\nover time, while also allowing for more flexible runtime configuration. For\nexample, with the Network Service in place we can now run the entire network\nstack either inside or outside of the browser process with the flip of a\ncommand-line switch. Client code using the Network Service stays the same,\nindependent of that switch.\n\nThis document focuses on helpful guidelines and patterns for servicifying parts\nof Chromium.\n\nAlso see general [Mojo &amp; Services](/docs/README.md#Mojo-Services)\ndocumentation for other introductory guides, API references, *etc.*\n\n## Setting Up The Service\n\nThis section briefly covers early decisions and implementation concerns when\nintroducing a new service.\n\n### Where in the Tree?\n\nBased on the\n[service development guidelines](/services/README.md), any service which could\nbe reasonably justified as a core system service in a hypothetical,\nwell-designed operating system may belong in the top-level `//services`\ndirectory. If that sounds super hand-wavy and unclear, that's because it is!\nThere isn't really a great universal policy here, so when in doubt, contact your\nfavorite local\n[services-dev@chromium.org](https://groups.google.com/a/chromium.org/forum#!forum/services-dev)\nmailing list and start a friendly discussion.\n\nOther common places where developers place services, and why:\n\n- `//components/services` for services which haven't yet made the cut for\n  `//services` but which are either used by Content directly or by multiple\n  Content embedders.\n- `//chrome/services` for services which are used exclusively within Chrome and\n  not shared with other Content embedders.\n- `//chromeos/services` for services which are used on Chrome OS by more than\n  just Chrome itself (for example, if the `ash` service must also connect to\n  them for use in system UI).\n\n### Launching Service Processes\n\nContent provides a simple\n[`ServiceProcessHost`](https://cs.chromium.org/chromium/src/content/public/browser/service_process_host.h?rcl=723edf64a56ef6058e886afc67adc786bea39e78&l=47)\nAPI to launch a new Service Process. The Mojo Remote corresponding to each\nprocess launch is effectively a lifetime control for the launched process.\n\nYou may choose to maintain only a single concurrent instance of your service\nat a time, similar to the Network or Storage services. In this case, typically\nyou will have some browser code maintain a lazy Mojo Remote to the service\nprocess, and any clients of the service will have their connections brokered\nthrough this interface.\n\nIn other cases you may want to manage multiple independent service processes.\nThe Data Decoder service, for example, allows for arbitrary browser code\nto launch a unique isolated instance to process a single decode operation or\na batch of related operations (e.g. to decode a bunch of different objects\nfrom the same untrusted origin).\n\nInsofar as the browser can use ServiceProcessLauncher however it likes, and the\ncorresponding Mojo Remotes can be owned just like any other object, developers\nare free to manage their service instances however they like.\n\n### Hooking Up the Service Implementation\n\nFor out-of-process service launching, Content uses its \"utility\" process type.\n\nFor services known to content, this is accomplished by adding an appropriate\nfactory function to\n[`//content/utility/services.cc`](https://cs.chromium.org/chromium/src/content/utility/services.cc)\n\nFor other services known only to Chrome, we have a similar file at\n[`//chrome/utility/services.cc`](https://cs.chromium.org/chromium/src/chrome/utility/services.cc).\n\nOnce an appropriate service factory is registered for your main service\ninterface in one of these places, `ServiceProcessHost::Launch` can be used to\nacquire a new isolated instance from within the browser process.\n\nTo run a service in-process, you can simply instantiate your service\nimplementation (e.g. on a background thread) like you would any other object,\nand you can then bind a Mojo Remote which is connected to that instance.\n\nThis is useful if you want to avoid the overhead of extra processes in some\nscenarios, and it allows the detail of where and how the service runs to be\nfully hidden behind management of the main interface's Mojo Remote.\n\n## Incremental Servicification\n\nFor large Chromium features it is not feasible to convert an entire subsystem\nto a service all at once. As a result, it may be necessary for the subsystem\nto spend a considerable amount of time (weeks or months) split between the old\nimplementation and your beautiful, sparkling new service implementation.\n\nIn creating your service, you likely have two goals:\n\n- Making the service available to its consumers\n- Making the service self-contained\n\nThose two goals are not the same, and to some extent are at tension:\n\n- To satisfy the first, you need to build out the API surface of the service to\n  a sufficient degree for the anticipated use cases.\n\n- To satisfy the second, you need to convert all clients of the code that you\n  are servicifying to instead use the service, and then fold that code into the\n  internal implementation of the service.\n\nWhatever your goals, you will need to proceed incrementally if your project is\nat all non-trivial (as they basically all are given the nature of the effort).\nYou should explicitly decide what your approach to incremental bringup and\nconversion will be. Here are some approaches that have been taken for various\nservices:\n\n- Build out your service depending directly on existing code,\n  convert the clients of that code 1-by-1, and fold the existing code into the\n  service implementation when complete ([Identity Service](https://docs.google.com/document/d/1EPLEJTZewjiShBemNP5Zyk3b_9sgdbrZlXn7j1fubW0/edit)).\n- Build out the service with new code and make the existing code\n  into a client library of the service. In that fashion, all consumers of the\n  existing code get converted transparently ([Preferences Service](https://docs.google.com/document/d/1JU8QUWxMEXWMqgkvFUumKSxr7Z-nfq0YvreSJTkMVmU/edit#heading=h.19gc5b5u3e3x)).\n- Build out the new service piece-by-piece by picking a given\n  bite-size piece of functionality and entirely servicifying that functionality\n  ([Device Service](https://docs.google.com/document/d/1_1Vt4ShJCiM3fin-leaZx00-FoIPisOr8kwAKsg-Des/edit#heading=h.c3qzrjr1sqn7)).\n\nThese all have tradeoffs:\n\n- The first lets you incrementally validate your API and implementation, but\n  leaves the service depending on external code for a long period of time.\n- The second can create a self-contained service more quickly, but leaves\n  all the existing clients in place as potential cleanup work.\n- The third ensures that you're being honest as you go, but delays having\n  the breadth of the service API up and going.\n\nWhich makes sense depends both on the nature of the existing code and on\nthe priorities for doing the servicification. The first two enable making the\nservice available for new use cases sooner at the cost of leaving legacy code in\nplace longer, while the last is most suitable when you want to be very exacting\nabout doing the servicification cleanly as you go.\n\n## Platform-Specific Issues: Android\n\nAs you servicify code running on Android, you might find that you need to port\ninterfaces that are served in Java. Here is an\n[example CL](https://codereview.chromium.org/2643713002) that gives a basic\npattern to follow in doing this.\n\nYou also might need to register JNI in your service. That is simple to set\nup, as illustrated in [this CL](https://codereview.chromium.org/2690963002).\n(Note that that CL is doing more than *just* enabling the Device Service to\nregister JNI; you should take the register_jni.cc file added there as your\nstarting point to examine the pattern to follow).\n\nFinally, it is possible that your feature will have coupling to UI process state\n(e.g., the Activity) via Android system APIs. To handle this challenging\nissue, see the section on [Coupling to UI](#Coupling-to-UI).\n\n## Platform-Specific Issues: iOS\n\n*** aside\n**WARNING:** Some of this content is obsolete and needs to be updated. When in\ndoubt, look approximately near the recommended bits of code and try to find\nrelevant prior art.\n***\n\nServices are supported on iOS insofar as Mojo is supported. However, Chrome on\niOS is strictly single-process, and all services thus must run in-process on\niOS.\n\nIf you have a use case or need for services on iOS, contact\nblundell@chromium.org. For general information on the motivations and vision for\nsupporting services on iOS, see the high-level\n[servicification design doc](https://docs.google.com/document/d/15I7sQyQo6zsqXVNAlVd520tdGaS8FCicZHrN0yRu-oU/edit).\nIn particular, search for the mentions of iOS within the doc.\n\n## Client-Specific Issues\n\n#### Mocking Interface Impls in JS\nIt is a common pattern in Blink's web tests to mock a remote Mojo interface\nin JS so that native Blink code requests interfaces from the test JS rather\nthan whatever would normally service them in the browser process.\n\nThe current way to set up that sort of thing looks like\n[this](https://cs.chromium.org/chromium/src/third_party/blink/web_tests/battery-status/resources/mock-battery-monitor.js?rcl=be6e0001855f7f1cfc26205d0ff5a2b5b324fcbd&l=19).\n\n#### Feature Impls That Depend on Blink Headers\nIn the course of servicifying a feature that has Blink as a client, you might\nencounter cases where the feature implementation has dependencies on Blink\npublic headers (e.g., defining POD structs that are used both by the client and\nby the feature implementation). These dependencies pose a challenge:\n\n- Services should not depend on Blink, as this is a dependency inversion (Blink\nis a client of services).\n- However, Blink is very careful about accepting dependencies from Chromium.\n\nTo meet this challenge, you have two options:\n\n1. Move the code in question from C++ to mojom (e.g., if it is simple structs).\n2. Move the code into the service's C++ client library, being very explicit\n   about its usage by Blink. See\n   [this CL](https://codereview.chromium.org/2415083002) for a basic pattern to\n   follow.\n\n#### Frame-Scoped Connections\nYou must think carefully about the scoping of the connection being made\nfrom Blink. In particular, some feature requests are necessarily scoped to a\nframe in the context of Blink (e.g., geolocation, where permission to access the\ninterface is origin-scoped). Servicifying these features is then challenging, as\nBlink has no frame-scoped connection to arbitrary services (by design, as\narbitrary services have no knowledge of frames or even a notion of what a frame\nis).\n\nAfter a\n[long discussion](https://groups.google.com/a/chromium.org/forum/#!topic/services-dev/CSnDUjthAuw),\nthe policy that we have adopted for this challenge is the following:\n\nCURRENT\n\n- The renderer makes a request through its frame-scoped connection to the\n  browser.\n- The browser obtains the necessary permissions before directly servicing the\n  request.\n\nAFTER SERVICIFYING THE FEATURE IN QUESTION\n\n- The renderer makes a request through its frame-scoped connection to the\n  browser.\n- The browser obtains the necessary permissions before forwarding the\n  request on to the underlying service that hosts the feature.\n\nNotably, from the renderer's POV essentially nothing changes here.\n\n## Strategies for Challenges to Decoupling from //content\n\n### Coupling to UI\n\nSome feature implementations have hard constraints on coupling to UI on various\nplatforms. An example is NFC on Android, which requires the Activity of the view\nin which the requesting client is hosted in order to access the NFC platform\nAPIs. This coupling is at odds with the vision of servicification, which is to\nmake the service physically isolatable. However, when it occurs, we need to\naccommodate it.\n\nThe high-level decision that we have reached is to scope the coupling to the\nfeature *and* platform in question (rather than e.g. introducing a\ngeneral-purpose FooServiceDelegate), in order to make it completely explicit\nwhat requires the coupling and to avoid the coupling creeping in scope.\n\nThe basic strategy to support this coupling while still servicifying the feature\nin question is to inject a mechanism of mapping from an opaque \"context ID\" to\nthe required context. The embedder (e.g., //content) maintains this map, and the\nservice makes use of it. The embedder also serves as an intermediary: it\nprovides a connection that is appropriately context-scoped to clients. When\nclients request the feature in question, the embedder forwards the request on\nalong with the appropriate context ID.  The service impl can then map that\ncontext ID back to the needed context on-demand using the mapping functionality\ninjected into the service impl.\n\nTo make this more concrete, see\n[this CL](https://codereview.chromium.org/2734943003).\n\n### Shutdown of Singletons\n\nYou might find that your feature includes singletons that are shut down as part\nof //content's shutdown process. As part of decoupling the feature\nimplementation entirely from //content, the shutdown of these singletons must be\neither ported into your service or eliminated:\n\n- In general, as Chromium is moving away from graceful shutdown, the first\n  question to analyze is: Do the singletons actually need to be shut down at\n  all?\n- If you need to preserve shutdown of the singleton, the naive approach is to\n  move the shutdown of the singleton to the destructor of your service\n- However, you should carefully examine when your service is destroyed compared\n  to when the previous code was executing, and ensure that any differences\n  introduced do not impact correctness.\n\nSee\n[this thread](https://groups.google.com/a/chromium.org/forum/#!topic/services-dev/Y9FKZf9n1ls)\nfor more discussion of this issue.\n\n## Additional Support\n\nIf this document was not helpful in some way, please post a message to your\nfriendly local\n[chromium-mojo@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/chromium-mojo)\nor\n[services-dev@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/services-dev)\nmailing list.\n"
  },
  {
    "path": "apis/README",
    "title": "API Documentation",
    "content": "# API Documentation\r\n\r\nThis section contains documentation for Chromium's APIs, IPC mechanisms, and service architecture.\r\n\r\n## Mojo & Services\r\n\r\n- [Intro to Mojo & Services](mojo_and_services.md) - Core concepts and usage\r\n- [Mojo IPC Conversion](mojo_ipc_conversion.md) - Converting legacy IPC to Mojo\r\n- [Mojo Testing](mojo_testing.md) - Testing Mojo interfaces\r\n- [Servicification](servicification.md) - Converting components to services\r\n\r\n## Key Concepts\r\n\r\n**Mojo** is Chromium's IPC system that enables communication between processes through strongly-typed interfaces. It provides:\r\n\r\n- **Type Safety**: Compile-time interface validation\r\n- **Performance**: Efficient message passing with shared memory\r\n- **Security**: Process isolation with capability-based access\r\n- **Cross-Platform**: Works across all supported platforms\r\n\r\n**Services** are discrete components that communicate through Mojo interfaces, enabling better modularity and testing.\r\n\r\n## Related Documentation\r\n\r\n- [Architecture Overview](../architecture/overview.md)\r\n- [Process Model](../architecture/process_model_and_site_isolation.md)\r\n- [Security Model](../security/security-model.md)\r\n"
  },
  {
    "path": "apis/overview",
    "title": "APIs & Services Overview",
    "content": "# APIs & Services Overview\r\n\r\nChromium's modern architecture relies heavily on well-defined APIs and service-oriented design. This section covers the key APIs and service architecture patterns used throughout the codebase.\r\n\r\n## 🎯 What You'll Learn\r\n\r\n- **Mojo IPC System**: Modern inter-process communication\r\n- **Service Architecture**: How Chromium's services are organized\r\n- **API Design Patterns**: Common patterns for building robust APIs\r\n- **Testing Strategies**: How to test complex service interactions\r\n\r\n## 📋 Topics Covered\r\n\r\n### Core Concepts\r\n- [Mojo & Services](mojo_and_services) - Introduction to Chromium's service architecture\r\n- [Servicification](servicification) - The process of converting legacy code to services\r\n- [Mojo IPC Conversion](mojo_ipc_conversion) - Migrating from legacy IPC to Mojo\r\n\r\n### Testing & Development\r\n- [Mojo Testing](mojo_testing) - Testing strategies for Mojo-based services\r\n\r\n## 🚀 Getting Started\r\n\r\nIf you're new to Chromium's service architecture, start with [Mojo & Services](mojo_and_services) to understand the fundamental concepts.\r\n\r\n## 🔗 Related Sections\r\n\r\n- [🏗️ Core Architecture](../architecture/overview) - Understanding Chromium's overall design\r\n- [🧪 Testing & QA](../development/testing/testing_in_chromium) - Testing methodologies\r\n- [⚙️ Core Modules](../modules/javascript-v8) - How modules interact with services\r\n"
  },
  {
    "path": "apis/mojo_testing",
    "title": "Testing With Mojo",
    "content": "# Testing With Mojo\n\nThis document outlines some best practices and techniques for testing code which\ninternally uses a Mojo service. It assumes familiarity with the\n[Mojo and Services] document.\n\n## Example Code & Context\n\nSuppose we have this Mojo interface:\n\n```mojom\nmodule example.mojom;\n\ninterface IncrementerService {\n  Increment(int32 value) => (int32 new_value);\n}\n```\n\nand this C++ class that uses it:\n\n```c++\nclass Incrementer {\n public:\n  Incrementer();\n\n  void SetServiceForTesting(\n      mojo::PendingRemote<mojom::IncrementerService> service);\n\n  // The underlying service is async, so this method is too.\n  void Increment(int32_t value,\n                 IncrementCallback callback);\n\n private;\n  mojo::Remote<mojom::IncrementerService> service_;\n};\n\nvoid Incrementer::SetServiceForTesting(\n    mojo::PendingRemote<mojom::IncrementerService> service) {\n  service_.Bind(std::move(service));\n}\n\nvoid Incrementer::Increment(int32_t value, IncrementCallback callback) {\n  if (!service_)\n    service_ = LaunchIncrementerService();\n  service_->Increment(value, std::move(callback));\n}\n```\n\nand we wish to swap a test fake in for the underlying IncrementerService, so we\ncan unit-test Incrementer. Specifically, we're trying to write this (silly) test:\n\n```c++\n// Test that Incrementer correctly handles when the IncrementerService fails to\n// increment the value.\nTEST(IncrementerTest, DetectsFailureToIncrement) {\n  Incrementer incrementer;\n  FakeIncrementerService service;\n  // ... somehow use `service` as a test fake for `incrementer` ...\n\n  incrementer.Increment(0, ...);\n\n  // ... Get the result and compare it with 0 ...\n}\n```\n\n## The Fake Service Itself\n\nThis part is fairly straightforward. Mojo generated a class called\nmojom::IncrementerService, which is normally subclassed by\nIncrementerServiceImpl (or whatever) in production; we can subclass it\nourselves:\n\n```c++\nclass FakeIncrementerService : public mojom::IncrementerService {\n public:\n  void Increment(int32_t value, IncrementCallback callback) override {\n    // Does not actually increment, for test purposes!\n    std::move(callback).Run(value);\n  }\n}\n```\n\n## Async Services\n\nWe can plug the FakeIncrementerService into our test using:\n\n```c++\n  mojo::Receiver<IncrementerService> receiver{&fake_service};\n  incrementer->SetServiceForTesting(receiver.BindNewPipeAndPassRemote());\n```\n\nwe can invoke it and wait for the response as we usually would:\n\n```c++\n  base::test::TestFuture test_future;\n  incrementer->Increment(0, test_future.GetCallback());\n  int32_t result = test_future.Get();\n  EXPECT_EQ(0, result);\n```\n\n... and all is well. However, we might reasonably want a more flexible\nFakeIncrementerService, which allows for plugging different responses in as the\ntest progresses. In that case, we will actually need to wait twice: once for the\nrequest to arrive at the FakeIncrementerService, and once for the response to be\ndelivered back to the Incrementer.\n\n## Waiting For Requests\n\nTo do that, we can instead structure our fake service like this:\n\n```c++\nclass FakeIncrementerService : public mojom::IncrementerService {\n public:\n  void Increment(int32_t value, IncrementCallback callback) override {\n    CHECK(!HasPendingRequest());\n    last_value_ = value;\n    last_callback_ = std::move(callback);\n    if (!signal_.IsReady()) {\n      signal_->SetValue();\n    }\n  }\n\n  bool HasPendingRequest() const {\n    return bool(last_callback_);\n  }\n\n  void WaitForRequest() {\n    if (HasPendingRequest()) {\n      return;\n    }\n    signal_.Clear();\n    signal_.Wait();\n  }\n\n  void AnswerRequest(int32_t value) {\n    CHECK(HasPendingRequest());\n    std::move(last_callback_).Run(value);\n  }\n private:\n  int32_t last_value_;\n  IncrementCallback last_callback_;\n  base::test::TestFuture signal_;\n};\n```\n\nThat having been done, our test can now observe the state of the code under test\n(in this case the Incrementer service) while the mojo request is pending, like\nso:\n\n```c++\n  FakeIncrementerService service;\n  mojo::Receiver<mojom::IncrementerService> receiver{&service};\n\n  Incrementer incrementer;\n  incrementer->SetServiceForTesting(receiver.BindNewPipeAndPassRemote());\n  incrementer->Increment(1, base::BindLambdaForTesting(...));\n\n  // This will do the right thing even if the Increment method later becomes\n  // synchronous, and exercises the same async code paths as the production code\n  // will.\n  service.WaitForRequest();\n  service.AnswerRequest(service.last_value() + 2);\n\n  // The lambda passed in above will now asynchronously run somewhere here,\n  // since the response is also delivered asynchronously by mojo.\n```\n\n## Intercepting Messages to Bound Receivers\n\nIn some cases, particularly in browser tests, we may want to take an existing,\nbound `mojo::Receiver` and intercept certain messages to it. This allows us to:\n - modify message parameters before the message is handled by the original\n   implementation,\n - modify returned values by intercepting callbacks,\n - introduce failures, or\n - completely re-implement the message handling logic\n\nTo accomplish this, Mojo autogenerates an InterceptorForTesting class for each\ninterface that can be subclassed to perform the interception. Continuing with\nthe example above, we can include `incrementer_service.mojom-test-utils.h` and\nthen use the following to intercept and replace the number to be incremented:\n\n```c++\nclass IncrementerServiceInterceptor\n    : public mojom::IncrementerServiceInterceptorForTesting {\n public:\n  // We'll assume RealIncrementerService implements the Mojo interface, owns the\n  // the bound mojo::Receiver, and makes it available to use via a testing\n  // method we added named `receiver_for_testing()`.\n  IncrementerServiceInterceptor(RealIncrementerService* service,\n                                int32_t value_to_inject)\n      : service_(service),\n        value_to_inject_(value_to_inject),\n        swapped_impl_(service->receiver_for_testing(), this) {}\n\n  ~IncrementerServiceInterceptor() override = default;\n\n  mojom::IncrementerService* GetForwardingInterface()\n      override {\n    return service_;\n  }\n\n  void Increment(int32_t value,\n                 IncrementCallback callback) override {\n    GetForwardingInterface()->Increment(value_to_inject_, std::move(callback));\n  }\n\n private:\n  raw_ptr<RealIncrementerService> service_;\n  int32_t value_to_inject_;\n  mojo::test::ScopedSwapImplForTesting<\n      mojo::Receiver<mojom::IncrementerService>>\n      swapped_impl_;\n};\n```\n\n## Ensuring Message Delivery\n\nBoth `mojo::Remote` and `mojo::Receiver` objects have a `FlushForTesting()`\nmethod that can be used to ensure that queued messages and replies have been\nsent to the other end of the message pipe, respectively. `mojo::Remote` objects\nalso have an asynchronous version of this method call `FlushAsyncForTesting()`\nthat accepts a `base::OnceCallback` that will be called upon completion. These\nmethods can be particularly helpful in tests where the `mojo::Remote` and\n`mojo::Receiver` might be in separate processes.\n\n[Mojo and Services]: mojo_and_services.md\n"
  },
  {
    "path": "apis/mojo_ipc_conversion",
    "title": "Converting Legacy IPC to Mojo",
    "content": "# Converting Legacy IPC to Mojo\n\n[TOC]\n\n## Overview\n\nA number of IPC messages sent (primarily between the browser and renderer\nprocesses) are still defined using Chrome's old IPC system in `//ipc`. This\nsystem uses\n[`base::Pickle`](https://cs.chromium.org/chromium/src/base/pickle.h?rcl=8b7842262ee1239b1f3ae20b9c851748ef0b9a8b&l=128)\nas the basis for message serialization and is supported by a number if `IPC_*`\npreprocessor macros defined in `//ipc` and used around the source tree.\n\nThere is an ongoing, distributed effort to get these messages converted to Mojo\ninterface messages. Messages that still need to be converted are tracked in two\nspreadsheets:\n\n- [Chrome IPC to Mojo migration](https://docs.google.com/spreadsheets/d/1pGWX_wxGdjAVtQOmlDDfhuIc3Pbwg9FtvFXAXYu7b7c/edit#gid=0) for non-web platform messages\n- [Mojoifying Platform Features](https://docs.google.com/spreadsheets/d/1VIINt17Dg2cJjPpoJ_HY3HI0uLpidql-1u8pBJtpbGk/edit#gid=1603373208) for web platform messages\n\nThis document is concerned primarily with rote conversion of legacy IPC messages\nto Mojo interface messages. If you are considering more holistic refactoring and\nbetter isolation of an entire subsystem of the browser, you may consider\n[servicifying](servicification.md) the feature instead of merely converting its\nIPCs.\n\nSee other [Mojo &amp; Services](/docs/README.md#Mojo-Services) documentation\nfor introductory guides, API references, and more.\n\n## Legacy IPC Concepts\n\nEach Content child process has a single **`IPC::Channel`** implementation going\nbetween it and the browser process, and this is used as the sole two-way FIFO\nto send legacy IPC messages between the processes.\n\nThere are two fundamental types of legacy IPC messages: **control** messages,\ndefined via `IPC_MESSAGE_CONTROLn` macros (where `n` is some small integer) and\n**routed** messages defined via `IPC_MESSAGE_ROUTEDn` macros.\n\nControl messages generally go between a browser-side process host (*e.g.*,\n`RenderProcessHost` or `GpuProcessHost`) and the child-side `ChildThreadImpl`\nsubclass. All of these classes implement `IPC::Sender` and thus have a `Send`\nmethod for sending a control message to their remote counterpart, and they\nimplement `IPC::Listener` to receive incoming control messages via\n`OnMessageReceived`.\n\nRouted messages are relegated to **routes** which have arbitrary meaning\ndetermined by their use within a given process. For example, renderers use\nroutes to isolate messages scoped to individual render frames, and so such\nrouted messages will travel between a `RenderFrameHostImpl` and its\ncorresponding `RenderFrameImpl`, both of which also implement `IPC::Sender` and\n`IPC::Listener`.\n\n## Mojo Interfaces as Routes\n\nRouted messages in the old IPC system always carry a **routing ID** to identify\nto the receiving endpoint which routed object (*e.g.* which `RenderFrameImpl`\nor `RenderViewImpl` or whatever) the message is targeting. Each endpoint is thus\nrequired to do some additional book-keeping to track what each routing ID means.\n\nMojo interfaces obviate the need for routing IDs, as new \"routes\" can be\nestablished by simply creating a new interface pipe and passing one endpoint to\nsomething which knows how to bind it.\n\nWhen thinking about an IPC message conversion to Mojo, it's important to\nconsider whether the message is a control message or a routed message, as this\ndetermines where you might find an existing Mojo interface to carry your\nmessage, or where you will want to add a new end-to-end Mojo interface for that\npurpose. This can mean the difference between a single per-process interface\ngoing between each `RenderProcessHostImpl` and its corresponding\n`RenderThreadImpl`, vs a per-frame interface going between each\n`RenderFrameHostImpl` and its corresponding `RenderFrameImpl`.\n\n## Ordering Considerations\n\nOne **very important** consideration when doing IPC conversions is the relative\nordering of IPC-driven operations. With the old IPC system, because every\nmessage between two processes is globally ordered, it is quite easy for parts\nof the system to (intentionally or often unintentionally) rely on strict\nordering guarantees.\n\nFor example, imagine a `WebContentsObserver` in the browser processes observes\na frame navigation and immediately sends an IPC message to the frame to\nconfigure some new behavior. The implementation may be inadvertently relying on\nthis message arriving *before* some other tangentially related message sent to\nthe same frame shortly after the same navigation event.\n\nWhile Mojo guarantees strict ordering within each message pipe, Mojo does not\n(and in fact cannot) make any strict ordering guarantees between separate\nmessage pipes, as message pipes may be freely moved across process boundaries\nand thus cannot necessarily share a common FIFO at all times.\n\nIf the two messages described above were moved to separate Mojo interfaces on\nseparate message pipes, renderer behavior could break as the first message may\narrive after the second message.\n\nThe best solution to this problem is to rethink the IPC surface and/or\nimplementation on either side to eliminate ordering dependencies between two\ninterfaces that otherwise seem to be logically distinct. Failing that, Mojo's\nsolution to this problem is to support\n[**associated interfaces**](/mojo/public/tools/bindings/README.md#Associated-Interfaces).\nIn a nutshell, these allow multiple distinct interfaces to be multiplexed over\na shared message pipe.\n\n## Channel-Associated Interfaces\n\nThe previous section mentions **associated interfaces** as a general-purpose\nsolution for establishing a mutual FIFO between multiple logical Mojo interfaces\nby having them share a single message pipe.\n\nIn Chrome, the `IPC::Channel` which carries all legacy IPC messages between\ntwo processes is itself a Mojo message pipe. We provide a mechanism for\nassociating arbitrary Mojo interfaces with this pipe, which means messages can\nbe converted to Mojo while preserving strict FIFO with respect to other legacy\nIPC messages. Such interfaces are designated in Chrome parlance as\n**Channel-associated interfaces**.\n\n*** aside\n**NOTE:** Channel-associated interface acquisition is not constrained by the\nService Manager in any way, so security reviewers need to be careful to inspect\nnew additions and uses of such interfaces.\n***\n\nUsage of Channel-associated interfaces should be rare but is considered a\nreasonable intermediate solution for incremental IPC conversions where it would\nbe too risky or noisy to convert a large IPC surface all at once, but it would\nalso be impossible to split the IPC surface between legacy IPC and a dedicated\nMojo interface pipe without introducing timing bugs.\n\nAt this point in Chrome's development, practical usage of Channel-associated\ninterfaces is restricted to the `IPC::Channel` between the browser process and\na renderer process, as this is the most complex IPC surface with the most\nimplicit ordering dependencies. A few simple APIs exist to support this.\n\n`RenderProcessHostImpl` owns an `IPC::Channel` to its corresponding\n`RenderThreadImpl` in the render process. This object has a\n`GetRemoteAssociatedInterfaces` method which can be used to pass arbitrary\nassociated interface requests:\n\n``` cpp\nmojo::PendingAssociatedRemote<magic::mojom::GoatTeleporter> teleporter;\nchannel_->GetRemoteAssociatedInterfaces()->GetInterface(teleporter.BindNewEndpointAndPassReceiver());\n\n// These messages are all guaranteed to arrive in the same order they were sent.\nchannel_->Send(new FooMsg_SomeLegacyIPC);\nteleporter->TeleportAllGoats();\nchannel_->Send(new FooMsg_AnotherLegacyIPC);\n```\n\nLikewise, `ChildThreadImpl` has an `IPC::Channel` that can be used in the same\nway to send such messages back to the browser.\n\nTo receive and bind incoming Channel-associated interface requests, the above\nobjects also implement `IPC::Listener::OnAssociatedInterfaceRequest`.\n\nFor supplementation of routed messages, both `RenderFrameHostImpl` and\n`RenderFrameImpl` define a `GetRemoteAssociatedInterfaces` method which works\nlike the one on `IPC::Channel`, and both objects also implement\n`IPC::Listener::OnAssociatedInterfaceRequest` for processing incoming associated\ninterface requests specific to their own frame.\n\nThere are some example conversion CLs which use Channel-associated interfaces\n[here](https://codereview.chromium.org/2381493003) and\n[here](https://codereview.chromium.org/2400313002).\n\n## Deciding How to Approach a Conversion\n\nThere are a few questions you should ask before embarking upon any IPC message\nconversion journey, and there are many potential approaches to consider. The\nright one depends on context.\n\nNote that this section assumes the message is traveling between the browser\nprocess and a renderer process. Other cases are rare and developers may wish to\nconsult\n[chromium-mojo@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/chromium-mojo)\nbefore proceeding with them. Otherwise, apply the following basic algorithm to\ndecide how to proceed:\n\n- General note: If the message is a reply to some other message (typically these\n  take a \"request ID\" argument), see the note about message replies at the\n  bottom of this section.\n- Consider whether or not the message makes sense as part of the IPC surface of\n  a new or existing service somewhere in `//services` or `//chrome/services`,\n  *etc.* This is less and less likely to be the case as time goes on, as many\n  remaining IPC conversions are quite narrowly dealing with specific\n  browser/renderer details rather than the browser's supporting subsystems. If\n  defining a new service, you may wish to consult some of the other\n  [Mojo &amp; Services documentation](/docs/README.md#Mojo-Services) first.\n- If the message is an `IPC_MESSAGE_CONTROL` message:\n    - If there are likely to be strict ordering requirements between this\n      message and other legacy IPC or Channel-associated interface messages,\n      consider using a new or existing\n      [Channel-associated interface](#Channel-Associated-Interfaces) between\n      `RenderProcessHostImpl` and `RenderThreadImpl`.\n    - If the message is sent from a renderer to the browser:\n        - If an existing interface is bound by `RenderProcessHostImpl` and\n          requested through `RenderThread`'s Connector and seems to be a good\n          fit for the message, add the equivalent Mojo message to that\n          interface.\n        - If no such interface exists, consider adding one for this message and\n          any related messages.\n    - If the message is sent from the browser to a renderer:\n        - If an existing interface is bound by `RenderThreadImpl` and requested\n          through a `BrowserContext` Connector referencing a specific\n          `RenderProcessHost` [identity](https://cs.chromium.org/chromium/src/content/public/browser/render_process_host.h?rcl=1497b88b7d6400a2a5cced258df03d53800d7848&l=327),\n          and the interface seems to be a good fit for the message, add the\n          equivalent Mojo message to that interface.\n        - If no such interface exists, consider adding one for this message and\n          any related messages.\n- If the message is an `IPC_MESSAGE_ROUTED` message:\n    - Determine what the routing endpoints are. If they are\n      `RenderFrameHostImpl` and `RenderFrameImpl`:\n        - If there are likely to be strict ordering requirements between this\n          message and other legacy IPC or Channel-associated interface messages,\n          consider using a new or existing\n          [Channel-associated interface](#Channel-Associated-Interfaces) between\n          `RenderFrameHostImpl` and `RenderFrameImpl`.\n        - If the message is sent from a renderer to the browser:\n            - If an existing interface is bound by `RenderFrameHostImpl` and\n              acquired via `RenderFrame::GetBrowserInterfaceBroker` and the interface seems\n              to be a good fit for this message, add the equivalent Mojo message\n              to that interface.\n            - If no such interface exists, consider adding one and registering it\n              with `RenderFrameHostImpl`'s `BrowserInterfaceBroker`. See the\n              [simple example](/docs/mojo_and_services.md#Example_Defining-a-New-Frame-Interface)\n              in the \"Intro to Mojo & Services\" document.\n        - If the message is sent from the browser to a renderer, consider\n          adding a Mojo equivalent to the `content.mojom.Frame` interface\n          defined\n          [here](https://cs.chromium.org/chromium/src/content/common/frame.mojom?rcl=138b66744ee9ee853cbb0ae8437b71eaa1fafaa9&l=42).\n    - If the routing endpoints are **not** frame objects (for example, they may\n      be `RenderView`/`RenderViewHost` objects), this is a special case which\n      does not yet have an easy conversion approach readily available. Contact\n      [chromium-mojo@chromium.org](https://groups.google.com/a/chromium.org/forum#!forum/chromium-mojo)\n      to propose or discuss options.\n\n*** aside\n**NOTE**: If you are converting a sync IPC, see the section on\n[Synchronous Calls](/mojo/public/cpp/bindings/README.md#Synchronous-Calls)\nin the Mojo documentation.\n***\n\n### Dealing With Replies\n\nIf the message is a **reply**, meaning it has a \"request ID\" which correlates it\nto a prior message in the opposite direction, consider converting the\n**request** message following the algorithm above. Unlike with legacy IPC, Mojo\nmessages support replies as a first-class concept. So for example if you have:\n\n``` cpp\nIPC_CONTROL_MESSAGE2(FooHostMsg_DoTheThing,\n                     int /* request_id */,\n                     std::string /* name */);\nIPC_CONTROL_MESSAGE2(FooMsg_DidTheThing,\n                     int /* request_id */,\n                     bool /* success */);\n```\n\nYou should consider defining an interface `Foo` which is bound in\n`RenderProcessHostImpl` and acquired from `RenderThreadImpl`, with the following\nmojom definition:\n\n``` cpp\ninterface Foo {\n  DoTheThing(string name) => (bool success);\n};\n```\nSee [Receiving responses](/mojo/public/cpp/bindings/README.md#receiving-responses)\nfor more information.\n\n## Repurposing `IPC::ParamTraits` and `IPC_STRUCT*` Invocations\n\nOccasionally it is useful to do partial IPC conversions, where you want to\nconvert a message to a Mojo interface method but you don't want to necessarily\nconvert every structure passed by the message. In this case, you can leverage\nMojo's\n[type-mapping](https://chromium.googlesource.com/chromium/src/+/main/mojo/public/cpp/bindings/README.md#Type-Mapping)\nsystem to repurpose existing `IPC::ParamTraits`.\n\n*** aside\n**NOTE**: Although in some cases `IPC::ParamTraits<T>` specializations are\ndefined manually in library code, the `IPC_STRUCT*` macro helpers also define\n`IPC::ParamTraits<T>` specializations under the hood. All advice in this section\npertains to both kinds of definitions.\n***\n\nIf a mojom struct is declared without a struct body and is tagged with\n`[Native]`, and a corresponding typemap is provided for the struct, the emitted\nC++ bindings will -- as if by magic -- replace the mojom type with the\ntypemapped C++ type and will internally use the existing `IPC::ParamTraits<T>`\nspecialization for that type in order to serialize and deserialize the struct.\n\nFor example, given the\n[`resource_messages.h`](https://cs.chromium.org/chromium/src/content/common/resource_messages.h?rcl=2e7a430d8d88222c04ab3ffb0a143fa85b3cec5b&l=215) header\nwhich defines an IPC mapping for `content::ResourceRequest`:\n\n``` cpp\nIPC_STRUCT_TRAITS_BEGIN(content::ResourceRequest)\n  IPC_STRUCT_TRAITS_MEMBER(method)\n  IPC_STRUCT_TRAITS_MEMBER(url)\n  // ...\nIPC_STRUCT_TRAITS_END()\n```\n\nand the\n[`resource_request.h`](https://cs.chromium.org/chromium/src/content/common/resource_request.h?rcl=dce9e476a525e4ff0304787935dc1a8c38392ac8&l=32) header\nwhich actually defines the `content::ResourceRequest` type:\n\n``` cpp\nnamespace content {\n\nstruct CONTENT_EXPORT ResourceRequest {\n  // ...\n};\n\n}  // namespace content\n```\n\nwe can declare a corresponding \"native\" mojom struct:\n\n``` cpp\nmodule content.mojom;\n\n[Native]\nstruct URLRequest;\n```\n\nand add a typemap like\n[`url_request.typemap`](https://cs.chromium.org/chromium/src/content/common/url_request.typemap?rcl=4b5963fa744a706398f8f06a4cbbf70d7fa3213d)\nto define how to map between them:\n\n``` python\nmojom = \"//content/public/common/url_loader.mojom\"\npublic_headers = [ \"//content/common/resource_request.h\" ]\ntraits_headers = [ \"//content/common/resource_messages.h\" ]\n...\ntype_mappings = [ \"content.mojom.URLRequest=content::ResourceRequest\" ]\n```\n\nNote specifically that public_headers includes the definition of the native C++\ntype, and traits_headers includes the definition of the legacy IPC traits.\n\nAs a result of all this, other mojom files can now reference\n`content.mojom.URLRequest` as a type for method parameters and other struct\nfields, and the generated C++ bindings will represent those values exclusively\nas `content::ResourceRequest` objects.\n\nThis same basic approach can be used to leverage existing `IPC_ENUM_TRAITS` for\ninvocations for `[Native]` mojom enum aliases.\n\n*** aside\n**NOTE:** Use of `[Native]` mojom definitions is strictly limited to C++\nbindings. If a mojom message depends on such definitions, it cannot be sent or\nreceived by other language bindings. This feature also depends on continued\nsupport for legacy IPC serialization and all uses of it should therefore be\ntreated as technical debt.\n***\n\n## Blink-Specific Advice\n\n### Variants\nLet's assume we have a mojom file such as this:\n\n``` cpp\nmodule example.mojom;\n\ninterface Foo {\n  SendData(string param1, array<int32> param2);\n};\n```\n\nThe following GN snippet will generate two concrete targets: `example` and\n`example_blink`:\n\n```\nmojom(\"example\") {\n  sources = [ \"example.mojom\" ]\n}\n```\n\nThe target `example` will generate Chromium-style C++ bindings using STL types:\n\n``` cpp\n// example.mojom.h\nnamespace example {\nnamespace mojom {\n\nclass Example {\n  virtual void SendArray(const std::string& param1, const std::vector<int32_t>& param2) = 0;\n}\n\n} // namespace mojom\n} // namespace example\n```\n\nThe target `example_blink` will generate Blink-style C++ bindings using WTF types:\n\n``` cpp\n// example.mojom-blink.h\nnamespace example {\nnamespace mojom {\nnamespace blink {\n\nclass Example {\n  virtual void SendArray(const WTF::String& param1, const WTF::Vector<int32_t>& param2) = 0;\n}\n\n} // namespace blink\n} // namespace mojom\n} // namespace example\n```\n\nThanks to these separate sets of bindings no work is necessary to convert types\nbetween Blink-style code and Chromium-style code. It is handled automatically\nduring message serialization and deserialization.\n\nFor more information about variants, see\n[this section](/mojo/public/cpp/bindings/README.md#Variants) of the C++ bindings\ndocumentation.\n\n### Binding callbacks\n\nMojo methods that return a value take an instance of `base::OnceCallback`.\nUse `WTF::BindOnce()` and an appropriate wrapper function depending on the type of\nobject and the callback.\n\nFor garbage-collected (Oilpan) classes owning the `mojo::Remote`, it is recommended\nto use `WrapWeakPersistent(this)` for connection error handlers since they\nare not guaranteed to get called in a finite time period (wrapping the object\nwith `WrapPersistent` in this case would cause memory leaks).\n\nIf the response can be discarded in case the object is not alive by the time\nthe response is received, use `WrapWeakPersistent(this)` for binding the response callback:\n\n``` cpp\n// src/third_party/blink/renderer/modules/device_orientation/device_sensor_entry.cc\nsensor_.set_connection_error_handler(WTF::BindOnce(\n    &DeviceSensorEntry::HandleSensorError, WrapWeakPersistent(this)));\nsensor_->ConfigureReadingChangeNotifications(/*enabled=*/false);\nsensor_->AddConfiguration(\n    std::move(config), WTF::BindOnce(&DeviceSensorEntry::OnSensorAddConfiguration,\n                                 WrapWeakPersistent(this)));\n```\n\nOtherwise (for example, if the response callback is used to resolve a Promise),\nuse `WrapPersistent(this)` to keep the object alive:\n\n``` cpp\n// src/third_party/blink/renderer/modules/nfc/nfc.cc\nScriptPromiseResolver* resolver = ScriptPromiseResolver::Create(script_state);\n... \nnfc_->CancelAllWatches(WTF::BindOnce(&NFC::OnRequestCompleted,\n                                 WrapPersistent(this),\n                                 WrapPersistent(resolver)));\n```\n\nNon-garbage-collected objects can use `WTF::Unretained(this)` for both response\nand error handler callbacks when the `mojo::Remote` is owned by the object bound\nto the callback or the object is guaranteed to outlive the Mojo connection for\nanother reason. Otherwise a weak pointer should be used. However, it is not a\ncommon pattern since using Oilpan is recommended for all Blink code.\n\n### Implementing Mojo interfaces in Blink\n\nOnly a `mojo::Receiver` or `mojo::ReceiverSet` should be used when implementing a\nMojo interface in an Oilpan-managed object. The object must then have a pre-finalizer\nto close any open pipes when the object is about to be swept as lazy sweeping\nmeans that it may be invalid long before the destructor is called. This requires\nsetup in both the object header and implementation.\n\n``` cpp\n// MyObject.h\nclass MyObject : public GarbageCollected,\n                 public example::mojom::blink::Example {\n  USING_PRE_FINALIZER(MyObject, Dispose);\n\n public:\n  MyObject();\n  void Dispose();\n\n  // Implementation of example::mojom::blink::Example.\n\n private:\n  mojo::Receiver<example::mojom::blink::Example> m_receiver{this};\n};\n\n// MyObject.cpp\nvoid MyObject::Dispose() {\n  m_receiver.Close();\n}\n```\n\nFor more information about Blink's Garbage Collector, see\n[Blink GC API Reference](/third_party/blink/renderer/platform/heap/BlinkGCAPIReference.md).\n\n### Typemaps For Content and Blink Types\n\nUsing typemapping for messages that go between Blink and content browser code\ncan sometimes be tricky due to things like dependency cycles or confusion over\nthe correct place for some definition\nto live. There are some example CLs provided here, but feel free to also contact\n[chromium-mojo@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/chromium-mojo)\nwith specific details if you encounter trouble.\n\n[This CL](https://codereview.chromium.org/2363533002) introduces a Mojom\ndefinition and typemap for `ui::WindowOpenDisposition` as a precursor to the\nIPC conversion below.\n\nThe [follow-up CL](https://codereview.chromium.org/2363573002) uses that\ndefinition along with several other new typemaps (including native typemaps as\ndescribed above) to convert the relatively large `ViewHostMsg_CreateWindow`\nmessage to Mojo.\n\n## Additional Support\n\nIf this document was not helpful in some way, please post a message to your\nfriendly\n[chromium-mojo@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/chromium-mojo)\nmailing list.\n"
  },
  {
    "path": "apis/mojo_and_services",
    "title": "Intro to Mojo &amp; Services",
    "content": "# Intro to Mojo &amp; Services\n\n[TOC]\n\n## Overview\n\nThis document contains the minimum amount of information needed for a developer\nto start using Mojo effectively in Chromium, with example Mojo interface usage,\nservice definition and hookup, and a brief overview of the Content layer's core\nservices.\n\nSee other [Mojo &amp; Services](/docs/README.md#Mojo-Services) documentation\nfor introductory guides, API references, and more.\n\n## Mojo Terminology\n\nA **message pipe** is a pair of **endpoints**. Each endpoint has a queue of\nincoming messages, and writing a message at one endpoint effectively enqueues\nthat message on the other (**peer**) endpoint. Message pipes are thus\nbidirectional.\n\nA **mojom** file describes **interfaces**, which are strongly-typed collections\nof **messages**. Each interface message is roughly analogous to a single proto\nmessage, for developers who are familiar with Google protobufs.\n\nGiven a mojom interface and a message pipe, one of the endpoints\ncan be designated as a **`Remote`** and is used to *send* messages described by\nthe interface. The other endpoint can be designated as a **`Receiver`** and is used\nto *receive* interface messages.\n\n*** aside\nNOTE: The above generalization is a bit oversimplified. Remember that the\nmessage pipe is still bidirectional, and it's possible for a mojom message to\nexpect a reply. Replies are sent from the `Receiver` endpoint and received by the\n`Remote` endpoint.\n***\n\nThe `Receiver` endpoint must be associated with (*i.e.* **bound** to) an\n**implementation** of its mojom interface in order to process received messages.\nA received message is dispatched as a scheduled task invoking the corresponding\ninterface method on the implementation object.\n\nAnother way to think about all this is simply that **a `Remote` makes\ncalls on a remote implementation of its interface associated with a\ncorresponding remote `Receiver`.**\n\n## Example: Defining a New Frame Interface\n\nLet's apply this to Chrome. Suppose we want to send a \"Ping\" message from a\nrender frame to its corresponding `RenderFrameHostImpl` instance in the browser\nprocess. We need to define a nice mojom interface for this purpose, create a\npipe to use that interface, and then plumb one end of the pipe to the right\nplace so the sent messages can be received and processed there. This section\ngoes through that process in detail.\n\n### Defining the Interface\n\nThe first step involves creating a new `.mojom` file with an interface\ndefinition, like so:\n\n``` cpp\n// src/example/public/mojom/pingable.mojom\nmodule example.mojom;\n\ninterface Pingable {\n  // Receives a \"Ping\" and responds with a random integer.\n  Ping() => (int32 random);\n};\n```\n\nThis should have a corresponding build rule to generate C++ bindings for the\ndefinition here:\n\n``` python\n# src/example/public/mojom/BUILD.gn\nimport(\"//mojo/public/tools/bindings/mojom.gni\")\nmojom(\"mojom\") {\n  sources = [ \"pingable.mojom\" ]\n}\n```\n\n### Creating the Pipe\n\nNow let's create a message pipe to use this interface.\n\n*** aside\nAs a general rule and as a matter of convenience when\nusing Mojo, the *client* of an interface (*i.e.* the `Remote` side) is\ntypically the party who creates a new pipe. This is convenient because the\n`Remote` may be used to start sending messages immediately without waiting\nfor the InterfaceRequest endpoint to be transferred or bound anywhere.\n***\n\nThis code would be placed somewhere in the renderer:\n\n```cpp\n// src/third_party/blink/example/public/pingable.h\nmojo::Remote<example::mojom::Pingable> pingable;\nmojo::PendingReceiver<example::mojom::Pingable> receiver =\n    pingable.BindNewPipeAndPassReceiver();\n```\n\nIn this example, ```pingable``` is the `Remote`, and ```receiver```\nis a `PendingReceiver`, which is a `Receiver` precursor that will eventually\nbe turned into a `Receiver`. `BindNewPipeAndPassReceiver` is the most common way to create\na message pipe: it yields the `PendingReceiver` as the return\nvalue.\n\n*** aside\nNOTE: A `PendingReceiver` doesn't actually **do** anything. It is an\ninert holder of a single message pipe endpoint. It exists only to make its\nendpoint more strongly-typed at compile-time, indicating that the endpoint\nexpects to be bound by a `Receiver` of the same interface type.\n***\n\n### Sending a Message\n\nFinally, we can call the `Ping()` method on our `Remote` to send a message:\n\n```cpp\n// src/third_party/blink/example/public/pingable.h\npingable->Ping(base::BindOnce(&OnPong));\n```\n\n*** aside\n**IMPORTANT:** If we want to receive the response, we must keep the\n`pingable` object alive until `OnPong` is invoked. After all,\n`pingable` *owns* its message pipe endpoint. If it's destroyed then so is\nthe endpoint, and there will be nothing to receive the response message.\n***\n\nWe're almost done! Of course, if everything were this easy, this document\nwouldn't need to exist. We've taken the hard problem of sending a message from\na renderer process to the browser process, and transformed it into a problem\nwhere we just need to take the `receiver` object from above and pass it to the\nbrowser process somehow where it can be turned into a `Receiver` that dispatches\nits received messages.\n\n### Sending a `PendingReceiver` to the Browser\n\nIt's worth noting that `PendingReceiver`s (and message pipe endpoints in general)\nare just another type of object that can be freely sent over mojom messages.\nThe most common way to get a `PendingReceiver` somewhere is to pass it as a\nmethod argument on some other already-connected interface.\n\nOne such interface which we always have connected between a renderer's\n`RenderFrameImpl` and its corresponding `RenderFrameHostImpl` in the browser\nis\n[`BrowserInterfaceBroker`](https://cs.chromium.org/chromium/src/third_party/blink/public/mojom/browser_interface_broker.mojom).\nThis interface is a factory for acquiring other interfaces. Its `GetInterface`\nmethod takes a `GenericPendingReceiver`, which allows passing arbitrary\ninterface receivers.\n\n``` cpp\ninterface BrowserInterfaceBroker {\n  GetInterface(mojo_base.mojom.GenericPendingReceiver receiver);\n}\n```\nSince `GenericPendingReceiver` can be implicitly constructed from any specific\n`PendingReceiver`, it can call this method with the `receiver` object it created\nearlier via `BindNewPipeAndPassReceiver`:\n\n``` cpp\nRenderFrame* my_frame = GetMyFrame();\nmy_frame->GetBrowserInterfaceBroker().GetInterface(std::move(receiver));\n```\n\nThis will transfer the `PendingReceiver` endpoint to the browser process\nwhere it will be received by the corresponding `BrowserInterfaceBroker`\nimplementation. More on that below.\n\n### Implementing the Interface\n\nFinally, we need a browser-side implementation of our `Pingable` interface.\n\n```cpp\n#include \"example/public/mojom/pingable.mojom.h\"\n\nclass PingableImpl : example::mojom::Pingable {\n public:\n  explicit PingableImpl(mojo::PendingReceiver<example::mojom::Pingable> receiver)\n      : receiver_(this, std::move(receiver)) {}\n  PingableImpl(const PingableImpl&) = delete;\n  PingableImpl& operator=(const PingableImpl&) = delete;\n\n  // example::mojom::Pingable:\n  void Ping(PingCallback callback) override {\n    // Respond with a random 4, chosen by fair dice roll.\n    std::move(callback).Run(4);\n  }\n\n private:\n  mojo::Receiver<example::mojom::Pingable> receiver_;\n};\n```\n\n`RenderFrameHostImpl` owns an implementation of `BrowserInterfaceBroker`.\nWhen this implementation receives a `GetInterface` method call, it calls\nthe handler previously registered for this specific interface.\n\n``` cpp\n// render_frame_host_impl.h\nclass RenderFrameHostImpl\n  ...\n  void GetPingable(mojo::PendingReceiver<example::mojom::Pingable> receiver);\n  ...\n private:\n  ...\n  std::unique_ptr<PingableImpl> pingable_;\n  ...\n};\n\n// render_frame_host_impl.cc\nvoid RenderFrameHostImpl::GetPingable(\n    mojo::PendingReceiver<example::mojom::Pingable> receiver) {\n  pingable_ = std::make_unique<PingableImpl>(std::move(receiver));\n}\n\n// browser_interface_binders.cc\nvoid PopulateFrameBinders(RenderFrameHostImpl* host,\n                          mojo::BinderMap* map) {\n...\n  // Register the handler for Pingable.\n  map->Add<example::mojom::Pingable>(base::BindRepeating(\n    &RenderFrameHostImpl::GetPingable, base::Unretained(host)));\n}\n```\n\nAnd we're done. This setup is sufficient to plumb a new interface connection\nbetween a renderer frame and its browser-side host object!\n\nAssuming we kept our `pingable` object alive in the renderer long enough,\nwe would eventually see its `OnPong` callback invoked with the totally random\nvalue of `4`, as defined by the browser-side implementation above.\n\n## Services Overview &amp; Terminology\nThe previous section only scratches the surface of how Mojo IPC is used in\nChromium. While renderer-to-browser messaging is simple and possibly the most\nprevalent usage by sheer code volume, we are incrementally decomposing the\ncodebase into a set of services with a bit more granularity than the traditional\nContent browser/renderer/gpu/utility process split.\n\nA **service** is a self-contained library of code which implements one or more\nrelated features or behaviors and whose interaction with outside code is done\n*exclusively* through Mojo interface connections, typically brokered by the\nbrowser process.\n\nEach service defines and implements a main Mojo interface which can be used\nby the browser to manage an instance of the service.\n\n## Example: Building a Simple Out-of-Process Service\n\nThere are multiple steps typically involved to get a new service up and running\nin Chromium:\n\n- Define the main service interface and implementation\n- Hook up the implementation in out-of-process code\n- Write some browser logic to launch a service process\n\nThis section walks through these steps with some brief explanations. For more\nthorough documentation of the concepts and APIs used herein, see the\n[Mojo](/mojo/README.md) documentation.\n\n### Defining the Service\n\nTypically service definitions are placed in a `services` directory, either at\nthe top level of the tree or within some subdirectory. In this example, we'll\ndefine a new service for use by Chrome specifically, so we'll define it within\n`//chrome/services`.\n\nWe can create the following files. First some mojoms:\n\n``` cpp\n// src/chrome/services/math/public/mojom/math_service.mojom\nmodule math.mojom;\n\ninterface MathService {\n  Divide(int32 dividend, int32 divisor) => (int32 quotient);\n};\n```\n\n``` python\n# src/chrome/services/math/public/mojom/BUILD.gn\nimport(\"//mojo/public/tools/bindings/mojom.gni\")\n\nmojom(\"mojom\") {\n  sources = [\n    \"math_service.mojom\",\n  ]\n}\n```\n\nThen the actual `MathService` implementation:\n\n``` cpp\n// src/chrome/services/math/math_service.h\n#include \"chrome/services/math/public/mojom/math_service.mojom.h\"\n\nnamespace math {\n\nclass MathService : public mojom::MathService {\n public:\n  explicit MathService(mojo::PendingReceiver<mojom::MathService> receiver);\n  MathService(const MathService&) = delete;\n  MathService& operator=(const MathService&) = delete;\n  ~MathService() override;\n\n private:\n  // mojom::MathService:\n  void Divide(int32_t dividend,\n              int32_t divisor,\n              DivideCallback callback) override;\n\n  mojo::Receiver<mojom::MathService> receiver_;\n};\n\n}  // namespace math\n```\n\n``` cpp\n// src/chrome/services/math/math_service.cc\n#include \"chrome/services/math/math_service.h\"\n\nnamespace math {\n\nMathService::MathService(mojo::PendingReceiver<mojom::MathService> receiver)\n    : receiver_(this, std::move(receiver)) {}\n\nMathService::~MathService() = default;\n\nvoid MathService::Divide(int32_t dividend,\n                         int32_t divisor,\n                         DivideCallback callback) {\n  // Respond with the quotient!\n  std::move(callback).Run(dividend / divisor);\n}\n\n}  // namespace math\n```\n\n``` python\n# src/chrome/services/math/BUILD.gn\n\nsource_set(\"math\") {\n  sources = [\n    \"math_service.cc\",\n    \"math_service.h\",\n  ]\n\n  deps = [\n    \"//base\",\n    \"//chrome/services/math/public/mojom\",\n  ]\n}\n```\n\nNow we have a fully defined `MathService` implementation that we can make\navailable in- or out-of-process.\n\n### Hooking Up the Service Implementation\n\nFor an out-of-process Chrome service, we simply register a factory function\nin [`//chrome/utility/services.cc`](https://cs.chromium.org/chromium/src/chrome/utility/services.cc).\n\n``` cpp\nauto RunMathService(mojo::PendingReceiver<math::mojom::MathService> receiver) {\n  return std::make_unique<math::MathService>(std::move(receiver));\n}\n\nvoid RegisterMainThreadServices(mojo::ServiceFactory& services) {\n  // Existing services...\n  services.Add(RunFilePatcher);\n  services.Add(RunUnzipper);\n\n  // We add our own factory to this list\n  services.Add(RunMathService);\n  //...\n```\n\nWith this done, it is now possible for the browser process to launch new\nout-of-process instances of MathService.\n\n### Launching the Service\n\nIf you're running your service in-process, there's really nothing interesting\nleft to do. You can instantiate the service implementation just like any other\nobject, yet you can also talk to it via a Mojo Remote as if it were\nout-of-process.\n\nTo launch an out-of-process service instance after the hookup performed in the\nprevious section, use Content's\n[`ServiceProcessHost`](https://cs.chromium.org/chromium/src/content/public/browser/service_process_host.h?rcl=e7a1f6c9a24f3151c875598174a05167fb12c5d5&l=47)\nAPI:\n\n``` cpp\nmojo::Remote<math::mojom::MathService> math_service =\n    content::ServiceProcessHost::Launch<math::mojom::MathService>(\n        content::ServiceProcessHost::Options()\n            .WithDisplayName(\"Math!\")\n            .Pass());\n```\n\nExcept in the case of crashes, the launched process will live as long as\n`math_service` lives. As a corollary, you can force the process to be torn\ndown by destroying (or resetting) `math_service`.\n\nWe can now perform an out-of-process division:\n\n``` cpp\n// NOTE: As a client, we do not have to wait for any acknowledgement or\n// confirmation of a connection. We can start queueing messages immediately and\n// they will be delivered as soon as the service is up and running.\nmath_service->Divide(\n    42, 6, base::BindOnce([](int32_t quotient) { LOG(INFO) << quotient; }));\n```\n*** aside\nNOTE: To ensure the execution of the response callback, the\n`mojo::Remote<math::mojom::MathService>` object must be kept alive (see\n[this section](/mojo/public/cpp/bindings/README.md#A-Note-About-Endpoint-Lifetime-and-Callbacks)\nand [this note from an earlier section](#sending-a-message)).\n***\n\n### Specifying a sandbox\n\nAll services must specify a sandbox. Ideally services will run inside the\n`kService` process sandbox unless they need access to operating system\nresources. For services that need a custom sandbox, a new sandbox type must be\ndefined in consultation with security-dev@chromium.org.\n\nThe preferred way to define the sandbox for your interface is by specifying a\n`[ServiceSandbox=type]` attribute on your `interface {}` in its `.mojom` file:\n\n```\nimport \"sandbox/policy/mojom/sandbox.mojom\";\n[ServiceSandbox=sandbox.mojom.Sandbox.kService]\ninterface FakeService {\n  ...\n};\n```\n\nValid values are those in\n[`//sandbox/policy/mojom/sandbox.mojom`](https://cs.chromium.org/chromium/src/sandbox/policy/mojom/sandbox.mojom). Note\nthat the sandbox is only applied if the interface is launched\nout-of-process using `content::ServiceProcessHost::Launch()`.\n\nAs a last resort, dynamic or feature based mapping to an underlying platform\nsandbox can be achieved but requires plumbing through ContentBrowserClient\n(e.g. `ShouldSandboxNetworkService()`).\n\n## Content-Layer Services Overview\n\n### Interface Brokers\n\nWe define an explicit mojom interface with a persistent connection\nbetween a renderer's frame object and the corresponding\n`RenderFrameHostImpl` in the browser process.\nThis interface is called\n[`BrowserInterfaceBroker`](https://cs.chromium.org/chromium/src/third_party/blink/public/mojom/browser_interface_broker.mojom?rcl=09aa5ae71649974cae8ad4f889d7cd093637ccdb&l=11)\nand is fairly easy to work with: you add a new method on `RenderFrameHostImpl`:\n\n``` cpp\nvoid RenderFrameHostImpl::GetGoatTeleporter(\n    mojo::PendingReceiver<magic::mojom::GoatTeleporter> receiver) {\n  goat_teleporter_receiver_.Bind(std::move(receiver));\n}\n```\n\nand register this method in `PopulateFrameBinders` function in `browser_interface_binders.cc`,\nwhich maps specific interfaces to their handlers in respective hosts:\n\n``` cpp\n// //content/browser/browser_interface_binders.cc\nvoid PopulateFrameBinders(RenderFrameHostImpl* host,\n                          mojo::BinderMap* map) {\n...\n  map->Add<magic::mojom::GoatTeleporter>(base::BindRepeating(\n      &RenderFrameHostImpl::GetGoatTeleporter, base::Unretained(host)));\n}\n```\n\nIt's also possible to bind an interface on a different sequence by specifying a task runner:\n\n``` cpp\n// //content/browser/browser_interface_binders.cc\nvoid PopulateFrameBinders(RenderFrameHostImpl* host,\n                          mojo::BinderMap* map) {\n...\n  map->Add<magic::mojom::GoatTeleporter>(base::BindRepeating(\n      &RenderFrameHostImpl::GetGoatTeleporter, base::Unretained(host)),\n      GetIOThreadTaskRunner({}));\n}\n```\n\nWorkers also have `BrowserInterfaceBroker` connections between the renderer and\nthe corresponding remote implementation in the browser process. Adding new\nworker-specific interfaces is similar to the steps detailed above for frames,\nwith the following differences:\n - For Dedicated Workers, add a new method to\n   [`DedicatedWorkerHost`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/worker_host/dedicated_worker_host.h)\n   and register it in\n   [`PopulateDedicatedWorkerBinders`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/browser_interface_binders.cc;l=1126;drc=e24e0a914ff0da18e78044ebad7518afe9e13847)\n - For Shared Workers, add a new method to\n   [`SharedWorkerHost`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/worker_host/shared_worker_host.h)\n   and register it in\n   [`PopulateSharedWorkerBinders`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/browser_interface_binders.cc;l=1232;drc=e24e0a914ff0da18e78044ebad7518afe9e13847)\n - For Service Workers, add a new method to\n   [`ServiceWorkerHost`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/service_worker/service_worker_host.h)\n   and register it in\n   [`PopulateServiceWorkerBinders`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/browser_interface_binders.cc;l=1326;drc=e24e0a914ff0da18e78044ebad7518afe9e13847)\n\nInterfaces can also be added at the process level using the\n`BrowserInterfaceBroker` connection between the Blink `Platform` object in the\nrenderer and the corresponding `RenderProcessHost` object in the browser\nprocess. This allows any thread (including frame and worker threads) in the\nrenderer to access the interface, but comes with additional overhead because\nthe `BrowserInterfaceBroker` implementation used must be thread-safe. To add a\nnew process-level interface, add a new method to `RenderProcessHostImpl` and\nregister it using a call to\n[`AddUIThreadInterface`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/renderer_host/render_process_host_impl.h;l=918;drc=ec5eaba0e021b757d5cbbf2c27ac8f06809d81e9)\nin\n[`RenderProcessHostImpl::RegisterMojoInterfaces`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/renderer_host/render_process_host_impl.cc;l=2317;drc=a817d852ea2f2085624d64154ad847dfa3faaeb6).\nOn the renderer side, use\n[`Platform::GetBrowserInterfaceBroker`](https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/public/platform/platform.h;l=781;drc=ee1482552c4c97b40f15605fe6a52565cfe74548)\nto retrieve the corresponding `BrowserInterfaceBroker` object to call\n`GetInterface` on.\n\nFor binding an embedder-specific document-scoped interface, override\n[`ContentBrowserClient::RegisterBrowserInterfaceBindersForFrame()`](https://cs.chromium.org/chromium/src/content/public/browser/content_browser_client.h?rcl=3eb14ce219e383daa0cd8d743f475f9d9ce8c81a&l=999)\nand add the binders to the provided map.\n\n*** aside\nNOTE: if BrowserInterfaceBroker cannot find a binder for the requested\ninterface, it will call `ReportNoBinderForInterface()` on the relevant\ncontext host, which results in a `ReportBadMessage()` call on the host's\nreceiver (one of the consequences is a termination of the renderer). To\navoid this crash in tests (when content_shell doesn't bind some\nChrome-specific interfaces, but the renderer requests them anyway),\nuse the\n[`EmptyBinderForFrame`](https://cs.chromium.org/chromium/src/content/browser/browser_interface_binders.cc?rcl=12e73e76a6898cb6df6a361a98320a8936f37949&l=407)\nhelper in `browser_interface_binders.cc`. However, it is recommended\nto have the renderer and browser sides consistent if possible.\n***\n\n### Navigation-Associated Interfaces\n\nFor cases where the ordering of messages from different frames is important,\nand when messages need to be ordered correctly with respect to the messages\nimplementing navigation, navigation-associated interfaces can be used.\nNavigation-associated interfaces leverage connections from each frame to the\ncorresponding `RenderFrameHostImpl` object and send messages from each\nconnection over the same FIFO pipe that's used for messages relating to\nnavigation. As a result, messages sent after a navigation are guaranteed to\narrive in the browser process after the navigation-related messages, and the\nordering of messages sent from different frames of the same document is\npreserved as well.\n\nTo add a new navigation-associated interface, create a new method for\n`RenderFrameHostImpl` and register it with a call to\n`associated_registry_->AddInterface` in\n[`RenderFrameHostImpl::SetUpMojoConnection`](https://source.chromium.org/chromium/chromium/src/+/main:content/browser/renderer_host/render_frame_host_impl.cc;l=8365;drc=a817d852ea2f2085624d64154ad847dfa3faaeb6).\nFrom the renderer, use\n[`LocalFrame::GetRemoteNavigationAssociatedInterfaces`](https://source.chromium.org/chromium/chromium/src/+/main:third_party/blink/renderer/core/frame/local_frame.h;l=409;drc=19f17a30e102f811bc90a13f79e8ad39193a6403)\nto get an object to call\n`GetInterface` on (this call is similar to\n`BrowserInterfaceBroker::GetInterface` except that it takes a\n[pending associated receiver](https://chromium.googlesource.com/chromium/src/+/main/mojo/public/cpp/bindings/README.md#associated-interfaces)\ninstead of a pending receiver).\n\n## Additional Support\n\nIf this document was not helpful in some way, please post a message to your\nfriendly\n[chromium-mojo@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/chromium-mojo)\nor\n[services-dev@chromium.org](https://groups.google.com/a/chromium.org/forum/#!forum/services-dev)\nmailing list.\n"
  }
]